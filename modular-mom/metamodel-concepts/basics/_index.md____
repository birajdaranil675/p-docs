





# Welcome Modular MOM wiki

```
Other cloudMES / revMOM websites
```
```
TFS Revolution MOM TFS
```
```
Sharepoint MES Cloud sharepoint
```
```
Some useful "external" links
```
```
MOM UX Styleguide MOM User eXperience Styleguide wiki
```
```
PLM UX Policies PLM User eXperience Policies (link not
working)
```
```
PL PaaS
documentation
```
```
PL PaaS Services documentation (link not
working)
```
```
Apollo documentation MOM Apollo Adoption wiki
```
```
Backlog
```
```
For an overall and contextualized view of the backlog from EPIC to US
use the EPIC backlog
```
```
For a detail ranking of US use the US backlog (but take care to look the
context of a US using the parent Feature and EPICs.
```
```
see the functional specification pages for a description of the existing
and coming functional capabilities of the product
```
```
see the product backlog pages for vision and minutes of refinement
meetings
```
```
How to grant access to the wiki
```
```
In order to access to the wiki (momwiki02) you need a SWQA Account.
If you do not have it please get it touch with you local resource manager.
Do not open a ticket, it is not handled by the ticket system.
```
```
If you want a read\write access, please ask one of the following wiki
admin:
```
```
Khairnar, Prashant , Napelyani , Amir
```
```
If all of them are not available, a ticket has to be opened. The ticket
procedure (because they'll require our confirmation to grant access) is
slower and so to be avoided if one of the wiki admin is available.
```
## Recent space activity

```
Balzer, Peter
NFR Dashboards updated about an hour ago view change
```
```
Trubini, Piergiorgio
Pub/Sub Communication updated about 2 hours ago view change
```
```
Balzer, Peter
Meeting BDD API 7/7/2022 updated about 2 hours ago view change
```
```
Trubini, Piergiorgio
Pub/Sub Communication commented about 2 hours ago
```
```
Dal Mas, Roberto
Monviso OOO's Calendar updated about 5 hours ago view change
```
## Space contributors

```
Balzer, Peter (59 minutes ago)
Trubini, Piergiorgio (an hour ago)
Dal Mas, Roberto (5 hours ago)
kesarwani, Arpita (5 hours ago)
Schmidt, Jan (23 hours ago)
...
```

# Modular MOM 2

```
Organization Modular Manufacturing
Feature Team "Mauna Kea" (Genoa - Mountain)
Mauna Kea's Out-of-Office Calendar
Feature Team "Pratapgad" (Pune - Fort)
Pratapgad's Retrospectives
Feature Team "Shivneri" (Pune - Fort)
Feature Team "Monviso" (Genoa - Mountain)
Monviso OOO's Calendar
Feature Team "Rajgad" (Pune -Fort)
Rajgad Retrospective
Feature Team "Sahyadri" (Pune - Mountain)
Feature Team "Sinhagad" (Pune - Fort)
Retrospective
Photogallery
Distribution Lists
ModMOM Status
Functional Specification
Factory Module
Detailed Information on Hierarchies
Factory Module APIs
Factory Module Documents
Factory Module Entities
Factory Module Messages
Wireframes Factory Modeling
Factory Module Permissions
Material Management Module
Material Management Module APIs
Material Management Module Documents
Material Management Module Entities
Material Management Module Messages
Material Management Module Use Cases
Material Management Module Permissions
Order Management Module
Order Management Module APIs
Order Management Module Documents
Order Management Module Entities
Order Management Module Messages
Order Management Module Use Cases
Order Management Module Permissions
AM Order Management Module Messages
TrackAndTrace Module
TrackAndTrace Module APIs
TrackAndTrace Module Entities (2207 version)
TrackAndTrace Module Messages
TrackAndTrace Module Use Cases
TrackAndTrace Module Permissions
AM TrackAndTrace Module Messages
AM TrackAndTrace Module
AM TrackAndTrace Module APIs
Access Management Module
PermissionGroups file
Access Management API
Access Management Entities
Access Management Use Cases
Access Management Permissions
Access Management - RBAC Overview
Sample Data
Sample data for Modular MOM 2.X - 202112 release
Acronyms Definitions
App Creation Workshop
Business Process Flow & Epics
Non Functional Requirements
NFR Dashboards
NFR Environment
NFR Definitions
General NFRs for all Modules
Order Management NFRs
Track and Trace NFRs
Home Page NFRs
Access Management NFRs
Factory Modeling NFRs
Material Modeling NFRs
NFR Parking Lot
Architectural Qualities
Supportability - Diagnostics
Supportability - Documentation
```

Supportability - Logging
Supportability - Tools
NFR Coverage
Before You Start
First Steps for Startup Users
OCMOD Concepts And First Steps To Use
Introduction of OCMOD Documentation
Quick Start to Developing with Opcenter Modular Manufacturing
Configuring the Development Environment
Siemens Web Framework Fundamentals
How to Create a Configurable Object
Creating a Metadata Model
Developing an App / Metadata Module
Common Guidelines
Glossary
Working with Documentation
Creating Documentation Pages pages in Confluence
User Story - Definition of Ready (DoR)
User Story - Definition of Done (DoD)
User Story - Definition of Done (checklist)
Product Security - Definition of Done for Feature and Release
Test Strategy
Interfacing with ModularMOM Adopter Startups
Procedure for Pull Request
Best practices for Git Branching and Merging
TFS Azure DevOps
Communication Rules
Procedure to set up first user in XCR
Manual configuration of User with Permission
OBM Flow
Service account registration
Product and App registration
Provider role creation
Provision Product and User
Agile Scrum Practice Modular MOM
Artifacts
Feature, User Story, Task, Epic
Bugs, Issues
Test Plan, Test Suite, Test Case
TFS Items - Creation Workflow
MomValueArea
Estimating
Events
Accountabilities (Roles)
Scrum Events
MM MES Retrospectives (from Sprint 24)
MM PLT Retrospectives (from Sprint 25)
MM MES Sprint Review Agenda
MM PLT Sprint Review Agenda
Scrum Events Preparation
Overall Retrospectives (Sprint 1 to Sprint 23)
Internal Collaborative Content
Nightly Test Run Follow-Up Plan 2.x
Building Redis on Windows for Development
Calculation of Code Coverage in Pipeline
Code Coverage using Unit Test
Debugging BDD API Framework and Common Steps
CI/CD Processes and Practices
Process And Phases
User Tasks
Devops Implementation
Enabling LongPath Support in Windows
Git How To
Monitoring Traces and Logs in Development Environment
Production Environment Allocation
Openshift Internal Project Links
Postman Examples
SF experts exchange topics
Solution Structure
SQL Server on deployment
SWF-6 UI Migration
TimeZone issues - find a meeting slot
Introducing Kustomize
ModMOM - SqlServer
Modular MOM on Cloud
How to run BDD UI against deployed environment
Executing BDD API Tests with AspNetCore Test Server
Executing BDD API Tests with AspNetCore Test Server in the Release Pipeline
Executing BDD API Tests with AspNetCore Test Server on VDI


Executing BDD API Tests from VDI on deployed system
Backend Localization
Localization(L10N) support provided by Siemens Web Framework (SWF)
Viewing Logs from OpenShift with Kibana for Dummies
How to initialize Access Management
How to add another NFR to the nightly test run & Sumo Dashboard
Custom component using SWF
How to use observability data in Sumologic
How to provide and use a new messaging model version
Signal R and SWF integration
Stable Builds Definition
Platform Upgrade 2.2.
Platform Internal Architecture
Platform & Model Versioning
Metadata/Metamodel
Model configuration
Revisioned & Revision Base Types
Enumerations(Enums)
Configuring Nullable Native, Enum, and Options Type Fields
List Persistence - Configuration & Characteristics
Metadata Model Query Definitions
Accessibility Configuration of CO Fields
Required Fields on Persistent Entities
Selection Values
System Objects & Persistence
Framework for Trackable & Audit Trail Objects
Message Model Extensions
Metamodel Override Features
Configurable Object Maps (COMap)
[Partially Obsolete] Metadata Module Architectural Concept
Internal Release Notes
Platform Version 2.2.
Platform Version 2.1.
Platform Version 2.1.
Platform Version 2.1.
Platform Version 2.1.
Platform Version 2.1.
Platform Version 2.1.
Platform Version 2.1.
Platform Version 2.1.
Platform Version 2.1.
Platform Version 2.1.
Platform Version 2.0.
Platform Version 1.0.
Platform Version 1.0.
Platform Version 1.0.
Platform Version 1.0.
Platform Version 1.0.
Platform Version 1.0.
Platform Version 1.0.
Platform Version 1.0.
Platform Version 1.0.
Platform Version 1.0.
Platform Version 1.0.
Platform Version 1.0.
Platform Version 2.2.1-prerelease
SaaS Architecture
Failure points and Recover Strategy
Policy configuration in SAM
Product Definition
SaaS Project Status for Limited Preview 2207
User Workflows
Configuration Layer (Former Governance Layer)
Application Configuration and Deployment
Appsettings.json - Messaging Configuration
Information Layer
Cache Strategy
Database Schema Strategy
Query Strategy
Integration Layer
Dapr Analysis
Grpc Message serialization changes (US 45509)
Message Consume Handling
Message Extensibility
Message resiliency - Detailed Design
Module name resolution
Pub/Sub Communication
Observability Layer
Instrumenting Applications


OpenTelemetry
Observability Architecture
Reference Materials
Distributed Tracing
Metrics
Structured Logging
How to use observability data?
Runtime Layer
API Interaction
Class Based Options and Flag
Exception Framework
Gateway Interaction - Pipeline call-chain
Healthchecks - K8 Healthiness probes
Localization Support for Metadata Runtime components and Platform
Spike: Localization Support for Metadata Runtime components
Unique Id Generation
Security Layer
Authenticated Api Request via Postman
Authorization Model
Dex Evaluation
Emissary Ingress (Ambassador API Gateway)
Enable Authentication in Business Module
Identity Brokering & SamAuth Integration
Oidc-Client library evaluation
Modules
Third-Party Software Used in Platform
Application Catalog
Business Model (Metadata) Development Guidelines
Developing a Metadata Module(V1)
Developing a Metadata Module(V2+)
User code execution risk & mitigation
Creating a new version of M1_Common
Technological Upgrade
Upgrade to .NET 6
Localization
Open points: Feature 32266: Ensure Localization on all modules
Internal Technical Documentation
Inspection Planning Shopfloor
Momwiki02 Structure Draft
App Catalogue
Access Management App
Factory App
Failure Catalog App
Inspection Evaluation Shopfloor App
Inspection Planning Shopfloor App
Material Management App
Order Management App
Track And Trace App
Modular MOM Configurator
API Specification
Build and Deployment
Configurator - Architecture
Git Integration
Squashing Git Repositories for release
System Configurations
UI
Configurator Development
Kubernetes Operators


# Organization Modular Manufacturing

```
The Modular Manufacturing team is spread around the globe: USA (West Coast and East Coast), Europe (Italy and Germany) and India.
```
```
Wintertime:
```
## Modular Manufacturing

### LEADERS

```
Role Name
```
```
Startup
Leader
```
```
Kaplan, Metin
```
```
Lean Agile
Leader
```
```
Pottigar,
Durgaprasad
```
```
Portfolio
Leader
```
```
Singh, Abhishek Jeet
```
```
Architecture
Leader
```
```
Nagamalli, Ramesh
```
```
Platform
Leader
```
```
Khairnar, Prashant
```
```
MES Leader Napelyani, Amir
```
```
MES SF
India Leader
```
```
Lodha, Vishal
```
```
Shared Resources
```
```
Role Name
```
```
Product Manager Anderson, Timothy (Engg. USA partner, Semantic Model, Platform)
```
```
Capponi, Diego (Lego/Additive, APS SU, Pharma SU, NFR)
```
```
Engelke, Felix (SaaS cost & pricing, MX PBCs)
```
```
Jaehnert, Christian (ADV Europe partner, BSCE, Sales Brazil, Backlog)
```
```
Schmidt, Jan (Alfmeier, Quality SU, Zug, Saas Readiness, Backlog)
```
```
Milligan, Patrick (Bopex)
```
```
Architecture Community Accorsi, Carlo
```
```
Aghazarian, Nick
```
```
Tabaei Befrouei, Mitra
```
```
Thivaharraja, Antony
```
```
DevOps Team
modularmom_devops.
sisw@siemens.com
```
```
De Pascale, Mauro
```
```
Poggi, Emiliano (Platform)
```
```
Sengupta, Ankita (Platform)
```
```
Chaudhari, Yogita (MES)
```
```
Kesarwani, Arpita (MES)
```
```
Test Manager Balzer, Peter
```
```
QA Infrastructure Team Di Rienzo, Andrea
```
```
Deshmukh, Dipti
```
```
Suryavanshi, Vikas
```
```
Release Manager / PSE Roncagliolo, Isabella
```
```
Documentation Manager Álvarez Villanueva, Beatriz (mainly MES)
```
```
Quality Officer Grosso, Stefania
```
```
UX Referent Thombare, Sagar (Platform)
```
```
Haas, Julia (MES)
```

Modular Manufacturing Platform

```
FEATURE TEAM Pune
Pratapgad TEAM
```
```
modularmom_pratapgad.sisw@siemens.com
```
```
Role Name
```
```
Product Owner Nagamalli, Ramesh
```
```
Scrum Master Moholkar, Sunetra
```
```
Team Lele, Pushkar
```
```
Patil, Shubhangi
```
```
Saktepar, Shrutika
```
```
Sinha, Dheeraj
```
```
Sheth Hemal
```
```
FEATURE TEAM Pune
Shivneri TEAM
```
```
modularmom_shivneri.sisw@siemens.com
```
```
Role Name
```
```
Product Owner Nagamalli, Ramesh
```
```
Scrum Master Kirve, Ajit
```
```
Team Das, Debarshi
```
```
Joshi, Deepak
```
```
Patil, Mahesh
```
```
Vashisht, Ram
```
```
Akshaykumar Patil
```
```
Hemant Naik
```
```
FEATURE TEAM Genoa
Mauna Kea TEAM
```
```
modularmom_maunakea.sisw@siemens.com
```
```
Role Name
```
```
Product Owner Nagamalli, Ramesh
```
```
Scrum Master Nitsche, Andrea
```
```
Team Arduini, Milvia Pierpaola
```
```
Di Rienzo, Andrea
```
```
Immordino, Giovanni
```
```
Occhiena, Marco
```
```
Trubini, Piergiorgio
```
```
Varoli, Guido
```
Modular Manufacturing MES

```
FEATURE TEAM Pune
Sinhagad TEAM
```
```
modularmom_sinhagad.sisw@siemens.com
```
```
Role Name
```
```
Product Owner Hegde, Ganesh (Heggi)
```
```
Scrum Master Malhotra, Parul
```
```
FEATURE TEAM Pune
Rajgad TEAM
```
```
modularmom_rajgad.sisw@siemens.com
```
```
Role Name
```
```
Product Owner Hegde, Ganesh (Heggi)
```
```
Scrum Master Girnar, Yuvraj
```

Team Salokhe, Niraj

```
Sharma, Pallavi
```
```
Shinde, Amit
```
```
Shirsale, Nikhilesh
```
```
Thakker, Hiren
```
```
Team Sablaka, Saurabh
```
```
Bajaj, Arjita
```
```
Pathak, Amit Chandra
```
```
Rawat, Amit
```
```
Chavhan, Bhushan
```
```
Khandave, Rohan
```
```
FEATURE TEAM Pune
Sahyadri TEAM
```
```
modularmom_Sahyadri.sisw@siemens.com
```
```
Role Name
```
Product Owner Hegde, Ganesh (Heggi)

Scrum Master Hingse, Sharad

Team Faizan, Syed

```
Hitesh, Rawal
```
```
Manoj, Baishya
```
```
Dattatray, Patil
```
```
Pratik, Mhatre
```
```
FEATURE TEAM Genoa
Monviso TEAM
```
```
modularmom_monviso.sisw@siemens.com
```
```
Role Name
```
```
Product Owner Bardini, Matteo
```
```
Scrum Master Montaldo, Ernesto
```
```
Team Abate, Francesco
```
```
Dal Mas, Roberto
```
```
Fioritoni, Marco
```
```
Peretti, Massimo
```
```
Simone Maurizio
```
```
Turco, Mariangela
```
```
Turolla, Andrea
```
### FEATURE TEAM BOPEX

```
Everest TEAM
```
```
modularmom_Everest.sisw@siemens.com
```
```
Role Name
```
Product Owner Giuffrida, Guido

Scrum Master Hasabnis, Unmesh

Team von Majowski, Martin

```
Lorenz, Juergen
```
```
Liao, Dong
```
```
T. V, Jishnu
```
```
Wang, Ning
```
```
Khandekar, Saurabh
```
```
Khadangale, Avinash
```
```
Khatale, Amit
```
```
Bhattad, Shailesh
```
```
Tapaswini, Snigdha
```
```
Beldar, Shubham
```

```
Musale, Varsha
```
```
Deidda, Manuela
```
```
Kesarwani, Arpita
```
Point(s) of Contact
"Point(s) of contact" for a given subject are meant to be the "driver" for the activities related to that subject (e.g. communicating subject related issues
inside the team, ensuring that the activity is performed, ensuring that at least one team member participates to a CoP meeting, ...).

```
Test Expert
```
```
Software Factory Experts (Product Experts)
```
```
CI/CD DevOps
```
```
Clearing
```
Documentation

Localization

```
UX Referent
```
```
Security
```
Future ideas:
Referents for SonarQube, Release Manager


# Feature Team "Mauna Kea" (Genoa - Mountain)

## Our Standup meeting: Each morning at 9:30 (Join Microsoft Teams Meeting)

```
Mauna Kea is a dormant volcano on the island of Hawaii.
```
```
Its peak is 4,207.3 m above sea level, making it the highest point in the
state of Hawaii. Most of the volcano is underwater, and when measured
from its underwater base, Mauna Kea is the tallest mountain in the
world, measuring 10,211 m in height.
```
```
It is about a million years old, and has thus passed the most active shield
stage of life hundreds of thousands of years ago. In its current post-
shield state, its lava is more viscous, resulting in a steeper profile.
```
```
Mauna Kea last erupted 6,000 to 4,000 years ago and is now considered
dormant.
```
```
Role Names Participates in ...
```
```
Team Member Alvarez Villanueva,
Beatriz
```
```
COP Documentation
```
```
Team Member Arduini, Milvia Pierpaola COP Test
```
```
Team Member Di Rienzo, Andrea
```
```
Team Member Immordino, Giovanni
```
```
Team Member Occhiena, Marco
```
```
Team Member Trubini, Piergiorgio COP Architecture, SF
Expert
```
```
Team Member Varoli, Guido
```
```
Scrum Master Nitsche, Andrea COP Agile
```

# Mauna Kea's Out-of-Office Calendar

```
June 2022
July 2022
August 2022
September 2022
October 2022
November 2022
December 2022
January 2023
January 2022
February 2022
March 2022
April 2022
May 2022
```
## June 2022

```
Monday Tuesday Wednesday Thursday Friday
```
```
01 Jun 2022 02 Jun 2022 03 Jun 2022
```
```
06 Jun 2022 07 Jun 2022 08 Jun 2022
```
```
Immordino, Giovanni
```
```
09 Jun 2022 10 Jun 2022
```
```
13 Jun 2022 14 Jun 2022 15 Jun 2022 16 Jun 2022 17 Jun 2022
```
```
Immordino, Giovanni
```
```
20 Jun 2022 21 Jun 2022 22 Jun 2022 23 Jun 2022 24 Jun 2022
```
```
27 Jun 2022
```
```
Nitsche, Andrea
```
```
28 Jun 2022
```
```
Nitsche, Andrea
```
```
29 Jun 2022
```
```
Nitsche, Andrea
```
```
30 Jun 2022
```
```
Nitsche, Andrea
```
## July 2022

```
Monday Tuesday Wednesday Thursday Friday
```

```
01 Jul 2022
```
```
Nitsche, Andrea
```
```
04 Jul 2022
```
```
Varoli, Guido
```
```
05 Jul 2022
```
```
Varoli, Guido
```
```
06 Jul 2022
```
```
Varoli, Guido
```
```
07 Jul 2022
```
```
Varoli, Guido
```
```
08 Jul 2022
```
```
Varoli, Guido
```
```
Immordino, Giovanni
```
```
11 Jul 2022 12 Jul 2022 13 Jul 2022 14 Jul 2022 15 Jul 2022
18 Jul 2022
```
```
Arduini, Milvia Pierpaola
```
```
19 Jul 2022
```
```
Arduini, Milvia Pierpaola
```
```
20 Jul 2022
```
```
Arduini, Milvia Pierpaola
```
```
21 Jul 2022
```
```
Arduini, Milvia Pierpaola
```
```
22 Jul 2022
```
```
Arduini, Milvia Pierpaola
```
```
Immordino, Giovanni
```
```
25 Jul 2022 26 Jul 2022 27 Jul 2022 28 Jul 2022
```
```
Nitsche, Andrea
```
```
29 Jul 2022
```
```
Nitsche, Andrea
```
August 2022

```
Monday Tuesday Wednesday Thursday
```
```
01 Aug 2022
Varoli, Guido
```
```
Nitsche, Andrea
```
```
02 Aug 2022
Varoli, Guido
```
```
Nitsche, Andrea
```
```
03 Aug 2022
Varoli, Guido
```
```
Nitsche, Andrea
```
```
04 Aug 2022
Varoli, Guido
```
```
Nitsche, Andrea
```
```
08 Aug 2022Arduini, Milvia Pierpaola 09 Aug 2022
Arduini, Milvia Pierpaola
```
```
10 Aug 2022
Arduini, Milvia Pierpaola
```
```
11 Aug 2022
Arduini, Milvia Pierpaola
```
```
15 Aug 2022 16 Aug 2022 17 Aug 2022 18 Aug 2022
```
```
22 Aug 2022 23 Aug 2022 24 Aug 2022 25 Aug 2022
```
```
29 Aug 2022 30 Aug 2022 31 Aug 2022
```
September 2022

```
Monday Tuesday Wednesday Thursday Friday
```
```
01 Sep 2022 02 Sep 2022
```
```
05 Sep 2022
```
```
Alvarez Villanueva, Beatriz
```
```
06 Sep 2022Alvarez Villanueva, Beatriz 07 Sep 2022
```
```
Alvarez Villanueva, Beatriz
```
```
08 Sep 2022
```
```
Alvarez Villanueva, Beatriz
```
```
09 Sep 2022
```
```
Alvarez Villanueva, Beatriz
```
```
Immordino, Giovanni
```
```
12 Sep 2022
```
```
Alvarez Villanueva, Beatriz
```
```
13 Sep 2022
```
```
Alvarez Villanueva, Beatriz
```
```
14 Sep 2022
```
```
Alvarez Villanueva, Beatriz
```
```
15 Sep 2022
```
```
Alvarez Villanueva, Beatriz
```
```
16 Sep 2022
```
```
Alvarez Villanueva, Beatriz
```
```
19 Sep 2022
```
```
Alvarez Villanueva, Beatriz
```
```
20 Sep 2022 21 Sep 2022 22 Sep 2022 23 Sep 2022
```
```
Immordino, Giovanni
```
```
26 Sep 2022 27 Sep 2022 28 Sep 2022 29 Sep 2022 30 Sep 2022
```

October 2022

```
Monday Tuesday Wednesday Thursday Friday
```
```
03 Oct 2022 04 Oct 2022 05 Oct 2022 06 Oct 2022 07 Oct 2022
```
```
10 Oct 2022 11 Oct 2022 12 Oct 2022 13 Oct 2022 14 Oct 2022
Immordino, Giovanni
```
```
17 Oct 2022 18 Oct 2022 19 Oct 2022 20 Oct 2022 21 Oct 2022
24 Oct 2022 25 Oct 2022 26 Oct 2022 27 Oct 2022 28 Oct 2022
```
```
Immordino, Giovanni
```
```
31 Oct 2022
```
November 2022

```
Monday Tuesday Wednesday Thursday Friday
```
```
01 Nov 2022 02 Nov 2022 03 Nov 2022 04 Nov 2022
```
```
07 Nov 2022 08 Nov 2022 09 Nov 2022 10 Nov 2022 11 Nov 2022
```
```
Immordino,
Giovanni
```
```
14 Nov 2022 15 Nov 2022 16 Nov 2022 17 Nov 2022 18 Nov 2022
```
```
21 Nov 2022 22 Nov 2022 23 Nov 2022 24 Nov 2022 25 Nov 2022
```
```
Immordino,
Giovanni
```
```
28 Nov 2022 29 Nov 2022 30 Nov 2022
```
December 2022

```
Monday Tuesday Wednesday Thursday
```
```
01 Dec 2022
```

## Monday Tuesday Wednesday Thursday Friday

- 1 Welcome Modular MOM wiki
   - 1 1 Modular MOM
      - 1 1 1 Organization Modular Manufacturing
         - 1 1 1 1 Feature Team "Mauna Kea" (Genoa - Mountain)
            - 1 1 1 1 1 Mauna Kea's Out-of-Office Calendar
         - 1 1 1 2 Feature Team "Pratapgad" (Pune - Fort)
            - 1 1 1 2 1 Pratapgad's Retrospectives
         - 1 1 1 3 Feature Team "Shivneri" (Pune - Fort)
         - 1 1 1 4 Feature Team "Monviso" (Genoa - Mountain)
            - 1 1 1 4 1 Monviso OOO's Calendar
         - 1 1 1 5 Feature Team "Rajgad" (Pune -Fort)
            - 1 1 1 5 1 Rajgad Retrospective
         - 1 1 1 6 Feature Team "Sahyadri" (Pune - Mountain)
         - 1 1 1 7 Feature Team "Sinhagad" (Pune - Fort)
            - 1 1 1 7 1 Retrospective
         - 1 1 1 8 Photogallery
         - 1 1 1 9 Distribution Lists
      - 1 1 2 ModMOM Status
      - 1 1 3 Functional Specification
         - 1 1 3 1 Factory Module
            - 1 1 3 1 1 Detailed Information on Hierarchies
            - 1 1 3 1 2 Factory Module APIs
            - 1 1 3 1 3 Factory Module Documents
            - 1 1 3 1 4 Factory Module Entities
            - 1 1 3 1 5 Factory Module Messages
            - 1 1 3 1 6 Wireframes Factory Modeling
            - 1 1 3 1 7 Factory Module Permissions
         - 1 1 3 2 Material Management Module
            - 1 1 3 2 1 Material Management Module APIs
            - 1 1 3 2 2 Material Management Module Documents
            - 1 1 3 2 3 Material Management Module Entities
            - 1 1 3 2 4 Material Management Module Messages
            - 1 1 3 2 5 Material Management Module Use Cases
            - 1 1 3 2 6 Material Management Module Permissions
         - 1 1 3 3 Order Management Module
            - 1 1 3 3 1 Order Management Module APIs
            - 1 1 3 3 2 Order Management Module Documents
            - 1 1 3 3 3 Order Management Module Entities
            - 1 1 3 3 4 Order Management Module Messages
            - 1 1 3 3 5 Order Management Module Use Cases
            - 1 1 3 3 6 Order Management Module Permissions
            - 1 1 3 3 7 AM Order Management Module Messages
         - 1 1 3 4 TrackAndTrace Module
            - 1 1 3 4 1 TrackAndTrace Module APIs
            - 1 1 3 4 2 TrackAndTrace Module Entities (2207 version)
            - 1 1 3 4 3 TrackAndTrace Module Messages
            - 1 1 3 4 4 TrackAndTrace Module Use Cases
            - 1 1 3 4 5 TrackAndTrace Module Permissions
            - 1 1 3 4 6 AM TrackAndTrace Module Messages
            - 1 1 3 4 7 AM TrackAndTrace Module
            - 1 1 3 4 8 AM TrackAndTrace Module APIs
         - 1 1 3 5 Access Management Module
            - 1 1 3 5 1 PermissionGroups file
            - 1 1 3 5 2 Access Management API
            - 1 1 3 5 3 Access Management Entities
            - 1 1 3 5 4 Access Management Use Cases
            - 1 1 3 5 5 Access Management Permissions
            - 1 1 3 5 6 Access Management - RBAC Overview
         - 1 1 3 6 Sample Data
            - 1 1 3 6 1 Sample data for Modular MOM 2.X - 202112 release
         - 1 1 3 7 Acronyms Definitions
         - 1 1 3 8 App Creation Workshop
            - 1 1 3 8 1 Business Process Flow & Epics
      - 1 1 4 Non Functional Requirements
         - 1 1 4 1 NFR Dashboards
         - 1 1 4 2 NFR Environment
         - 1 1 4 3 NFR Definitions
            - 1 1 4 3 1 General NFRs for all Modules
            - 1 1 4 3 2 Order Management NFRs
            - 1 1 4 3 3 Track and Trace NFRs
            - 1 1 4 3 4 Home Page NFRs
            - 1 1 4 3 5 Access Management NFRs
            - 1 1 4 3 6 Factory Modeling NFRs
            - 1 1 4 3 7 Material Modeling NFRs
         - 1 1 4 4 NFR Parking Lot
            - 1 1 4 4 1 Architectural Qualities
            - 1 1 4 4 2 Supportability - Diagnostics
            - 1 1 4 4 3 Supportability - Documentation
            - 1 1 4 4 4 Supportability - Logging
      - 1 1 4 4 5 Supportability - Tools
   - 1 1 4 5 NFR Coverage
- 1 1 5 Before You Start
   - 1 1 5 1 First Steps for Startup Users
   - 1 1 5 2 OCMOD Concepts And First Steps To Use
   - 1 1 5 3 Introduction of OCMOD Documentation
   - 1 1 5 4 Quick Start to Developing with Opcenter Modular Manufacturing
      - 1 1 5 4 1 Configuring the Development Environment
      - 1 1 5 4 2 Siemens Web Framework Fundamentals
      - 1 1 5 4 3 How to Create a Configurable Object
      - 1 1 5 4 4 Creating a Metadata Model
      - 1 1 5 4 5 Developing an App / Metadata Module
- 1 1 6 Common Guidelines
   - 1 1 6 1 Glossary
   - 1 1 6 2 Working with Documentation
      - 1 1 6 2 1 Creating Documentation Pages pages in Confluence
   - 1 1 6 3 User Story - Definition of Ready (DoR)
   - 1 1 6 4 User Story - Definition of Done (DoD)
   - 1 1 6 5 User Story - Definition of Done (checklist)
   - 1 1 6 6 Product Security - Definition of Done for Feature and Release
   - 1 1 6 7 Test Strategy
   - 1 1 6 8 Interfacing with ModularMOM Adopter Startups
   - 1 1 6 9 Procedure for Pull Request
   - 1 1 6 10 Best practices for Git Branching and Merging
   - 1 1 6 11 TFS Azure DevOps
   - 1 1 6 12 Communication Rules
   - 1 1 6 13 Procedure to set up first user in XCR
      - 1 1 6 13 1 Manual configuration of User with Permission
      - 1 1 6 13 2 OBM Flow
      - 1 1 6 13 3 Service account registration
      - 1 1 6 13 4 Product and App registration
      - 1 1 6 13 5 Provider role creation
      - 1 1 6 13 6 Provision Product and User
- 1 1 7 Agile Scrum Practice Modular MOM2
   - 1 1 7 1 Artifacts
      - 1 1 7 1 1 Feature, User Story, Task, Epic
      - 1 1 7 1 2 Bugs, Issues
      - 1 1 7 1 3 Test Plan, Test Suite, Test Case
      - 1 1 7 1 4 TFS Items - Creation Workflow
      - 1 1 7 1 5 MomValueArea
      - 1 1 7 1 6 Estimating
   - 1 1 7 2 Events
   - 1 1 7 3 Accountabilities (Roles)
- 1 1 8 Scrum Events
   - 1 1 8 1 MM MES Retrospectives (from Sprint 24)
   - 1 1 8 2 MM PLT Retrospectives (from Sprint 25)
   - 1 1 8 3 MM MES Sprint Review Agenda
   - 1 1 8 4 MM PLT Sprint Review Agenda
   - 1 1 8 5 Scrum Events Preparation
   - 1 1 8 6 Overall Retrospectives (Sprint 1 to Sprint 23)
- 1 1 9 Internal Collaborative Content
   - 1 1 9 1 Nightly Test Run Follow-Up Plan 2.x
   - 1 1 9 2 Building Redis on Windows for Development
   - 1 1 9 3 Calculation of Code Coverage in Pipeline
   - 1 1 9 4 Code Coverage using Unit Test
   - 1 1 9 5 Debugging BDD API Framework and Common Steps
   - 1 1 9 6 CI/CD Processes and Practices
      - 1 1 9 6 1 Process And Phases
      - 1 1 9 6 2 User Tasks
      - 1 1 9 6 3 Devops Implementation
   - 1 1 9 7 Enabling LongPath Support in Windows
   - 1 1 9 8 Git How To
   - 1 1 9 9 Monitoring Traces and Logs in Development Environment
   - 1 1 9 10 Production Environment Allocation
   - 1 1 9 11 Openshift Internal Project Links
   - 1 1 9 12 Postman Examples
   - 1 1 9 13 SF experts exchange topics
   - 1 1 9 14 Solution Structure
   - 1 1 9 15 SQL Server on deployment
   - 1 1 9 16 SWF-6 UI Migration
   - 1 1 9 17 TimeZone issues - find a meeting slot
   - 1 1 9 18 Introducing Kustomize
      - 1 1 9 18 1 ModMOM - SqlServer
   - 1 1 9 19 Modular MOM on Cloud
   - 1 1 9 20 How to run BDD UI against deployed environment
   - 1 1 9 21 Executing BDD API Tests with AspNetCore Test Server
      - 1 1 9 21 1 Executing BDD API Tests with AspNetCore Test Server in the Release Pipeline
      - 1 1 9 21 2 Executing BDD API Tests with AspNetCore Test Server on VDI
   - 1 1 9 22 Executing BDD API Tests from VDI on deployed system
   - 1 1 9 23 Backend Localization
   - 1 1 9 24 Localization(L10N) support provided by Siemens Web Framework (SWF)
   - 1 1 9 25 Viewing Logs from OpenShift with Kibana for Dummies
   - 1 1 9 26 How to initialize Access Management
   - 1 1 9 27 How to add another NFR to the nightly test run & Sumo Dashboard
   - 1 1 9 28 Custom component using SWF
   - 1 1 9 29 How to use observability data in Sumologic
   - 1 1 9 30 How to provide and use a new messaging model version
   - 1 1 9 31 Signal R and SWF integration
   - 1 1 9 32 Stable Builds Definition
   - 1 1 9 33 Platform Upgrade 2.2.0
- 1 1 10 Platform Internal Architecture
   - 1 1 10 1 Platform & Model Versioning
   - 1 1 10 2 Metadata/Metamodel
      - 1 1 10 2 1 Model configuration
      - 1 1 10 2 2 Revisioned & Revision Base Types
      - 1 1 10 2 3 Enumerations(Enums)
      - 1 1 10 2 4 Configuring Nullable Native, Enum, and Options Type Fields
      - 1 1 10 2 5 List Persistence - Configuration & Characteristics
      - 1 1 10 2 6 Metadata Model Query Definitions
      - 1 1 10 2 7 Accessibility Configuration of CO Fields
      - 1 1 10 2 8 Required Fields on Persistent Entities
      - 1 1 10 2 9 Selection Values
      - 1 1 10 2 10 System Objects & Persistence
      - 1 1 10 2 11 Framework for Trackable & Audit Trail Objects
      - 1 1 10 2 12 Message Model Extensions
      - 1 1 10 2 13 Metamodel Override Features
      - 1 1 10 2 14 Configurable Object Maps (COMap)
      - 1 1 10 2 15 [Partially Obsolete] Metadata Module Architectural Concept
   - 1 1 10 3 Internal Release Notes
      - 1 1 10 3 1 Platform Version 2.2.0
      - 1 1 10 3 2 Platform Version 2.1.10
      - 1 1 10 3 3 Platform Version 2.1.8
      - 1 1 10 3 4 Platform Version 2.1.7
      - 1 1 10 3 5 Platform Version 2.1.6
      - 1 1 10 3 6 Platform Version 2.1.5
      - 1 1 10 3 7 Platform Version 2.1.4
      - 1 1 10 3 8 Platform Version 2.1.3
      - 1 1 10 3 9 Platform Version 2.1.2
      - 1 1 10 3 10 Platform Version 2.1.1
      - 1 1 10 3 11 Platform Version 2.1.0
      - 1 1 10 3 12 Platform Version 2.0.0
      - 1 1 10 3 13 Platform Version 1.0.14
      - 1 1 10 3 14 Platform Version 1.0.13
      - 1 1 10 3 15 Platform Version 1.0.12
      - 1 1 10 3 16 Platform Version 1.0.11
      - 1 1 10 3 17 Platform Version 1.0.10
      - 1 1 10 3 18 Platform Version 1.0.9
      - 1 1 10 3 19 Platform Version 1.0.8
      - 1 1 10 3 20 Platform Version 1.0.7
      - 1 1 10 3 21 Platform Version 1.0.6
      - 1 1 10 3 22 Platform Version 1.0.5
      - 1 1 10 3 23 Platform Version 1.0.4
      - 1 1 10 3 24 Platform Version 1.0.3
      - 1 1 10 3 25 Platform Version 2.2.1-prerelease
   - 1 1 10 4 SaaS Architecture
      - 1 1 10 4 1 Failure points and Recover Strategy
      - 1 1 10 4 2 Policy configuration in SAM
      - 1 1 10 4 3 Product Definition
      - 1 1 10 4 4 SaaS Project Status for Limited Preview
      - 1 1 10 4 5 User Workflows
   - 1 1 10 5 Configuration Layer (Former Governance Layer)
      - 1 1 10 5 1 Application Configuration and Deployment
      - 1 1 10 5 2 Appsettings.json - Messaging Configuration
   - 1 1 10 6 Information Layer
      - 1 1 10 6 1 Cache Strategy
      - 1 1 10 6 2 Database Schema Strategy
      - 1 1 10 6 3 Query Strategy
   - 1 1 10 7 Integration Layer
      - 1 1 10 7 1 Dapr Analysis
      - 1 1 10 7 2 Grpc Message serialization changes (US 45509)
      - 1 1 10 7 3 Message Consume Handling
      - 1 1 10 7 4 Message Extensibility
      - 1 1 10 7 5 Message resiliency - Detailed Design
      - 1 1 10 7 6 Module name resolution
      - 1 1 10 7 7 Pub/Sub Communication
   - 1 1 10 8 Observability Layer
      - 1 1 10 8 1 Instrumenting Applications
      - 1 1 10 8 2 OpenTelemetry
         - 1 1 10 8 3 Observability Architecture
         - 1 1 10 8 4 Reference Materials
         - 1 1 10 8 5 Distributed Tracing
         - 1 1 10 8 6 Metrics
         - 1 1 10 8 7 Structured Logging
         - 1 1 10 8 8 How to use observability data?
      - 1 1 10 9 Runtime Layer
         - 1 1 10 9 1 API Interaction
         - 1 1 10 9 2 Class Based Options and Flag
         - 1 1 10 9 3 Exception Framework
         - 1 1 10 9 4 Gateway Interaction - Pipeline call-chain
         - 1 1 10 9 5 Healthchecks - K8 Healthiness probes
         - 1 1 10 9 6 Localization Support for Metadata Runtime components and Platform
         - 1 1 10 9 7 Spike: Localization Support for Metadata Runtime components
         - 1 1 10 9 8 Unique Id Generation
      - 1 1 10 10 Security Layer
         - 1 1 10 10 1 Authenticated Api Request via Postman
         - 1 1 10 10 2 Authorization Model
         - 1 1 10 10 3 Dex Evaluation
         - 1 1 10 10 4 Emissary Ingress (Ambassador API Gateway)
         - 1 1 10 10 5 Enable Authentication in Business Module
         - 1 1 10 10 6 Identity Brokering & SamAuth Integration
         - 1 1 10 10 7 Oidc-Client library evaluation
      - 1 1 10 11 Modules
         - 1 1 10 11 1 Third-Party Software Used in Platform
         - 1 1 10 11 2 Application Catalog
         - 1 1 10 11 3 Business Model (Metadata) Development Guidelines
         - 1 1 10 11 4 Developing a Metadata Module(V1)
         - 1 1 10 11 5 Developing a Metadata Module(V2+)
         - 1 1 10 11 6 User code execution risk & mitigation
         - 1 1 10 11 7 Creating a new version of M1_Common
      - 1 1 10 12 Technological Upgrade
         - 1 1 10 12 1 Upgrade to .NET
      - 1 1 10 13 Localization
         - 1 1 10 13 1 Open points: Feature 32266: Ensure Localization on all modules
   - 1 1 11 Internal Technical Documentation
      - 1 1 11 1 Inspection Planning Shopfloor
      - 1 1 11 2 Momwiki02 Structure Draft
   - 1 1 12 App Catalogue
      - 1 1 12 1 Access Management App
      - 1 1 12 2 Factory App
      - 1 1 12 3 Failure Catalog App
      - 1 1 12 4 Inspection Evaluation Shopfloor App
      - 1 1 12 5 Inspection Planning Shopfloor App
      - 1 1 12 6 Material Management App
      - 1 1 12 7 Order Management App
      - 1 1 12 8 Track And Trace App
   - 1 1 13 Modular MOM Configurator
      - 1 1 13 1 API Specification
      - 1 1 13 2 Build and Deployment
      - 1 1 13 3 Configurator - Architecture
      - 1 1 13 4 Git Integration
         - 1 1 13 4 1 Squashing Git Repositories for release
      - 1 1 13 5 System Configurations
      - 1 1 13 6 UI
         - 1 1 13 6 1 Configurator Development
   - 1 1 14 Kubernetes Operators
- 1 2 Quality MOM
   - 1 2 1 Organization Quality MOM
      - 1 2 1 1 Feature Team "Acquisition"
         - 1 2 1 1 1 Definition of Ready (Acquisition Team)
         - 1 2 1 1 2 Definition of Done (Acquisition Team)
      - 1 2 1 2 Feature Team "Business"
      - 1 2 1 3 Feature Team "Engineering"
         - 1 2 1 3 1 Definition of Done (Engineering team)
         - 1 2 1 3 2 Definition of Ready (Engineering team)
         - 1 2 1 3 3 Story Points
      - 1 2 1 4 Feature Team "Integration"
         - 1 2 1 4 1 Definition of Done (Integration Team)
         - 1 2 1 4 2 Definition of Ready (Integration team)
   - 1 2 2 Functional Specification
      - 1 2 2 1 Action Management
         - 1 2 2 1 1 Action Management APIs
         - 1 2 2 1 2 Action Management Entities
         - 1 2 2 1 3 Action Management Messages
      - 1 2 2 2 Failure Catalogue
      - 1 2 2 3 Inspection Evaluation Shopfloor
         - 1 2 2 3 1 IES - Module Permissions
      - 1 2 2 4 Inspection Planning Shopfloor #
      - 1 2 2 5 NonConformance Management
      - 1 2 2 6 Variant Management
   - 1 2 3 Internal Collaborative Content
      - 1 2 3 1 Azure DevOps
      - 1 2 3 2 Development
         - 1 2 3 2 1 Access to database on OpenShift environment
         - 1 2 3 2 2 Cheat sheets
         - 1 2 3 2 3 Create and use local NuGet package
         - 1 2 3 2 4 Working with MetadataRuntime
      - 1 2 3 3 Optimistic Locking in ModMom Quality
      - 1 2 3 4 Testing
         - 1 2 3 4 1 UI BDD Tests
   - 1 2 4 Development Runtime
      - 1 2 4 1 Iteration Result
   - 1 2 5 Operational Runtime (PLM)
- 1 3 APS MOM
- 1 4 Security
   - 1 4 1 Security Tasks
   - 1 4 2 Security Training
- 1 5 Technical Knowledge Base
   - 1 5 1 AWS
      - 1 5 1 1 ElastiCache
      - 1 5 1 2 SWF UI builder on AWS ECS with Fargate Container
   - 1 5 2 Developer's Jumpstart for ModMOM 1.x
      - 1 5 2 1 Adding the ASP.NET Core Server Project manually
      - 1 5 2 2 Component Tests
      - 1 5 2 3 Debugging a Microservice locally
      - 1 5 2 4 Kubernetes Cheat Sheet
      - 1 5 2 5 Minishift Scenario Guidelines
      - 1 5 2 6 Working with Git submodules in Visual Studio
      - 1 5 2 7 Generate token for ModularMOM via Postman using Token Service
      - 1 5 2 8 Modular MOM web apps development using SWF
         - 1 5 2 8 1 Http request methods for Modular MOM web apps
      - 1 5 2 9 Guidelines for declaring MS permissions via Permission.xml
      - 1 5 2 10 Adding Nuget packages in ModularMOM MS
      - 1 5 2 11 Getting started with OpenShift CLI
      - 1 5 2 12 Steps to Update Engine references in Modular MOM project
      - 1 5 2 13 How to setup the SQL Server sa user on Opcenter X VDI
      - 1 5 2 14 Tips to design a Use Case flow diagram
      - 1 5 2 15 Step by step guide to run BDD automated tests for any UI microservices
      - 1 5 2 16 Event Implementation Guide for ModularMOM MS
      - 1 5 2 17 MIOUMC0x: Installation of Additional Resources
         - 1 5 2 17 1 How to Install and Configure UMC
         - 1 5 2 17 2 UMC Endpoints Configuration
         - 1 5 2 17 3 How to Install and Configure MIO
      - 1 5 2 18 Convert an object to a B2MML (ERP)
      - 1 5 2 19 How to seed data using xxx.dbInit.xml
      - 1 5 2 20 Working with Localization file for Web App
      - 1 5 2 21 How to enable HA Endpoints on UI Services
      - 1 5 2 22 Practices to be followed while Creating Branches
      - 1 5 2 23 Checklist for AC and Description
      - 1 5 2 24 Installation & Configuration of the Monitoring Stack for NFR Testing
      - 1 5 2 25 Setting breadcrumb title using mom-config
      - 1 5 2 26 How to use Error parameters in Modular MOM?
      - 1 5 2 27 How to Install, Upgrade, and Uninstall Fake Microservices
         - 1 5 2 27 1 Installing and Upgrading Account MS
         - 1 5 2 27 2 Installing and Upgrading MaterialFlow MS
         - 1 5 2 27 3 Installing and Upgrading Process MS
         - 1 5 2 27 4 Uninstalling Account MS
         - 1 5 2 27 5 Uninstalling MaterialFlow MS
         - 1 5 2 27 6 Uninstalling Process MS
      - 1 5 2 28 How to Upgrade Foundation Installation with ModMOM custom DSL Plug-ins
      - 1 5 2 29 How to Integrate Apps with Token MS Locally
      - 1 5 2 30 Getting started with UI Auto tests
      - 1 5 2 31 Generate Role based token via Postman from Token Service
      - 1 5 2 32 SonarLint for Client Static Code Analysis on Visual Studio
      - 1 5 2 33 How to set Automation Test Execution Timeout
      - 1 5 2 34 Configure keyboard shortcut for ModularMOM UI Apps
   - 1 5 3 DevOps
      - 1 5 3 1 Kafka SASL-SCRAM configuration
   - 1 5 4 Opcenter X with MIO Integration
      - 1 5 4 1 How to Configure MIO for Communicating with Serial Scales
         - 1 5 4 1 1 Defining a Message Channel For Serial Scales
         - 1 5 4 1 2 Defining the Channel Sources For Serial Scales
         - 1 5 4 1 3 Defining the Client Gateway for Serial Scales
         - 1 5 4 1 4 Defining the Inbound Message types for Serial Scales
         - 1 5 4 1 5 Defining the Outbound Message Types for Serial Scales
         - 1 5 4 1 6 Defining the Serial Port Adapters for Serial Scales
      - 1 5 4 2 How to Configure MIO for Exchanging Messages with Opcenter X
         - 1 5 4 2 1 Configuring the MOM Connection
         - 1 5 4 2 2 How to Download a Message
         - 1 5 4 2 3 How to Upload a Message
      - 1 5 4 3 How to Extend XML Files
      - 1 5 4 4 Messages for Manufacturing Processes
         - 1 5 4 4 1 Production Order Download Message
- 1 6 Traveller's Guide
   - 1 6 1 Paperwork before and after a business trip (for Siemens PL employees)
- 1 7 What's the name?
- 1 8 NFR+Assurance_+Chaos+Engineering
   - 05 Dec 2022 06 Dec 2022 07 Dec 2022 08 Dec
   - 12 Dec 2022 13 Dec 2022 14 Dec 2022 15 Dec
   - 19 Dec 2022 20 Dec 2022 21 Dec 2022 22 Dec
   - 26 Dec 2022 27 Dec 2022 28 Dec 2022 29 Dec
- January


```
02 Jan 2023 03 Jan 2023 04 Jan 2023 05 Jan 2023 06 Jan 2023
```
```
09 Jan 2023 10 Jan 2023 11 Jan 2023 12 Jan 2023 13 Jan 2023
16 Jan 2023 17 Jan 2023 18 Jan 2023 19 Jan 2023 20 Jan 2023
```
```
23 Jan 2023 24 Jan 2023 25 Jan 2023 26 Jan 2023 27 Jan 2023
30 Jan 2023 31 Jan 2023
```
January 2022

```
Monday Tuesday Wednesday Thursday Friday
```
```
03 Jan 2022
```
```
Varoli, Guido
```
```
Alvarez Villanueva,
Beatriz
```
```
Arduini, Milvia
Pierpaola
```
```
04 Jan 2022
```
```
Varoli, Guido
```
```
Alvarez Villanueva,
Beatriz
```
```
Arduini, Milvia
Pierpaola
```
```
05 Jan 2022
```
```
Varoli, Guido
```
```
Alvarez Villanueva,
Beatriz
```
```
Arduini, Milvia
Pierpaola
```
```
06 Jan 2022 07 Jan 2022
```
```
10 Jan 2022
```
```
Alvarez Villanueva,
Beatriz
```
```
11 Jan 2022 12 Jan 2022 13 Jan 2022 14 Jan 2022
```
```
Alvarez Villanueva, Beatriz
(1 h in the afternoon)
```
```
17 Jan 2022 18 Jan 2022 19 Jan 2022 20 Jan 2022 21 Jan 2022
24 Jan 2022
```
```
Occhiena, Marco
```
```
25 Jan 2022 26 Jan 2022 27 Jan 2022 28 Jan 2022
```
```
31 Jan 2022
```
February 2022

```
Monday Tuesday Wednesday Thursday Friday
```
```
01 Feb 2022 02 Feb 2022 03 Feb 2022 04 Feb 2022
```
```
07 Feb 2022 08 Feb 2022 09 Feb 2022 10 Feb 2022 11 Feb 2022
```
```
14 Feb 2022 15 Feb 2022 16 Feb 2022 17 Feb 2022 18 Feb 2022
21 Feb 2022 22 Feb 2022 23 Feb 2022 24 Feb 2022 25 Feb 2022
```
```
28 Feb 2022
```
```
Nitsche, Andrea
```
March 2022

```
Monday Tuesday Wednesday Thursday Friday
```

```
01 Mar 2022
```
```
Nitsche, Andrea
```
```
02 Mar 2022 03 Mar 2022 04 Mar 2022
```
```
07 Mar 2022 08 Mar 2022
```
```
Alvarez Villanueva, Beatriz
```
```
09 Mar 2022 10 Mar 2022 11 Mar 2022
```
```
Alvarez Villanueva, Beatriz
```
```
14 Mar 2022 15 Mar 2022 16 Mar 2022 01 Mar 2022 18 Mar 2022
```
```
Alvarez Villanueva, Beatriz
```
```
21 Mar 2022
```
```
Alvarez Villanueva, Beatriz
```
```
22 Mar 2022 23 Mar 2022 24 Mar 2022 25 Mar 2022
```
```
Alvarez Villanueva, Beatriz Immordino, Giovanni
```
```
28 Mar 2022 29 Mar 2022 30 Mar 2022 31 Mar 2022
```
April 2022

```
Monday Tuesday Wednesday Thursday Friday
```
```
01 Apr 2022
```
```
04 Apr 2022 05 Apr
2022
```
```
06 Apr 2022 07 Apr 2022
```
```
Alvarez Villanueva,
Beatriz
```
```
(3h in the morning)
```
```
08 Apr 2022
```
```
11 Apr 2022 12 Apr
2022
```
```
13 Apr 2022 14 Apr 2022 15 Apr 2022
```
```
18 Apr 2022 19 Apr
2022
```
```
20 Apr 2022 21 Apr 2022
Nitsche, Andrea
```
```
22 Apr 2022
Nitsche, Andrea
```
```
25 Apr 2022 26 Apr
2022
```
```
27 Apr 2022
```
```
Alvarez Villanueva,
Beatriz
```
```
(3h in the afternoon)
```
```
28 Apr 2022 29 Apr 2022
```
May 2022

```
Monday Tuesday Wednesday Thursday Friday
```
```
02 May 2022
```
```
Alvarez Villanueva, Beatriz
```
```
03 May 2022 04 May 2022 05 May 2022 06 May 2022
```
```
Alvarez Villanueva, Beatriz
```

09 May 2022Alvarez Villanueva, Beatriz (3h in the afternoon) 10 May 2022 11 May 2022 12 May 2022 13 May 2022

```
Immordino, Giovanni
```
16 May 2022 17 May 2022 18 May 2022 19 May 2022 20 May 2022

23 May 2022 24 May 2022 25 May 2022 26 May 2022 27 May 2022

```
Immordino, Giovanni
```
30 May 2022 31 May 2022


# Feature Team "Pratapgad" (Pune - Fort)

## Our Standup meeting: xxx (Join Microsoft Teams Meeting)

```
Pratapgad literally 'Valour Fort' is a large mountain fort located in Satara
district, in the Western Indian state of Maharashtra. The fort's historical
significance is due to the Battle of Pratapgad in 1659. The fort is now a
popular tourist destination.
```
```
Role Contacts Participates in
...
```
```
Team
Member
```
```
Lele, Pushkar QA CoP
```
```
Team
Member
```
```
Sheth, Hemal
```
```
Team
Member
```
```
Patil, Dattatray
```
```
Team
Member
```
```
Saktepar, Shrutika
```
```
Team
Member
```
```
Patil, Shubhangi
```
```
Team
Member
```
```
Rawal, Hitesh
```
```
Team
Member
```
```
Sinha, Dheeraj
```
```
Scrum
Master
```
```
Moholkar, Sunetra Agile CoP
```

# Pratapgad's Retrospectives


# old


# Sprint 12

Attendees:

Aditya, Mallikarjun , Siddhu , Yuvraj , Manali, Mangesh and Parul

Action Items :

```
Raise the concern in COP to re-scheduled as per availability of all the members in the meeting - Unknown User (aditya.kaminwar) Unknown User
(mallikarjun.hiremath)
```
```
Approve the pull request after getting it reviewed from 2 team members.
```
```
We should have single story active at a time and use 'onHold' tags at story level with an explanation in comments.
```
```
Feature level testing - TBD from next sprint : It should be included in FI test
```
```
Create a process to assign bugs - TBD in QA COP : Overall Retrospective
```
```
Internal team meetings post BR1 and after BR2 , to analyse the issues/ challenges. Create US and update AC in the meeting Malhotra, Parul -
Team
```
```
Mock up should be approved before the implementation. Risk must be highlighted in case there is any change in requirement. - Story Owner
```
```
Add Dev End date to story and try to finish test preparation in first 2 week of the sprint. - Team/ QA/Story Owner
```
```
Highlight the risk on the stories in teams Excel as well as in the story as comment - Story owner
```
```
OS Environment level changes done by other Teams were not Conveyed properly so it Impacted development and 02 Scenario. - Overall-
Retrospective
```
```
Plan 1 team building activity per sprint
```

# Sprint 11

Attendees:

Varun, Aditya, Mallikarjun , Siddhu , Yuvraj and Parul

Action Items :

```
Add query to verify the estimation Malhotra, Parul
```
```
Raise the concern in COP to re-scheduled as per availability of all the members in the meeting - Unknown User (aditya.kaminwar) Unknown User
(mallikarjun.hiremath)
```
```
Approve the pull request after getting it reviewed from 2 team members.
```
```
We should have single development task active at a time and use 'onHold' tags at task level with an explanation in comments.
```
```
Show color coding in TFS based on tasks. Malhotra, Parul
```
```
Bug Bash activity for 2 hrs on first day of Sprint Malhotra, Parul
```
```
Feature level testing - TBD from next sprint
```
```
Create a process to assign bugs - TBD in QA COP
```

# Sprint 10

```
Mock up should finalized max in 1st week of sprint. If new improvements come, we should revise estimation.
```
- we will not mark US as committed till mock up gets finalized. - @Team

```
User documentation tasks got delayed due to much unknowns
```
- Try to co-ordinate well and minimize the unknowns - Unknown User (subha.gopalakrishnan)and Team

Improvements:

```
To avoid last minute rush/pressure, keep a practise to make 03 up with all latest changes from master. This activity will be on rotation basis - Seng
upta, Ankita
```
```
PR must be raised even for small change - @Team
```
```
Add TCs to sprint specific and master test plan - @Whoever execute TCs
```
```
Run API automations on 03 and 07 after merging to master - @Team
```

# Sprint 9

```
Internal Code Freeze on Wednesday , 3rd week. Malhotra, Parul
```
```
Every COP Member should update of MOM of COP meeting on regular basis to the team
```
```
Dedicated discussion time for task analysis - Team
```
```
Task Analysis ,Review and Estimation should be done for all the tasks _ Team
```
```
If feature team creates mockup, make sure we are aligned with SWF elements. If UX team provide mockup, check for elements not supported by
SWF and inform the risk - Story Owner
```
```
Highlight the dependency at earliest and raise it in first week of sprint within team and communicate to SMs and PO if required - Story Owner
```
```
Add query to verify the estimation
```
Attendees:

Siddhu, Aditya, Mallikarjun, Yuvraj and Parul


# Sprint 8

Action Items:

```
One time session on deployment using OS - Malhotra, Parul
```
```
Test case preparation should consider all possible scenario - -QA and Story Owner
```
```
Test case review should be done more thoroughly - Story Owner
```
```
If feature team creates mockup, make sure we are aligned with SWF elements. If UX team provide mockup, check for elements not supported by
SWF and inform the risk
```

### 1.

### 2.

### 3.

# Sprint 7

Action Item:

```
Cross Team collaboration must be better , so that re-work can be avoided or any important piece is missed.
```
```
Development should be completed by early 3rd week Malhotra, Parul
```
```
QA should spend committed hours to Team, Priority of stories cannot take precedence over committed time. Committed hours must be pre-
defined based on there bandwidth. Malhotra, Parul
Discussed with QA Experts, about the issues and how to rectify.
Resolution :
BR2s are optional for QA Experts
Join daily Scrum meeting , share update and can drop or share update via message( acceptable)
Will add them in planning for 1 hr when task is being allocated, from there they will connect with Story owners
```
```
Highlight the dependency at earliest and raise it in first week of sprint within team and communicate to SMs and PO if required. – Team
```
```
Add query to verify the estimation
```
```
More accountability of story owners for better task distribution and analyse the dependency - Team
```
```
Add stories for Refinement of Feature
```
```
Focus more in BR2 for better planning and estimation- Team
```

# Feature Team "Shivneri" (Pune - Fort)

## Our Standup meeting: Each morning at xxx (Join Microsoft Teams Meeting)

```
Shivneri Fort is a 17th-century military fortification located near Junnar in
Pune district in Maharashtra, India. It is the birthplace of Chhatrapati
Shivaji Maharaj , the emperor and founder of Maratha Empire.
```
```
It is one of the strongest forts of India.
```
```
Role Contacts Participates in ...
```
```
Team Member Das, Debarshi Architectural CoP
```
```
Team Member Naik, Hemant CI/CD CoP
```
```
Team Member Vashisht, Ram
```
```
Scrum Master Kirve, Ajit Agile CoP
```
```
Team Member Joshi, Deepak QA CoP
```
```
Team Member Patil, Mahesh
```
```
Team Member Patil, Akshaykumar
```

# Feature Team "Monviso" (Genoa - Mountain)

## Welcome! You just landed in the Monviso's Wiki Page.

## You can meet the team in one of our Standups held daily at 9:30 AM (CET)

```
Monte Viso or Monviso (Italian pronunciation: [movizo]) is the highest
mountain of the Cottian Alps. It is located in Italy close to the French
border. The Monviso is well known for its pyramid-like shape and,
because it is higher than all its neighboring peaks by about 500 m, it can
be seen from a great distance, including from the Piedmontese plateau,
the Langhe, the Theodulpass in the Zermatt ski area and the summits of
the Mont Blanc massif. On a very clear day it can be seen from the
spires of Milan Cathedral.
```
```
It has been suggested that Monviso could be one of the mountains
which inspired the Paramount logo. In Italy it is also known as Il Re di
Pietra ("The Stone King") because of its prominence within the western
Italian Alps. It was declared a cross-border UNESCO biosphere reserve
in 2013.
```
```
It is also a mountain of the birth of the longest river of Italy, River Po.
```
```
Role Name Participates in ...
```
```
Team Member Abate, Francesco
```
```
Team Member Dal Mas, Roberto
```
```
Team Member Fioritoni, Marco
```
```
Scrum Master Montaldo, Ernesto Agile CoP
```
```
Team Member Peretti, Massimo
```
```
Team Member Simone, Maurizio QA CoP
```
```
Team Member Turco, Mariangela
```
```
Team Member Turolla, Andrea
```

# Monviso OOO's Calendar

## January 2022

```
Monday Tuesday Wednesday Thursday Friday
```
```
03 Jan 2022
```
```
Fioritoni, Marco
```
```
Turolla, Andrea
```
```
Abate, Francesco
```
```
Dal Mas, Roberto
```
```
Simone, Maurizio
```
```
04 Jan 2022
```
```
Fioritoni, Marco
```
```
Turolla, Andrea
```
```
Abate, Francesco
```
```
Dal Mas, Roberto
```
```
Simone, Maurizio
```
```
05 Jan 2022
```
```
Fioritoni, Marco
```
```
Turolla, Andrea
```
```
Abate, Francesco
```
```
Dal Mas, Roberto
```
```
Simone, Maurizio
```
```
06 Jan 2022
```
```
Holiday - Epiphany
```
```
07 Jan 2022
```
```
Company Holiday Period
```
```
10 Jan 2022 11 Jan 2022 12 Jan 2022
```
```
Turolla, Andrea
```
```
13 Jan 2022 14 Jan 2022
```
```
Simone, Maurizio
```
```
17 Jan 2022 18 Jan 2022 19 Jan 2022 20 Jan 2022 21 Jan 2022
```
```
Simone, Maurizio
```
```
24 Jan 2022 25 Jan 2022 26 Jan 2022 27 Jan 2022 28 Jan 2022
Simone, Maurizio
```
```
31 Jan 2022
```
## February 2022

```
Monday Tuesday Wednesday Thursday Friday
```
```
01 Feb 2022 02 Feb 2022 03 Feb 2022 04 Feb 2022
```
```
07 Feb 2022 08 Feb 2022 09 Feb 2022 10 Feb 2022 11 Feb 2022
```
```
14 Feb 2022 15 Feb 2022 16 Feb 2022 17 Feb 2022 18 Feb 2022
Simone, Maurizio
```
```
21 Feb 2022
```
```
Montaldo, Ernesto
```
```
22 Feb 2022
```
```
Montaldo, Ernesto
```
```
23 Feb 2022
```
```
Montaldo, Ernesto
```
```
24 Feb 2022
```
```
Montaldo, Ernesto
```
```
25 Feb 2022
```
```
Montaldo, Ernesto
```
```
28 Feb 2022
```
## March 2022

```
Monday Tuesday Wednesday Thursday Friday
```
```
01 Mar 2022 02 Mar 2022 03 Mar 2022 04 Mar 2022
```
```
07 Mar 2022 08 Mar 2022 09 Mar 2022 10 Mar 2022 11 Mar 2022
14 Mar 2022 15 Mar 2022 16 Mar 2022 17 Mar 2022 18 Mar 2022
21 Mar 2022 22 Mar 2022 23 Mar 2022 24 Mar 2022 25 Mar 2022
28 Mar 2022 29 Mar 2022 30 Mar 2022 31 Mar 2022
```
## April 2022

```
Monday Tuesday Wednesday Thursday Friday
```
```
01 Apr 2022
```
```
04 Apr 2022 05 Apr 2022 06 Apr 2022 07 Apr 2022 08 Apr 2022
```
```
11 Apr 2022 12 Apr 2022 13 Apr 2022
```
```
Turco, Mariangela
```
```
14 Apr 2022
```
```
Turco, Mariangela
```
```
Dal Mas, Roberto
```
```
15 Apr 2022
```
```
We are closed
```

```
18 Apr 2022
```
```
Easter Monday (Holiday)
```
```
19 Apr 2022 20 Apr 2022 21 Apr 2022 22 Apr 2022
```
```
25 Apr 2022 26 Apr 2022 27 Apr 2022 28 Apr 2022 29 Apr 2022
```
May 2022

```
Monday Tuesday Wednesday Thursday Friday
```
```
02 May 2022 03 May 2022 04 May 2022 05 May 2022 06 May 2022
```
```
09 May 2022 10 May 2022 11 May 2022 12 May 2022 13 May 2022
```
```
16 May 2022 17 May 2022 18 May 2022 19 May 2022 20 May 2022
```
```
23 May 2022
Simone, Maurizio
```
```
24 May 2022 25 May 2022 26 May 2022 27 May 2022
```
```
30 May 2022 31 May 2022
```
June 2022

```
Monday Tuesday Wednesday Thursday Friday
```
```
01 Jun 2022 02 Jun 2022
```
```
Republic Day (Holiday)
```
```
03 Jun 2022
```
```
We are closed
```
```
06 Jun 2022 07 Jun 2022 08 Jun 2022
```
```
Dal Mas, Roberto
```
```
09 Jun 2022
```
```
Dal Mas, Roberto
```
```
10 Jun 2022
```
```
Peretti, Massimo
```
```
13 Jun 2022 14 Jun 2022 15 Jun 2022 16 Jun 2022
```
```
Montaldo, Ernesto (afternoon only)
```
```
Dal Mas, Roberto
```
```
17 Jun 2022
```
```
20 Jun 2022
```
```
Montaldo, Ernesto (afternoon only)
```
```
21 Jun 2022 22 Jun 2022 23 Jun 2022
```
```
Dal Mas, Roberto
```
```
24 Jun 2022
```
```
We are closed
```
```
27 Jun 2022
```
```
Montaldo, Ernesto (afternoon only)
```
```
Peretti, Massimo
```
```
28 Jun 2022
```
```
Peretti, Massimo
```
```
29 Jun 2022
```
```
Peretti, Massimo
```
```
30 Jun 2022
```
```
Peretti, Massimo
```
July 2022

```
Monday Tuesday Wednesday Thursday Friday
```
```
01 Jul 2022
```
```
Montaldo, Ernesto (afternoon only)
```
```
Peretti, Massimo
```
```
Fioritoni, Marco
```
```
04 Jul 2022
Peretti, Massimo
```
```
05 Jul 2022
Peretti, Massimo
```
```
06 Jul 2022 07 Jul 2022
Turco, Mariangela
```
```
08 Jul 2022
Montaldo, Ernesto (afternoon only)
```
```
Turco, Mariangela
```
```
11 Jul 2022 12 Jul 2022 13 Jul 2022 14 Jul 2022 15 Jul 2022
```
```
18 Jul 2022
```
```
Montaldo, Ernesto (afternoon only)
```
```
Fioritoni, Marco
```
```
19 Jul 2022
```
```
Fioritoni, Marco
```
```
20 Jul 2022
```
```
Fioritoni, Marco
```
```
21 Jul 2022
```
```
Fioritoni, Marco
```
```
22 Jul 2022
```
```
Montaldo, Ernesto (afternoon only)
```
```
Fioritoni, Marco
```
```
25 Jul 2022
```
```
Peretti, Massimo
```
```
Montaldo, Ernesto
```
```
26 Jul 2022
```
```
Peretti, Massimo
```
```
Montaldo, Ernesto
```
```
27 Jul 2022
```
```
Peretti, Massimo
```
```
Montaldo, Ernesto
```
```
28 Jul 2022
```
```
Montaldo, Ernesto
```
```
Peretti, Massimo
```
```
29 Jul 2022
```
```
Peretti, Massimo
```
```
Montaldo, Ernesto
```

August 2022

```
Monday Tuesday Wednesday Thursday Friday
```
```
01 Aug 2022
```
```
Turolla, Andrea
```
```
02 Aug 2022
```
```
Turolla, Andrea
```
```
03 Aug 2022
```
```
Turolla, Andrea
```
```
04 Aug 2022
```
```
Turolla, Andrea
```
```
05 Aug 2022
```
```
Montaldo, Ernesto
```
```
Turolla, Andrea
```
```
08 Aug 2022
```
```
Montaldo, Ernesto Abate,
Francesco
```
```
Peretti, Massimo
```
```
Turolla, Andrea
```
```
09 Aug 2022
```
```
Montaldo, Ernesto Abate,
Francesco
```
```
Peretti, Massimo
```
```
Turolla, Andrea
```
```
10 Aug 2022
```
```
Montaldo, Ernesto Abate,
Francesco
```
```
Peretti, Massimo
```
```
Turolla, Andrea
```
```
Dal Mas, Roberto
```
```
11 Aug 2022
```
```
Montaldo, Ernesto Abate,
Francesco
```
```
Peretti, Massimo
```
```
Turolla, Andrea
```
```
Dal Mas, Roberto
```
```
12 Aug 2022
```
```
Montaldo, Ernesto Abate,
Francesco
```
```
Peretti, Massimo
```
```
Turolla, Andrea
```
```
Dal Mas, Roberto
```
```
15 Aug 2022
```
```
Assumption (Holiday)
```
```
16 Aug 2022
```
```
We are closed
```
```
17 Aug 2022
```
```
We are closed
```
```
18 Aug 2022
```
```
We are closed
```
```
19 Aug 2022
```
```
We are closed
```
```
22 Aug 2022
```
```
Montaldo, Ernesto Abate,
Francesco
```
```
Dal Mas, Roberto
```
```
23 Aug 2022
```
```
Montaldo, Ernesto Abate,
Francesco
```
```
Dal Mas, Roberto
```
```
24 Aug 2022
```
```
Montaldo, Ernesto Abate,
Francesco
```
```
Dal Mas, Roberto
```
```
25 Aug 2022
```
```
Montaldo, Ernesto Abate,
Francesco
```
```
Dal Mas, Roberto
```
```
26 Aug 2022
```
```
Montaldo, Ernesto Abate,
Francesco
```
```
Dal Mas, Roberto
```
```
29 Aug 2022Abate, Francesco
```
```
Peretti, Massimo
```
```
Dal Mas, Roberto
```
```
30 Aug 2022Abate, Francesco
```
```
Peretti, Massimo
```
```
Dal Mas, Roberto
```
```
31 Aug 2022
```
```
Peretti, Massimo
```
```
Dal Mas, Roberto
```
September 2022

```
Monday Tuesday Wednesday Thursday Friday
```
```
01 Sep 2022
```
```
Peretti, Massimo
```
```
02 Sep 2022
```
```
Montaldo, Ernesto (afternoon only)
```
```
Peretti, Massimo
```
```
05 Sep 2022
```
```
Montaldo, Ernesto (afternoon only)
```
```
Turolla, Andrea
```
```
06 Sep 2022
```
```
Turolla, Andrea
```
```
07 Sep 2022
```
```
Turolla, Andrea
```
```
08 Sep 2022
```
```
Turolla, Andrea
```
```
09 Sep 2022
```
```
Turolla, Andrea
```
```
12 Sep 2022
```
```
Montaldo, Ernesto (afternoon only)
```
```
13 Sep 2022 14 Sep 2022 15 Sep 2022 16 Sep 2022
```
```
19 Sep 2022 20 Sep 2022 21 Sep 2022 22 Sep 2022 23 Sep 2022
26 Sep 2022 27 Sep 2022 28 Sep 2022 29 Sep 2022 30 Sep 2022
```
October 2022

```
Monday Tuesday Wednesday Thursday Friday
```
```
03 Oct 2022 04 Oct 2022 05 Oct 2022 06 Oct 2022 07 Oct 2022
10 Oct 2022 11 Oct 2022 12 Oct 2022 13 Oct 2022 14 Oct 2022
```
```
17 Oct 2022 18 Oct 2022 19 Oct 2022 20 Oct 2022 21 Oct 2022
24 Oct 2022 25 Oct 2022 26 Oct 2022 27 Oct 2022 28 Oct 2022
31 Oct 2022
```
```
We are closed
```

November 2022

```
Monday Tuesday Wednesday Thursday Friday
```
```
01 Nov 2022
```
```
All Saints' Day (Holiday)
```
```
02 Nov 2022 03 Nov 2022 04 Nov 2022
```
```
07 Nov 2022 08 Nov 2022 09 Nov 2022 10 Nov 2022 11 Nov 2022
14 Nov 2022 15 Nov 2022 16 Nov 2022 17 Nov 2022 18 Nov 2022
21 Nov 2022 22 Nov 2022 23 Nov 2022 24 Nov 2022 25 Nov 2022
```
```
28 Nov 2022 29 Nov 2022 30 Nov 2022
```
December 2022

```
Monday Tuesday Wednesday Thursday Friday
```
```
01 Dec 2022 02 Dec 2022
```
```
05 Dec 2022 06 Dec 2022 07 Dec 2022 08 Dec 2022
```
```
Immaculate Conception (Holiday)
```
```
09 Dec 2022
```
```
We are closed
```
```
12 Dec 2022 13 Dec 2022 14 Dec 2022 15 Dec 2022 16 Dec 2022
19 Dec 2022 20 Dec 2022 21 Dec 2022 22 Dec 2022 23 Dec 2022
```
```
We are closed
```
```
26 Dec 2022
```
```
Saint Stephen's Day (Holiday)
```
```
27 Dec 2022
```
```
We are closed
```
```
28 Dec 2022
```
```
We are closed
```
```
29 Dec 2022
```
```
We are closed
```
```
30 Dec 2022
```
```
We are closed
```

# Feature Team "Rajgad" (Pune -Fort)


# Rajgad Retrospective

```
Sprint Link
```
```
Sprint 22 https://siemens.conceptboard.com/board/0g79-drhy-76k0-tpan-m607
```

# Feature Team "Sahyadri" (Pune - Mountain)

## Welcome to Sahyadri's Wiki Page.

## Feel free to join the team in one of our Standups held daily at 10:30 AM (IST)

```
Sahyadri is a mountain range that covers an area of 160,000 km² in a
stretch of 1,600 km parallel to the western coast of the Indian peninsula,
traversing the states of Karnataka, Goa, Maharashtra, Gujarat, Kerala,
and Tamil Nadu. The range runs north to south along the western edge
of the Deccan Plateau and separates the plateau from a narrow coastal
plain called Konkan along the Arabian Sea. A total of 39 areas in the
Western Ghats, including national parks, wildlife sanctuaries and
reserve forests, were designated as world heritage sites in 2012.
```
```
Role Contacts Participates in ...
```
```
Scrum Master Hingse, Sharad Agile CoP
```
```
Team Member Syed, Faizan Mohammad CI CD COP
```
```
Team Member Rawal, Hitesh
```
```
Team Member Baishya, Manoj
```
```
Team Member Dattatray, Patil QA COP
```
```
Team Member Mhatre, Pratik
```
```
Team Member Sablaka, Saurabh
```

# Feature Team "Sinhagad" (Pune - Fort)

## Our Standup meeting: Each morning at xxx (Join Microsoft Teams Meeting)

```
Sinhagad is a hill fortress located at around 35 km southwest of the city
of Pune, India. The fort could have been built 2000 years ago. The
caves and the carvings in the Kaundinyeshwar temple stand as proofs
for the same.
```
```
Role Contacts Participates in ...
```
```
Team
Member
```
```
Thakker, Hiren
```
```
Team
Member
```
```
Suryavanshi, Vikas Unknow
n User (ashwini.patil)
```
### QA COP

```
Team
Member
```
```
Chaudhari, Yogita CI CD COP
```
```
Team
Member
```
```
Sharma, Pallavi
```
```
Team
Member
```
```
Shirsale, Nikhilesh
```
```
Team
Member
```
```
Syed, Faizan Mohammad
```
```
Team
Member
```
```
Shinde, Amit
```
```
Team
Member
```
```
Salokhe, Niraj
```
```
Scrum
Master
```
```
Malhotra, Parul Agile CoP
```

# Retrospective


# Sinhagad_Sprint 13

## Attendees:

Parul, Hiren, Nikhlesh, Faizan, Amit, Pallavi, Niraj

## Retro points:

https://siemens.conceptboard.com/board/ou44-5yic-xd1q-8zkf-z9cb

## Action Items for sprint:

```
Overview of MES Domain and its technical jargons with Heggi Malhotra, Parul
```
```
Team will observe and prioritize the meetings - Team
```
## Practices:


# Sinhagad_Sprint 14

## Attendees:

Hiren, Nikhlesh, Faizan, Amit, Pallavi, Parul

## Retro points:

https://siemens.conceptboard.com/board/ypy3-12et-1huc-7nu6-40np

## Action Items for sprint:

```
Overview of MES Domain and its technical jargons with Heggi Malhotra, Parul
```
```
Check with POs , to pull in analysis of medium or lo priority bugs to touch base the server side code. Team (18 Nov 2021 BR2)
```
```
More detail discussion on stories for better estimation
```
## Practices:


# Sinhagad_Sprint 15

## Attendees:

Hiren, Nikhlesh, Faizan, Amit, Pallavi, Parul, Niraj

## Retro points:

https://siemens.conceptboard.com/board/ts0k-56h5-g61s-nfsb-ph5o

## Action Items for sprint:

```
Overview of MES Domain and its technical jargons with Heggi Malhotra, Parul
```
```
More detail discussion on stories for better estimation
```
```
Any change impacting all the FTs, must be communicated via Teams channel. Global Retrospective.
```
```
Deploy and Verify : must not be closed without verification on 10.
```
## Practices:

```
Raise issue on time across FT if needed.
```

# Sinhagad_Sprint 17

## Attendees:

Hiren, Faizan, Amit, Pallavi, Parul, Niraj

## Retro points:

https://siemens.conceptboard.com/board/imqu-71qo-0tkr-782p-7esg

## Action Items for sprint:

```
Overview of MES Domain and its technical jargons with Heggi Malhotra, Parul
```
```
More detail discussion on stories for better estimation
```
```
Any change impacting all the FTs, must be communicated via Teams channel. Global Retrospective.
```
```
Deploy and Verify : must not be closed without verification on 10.
```
```
Verification must be done by person who has not done the development.(Monitor foe a sprint and verify the results)
```
```
BDD is not covering all scenarios. Global Retrospective.
```
```
Feature ownership must be taken by team members- All
```

# Sinhagad_Sprint 20

## Attendees:

Hiren, Sharad, Nikhlesh, Amit, Pallavi, Parul, Niraj

## Retro points:

https://siemens.conceptboard.com/board/nyfh-m2m8-fx1i-0ai9-ihz4

## Action Items for sprint:

```
Overview of MES Domain and its technical jargons with Heggi Malhotra, Parul
```
```
Deploy and Verify : must not be closed without verification on 10.
```
```
Feature ownership must be taken by team members- All
```
```
30 min sync up on every Friday (except for Review day) to share understanding on tasks Malhotra, Parul
```

# Sinhagad_Sprint 21

## Attendees:

Hiren, Amit, Pallavi, Parul, Niraj

## Retro points:

https://siemens.conceptboard.com/board/au3m-5rcr-bxtt-iopk-pex8

## Action Items for sprint:

```
Overview of MES Domain and its technical jargons with Heggi Malhotra, Parul
```
```
Multiple teams should not work on same task - Global retrospective. For Example: AM disable functionality, de-selecting list item on OT.
```
```
We should raise bugs , if we identify. Also add initial findings as the comment.
```

# Sinhagad_Sprint 22

## Attendees:

Hiren, Amit, Pallavi, Parul, Niraj, Nikhlesh

## Retro points:

https://siemens.conceptboard.com/board/hk0a-oaca-guqr-66of-uxny

## Action Items for sprint:

```
Overview of MES Domain and its technical jargons with Heggi Malhotra, Parul
```
```
Multiple teams should not work on same task - Global retrospective. For Example: AM disable functionality, de-selecting list item on OT.
```
```
Similar changes across the modules must be taken care in scope of same task. For Example: If refresh functionality is to be added in PO grid ,
same team can replicate the changes across other modules(WO or Operations etc) - Global Retrospective
```
```
BDD UI configuration in pipeline so that the developer is fixing the TCs in scope of the same task. - Global Retrospective
```

# Sinhagad_Sprint 23

## Attendees:

Hiren, Amit, Pallavi, Parul, Niraj, Nikhlesh

## Retro points:

https://siemens.conceptboard.com/board/okhh-87to-sok1-npf8-9ofx

## Action Items for sprint:

```
Overview of MES Domain and its technical jargons with Heggi Malhotra, Parul
```
```
Multiple teams should not work on same task - Global retrospective. For Example: AM disable functionality, de-selecting list item on OT.
```
```
Create checklist for feature refinement - Team
```
```
Session on B2MML , creation of payload - Team
```
```
Pallavi: TNT, Niraj: OM, Hiren : UM, Nikhlesh: MM, Amit: Factory , Parul : Landing page Integration Module
```

# Sinhagad_Sprint 24

## Attendees:

Hiren, Amit, Pallavi, Parul, Niraj, Nikhlesh

## Retro points:

https://siemens.conceptboard.com/board/hs2q-pns8-ditr-17a3-6nyi

## Action Items for sprint:

```
Overview of MES Domain and its technical jargons with Heggi Malhotra, Parul
```
```
Multiple teams should not work on same task - Global retrospective. For Example: AM disable functionality, de-selecting list item on OT.
```
```
Create checklist for feature refinement - Team
```
```
Check how to test inquiry service - SFE meeting Thakker, Hiren
```

# Sinhagad_Sprint 25

## Attendees:

Hiren, Amit, Pallavi, Parul, Niraj, Nikhlesh

## Retro points:

https://siemens.conceptboard.com/board/cknu-0ynu-7msi-e6g3-fath

## Action Items for sprint:

```
Create checklist for feature refinement - Team
```
```
Adding Integration TCs for Inquiry - Take Matteo - create a task after release sprint Malhotra, Parul
```
```
Check with UX and POs if we can update mockup with US ID , to ensure nothing is missed in mockup. Team
```
```
Discuss mockups on every Monday for any new change 15 min - Team
```
```
Create a meeting post BR for teams understanding 30 min - team
```

# Photogallery

Grosso, Stefania

### MANAGEMENT

```
Name: Lange, Tobias
```
```
Location:
```
```
Duty: Startup Leader
```
```
Hobbies:
```
```
Name: Unknown User (metin.kaplan)
```
```
Location: South California, USA
```
```
Duty: Head of R&D / LAL
```
```
Hobbies: Long-distance running, superbikes (in particular
Honda Fireblade), flying (student PPL: private pilot license).
```
```
Name: Abhishek Singh
```
```
Location:
```
```
Duty:
```
```
Hobbies:
```
```
Name: Trifoglio, Giuliano
```
```
Location: Genoa (Italy)
```
```
Duty: Agile Leader Genoa
```
```
Hobbies:
```
```
Name: Lodha, Vishal
```
```
Location: Pune (India)
```
```
Duty: Agile Leader Pune
```
```
Hobbies: Bike, Kites, Cricket, Movies
```
```
PM, PO, ARCH, SUPPORTING ROLES
```

Name: Schmidt, Jan

Location: Dusseldorf (Germany)

Duty: Product Manager

Hobbies: CrossFit, Soccer, Snowboarding/Surfing (depending on the
season )

```
Name: Jaehnert, Christian
```
```
Location: Nuremberg (Germany)
```
```
Duty: Product Manager
```
```
Hobbies:
```
```
Name: Anderson, Timothy
```
```
Location:
```
```
Duty:
```
```
Hobbies:
```
Name: Hegde , Ganesh (Heggi)

Location: Pune (India)

Duty: Product Owner

Hobbies: Indian Classical Music ( listening / playing Tabla) / Yoga
Practice / Treks

```
Name: Bardini, Matteo
```
```
Location: Genoa (Italy)
```
```
Duty: Product Owner
```
```
Hobbies: (too many!!) Anime (Japanese Cartoon) / RPG (Dungeons &
Dragons, and so on) / Miniature Painting / Tabletop games / Fantasy &
SciFI movie\tv series
```
```
Name: Nagamalli, Ramesh
```
```
Location:
```
```
Duty: Architect
```
```
Hobbies:
```

Name: De Pascale, Mauro

Location: Genoa (Italy)

Duty: Configuration Manager

Hobbies:

```
Name: Poggi, Emiliano
```
```
Location: Genoa (Italy)
```
```
Duty: Configuration Manager
```
```
Hobbies:
```
```
Name: Balzer, Peter
```
```
Location: Genoa (Italy)
```
```
Duty: Test Manager
```
```
Hobbies:
```
Name: Accorsi, Carlo

Location: (US)

Duty:

Hobbies:

```
Name: Aghazarian, Nick
```
```
Location: (US)
```
```
Duty: Principal Engineer
```
```
Hobbies: Whitewater Rafting
```
```
Name: Tabaei Befrouei, Mitra
```
```
Location: Vienna (Austria)
```
```
Duty:
```
```
Hobbies:
```

Name: Haas, Julia

Location:

Duty: UX Referent for Italy

Hobbies:

```
Name: Thombare, Sagar
```
```
Location:
```
```
Duty: UX Referent for India
```
```
Hobbies:
```
```
Name: Grosso, Stefania
```
```
Location: Genoa, Italy
```
```
Duty: Quality Officer
```
```
Hobbies:
```
```
Mauna Kea Team
```
Name: Arduini, Milvia Pierpaola

Location: Genoa (Italy)

Duty: Developer

Hobbies:

```
Name: Alvarez Villanueva, Beatriz (Bea)
```
```
Location: Genoa (Italy) since 1993; Spanish
```
```
Duty: Technical Writer since 2018
```
```
Hobbies:
```
```
Name: Di Rienzo, Andrea
```
```
Location: Genoa (Italy)
```
```
Duty: Developer
```
```
Hobbies:
```

Name: Occhiena, Marco

Location: Genoa (Italy)

Duty: Developer

Hobbies:

```
Name: Trubini, Piergiorgio
```
```
Location: Genoa (Italy)
```
```
Duty: Developer
```
```
Hobbies:
```
```
Name: Varoli, Guido
```
```
Location: Genoa (Italy)
```
```
Duty: Developer
```
```
Hobbies:
```
```
MONVISO TEAM
```
Name: Abate, Francesco

Location: Genoa (Italy)

Duty: Developer

Hobbies:

```
Name: Dal Mas, Roberto
```
```
Location: Genoa (Italy)
```
```
Duty: Developer
```
```
Hobbies:
```

Name: Turco, Mariangela

Location: Genoa (Italy)

Duty: Developer

Hobbies:

```
Name: Turolla, Andrea
```
```
Location: Genoa (Italy)
```
```
Duty: Developer
```
```
Hobbies:
```
```
PRATAPGAD TEAM
```
Name: Pushkar, Lele

Location: Pune (India)

Duty:

Hobbies:

```
Name: Moholkar, Sunetra
```
```
Location: Pune (India)
```
```
Duty:
```
```
Hobbies:
```
```
Name: Dattatray, Patil
```
```
Location: Pune (India)
```
```
Duty:
```
```
Hobbies: Sketching Portraits
```

Name: Patil, Shubhangi

Location: Pune (India)

Duty:

Hobbies:

```
Name: Sengupta, Ankita
```
```
Location: Pune (India)
```
```
Duty:
```
```
Hobbies:
```
```
Name: Sinha , Dheeraj
```
```
Location: Pune (India)
```
```
Duty:
```
```
Hobbies:
```
```
SHIVNERI TEAM
```
Name: Baishya, Manoj

Location: Pune (India)

Duty:

Hobbies:

```
Name: Das, Debarshi
```
```
Location: Pune (India)
```
```
Duty:
```
```
Hobbies:
```
```
Name: Naik, Hemant
```
```
Location: Pune (India)
```
```
Duty: Developer
```
```
Hobbies: Reading, Wrestling
```

Name: Kirve, Ajit

Location: Pune (India)

Duty: Scrum Master, Developer

Hobbies: Trekking, Cricket, Painting

```
Name: Patil, Mahesh
```
```
Location: Pune (India)
```
```
Duty: Developer
```
```
Hobbies:
```
```
Name: Joshi, Deepak
```
```
Location: Pune (India)
```
```
Duty: Developer
```
```
Hobbies: Music, Movies, Travelling
```
```
SINHAGAD TEAM (LION)
```
Name: Thakker, Hiren

Location: Pune

Duty: Developer

Hobbies: Trek, Travel, Food, Movie, Peace

```
Name: Unknown User (ashwini.patil)
```
```
Location: Pune
```
```
Duty: Developer
```
```
Hobbies:
```
```
Name: Sharma, Pallavi
```
```
Location: Pune
```
```
Duty: Developer
```
```
Hobbies: Hiking, Gardening, Cooking, DIY crafts, playing keyboard
(learning )
```

Name: Syed, Faizan Mohammad

Location: Pune

Duty: Developer

Hobbies:

```
Name: Shirsale, Nikhilesh
```
```
Location: Pune
```
```
Duty: Developer
```
```
Hobbies: Hiking, exploring new places,
```
```
Chess, Yoga and good food
```
```
Name: Suryavanshi, Vikas
```
```
Location: Pune
```
```
Duty: Developer
```
```
Hobbies:
```
Name: Salokhe, Niraj

Location: Pune

Duty: Developer

Hobbies: Hiking & Trekking, Backpacking, Gaming (Dota2), Football,
Cricket

```
Name: Malhotra, Parul
```
```
Location: Pune
```
```
Duty: SM/ Developer
```
```
Hobbies: Solo travelling, DIY craft, Cooking
```

# Distribution Lists

```
Organization
Feature Teams
Communities of Practice
Others
```
## Organization

## Feature Teams

```
Distribution List Platform Members Last Update
```
```
ModularMOM_MaunaKea
(modularmom_maunakea.sisw@siemens.com)
```
```
Owner: Nitsche, Andrea
```
```
Arduini, Milvia Pierpaola
Di Rienzo, Andrea (name is not visible)
Immordino, Giovanni
Nitsche, Andrea
Occhiena, Marco (name is not visible)
Trubini, Piergiorgio
Varoli, Guido
```
```
18 May 2022
```
```
ModularMOM__Pratapgad
(modularmom_pratapgad.sisw@siemens.com)
```
```
Owner: Sheth, Hemal
```
```
Lele, Pushkar
Moholkar, Sunetra
Patil, Dattatray
Patil, Shreneek
Patil, Shubhangi
Rawal, Hitesh
Sengupta, Ankita
Sheth, Hemal
Sinha, Dheeraj
```
```
10 Nov 2021
```

```
ModularMOM_Shivneri
(modularmom_shivneri.sisw@siemens.com)
```
```
Owner: Kirve, Ajit and Lodha, Vishal
```
```
Patil, Akshaykumar
Das, Debarshi
Joshi, Deepak
Kirve, Ajit
Naik, Hemant
Vashisht, Ram
Patil, Mahesh
```
```
18 Nov 2021
```
```
Distribution List MES Members Last Update
```
```
ModularMOM_Monviso
(modularmom_monviso.sisw@siemens.com)
```
```
Owner: Montaldo, Ernesto
```
```
Abate, Francesco
Dal Mas, Roberto
Fioritoni, Marco
Montaldo, Ernesto
Peretti, Massimo
Simone, Maurizio
Turco, Mariangela
Turolla, Andrea
```
```
06 Oct 2021
```
```
ModularMOM_Rajgad
(modularmom_rajgad.sisw@siemens.com)
```
```
Owner: Girnar, Yuvraj
```
```
Bajaj, Arjita
Chauhan, Saurabh
Deshmukh, Dipti
Girnar, Yuvraj
Jagtap, Harshila
Lawate, Manali
Pathak, Chandra
Rawat, Amit
Sablaka, Saurabh
```
```
05 Jan 2022
```
```
ModularMOM_Sinhagad
(modularmom_sinhagad.sisw@siemens.com)
```
```
Owner: Malhotra, Parul
```
```
Chaudhari, Yogita
Malhotra, Parul
Patil, Ashwini
Salokhe, Niraj
Sharma, Pallavi
Shinde, Amit
Shirsale, Nikhilesh
Suryavanshi, Vikas
Syed, Faizan Mohammad
Thakker, Hiren
```
```
06 Oct 2021
```
```
ModularMOM_Everest
(modularmom_everest.sisw@siemens.com)
```
```
Owner: Amir Napelyani and Unmesh Hasabnis
```
```
Giuffrida, Guido
Hasabnis, Unmesh
von Majowski, Martin
Lorenz, Juergen
Liao, Dong
T. V, Jishnu
Wang, Ning
Khandekar, Saurabh
Khadangale, Avinash
Khatale, Amit
Bhattad, Shailesh
Tapaswini, Snigdha (to be added)
Beldar, Shubham (to be added)
Musale, Varsha (to be added)
Deidda, Manuela (to be added)
Kesarwani, Arpita
Napelyani, Amir
Milligan, Patrick
```
```
11 Mar 2022
```
Communities of Practice

```
Distribution List Members Last Update
```

```
ModularMOM_ScrumMasters
(modularmom_scrummasters.sisw@siemens.com)
```
```
Owner: Lodha, Vishal
```
```
Girnar, Yuvraj
Kirve, Ajit
Malhotra, Parul
Montaldo, Ernesto
Nitsche, Andrea
Moholkar, Sunetra
Hingse, Sharad
```
```
11 Oct 2021
```
```
ModularMOM_CoP_QA
(modularmom_cop_qa.sisw@siemens.com)
```
```
Owner: Balzer, Peter
```
```
Arduini, Milvia
Balzer, Peter
Deshmukh, Dipti
Joshi, Deepak
Malhotra, Parul
Moholkar, Sunetra
Simone, Maurizio
```
```
07 Oct 2021
```
```
modularmom_devops
```
```
(modularmom_devops.sisw@siemens.com)
```
```
Owner: De Pascale, Mauro
```
```
De Pascale, Mauro
Ankita Sengupta
Poggi, Emiliano
Yogita Chaudhari
Kesarwani, Arpita
```
```
23 Nov 2021
```
Others

```
Distribution List Members Last Update
```
```
modularmom_rd
(modularmom_rd.sisw@siemens.com)
```
```
Owner: Napelyani, Amir
```
```
ModularMOM_Pratapgad
ModularMOM_Rajgad
ModularMOM_Shivneri
ModularMOM_Sinhagad
ModularMOM_Everest
ModularMOM_Monviso
ModularMOM_MaunaKea
modularmom_rd_leadership
modularmom_rd_archs
```
```
11 Mar 2022
```
```
modularmom_rd_leadership
(modularmom_rd_leadership.sisw@siemens.com)
```
```
Owner: Napelyani, Amir
```
```
Amir Napelyani
Balzer, Peter
Bardini, Matteo
De Pascale, Mauro
Hedge (Heggi), Ganesh
Kaplan, Metin
Lodha, Vishal
Nagamalli, Ramesh
Palkar, Yogesh
Pottigar, Durgaprasad Anand
Usykov, Igor
Roncagliolo, Isabella
```
```
11 Mar 2022
```
```
modularmom_prm
(modularmom_prm.sisw@siemens.com)
```
```
Owner: Pottigar, Durgaprasad Anand
```
```
Anderson, Timothy
Jaehnert, Christian
Schmidt, Jan
Singh (AGT), Abhishek Jeet
Capponi, Diego
Pottigar, Durgaprasad Anand
Engelke (AGT), Felix
```
```
06 Oct 2021
```
```
modularmom_leadership
```
```
(modularmom_leadership.sisw@siemens.com)
```
```
Owner: Napelyani, Amir; Kaplan, Metin
```
```
Dellucca, Alessandra
Kaplan, Metin
McGillivray, Neil
Singh, Abhishek Jeet
Pottigar, Durgaprasad Anand
```
```
06 Oct 2021
```

# ModMOM Status

## Summary

This page is to give a quick report on each ModMOM Modules and their current status in terms of provided macro functionalities.

## Material Modeling (Demo)

```
Functionality Status Notes
```
```
Import Material Definition via B2MML DONE 2112
```
```
View Material Definition DONE 2112
```
```
Allocate Material DONE 2112
```
## Factory Modeling (Demo)

```
Functionality Status Notes
```
```
View Factory Hierarchy DONE 2112
```
```
Create Factory Hierarchy Item DONE 2207
```
```
Edit Factory Hierarchy Item PARTIAL 2112 Only via Web API
```
```
Delete Factory Hierarchy Item PARTIAL 2112 Only via Web API
Front end delete planned for 2207
```
```
Expose UOM PLANNED
```
```
Edit UOM PLANNED
```
```
Allocate Site DONE 2112
```
```
Import Factory Data PARTIAL 2204 API Only
```
```
Move Factory Item PLANNED 2207/2210
```
## Order Management (Demo)

```
Functionality Status Notes
```
```
Import Production Request via B2MML DONE 2112
```
```
View Production Order (PO) DONE 2112
```
```
Change PO Quantity DONE 2112
```
```
Change PO Priority PARTIAL Enhanced UI Required
```
```
Create Operation Definition DONE 2112
```
```
Launch PO DONE 2112
```
```
Filter View PO DONE 2112
```
```
All Demos needs to be opened inside a SWQA Machine (VDI or VCloud).
```
```
Demos do not show the latest status but the latest status shown during an official product demo.
```
```
For credential info, please contact Bardini, Matteo or Hegde , Ganesh.
```

```
View PO Details DONE 2112
```
```
Import Production Order CSV DONE 2112
```
```
Automatic Launch DONE 2204
```
```
Sort PO in PO View DONE 2204
```
```
Launch PO Rework DONE 2204
```
```
Custom User Field Import IN PROGRESS 2207
```
```
Updater PO via B2MML PLANNED 2207
```
```
Cancel PO (and related WOs) PLANNED 2207
```
Track And Trace (Demo)

```
Functionality Status Notes
```
```
Create Work Order (WO) DONE 2112
```
```
Collect Throughput (TP) API DONE 2112
```
```
View Work Order Operation (WO OP) - View Operations DONE 2112
```
```
Enable full filters in View WO OP DONE 2204
```
```
View WO DONE 2112
```
```
View WO Details DONE 2112
```
```
Filter WO View DONE 2204
```
```
WO and WO OP History (View WO Details) DONE 2204
```
```
Collect TP Page (Operator Terminal) DONE 2112
```
```
Container Validation DONE 2204
```
```
Sort WO OP by Start Date + Time DONE 2204
```
```
Default Collection Quantity DONE 2204
```
```
Work Order Model Rework IN PROGRESS 2207
```
```
Collect Model Rework IN PROGRESS 2207
```
```
Show Barcode in Operator Terminal PLANNED 2207
```
```
Propagate change PO Quantity and Priority to TNT PLANNED 2207
```
```
Comment WO and WO OP PLANNED 2207
```
```
NC Integration PLANNED 2207
```
```
Milestone PLANNED 2207
```
Access Management (Demo - It was not delivered in 2112 Release)

```
Functionality Status Notes
```
```
Add\Delete UserGroup DONE 2204
```

```
Add\Delete Roles DONE 2204
```
```
Import Users DONE 2204
```
```
Auth-z DONE 2204
```
```
Auth-n DONE 2112
```
```
Pre-Defined Permission Groups DONE 2204
```
```
Associate Permission Groups DONE 2204
```
```
Associate User to UserGroup DONE 2204
```
```
Associate UserGroup to Role DONE 2204
```
Home Page (Demo)

```
Functionality Status Notes
```
```
Central Home Page DONE 2112.1
```
```
APIGW configuration DONE 2112.1
```
```
Drive Visualization with Permissions DONE 2204
```
SaaS Readiness

```
Functionality Status Notes
```
```
On-Board Manager DONE 2204
```
```
SAM Auth Integration DONE 2204
```
```
Keycloak provisioning DONE 2204
```
```
Notification Service Integration DONE 2204
```
```
AM Initialization DONE 2204
```
```
Operator Integration IN PROGRESS 2207
```
```
Postgres SQL Integration PARTIAL 2204
```

# Functional Specification


# Factory Module

### UI

```
Services
Data Model
Detailed Information on hierarchies
```

# Detailed Information on Hierarchies

Hierarchies include:
1 - Enterprise

- Collection of Sites
- Top Level Entity
- not mandatory

2 - Site [Mandatory]

- Physical, geographical, or logical grouping determined by the enterprise
- May contain areas, production lines, process cells, and production units.
- A geographical location and main production capability usually identifies a site. Sites generally have well-defined manufacturing capabilities.
- Example: "Johnson City Manufacturing Facility"


3 - Area

- An area is a physical, geographical, or logical grouping determined by the site.
- It may contain work centers such as process cells, production units, production lines, and storage zones.
- Areas generally have well-defined manufacturing capabilities and capacities.
- An area is made up of lower-level elements that perform the manufacturing functions. An area may have one or more of any of the lower-level elements
depending upon the manufacturing requirements.
- Example: "Building 2 Electronic Assembly"

4 - Work Center

- Work centers are elements of the equipment hierarchy under an area. The term work center may be used when the specific type of the equipment
element is not significant for the purpose of the discussion.
- For manufacturing operations management there are specific terms for work centers and work units that apply to batch production, continuous
production, discrete or repetitive production, and for storage and movement of materials and equipment.
- A work center is any equipment element subordinate to an area that may be defined by the user in an extension to the role based equipment hierarchy
model.
- The types of work centers may be extended when required for application specific role based equipment hierarchies where the defined types do not
apply. When a new type is added it shall maintain the same relationship within the hierarchy as the defined work center types (within an area and contains
work units). [Not MVP Scope]
- Work Center Types:
- Process Cell (Batch)
- Production Unit (Continuous)
- Production Line (Repetitive / Discrete)
- Storage Zone (Logistics)
- Also: Laboratory, Mobile Equipment Pool, Unused Equipment Store, Transportation Center (Transportation Modeling causes issues in Scheduling)
- Work Center Types might be extended by the customer
5 - Work Unit [discussion ongoing if mandatory]
- A work unit is any element of the equipment hierarchy under a work center. Work units are the lowest level in the factory model.

Properties / attributes tbd.

At a later point:

- "Sub-" and "super"-levels should be possible for Enterprise, Site, Area
- e.g.:

```
Level Type Name (Instance)
```
```
1 Enterprise Automobile AG
```
```
2 Site Berlin Plant
```
```
2 Sub-Site Body-in-White
```
```
3 Area Welding
```
```
3 Sub-Area Welding Front End
```
```
4 WorkCenter Welding Cell #1
```
```
5 WorkUnit Welding Machine #1
```
- This is a vertical scaling, not a horizontal one (WC / WU types)
- It's necessary to accompany the needs of enterprise customers with extraordinary complex hierarchies


# Factory Module APIs

Here follows all the services exposed by the Factory Module


# Create Hierarchy Item

## Summary

Each Object will have their own maintenance service (NamedObjectService).

## Errors

```
Code Message Note
```
```
100 Enterprise Already Defined
```
```
101 Invalid Parent Object Type Name
```

# Delete Hierarchy Item

## Summary

Each Object will have their own maintenance service (NamedObjectService) do deal with delete.


# Factory Module Documents


# Import Factory Document

## Example

```
<?xml version="1.0" encoding="utf-8"?>
<EquipmentInformation
xmlns="http://www.mesa.org/xml/B2MML-V0600"
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
xsi:schemaLocation="http://www.mesa.org/xml/B2MML-V0600 B2MML-V0600-ProductionSchedule.xsd"
xmlns:Extended="http://www.mesa.org/xml/B2MML-V0600-AllExtensions">
<Equipment>
<ID>Siemens-DISW</ID>
<Description>Siemens DISW</Description>
<Extended:EquipmentTypeValue>Enterprise</Extended:EquipmentTypeValue>
<Equipment>
<Equipment>
<ID>Nuremberg-1</ID>
<Description>Nuremberg Plant Number 1</Description>
<Extended:EquipmentTypeValue>Site</Extended:EquipmentTypeValue>
<Equipment>
<Equipment>
<ID>Assembly-1</ID>
<Description>Assembly Area 1</Description>
<Extended:EquipmentTypeValue>Area</Extended:EquipmentTypeValue>
<Equipment>
<Equipment>
<ID>Case-Assy-Bottom-1</ID>
<Description>Case Assembly Bottom</Description>
<Extended:EquipmentTypeValue>WorkCenter<
/Extended:EquipmentTypeValue>
</Equipment>
<Equipment>
<ID>Case-Assy-Top-1</ID>
<Description>Case Assembly Top</Description>
<Extended:EquipmentTypeValue>WorkCenter<
/Extended:EquipmentTypeValue>
</Equipment>
<Equipment>
<ID>Battery-Assy-1</ID>
<Description>Battery Assembly</Description>
<Extended:EquipmentTypeValue>WorkCenter<
/Extended:EquipmentTypeValue>
</Equipment>
</Equipment>
</Equipment>
<Equipment>
<ID>Packaging Area-1</ID>
<Description>Packaging Area 1</Description>
<Extended:EquipmentTypeValue>Area</Extended:EquipmentTypeValue>
<Equipment>
<Equipment>
<ID>Packaging-Battery-Pack-1</ID>
<Description>Packaging Battery Pack<
/Description>
<Extended:EquipmentTypeValue>WorkCenter<
/Extended:EquipmentTypeValue>
</Equipment>
</Equipment>
</Equipment>
</Equipment>
</Equipment>
<Equipment>
<ID>Nuremberg-2</ID>
<Description>Nuremberg Plant 2</Description>
```
```
Obsolete
```

```
<Extended:EquipmentTypeValue>Site</Extended:EquipmentTypeValue>
<Equipment>
<Equipment>
<ID>Assembly-2</ID>
<Description>Assembly Area 2</Description>
<Extended:EquipmentTypeValue>Area</Extended:EquipmentTypeValue>
<Equipment>
</Equipment>
</Equipment>
</Equipment>
</Equipment>
</Equipment>
</Equipment>
</EquipmentInformation>
```
Schema

Standard B2MML + Extensions

```
<xsd:complexType name="EquipmentType">
<xsd:sequence>
<xsd:element name="EquipmentTypeValue" type="xsd:string" minOccurs ="1" maxOccurs="1"/>
</xsd:sequence>
</xsd:complexType>
```
```
To be discussed: do we need an extension for the Type?
```

# Factory Module Entities

## Hierarchy Model

We have a division of entities depending on the ISA95 hierarchy.

Organization of COs

```
Enterprise: NamedObject
Site and Area: Subentity (Site is NamedSubentity<Enterprise>, Area is NamedSubentity<Site>)
Workcenter & WorkUnit: NamedObject, they are not subentities of area
```
## Properties

## Enterprise: NamedObject

```
Property
Name
```
```
Label Property
Type
```
```
Data
Type
```
```
Natural
Key
```
```
Mandatory Note
```
```
Name Name Name nvarchar
(64)
```
```
Yes Yes ID (UID) is a system field (which is not displayed here), we use Name as our key.
For the not subentity objects, this value may be meaningless.
```
```
ShortName Factory
Unit
```
```
Name nvarchar
(64)
```
```
Yes The short name, a kind of short name.
```
```
Description Description Description nvarchar
(1024)
```
```
FullPath Full Path Name nvarchar
(1024)
```
```
IsVirtuallyDe
leted
```
```
<not to be
shown>
```
```
Boolean boolean
```
```
Sites <not to be
shown>
```
```
List of Site Subentiti
es List
```
```
Mandatory for Subentity pattern.
```
## Site: NamedSubentity<Enterprise>

```
Property
Name
```
```
Label Property
Type
```
```
Data
Type
```
```
Natural
Key
```
```
Mandatory Note
```
```
Name Name Name nvarchar
(64)
```
```
Yes Yes ID (UID) is a system field (which is not displayed here), we use Name as our key.
For the not subentity objects, this value may be meaningless.
```
```
ShortName Factory
Unit
```
```
Name nvarchar
(64)
```
```
Yes The short name, a kind of short name.
```
```
Description Description Description nvarchar
(1024)
```
```
FullPath Full Path Name nvarchar
(1024)
```
```
IsVirtuallyDe
leted
```
```
<not to be
shown>
```
```
Boolean boolean Not handled in MVP 2112
```
```
Parent <not to be
shown>
```
```
Enterprise Reference Mandatory for Subentity pattern.
```
```
Areas <not to be
shown>
```
```
List of Area Subentitie
s List
```
```
Mandatory for Subentity pattern.
```
```
WorkCenters <not to be
shown>
```
```
List of
WorkCenter
```
```
List of
Reference
```
```
To handle special use case of having WC directly under Site
```
```
WorkUnits <not to be
shown>
```
```
List of
WorkUnit
```
```
List of
Reference
```
```
To handle special use case of having UC directly under Site
```
## Area: NamedSubentity<Site>

```
Property
Name
```
```
Label Property
Type
```
```
Data
Type
```
```
Natural
Key
```
```
Mandatory Note
```
```
Name Name Name nvarchar
(64)
```
```
Yes Yes ID (UID) is a system field (which is not displayed here), we use Name as our key.
For the not subentity objects, this value may be meaningless.
```

```
ShortName Factory
Unit
```
```
Name nvarchar
(64)
```
```
Yes The short name, a kind of short name.
```
```
Description Description Description nvarchar
(1024)
```
```
FullPath Full Path Name nvarchar
(1024)
```
```
IsVirtuallyDe
leted
```
```
<not to be
shown>
```
```
Boolean boolean Not handled in MVP 2112
```
```
Parent <not to be
shown>
```
```
Site Reference Mandatory for Subentity pattern.
```
```
WorkCenters <not to be
shown>
```
```
List of
WorkCenter
```
```
List of
Reference
```
```
To handle special use case of having WC directly under Site
```
```
WorkUnits <not to be
shown>
```
```
List of
WorkUnit
```
```
List of
Reference
```
```
To handle special use case of having UC directly under Site
```
WorkCenter: NamedObject

```
Property
Name
```
```
Label Property
Type
```
```
Data
Type
```
```
Natural
Key
```
```
Mandatory Note
```
```
Name Name Name nvarchar
(64)
```
```
Yes Yes ID (UID) is a system field (which is not displayed here), we use Name as our key.
For the not subentity objects, this value may be meaningless.
```
```
ShortName Factory
Unit
```
```
Name nvarchar
(64)
```
```
Yes
```
```
Description Description Description nvarchar
(1024)
```
```
FullPath Full Path Name nvarchar
(1024)
```
```
IsVirtuallyDe
leted
```
```
<not to be
shown>
```
```
Boolean boolean Not handled in MVP 2112
```
```
WorkCenter
Type
```
```
Type WorkCenter
Type
```
```
Enum
```
```
WorkCenters <not to be
shown>
```
```
List of
WorkCenter
```
```
List of
Reference
```
```
To handle special use case of having WC directly under Site
```
```
WorkUnits <not to be
shown>
```
```
List of
WorkUnit
```
```
List of
Reference
```
```
To handle special use case of having UC directly under Site
```
WorkUnit: NamedObject

```
Property
Name
```
```
Label Property
Type
```
```
Data
Type
```
```
Natural
Key
```
```
Mandatory Note
```
```
Name Name Name nvarchar
(64)
```
```
Yes Yes ID (UID) is a system field (which is not displayed here), we use Name as our key.
For the not subentity objects, this value may be meaningless.
```
```
ShortName Factory
Unit
```
```
Name nvarchar
(64)
```
```
Yes
```
```
Description Description Description nvarchar
(1024)
```
```
FullPath Full Path Name nvarchar
(1024)
```
```
IsVirtuallyDe
leted
```
```
<not to be
shown>
```
```
Boolean boolean Not handled in MVP 2112
```
```
WorkUnitTy
pe
```
```
Type WorkUnitTy
pe
```
```
Enum
```
```
WorkUnits <not to be
shown>
```
```
List of
WorkUnit
```
```
List of
Reference
```
```
To handle special use case of having UC directly under Site
```
EquipmentType

Here follows the ISA95 equipment model types.

Only WorkCenter and WorkUnit object will have a "Type" property. The other objects are mapped 1:1 with the standard types so no specific property is
needed.


```
Value Macro-Level Enum
```
```
Enterprise Enterprise
```
```
Site Site
```
```
Area Area
```
```
Process Cell Work Center WorkCenter
Type
```
```
Production Unit Work Center WorkCenter
Type
```
```
Production Line Work Center WorkCenter
Type
```
```
Storage Zone Work Center WorkCenter
Type
```
```
Laboratory Work Center WorkCenter
Type
```
```
Mobile Equipment Pool Work Center WorkCenter
Type
```
```
Unused Equipment Store Work Center WorkCenter
Type
```
```
Transportation Center Work Center WorkCenter
Type
```
```
Work Center Work Center WorkCenter
Type
```
```
Work Unit Work Unit WorkUnitTy
pe
```
```
Work Cell Work Unit WorkUnitTy
pe
```
```
Storage Unit Work Unit WorkUnitTy
pe
```
UoM Model

UOM: NamedObject

```
Property
Name
```
```
Property
Type
```
```
Data
Type
```
```
Natural
Key
```
```
Mandatory Note Example
```
```
Name Name nvarchar
(64)
```
```
Yes Yes Id is a system field, we use Name as our key. Name is
unique in the system
```
```
Kg
```
```
Description Description nvarchar
(1024)
```
```
Inherited from NamedObject Kilogram
```
```
UOMType String nvarchar
(64)
```
```
Mass / Length / Time /
Volume
```
```
System String nvarchar
(64)
```
```
Metric / British
```
```
Conversion
Formula
```

# Work-Unit and Equipment

## ISA-95 WorkUnit

According to ISA-95 all items defined in site hierarchy are called "equipment". This term, however, may be misleading depending on which industry you are
more familiar with. In particular, the issue may raise when we need to define what is the Work-Unit, the last level of the hierarchy. The ISA-95 standard
itself is a little vague on that concept, and depending on the industry it seems more oriented towards a physical equipment (process industry\batch
processing) or more toward a (virtual or physical) location where the work is done (discrete industry, with the concept of work-cell and work-station). It's
clear anyway, that a work-unit is "the most granular place" where work is performed.

## Key Decision

We decided that when we are speaking of WorkUnit but also all the other levels of the Hierarchy as Location. So, our Factory Hierarchy should describe
only locations.

Physical Equipment management as well as other physical resources (like tools) will be handled separately.

```
The customer is anyway free to detail his physical equipment/machinery in the Hierarchy if he really wants it using the Work-Unit. However, for
now, he will need to duplicate this information also in the Equipment\Resource management.
```

# Factory Module Messages


# Allocate Site

## Summary

Automatic message, start the Allocate Site Use Case

more details, here

## Input

```
Name Value Mandatory Note
```
```
SiteName string Yes
```

# FactoryItemNotification

## Summary

Notify the modelling operations performed on Factory entities

## Input

```
Name Type Note
```
```
FactoryItems List of FactoryItem
```
## FactoryItem

```
Name Data Type
```
```
Name string
```
```
Description string
```
```
ProductName string
```
```
ProductRevision string
```
```
WorkOrderQuantity decimal
```
```
UOM string
```
```
WorkOrderPriority string
```
```
LocationName string
```
```
ProductionOrderTypeName string
```
```
PlannedStartDate datetime
```
```
PlannedEndDate datetime
```
```
SubentityListField<CreateWorkOrderInputOperationType> CreateWorkOrderInputOperationType
```
## CreateWorkOrderInputOperationType

```
Name Data Type
```
```
Name string
```
```
Description string
```
```
Sequence int
```
```
OperationType string
```
```
WorkUnitName string
```
```
ProcessTime decimal
```
```
ProcessTimeUOM string
```
```
SetupTime decimal
```
```
work-in-progress
```

```
SetupTimeUOM string
```
```
CollectContainer int
```
```
StandardCollectionQuantity decimal
```
```
ProductionOrderTypeMaterials SubentityListField<CreateWorkOrderInputMaterialType>
```
```
StartDate StringField
```
```
OpdefAttachedDocuments SubentityListField<OpdefAttachement>
```
```
OpdefName string
```
```
OpdefDescription string
```
```
OpdefRevision string
```
CreateWorkOrderInputMaterialType

```
Name Data Type
```
```
Name Data Type
```
```
Name string
```
```
Description string
```
```
MaterialQuantityRequired decimal
```
```
MaterialDefinitionID decimal
```
```
MaterialUOM string
```
```
StoreLocation string
```
```
LineLocation string
```
OpdefAttachement

```
Name Data Type
```
```
AttachedDoc string
```

# Wireframes Factory Modeling

Preliminary:

1 - View the factory hierarchy

```
always sorted ascending by level sorting for MVP is not needed
search by Level, Type and Name
Buttons (Add, Edit) on the main table can be treated as shortcuts and are not of the highest priority, because the same functionality can be
achieved via the navigation bar on the left
```
2 - Display details

```
As soon as the user clicks on an element, details about this will be shown in a flyout (read only)
Selecting an item is shown by colouring its background
Alignment pending, if this behaviour is desired over clicking a dedicated button
```

3 - Edit a hierarchy item

```
Pressing the pen icon enables the user to make changes to an element
Needed to avoid frustration, when an element would have to be scrapped and recreated in case of typos, etc.
Ability to change the status
```

4 - Add a new element to the hierarchy

```
Adding a workcenter by clicking on the "+" Button.
Discussion ongoing if flyout of popup
```
Drop downs should be provided where appropriate, text input fields / search+select elsewhere. Mandatory field should be marked as such.

Position of the search needs to be evaluated:

```
Names of elements are not globally unique to identify the right one, context in the form of the position in the hierarchy must be provided
multiple options exist:
```

```
Full path
First discriminator
Truncated path
Full path was chosen for MVP for the simplicity of its implementation
The space needed to display this information might be quite large, making a wider search bar possibly preferable
```
SWF Implementation

We could try to leverage the following widget:

[http://swf/swf-showcase/#/showcase/Declarative%20Building%20Blocks/Tables%20and%20Trees/aw-splm-table%20tree](http://swf/swf-showcase/#/showcase/Declarative%20Building%20Blocks/Tables%20and%20Trees/aw-splm-table%20tree)

and having a button on the right panel for adding an item in hierarchy if an existing item in the tree is selected.

Why using this one instead of aw-tree? we want to leverage its out of the box capacity of splitting info in properties. With the aw-tree we would need a
custom inner content.


# Factory Module Permissions

```
Name Effect Note Home Card Relevant
```
```
View Factory Hierarchy Allows to visualize Hierarchy Items on the treeview
Factory Modeling Card
View Hierarchy Item Tile
```
```
Add Factory Item Allow to add a new Hierarchy Item
```
```
Delete Factory Item Allow to delete a new Hierarchy Item
```
```
Import Plant Allow to Import a Plant
```

# Material Management Module


# Material Management Module APIs


# Import Material Definitions

## Summary

Allows to import material in the system

## Input

```
Name Type Mandatory Note
```
```
Document String or Blob Yes
```
The document should follow this format: MaterialInformation B2MML

## Output

No output for now

## Errors

```
Code Message Note
```
```
1 Document Not properly Formatted
```
## Flow



# Material Management Module Documents


# MaterialInformation B2MML

## Example

```
<?xml version="1.0" encoding="utf-8"?>
<MaterialInformation xmlns:bml="http://www.mesa.org/xml/B2MML-V0600"
xmlns:Extended="http://www.mesa.org/xml/B2MML-V0600-AllExtensions">
<MaterialDefinition>
<ID>PRT_CBTM_BP</ID>
<Description>Case Bottom, Battey Pack.</Description>
<Extended:Name>Case Bottom, Battey Pack</Extended:Name>
<Extended:Revision>A</Extended:Revision>
<Extended:UoM>each</Extended:UoM>
<Location>Nuremberg-WH-1</Location>
</MaterialDefinition>
<MaterialDefinition>
<ID>PRT_NCCL_BP</ID>
<Description>Nickel Card Cell 1.2V x15.</Description>
<Extended:Name>Nickel Card Cell 1.2V x15</Extended:Name>
<Extended:Revision>A</Extended:Revision>
<Extended:UoM>each</Extended:UoM>
<Location>Nuremberg-WH-2</Location>
</MaterialDefinition>
<MaterialDefinition>
<ID>PRT_LTCH_BP</ID>
<Description>Latch, Battey x 2.</Description>
<Extended:Name>Latch, Battey x 2</Extended:Name>
<Extended:Revision>B</Extended:Revision>
<Extended:UoM>each</Extended:UoM>
<Location>Nuremberg-WH-3</Location>
</MaterialDefinition>
</MaterialInformation>
```
## Extensions

```
<xsd:group name = "MaterialDefinition">
<xsd:sequence>
<xsd:element name="Revision" type="xsd:string" minOccurs="1" maxOccurs = "1"/>
<xsd:element name="Name" type="xsd:string" minOccurs="0" maxOccurs = "1"/>
<xsd:element name="UoM" type="xsd:string" minOccurs="0" maxOccurs = "1"/>
</xsd:sequence>
</xsd:group>
```

# Material Management Module Entities

## Material Definition

```
Property
Name
```
```
Label Property
Type
```
```
Mandatory Natural
Key
```
```
Note Example
```
```
ObjectID <not to be
shown>
```
```
Internal ID of Object
```
```
MaterialName Material Name Yes Yes Unique identifier mapped to Object Name in
Platform
```
```
PRT_CBTM_BP
```
```
MaterialRevisi
on
```
```
Revision String Yes Yes Revision of Material mapped to Object Revision of
revisable Object
```
```
A
```
```
ShortDescripti
on
```
```
Description String No Short Description of the Part/Material Battery Case, Bottom
```
```
UOM UoM String No Basic UOM of material Each / Piece / Gram...
```
```
RevisionOfRec
ord
```
```
Active Boolean Current revision identifier in platform
```

# Material Management Module Messages


# MaterialCreated

## Summary

Message needed to communicate to third party about the material creation

## MaterialCreated

```
Name Value Note
```
```
MaterialInformation SubentityListField<MaterialMessageConsume Subentity List field of MaterialMessageConsume
```
## MaterialMessageConsume

```
Name Value Note
```
```
MaterialName String
```
```
Revision String
```
```
ProductionOrderHeaderName String
```
```
UOM string
```

# Material Management Module Use Cases


# MaterialValidation Message Flow

## Summary

Automatic message, start the Validate Material Use Case

## UML/Flow diagram creation for communication between PO header and Material


# Material Management Module Permissions

```
Name Effect Note Home Card Relevant
```
```
View Material Definitions Allows to visualize Material Definition on the main view
View Material Definition Tile
Material Management Card
```

# Order Management Module


# Order Management Module APIs


# AM


# GetProcessDefinitionListInquiryService

## Summary

This "GetProcessDefinitionListInquiryService" inquiry service allows user to retrieve Process Definition for given "group definition type". This service can
return filtered list of Process Definition Id, Name and Revision fields.

Endpoint

```
Name Verb Endpoint
```
```
GetProcessDefinitionListInquiryService POST api/ GetProcessDefinitionListInquiryService /Execute
```
Input

```
Name Type Mandatory Note
```
```
ProcessDefinitionType string Yes
```
```
Skip integer No
```
```
Limit integer No
```
Output

```
Response
```
### {

```
"dataLst": {
"__Value": ""
}
}
```
API usage example

```
Request Body
```
### {

```
"input": {
"ProcessDefinitionType": "coupon",
"Skip": 0,
"Limit": 10
},
"__RequestData": {
"DataLst": {}
}
}
```

Response Body

### {

"dataLst": {
"__Value": "[{\"ProcessDefinitionId\":\"0dHzLcRRK042UejFKd\",\"Name\":\"coupon-fd\",\"Revision\":\"
1\"}]"
},
"id": "0e0qRWTw_032a6DnHo"
}


# PrintGroupDefinitionsInquiryService

## Summary

This "PrintGroupDefinitionsInquiryService" inquiry service allows user to retrieve all the Print Group Definitions for selected Production Order.

Endpoint

```
Name Verb Endpoint
```
```
PrintGroupDefinitionsInquiryService POST api/PrintGroupDefinitionsInquiryService/Execute
```
Input

```
Name Type Mandatory Note
```
```
ProductionOrderName string Yes
```
```
Skip integer No
```
```
Limit integer No
```
Output

Response

### {

```
"groupDefinitionData":
{
"__Value":"{
"printGroupDefinitions":
[
{ "printFileName" :
{
"__Value" : ""
},
"partsPerGroup" :
{
"__Value" : ""
},
"couponsPerGroup" :
{
"__Value" : ""
},
"numberOfGroup" :
{
"__Value" : ""
},
"groupTemplate" :
{
"__Value" : ""
}
}
],
"partsCovered" : ""
}"
}
}
```
API usage example

Request Body


### {

```
"Input":
{
"name": "sample_name"
},
"__RequestData":
{
"__Options":
{
"__Label": true,
"__Value": true,
"__Type": true
},
"Name":{}
}
}
```
Response Body

### {

```
"groupDefinitionData":
{
"__Value":"{
\"printGroupDefinitions\":
[
{ \"printFileName\" :
{
\"__Value\" : \"printFileName1.jz\"
},
\"partsPerGroup\" :
{
\"__Value\" : \"3\"
},
\"couponsPerGroup\" :
{
\"__Value\" : \"3\"
},
\"numberOfGroup\" :
{
\"__Value\" : \"2\"
},
\"groupTemplate\" :
{
\"__Value\" : \"fd-template\"
}
}
],
\"partsCovered\" : \"6\"
}"
},
"id":"0e6Py6BJe042Zjs7o2"
}
```

# Production Order Header AM Inquiry Service

## Summary

It allows Supervisor to filter and sort the production order view data based on production order properties.

Filterable ColumnFilter and Sort Production Order views:

```
ProductionOrderName
ProductName
ProductVersion
Status
LocationName
Customer
PurchaseOrder
ProjectManager
ContractDeliveryDate
```
## Endpoint

```
Name Verb Endpoint
```
```
ProductionOrderInquiryService POST api/ProductionOrderInquiryService/Execute
```
## Input

```
Name Type Mandatory Note
```
```
ProductionOrderName string no
```
```
ProductName string no
```
```
ProductVersion int no
```
```
LocationName string no
```
```
Status string no
```
```
QueryOption object yes
```
```
ResponsibleEngineer string no
```
```
Customer string no
```
```
PurchaseOrder string no
```
```
ProjectManager string no
```
```
ContractDeliveryDate DateTime no
```
## Output

"DataLst":{}

## API usage example

Request body example

For default load


### {

"Input": {

"QueryOption":
{

"skip": 0 ,
"limit": 10

}

### },

"__RequestData": {
"DataLst": {}
}
}

For filter and sort on any specific column

{
"Input": {
"ProductName": "Mat1",
"Status":"New",
"QueryOption":
{
"SortingLst": [
{
"Value":

{

"SortProperty": "plannedEnd",
"SortDirection": "asc"
},
"ListItemAction": "Add"
}

],
"skip": 0 ,
"limit": 10

}

### },

"__RequestData": {
"DataLst": {}
}
}


# Associate Process Definition to Production Order

## Summary

```
Assign/attach a Process Definition to Production Order.
```
## API

```
The API is exposed through endpoint:
```
```
Name Verb Endpoint
```
```
AssociateProcessDefToPOService POST /api/AssociateProcessDefToPOService/Execute
```
## Input

```
Name Value Mandatory Note
```
```
ProductionOrderName string Yes Name of ProductionOrder to which a Process is to be attached.
```
```
ProcessDefinitionName string Yes Name of Process which is to be attached to PO.
```
## Output

```
No output required.
```
## missing mandatory parametersErrors

```
Code Message Note
```
```
0000001 Process definition cannot be assigned to production order ‘PO_Name’.
```
```
Production order needs to be in Engineering state.
```
```
0000002 Process definition cannot be assigned to production order ‘PO_Name’.
```
```
Process definition is already assigned.
```
```
0000003 Process definition cannot be assigned to production order ‘PO_Name’.
```
```
Production order was not found.
```
```
0000004 Process definition cannot be assigned to production order ‘PO_Name’.
```
```
Process definition ‘ProcessDef_Name’ was not found.
```
```
For other error cases such as missing mandatory parameters, platform behavior is leveraged.
```
## API Usage Example

```
Request Body
```
### {

```
"Input":{
"ProductionOrderName":"PO_Dummy_1",
"ProcessDefinitionName" :"ProcessDef_Dummy_1"
}
}
```

# Change PO Priority

## Summary

Change from a given Production Order the priority.

Valid state:

```
New
Launched
```
Invalid states:

```
Pending
Invalid
Launching
Completed
```
## Endpoint

The API is exposed through the endpoint:

```
api/ChangePOPriorityService/Execute
```
## Input

```
Name Type Mandatory Note
```
```
ProductionOr
der
```
```
INamedObjectRefField<ProductionOrderHe
ader>
```
```
yes Named reference to the ProductionOrderHeader.
In case the referenced ProductionOrderHeader does not exist, the platform provides
proper error code.
```
```
NewPriority int yes
```
## Output

No output required

## Errors

```
Code Message Note
```
```
400 Invalid Priority value (NewPriority). Priority cannot be negative.
```
```
400 Priority cannot be updated for production order (ProductionOrderName) as it is in
(ProductionOrderStatus) state.
```
```
Priority can be updated for POs having status as
New and Launched
```
```
400 Production Order (ProductionOrderName) does not exist
```
```
400 Production order {ProductionOrderName} can not be updated as priority is already set
to {NewPriority}.
```
For other error cases such as invalid ProductionOrderHeader reference or missing manadatiry parameters, platform behavior is leveraged.

## API usage example


Request body example

### {

"Input": {
"NewPriority": 2,
"ProductionOrder": {
"__Name": "PO-23009-OrderPriorityDemo-1",
"__TypeOfRefDto": "NamedRefDTO"
}
}
}


# Change PO Quantity

## Summary

Change from a given Production Order the quantiy.

Valid state:

```
New
```
## Input

```
Name Type Mandatory Note
```
```
ProductionOrderName string yes to be changed INamedObjectRefField<ProductionOrderHeader>
```
```
New Quantity int yes
```
```
Old Quantity int yes
```
## Output

No output required

## Errors

```
Code Message Example Text Note
```
```
1 Production Order ‘(ProductionOrderName)’ was not found. Production Order ‘PO_125’ was not found.
```
```
2 Production Order ‘(ProductionOrderName)’ has the state ‘(State)’. This state is
not supported.
```
```
Production Order ‘myOrder’ has the state ‘Paused’. This state is
not supported.
```
```
3 The Quantity cannot be set because it was changed in the meantime. Please
refresh the screen.
```
-

```
4 The new Quantity must be less than the old value '(OldQuantity)', but greater
than zero.
```
```
The new Quantity must be less than the old value '9', but greater
than zero.
```
## Example of Flow

```
PO - Quantiy 100
Op1
Mat1 - Quantity 100
Op2
Mat2 - Quantity 200
Mat3 - Quantity 300
```
```
Change Quanity 100 --> 80
```
```
Compute the Material Ratios
```
```
mat1_ratio = Mat1_qnty / po_qntyt = 100/100 = 1
new_mat1_qnty = old_mat1_qnty - PO_qnty_dif * ratio = 100 - 20 = 80
```
```
mat1_ratio = Mat1_qnty / po_qntyt = 200/100 = 2
new_mat1_qnty = old_mat1_qnty - PO_qnty_dif * ratio = 200 - 2*20 = 160
```
```
mat1_ratio = Mat1_qnty / po_qntyt = 300/100 = 3
new_mat1_qnty = old_mat1_qnty - PO_qnty_dif * ratio = 300 - 3*20 = 240
```

# Filter and Sort Production Order view

## Summary

It allows Supervisor to filter and sort the production order view data based on production order properties.

Filterable Columns:

```
ProductionOrderName
ProductName
ProductVersion
Status
LocationName
```
## Endpoint

```
Name Verb Endpoint
```
```
ProductionOrderInquiryService POST api/ProductionOrderInquiryService/Execute
```
## Input

```
Name Type Mandatory Note
```
```
ProductionOrderName string no
```
```
ProductName string no
```
```
ProductVersion int no
```
```
LocationName string no
```
```
Status string no
```
```
QueryOption object yes
```
## Output

"DataLst":{}

## API usage example

Request body example

For default load

{
"Input": {

"QueryOption":
{

"skip": 0 ,
"limit": 10

}

### },

"__RequestData": {
"DataLst": {}
}
}


For filter and sort on any specific column

{
"Input": {
"ProductName": "Mat1",
"Status":"New",
"QueryOption":
{
"SortingLst": [
{
"Value":

{

"SortProperty": "plannedEnd",
"SortDirection": "asc"
},
"ListItemAction": "Add"
}

],
"skip": 0 ,
"limit": 10

}

### },

"__RequestData": {
"DataLst": {}
}
}

Note

Values are provided by Provide Values for Filter In Production Order View


# Import Operation Definitions from CSV

## Summary

It allows to import new Operation Definitions in the system

## API

```
Name Verb Endpoint
```
```
ProcessCSVProductionScheduleObject POST /api/ProcessCSVProductionScheduleObject/Execute
```
## Input

```
Name Value Mandatory Note
```
```
Document string (or blob) Yes
```
```
Separator string Yes
```
## Output

No output for the moment.

## API usage example

```
Postman input
```
### {

```
"input": {
"Separator": ";",
"Document": "Name;Revision;AttachedDocuments\r\nOP_Def_1;A;\"link1;link2\"\r\nOP_Def_2;;\"link4;link5\"
\r\nOP_Def_3;;\r\n"
}
}
```

# Import Production Order

## Summary

It allows to import new Production Orders in the system, or update them if they are in the "Invalid" state. It also triggers the creation of the corresponding
Work Order(s) if in the Production Order(s) the AutomaticLaunch is not set to False.

Also, if the flag for ImportedInEngineering is set to true, then Production Order will remain in Engineering state and if any associated operations &
materials are defined then they will get skipped along with AutomaticLaunch property of flag.

Each production order that has been validated by Material Manager ( so in state New ) and with AutomaticLauch set to true, is launched by sending a
sinlge CreateWorkOrders message to TNT which handle the WO creation in a transactionl way.

See Modular MOM 2.x UseCase ImportProductionOrder for Use Case and Work Flow.

## Api

```
Name Verb Endpoint
```
```
ProcessB2MMLProductionScheduleObject POST /api/ProcessB2MMLProductionScheduleObject/Execute
```
## Input

```
Name Value Mandatory Note
```
```
b2MMLDocument string (or blob) Yes
```
The document will follow this structure: ProcessProductionSchedule B2MML

## Output

No output for the moment.

## Errors

```
Code Message Note
```
```
1 Document Not properly Formatted
```
```
2 The required order (OrderName) was already imported and it's not in the new state - Current State (State)
```
```
3 Object name(Name) should not contain '#'.
```
## API usage example


```
ProcessB2MMLProductionScheduleObject
```
### {

```
"Input": {
"b2MMLDocument": "<?xml version='1.0' encoding='utf-8'?> <ProcessProductionSchedule xmlns='http://www.
mesa.org/xml/B2MML-V0600' xmlns:xsi='http://www.w3.org/2001/XMLSchema-instance' xsi:schemaLocation='http://www.
mesa.org/xml/B2MML-V0600' xmlns:Extended='http://www.mesa.org/xml/B2MML-V0600-AllExtensions'> <DataArea>
<Process></Process> <ProductionSchedule> <ProductionRequest> <ID>PO_testID</ID>
<StartTime></StartTime> <EndTime></EndTime> <Priority>1</Priority> <Extended:Type>A<
/Extended:Type> <Extended:LocationName>Location2</Extended:LocationName> <Extended:
AutomaticLaunch>true</Extended:AutomaticLaunch> <Extended:ImportedInEngineering>false</Extended:
ImportedInEngineering> <Extended:Material> <Extended:ID>Samsung</Extended:ID>
<Extended:Revision>rev1.0</Extended:Revision> </Extended:Material> <Extended:
Quantity> <Extended:QuantityString>80</Extended:QuantityString> <Extended:
UnitOfMeasure>each</Extended:UnitOfMeasure> </Extended:Quantity> <SegmentRequirement>
<ID>0010</ID> <Description>SegmentRequirement-0010-Description</Description> <Extended:
Sequence>10</Extended:Sequence> <Extended:Type>OT1</Extended:Type> <Extended:
WorkCenterName>WC1</Extended:WorkCenterName> <Extended:StartDate>2022-01-09T07:00:20Z</Extended:
StartDate> <Extended:ProcessTime> <Extended:ProcessTimeString>4.0</Extended:
ProcessTimeString> <Extended:UnitOfMeasure>Each</Extended:UnitOfMeasure> </Extended:
ProcessTime> <Extended:SetupTime> <Extended:SetupTimeString>10.0</Extended:
SetupTimeString> <Extended:UnitOfMeasure>Minutes</Extended:UnitOfMeasure> </Extended:
SetupTime> <Extended:CollectContainer>true</Extended:CollectContainer> <Extended:
OperationDefinition> <Extended:Name>OPD1</Extended:Name> <Extended:Revision>A</Extended:
Revision> <Extended:Description>Desc</Extended:Description> <Extended:
AttachedDocument>link1</Extended:AttachedDocument> <Extended:AttachedDocument>link2</Extended:
AttachedDocument> </Extended:OperationDefinition> <MaterialRequirement>
<Quantity> <QuantityString>600</QuantityString> <UnitOfMeasure>Unit<
/UnitOfMeasure> </Quantity> <Extended:MaterialDefName>Mat-1</Extended:
MaterialDefName> <Extended:MaterialDefRevision>Mat-A</Extended:MaterialDefRevision>
<Extended:LineLocation>Line Location 1</Extended:LineLocation> <Extended:StoreLocation>Warehouse
Location 1</Extended:StoreLocation> <Extended:Description>This material use for creating tyre<
/Extended:Description> </MaterialRequirement> </SegmentRequirement> <
/ProductionRequest> </ProductionSchedule> </DataArea> </ProcessProductionSchedule>"
}
}
```
Flow

Mapping


The main mapping in terms of B2MML entities to Modular MOM entities defined in OM\MM components is as follows:

```
ProdutionSchedule (none)
ProductionRequest ProductionOrderHeader
SegmentRequirement ProductionOrderOperation\OperationDefinition
SegmentRequirement:MaterialRequirement ProductionOrderMaterial
```
```
ProductionRequest ProductionOrderHeader
```
```
Property Name Mandatory Property Name Mandatory
```
```
ID Name
```
```
Description Description
```
```
StartTime PlannedStart
```
```
EndTime PlannedEnd
```
```
Priority ProductionOrderPriority
```
```
Extended:Type ProductionOrderTypeName
```
```
Extended:LocationName LocationName
```
```
Extended:AutomaticLaunch AutomaticLaunch
```
```
Extended:ImportedInEngineering ImportedInEngineering
```
```
Extended:Material.ID ProductName
```
```
Extended:Material.Revision ProductRevision
```
```
Extended:Quantity.QuantityString ProductionOrderQuantity
```
```
Extended:Quantity.UnitOfMeasure UOM
```
```
Sequence of SegmentRequirement List of ProductionOrderOperation
```
```
SegmentRequirement ProductionOrderOperation
```
```
Property Name Mandatory Property Name Mandatory
```
```
ID Name
```
```
Description Description
```
```
Extended:Type OperationType
```
```
Extended:WorkCenterName WorkCenterName
```
```
Extended:Sequence Sequence
```
```
Extended:ProcessTime.ProcessTimeString ProcessTime
```
```
Extended:ProcessTime.UnitOfMeasure ProcessTimeUOM
```
```
Extended:StartDate StartDate
```
```
Extended:SetupTime.SetupTimeString SetupTime
```
```
Extended:SetupTime.UnitOfMeasure SetupTimeUOM
```
```
Extended:CollectContainer CollectContainer
```
```
Extended:OperationDefintion.Name
Extended:OperationDefintion.Description
Extended:OperationDefintion.Revision
Extended:OperationDefintion.AttachedDocument
```
```
OperationDefinition
(ref see next table)
```
```
Sequence of MaterialRequirement List of ProductionOrderMaterial
```
```
SegmentRequirement OperationDefinition
```
```
Property Name Mandatory Property Name Mandatory
```
```
Extended:OperationDefintion.Name Name
```
```
Extended:OperationDefintion.Description Description
```
```
Extended:OperationDefintion.Revision Revision
```
```
Extended:OperationDefintion.AttachedDocument AttachedDocuments
```
```
MaterialRequirement ProductionOrderMaterial
```

```
Property Name Mandatory Property Name Mandatory
```
ID Name

Description Description

Quantity.QuantityString MaterialQuantity

Quantity.UnitOfMeasure MaterialUOM

Extended:Description Description

Extended:LineLocation LineLocation

Extended:StoreLocation StoreLocation

Extended:MaterialDefName

Extended:MaterialDefRevision


# Import Production Order from CSV

## Summary

It allows to import new Production Orders in the system, or update them if they are in the "Invalid" state. It also triggers the creation of the corresponding
Work Order(s) if in the Production Order(s) the AutomaticLaunch is not set to False.

Also, if the flag for ImportedInEngineering is set to true, then Production Order will remain in Engineering state and if any associated operations &
materials are defined then they will get skipped along with AutomaticLaunch property of flag.

See Modular MOM 2.x UseCase ImportProductionOrder from CSV File for Use Case and Work Flow.

## API

```
Name Verb Endpoint
```
```
ProcessCSVProductionScheduleObject POST /api/ProcessCSVProductionScheduleObject/Execute
```
## Input

```
Name Value Mandatory Note
```
```
CSVDocument string (or blob) Yes
```
## Output

No output for the moment.

## Errors

```
Code Message Note
```
```
1 Document Not properly Formatted
```
```
2 The required order (OrderName) was already imported and it's not in the new state - Current State (State)
```
## API usage example

```
Postman input
```
### {

```
"input": {
```
```
"CSVDocument": "ID;ImportedInEngineering;StartTime;EndTime;Priority;Type;Location;AutomaticLaunch;
ProductName;Description;ProductVersion;Quantity;QuantityUOM;OperationName;OperationDescription;
OperationSequence;OperationType;WorkCenter;ProcessTime;ProcessTimeUOM;SetupTime;SetupTimeUOM;CollectContainer;
StartDate;OperationDefinitionName;OperationDefinitionVersion;OperationDefinitionAttachedDocuments;
MaterialQuantity;MaterialQuantityUOM;MaterialDefName;MaterialDefVersion;MaterialDesctiption;
MaterialLineLocation;MaterialStorageLocation\r\nProduction-Order-Name;false;08.11.2021 11:09;11.11.2021 11:09;
1;;location_name;false;Product_Name;product_desc;1;50;pcs;op_name;operation-description;10;TypeA;WC-1;10;s;2;s;;
11.11.2021 11:09;OP_D_010;1;\"link1;link2;link3\";100;pcs;Mat_D_1;1;;L_1;S_1\r\n"
}
```
### }

## Note

Please make sure the csv has all the fields mentioned in above example. If the fields are not matching in mentioned sequence above, the import might fail
or given incorrect results.



# Launch Production Order (V2)

## Summary

Create a Work Order from each Production Order Name in the in the ProductionOrderNames list, in New state. This method will consumer the BOP, BOR
and BOM associated with the PO in order to create all the required items.

## Behavior

The ProductionOrders are put in the state "Launching" and a message is invoked to TnT, named CreateWorkOrders

Once the Work Orders are created, TnT will sent back a message, WorkOrderCreated that will be fetched by Order Management that will change the
status of the ProductionOrder to "Launched"

Together with the Status change, we save in the database (internal property never to be displayer) the LastLaunchingTime.

## Api

```
Name Verb Endpoint
```
```
Launch POST /api/LaunchProductionOrder/Execute
```
## Input

```
Name Type Mandatory Note
```
```
ProductionOrderNames List<string> yes List of ProductionOrder's name
```
## Output

No output required

## Errors

```
Code Message Note
```
```
1 The chosen Order (OrderName) is the wrong state - (State)
```
```
2 An error occurred while retrieving the information related to the Production Order.
Unable to read the (BOM|BOP|BOR)
```
```
depending on the BL the system may return error only
for the first not found bill.
```
```
3 An error occurred while creating the Work Order, Work Order already exists -
(WorkOrder Name)
```
```
4 A not empty list of production order name ust be
```
## Messages

```
Type Destination Note
```
```
CreateWorkOrders TNT
```
```
This API refers to ModMOM V2
```
```
We can Launch the PO again if the status is "Launching" and the LastLaunchingTime is > 10 min.
This mechanism is quick and dirty back up solution to compensate issue during the WO creation on TnT module (failure in commit, failure in
sending back the message) but also failure in handling the WOCreated message in OM.
```

API usage example

<om-module-url>/api/Launch/Execute

### {

```
"Input": {
"ProductionOrderNames": [
{
"value": "ProductionOrderName_1",
"ListItemAction": "Add"
},
{
"value": "ProductionOrderName_2",
"ListItemAction": "Add"
},
{
"value": "ProductionOrderName_3",
"ListItemAction": "Add"
},
{
"value": "ProductionOrderName_4",
"ListItemAction": "Add"
}
]
}
}
```

# ProcessDefinitionInquiryService

## Summary

It returns the following ProcessDefinition information: Name, RevOfRcd, Description, Revision, ProcessDefinitionType, InstanceId, createdBy, createdOn,
lastChangedBy, lastChangedOn". Data can be filtered by specifying a filter value in the corresponding parameter (wild cards '%' and '_' are supported).

The default sorting applied to the retrieved data is by ProcessName(ascending).

## Endpoint

```
Name Verb Endpoint
```
```
ProcessDefinitionInquiryService POST api/ProcessDefinitionInquiryService/Execute
```
## Input

```
Name Type Mandatory Note
```
```
ProcessName string no
```
```
QueryOptions object yes
```
QueryOptions

```
Name Type Mandatory Note
```
```
SortingLst list of SortingOptions no
```
```
Skip int no
```
```
Limit int no
```
SortingOptions

```
Name Type Mandatory Note
```
```
SortProperty string no
```
```
SortDirection string no "ASC"|"DESC"
```
## Output

```
Name Type Mandatory Note
```
```
name string no
```
```
revOfRcd string no
```
```
description string no
```
```
revision string no
```
```
processDefinitionType string no
```
```
instanceId string no
```
```
createdBy string no
```
```
createdOn datetime no
```
```
lastChangedBy string no
```
```
lastChangedOn datetime no
```
## API usage example

<om-module-url>/api/ProcessDefinitionInquiryService/Execute


Request body

### {

"Input": {
"ProcessName": "",
"QueryOptions": {
"SortingLst": [
{
"Value": {
"SortProperty": "ProcessName",
"SortDirection": "asc"
},
"ListItemAction": "Add"
}
],
"skip": 0,
"limit": "200000000"
}
},
"__RequestData": {
"ProcessLst": {
"name": {},
"revOfRcd": {},
"description": {},
"revision": {},
"processDefinitionType": {},
"base": {},
"createdBy": {},
"createdOn": {},
"lastChangedBy": {},
"lastChangedOn": {}
}
}
}


Responde body example

### {

"processLst": {
"__Value": "[{\"name\":\"ExampleProcess-001\",\"revOfRcd\":null,\"description\":null,\"revision\":\"
1652374374\",\"processDefinitionType\":null,\"instanceId\":\"0cwKnEJR50M2UejFKd\",\"createdBy\":\"Test Test\",\"
createdOn\":\"2022-05-12T16:52:54.5366252+00:00\",\"lastChangedBy\":\"Test Test\",\"lastChangedOn\":\"2022-05-
12T16:52:54.5366252+00:00\"},{\"name\":\"ExampleProcess-003\",\"revOfRcd\":null,\"description\":null,\"
revision\":\"1652789073\",\"processDefinitionType\":null,\"instanceId\":\"0dK2k8HwA032UejFKd\",\"createdBy\":\"
Test Test\",\"createdOn\":\"2022-05-17T12:04:33.6702595+00:00\",\"lastChangedBy\":\"Test Test\",\"
lastChangedOn\":\"2022-05-17T12:04:33.6702595+00:00\"},{\"name\":\"ExampleProcess-004\",\"revOfRcd\":null,\"
description\":null,\"revision\":\"1652803242\",\"processDefinitionType\":\"Production\",\"instanceId\":\"
0dKtnN2pI062UejFKd\",\"createdBy\":\"Test Test\",\"createdOn\":\"2022-05-17T16:00:42.5823387+00:00\",\"
lastChangedBy\":\"Test Test\",\"lastChangedOn\":\"2022-05-17T16:00:42.5823387+00:00\"},{\"name\":\"
ExampleProcess-005\",\"revOfRcd\":null,\"description\":null,\"revision\":\"1652896109\",\"
processDefinitionType\":\"Production\",\"instanceId\":\"0dQR2wuKK0X2UejFKd\",\"createdBy\":\"Test Test\",\"
createdOn\":\"2022-05-18T17:48:29.549074+00:00\",\"lastChangedBy\":\"Test Test\",\"lastChangedOn\":\"2022-05-
18T17:48:29.549074+00:00\"},{\"name\":\"ImportProcessCSV-001\",\"revOfRcd\":null,\"description\":null,\"
revision\":\"1652967798\",\"processDefinitionType\":\"Production\",\"instanceId\":\"0dUhX0~e6042UejFKd\",\"
createdBy\":\"Test Test\",\"createdOn\":\"2022-05-19T13:43:18.0651524+00:00\",\"lastChangedBy\":\"Test Test\",\"
lastChangedOn\":\"2022-05-19T13:43:18.0651524+00:00\"},{\"name\":\"ImportProcessCSV-002\",\"revOfRcd\":null,\"
description\":null,\"revision\":\"1652977355\",\"processDefinitionType\":\"Production\",\"instanceId\":\"
0dVGzRNkv0B2UejFKd\",\"createdBy\":\"Test Test\",\"createdOn\":\"2022-05-19T16:22:35.7246167+00:00\",\"
lastChangedBy\":\"Test Test\",\"lastChangedOn\":\"2022-05-19T16:22:35.7246167+00:00\"},{\"name\":\"
ExampleProcess-006\",\"revOfRcd\":\"0dVRN9PPJ062UejFKd\",\"description\":null,\"revision\":\"1652980078\",\"
processDefinitionType\":\"Production\",\"instanceId\":\"0dVRN9PPJ062UejFKd\",\"createdBy\":\"Test Test\",\"
createdOn\":\"2022-05-19T17:07:58.408163+00:00\",\"lastChangedBy\":\"Test Test\",\"lastChangedOn\":\"2022-05-
19T17:07:58.408163+00:00\"},{\"name\":\"ImportProcessCSV-003\",\"revOfRcd\":\"0dYbQDCBn0E2UejFKd\",\"
description\":null,\"revision\":\"1653033306\",\"processDefinitionType\":\"Production\",\"instanceId\":\"
0dYbQDCBn0E2UejFKd\",\"createdBy\":\"Test Test\",\"createdOn\":\"2022-05-20T07:55:06.1773245+00:00\",\"
lastChangedBy\":\"Test Test\",\"lastChangedOn\":\"2022-05-20T07:55:06.1773245+00:00\"},{\"name\":\"
ImportProcessCSV-004\",\"revOfRcd\":\"0dZDjcZ2F082UejFKd\",\"description\":null,\"revision\":\"1653043613\",\"
processDefinitionType\":\"Production\",\"instanceId\":\"0dZDjcZ2F082UejFKd\",\"createdBy\":\"Test Test\",\"
createdOn\":\"2022-05-20T10:46:53.3952391+00:00\",\"lastChangedBy\":\"Test Test\",\"lastChangedOn\":\"2022-05-
20T10:46:53.3952391+00:00\"},{\"name\":\"ImportProcessCSV-005\",\"revOfRcd\":\"0dZG8JjEX042UejFKd\",\"
description\":null,\"revision\":\"1653044242\",\"processDefinitionType\":\"Production\",\"instanceId\":\"
0dZG8JjEX042UejFKd\",\"createdBy\":\"Test Test\",\"createdOn\":\"2022-05-20T10:57:22.9160859+00:00\",\"
lastChangedBy\":\"Test Test\",\"lastChangedOn\":\"2022-05-20T10:57:22.9160859+00:00\"}]"
},
"id": "0dZjzEiSX062ZppD2B"
}


# Provide Values for Filter In Production Order View

## Summary

It provides all the value of a given PO Header column given a set of filter

## Endpoint

```
Name Verb Endpoint
```
```
FillComboProductionOrderInquiryService POST api/FillComboProductionOrderInquiryService/Execute
```
## Input

```
Name Type Mandatory Note
```
```
ProductionOrderName string no
```
```
ProductName string no
```
```
ProductVersion int no
```
```
LocationName string no
```
```
Status string no
```
```
ParamName string yes
```
```
Skip int yes
```
```
Limit int yes
```
## Output

"DataLst":{}

## API usage example

Request body example

### {

"Input": {
"paramName":"ProductionOrderName",
"ProductionOrderName":"","ProductName":"Mat1","ProductVersion":"","Status":"","LocationName":"", "Skip":0,
"Limit":4000
},
"__RequestData":{"DataLst":{
}}
}


# Order Management Module Documents


# ProcessProductionSchedule B2MML

This document provide the minimum information that the B2MML document must contain in order to make it possible to import the Production Order in
Modular MOM.

```
Document Name Document Type
```
```
ProcessProductionSchedule B2MML
```
## Document Sample

This is a sample of B2MML document that ModularMOM can process

```
ProcessProductionSchedule B2MML sample
```
```
<?xml version="1.0" encoding="utf-8"?>
<ProcessProductionSchedule xmlns='http://www.mesa.org/xml/B2MML-V0600' xmlns:xsi='http://www.w3.org/2001
/XMLSchema-instance' xsi:schemaLocation='http://www.mesa.org/xml/B2MML-V0600' xmlns:Extended='http://www.mesa.
org/xml/B2MML-V0600-AllExtensions'>
<DataArea>
<Process></Process>
<ProductionSchedule>
<ProductionRequest>
<ID>PO_25933</ID>
<StartTime></StartTime>
<EndTime></EndTime>
<Priority>2</Priority>
<Extended:Type>A</Extended:Type>
<Extended:UserFields><Extended:UserField><Extended:Name>ResponsibleEngineer<
/Extended:Name><Extended:Type>String</Extended:Type> <Extended:Value>ResponsibleEngineer1<
/Extended:Value></Extended:UserField><Extended:UserField><Extended:Name>Customer</Extended:Name><Extended:
Type>String</Extended:Type><Extended:Value>Customer1</Extended:Value></Extended:UserField><Extended:
UserField><Extended:Name>PurchaseOrder</Extended:Name><Extended:Type>String</Extended:Type><Extended:
Value>PurchaseOrder1</Extended:Value></Extended:UserField><Extended:UserField><Extended:Name>ProjectManager<
/Extended:Name><Extended:Type>String</Extended:Type><Extended:Value>ProjectManager1</Extended:Value></Extended:
UserField><Extended:UserField><Extended:Name>ContractDeliveryDate</Extended:Name><Extended:Type>Date</Extended:
Type><Extended:Value>2022-01-09T07:00:20Z</Extended:Value></Extended:UserField></Extended:
UserFields> <Extended:LocationName>Location2</Extended:LocationName>
<Extended:AutomaticLaunch>0</Extended:AutomaticLaunch>
<Extended:ImportedInEngineering>false</Extended:ImportedInEngineering>
<Extended:UserFields>
<Extended:UserField>
<Name>PCB_ID</Extended:Name>
<Type>String</Extended:Type>
<Value>P012345</Extended:Value>
</Extended:UserField>
</Extended:UserFields>
<Extended:Material>
<Extended:ID>Samsung006</Extended:ID>
<Extended:Revision>A</Extended:Revision>
</Extended:Material>
<Extended:Quantity>
<Extended:QuantityString>1</Extended:QuantityString>
<Extended:UnitOfMeasure>each</Extended:UnitOfMeasure>
</Extended:Quantity>
<SegmentRequirement>
<ID>0010</ID>
<Description>SegmentRequirement-0010-Description</Description>
<Extended:Sequence>10</Extended:Sequence>
<Extended:Type>OT1</Extended:Type>
<Extended:WorkCenterName>WC1</Extended:WorkCenterName>
<Extended:UserFields>
<Extended:UserField>
<Extended:Name>Process Parameter</Extended:Name>
<Extended:Type>String</Extended:Type>
<Extended:Value>Temperature</Extended:Value>
</Extended:UserField>
</Extended:UserFields>
```

```
<Extended:StartDate>2022-01-09T07:00:20Z</Extended:StartDate>
<Extended:ProcessTime>
<Extended:ProcessTimeString>4.0</Extended:ProcessTimeString>
<Extended:UnitOfMeasure>Each</Extended:UnitOfMeasure>
</Extended:ProcessTime>
<Extended:SetupTime>
<Extended:SetupTimeString>10.0</Extended:SetupTimeString>
<Extended:UnitOfMeasure>Minutes</Extended:UnitOfMeasure>
</Extended:SetupTime>
<MaterialRequirement>
<Quantity>
<QuantityString>600</QuantityString>
<UnitOfMeasure>Unit</UnitOfMeasure>
</Quantity>
<Extended:MaterialDefName>Mat-1</Extended:MaterialDefName>
<Extended:MaterialDefRevision>Mat-A</Extended:
MaterialDefRevision>
<Extended:LineLocation>Line Location 1</Extended:LineLocation>
<Extended:StoreLocation>Warehouse Location 1</Extended:
StoreLocation>
<Extended:Description>This material use for creating tyre<
/Extended:Description>
<Extended:UserFields>
<Extended:UserField>
<Extended:Name>Material Line</Extended:Name>
<Extended:Type>String</Extended:Type>
<Extended:Value>Line01</Extended:Value>
</Extended:UserField>
</Extended:UserFields>
</MaterialRequirement>
<Extended:OperationDefinition>
<Extended:Name>Additive Pre-Mix</Extended:Name>
<Extended:Revision>1.0</Extended:Revision>
<Extended:AttachedDocument>something 1</Extended:
AttachedDocument>
<Extended:AttachedDocument>something 2</Extended:
AttachedDocument>
</Extended:OperationDefinition>
</SegmentRequirement>
</ProductionRequest>
</ProductionSchedule>
</DataArea>
</ProcessProductionSchedule>
```
ProductionSchedule Standard Schema

The standard validation of the document is based on the standard schema B2MML-V0600-ProductionSchedule.xsd

ProductionSchedule Extended Schema

This is the extended schema that can validate Modular MOM Extensions. For validating XML structure above we have to use B2MML-V0600-
ProductionSchedule.xsd schema + groups of elements from B2MML-V0600-Extensions.xsd (M1_Common/ Code/Schema/B2MML-V0600-Extensions.xsd).
Extensions for this validation have been added to ProductionRequest and SegmentRequirment elements

```
mod mom extension for B2MML
```
```
<xsd:complexType name ="UserFieldType">
<xsd:sequence>
<xsd:element name="Name" type="xsd:string" minOccurs = "0" maxOccurs = "1" />
<xsd:element name="Type" type="xsd:string" minOccurs = "0" maxOccurs = "1" />
<xsd:element name="Value" type="xsd:string" minOccurs = "0" maxOccurs = "1" />
</xsd:sequence>
</xsd:complexType>
```
```
<xsd:group name = "ProductionRequest">
<xsd:sequence>
<xsd:element name="Type" type="xsd:string" minOccurs = "0" maxOccurs = "1" />
<xsd:element name="AutomaticLaunch" type="xsd:boolean" minOccurs = "0" maxOccurs = "1" />
```

<xsd:element name="ImportedInEngineering" type="xsd:boolean" minOccurs = "0" maxOccurs = "1" />
<xsd:element name="LocationName" type="xsd:string" minOccurs = "0" maxOccurs = "1" />
<xsd:element name="UserFields" minOccurs = "0" maxOccurs = "1">
<xsd:complexType>
<xsd:sequence>
<xsd:element name="UserField" type="UserFieldType" minOccurs = "0" maxOccurs = "unbounded"/>
</xsd:sequence>
</xsd:complexType>
</xsd:element>
<xsd:element name="Material" minOccurs = "0" maxOccurs = "unbounded">
<xsd:complexType>
<xsd:sequence>
<xsd:element name="ID" type="xsd:string" minOccurs = "1" maxOccurs = "1"/>
<xsd:element name="Revision" type="xsd:string" minOccurs = "0" maxOccurs = "1"/>
</xsd:sequence>
</xsd:complexType>
</xsd:element>
<xsd:element name="Quantity" minOccurs = "0" maxOccurs = "1">
<xsd:complexType>
<xsd:sequence>
<xsd:element name="QuantityString" type="xsd:decimal" minOccurs = "1" maxOccurs = "1"/>
<xsd:element name="UnitOfMeasure" type="xsd:string" minOccurs = "1" maxOccurs = "1" />
</xsd:sequence>
</xsd:complexType>
</xsd:element>
</xsd:sequence>
</xsd:group>

<xsd:group name = "SegmentRequirement">
<xsd:sequence>
<xsd:element name="Type" type="xsd:string" minOccurs = "0" maxOccurs = "1"/>
<xsd:element name="WorkCenterName" type="xsd:string" minOccurs = "0" maxOccurs = "1"/>
<xsd:element name="Sequence" type="xsd:decimal" minOccurs = "0" maxOccurs = "1"/>
<xsd:element name="StandardCollectionQuantity" type="xsd:decimal" minOccurs = "0" maxOccurs = "1"/>
<xsd:element name="Milestone" type="xsd:boolean" minOccurs = "0" maxOccurs = "1" />
<xsd:element name="UserFields" minOccurs = "0" maxOccurs = "1">
<xsd:complexType>
<xsd:sequence>
<xsd:element name="UserField" type="UserFieldType" minOccurs = "0" maxOccurs = "unbounded"/>
</xsd:sequence>
</xsd:complexType>
</xsd:element>
<xsd:element name="StartDate" type="StartDateType" minOccurs = "1"/>
<xsd:element name="ProcessTime" minOccurs = "0" maxOccurs = "1" >
<xsd:complexType>
<xsd:sequence>
<xsd:element name="ProcessTimeString" type="xsd:string" minOccurs = "0" maxOccurs = "1"/>
<xsd:element name="UnitOfMeasure" type="xsd:string" minOccurs = "0" maxOccurs = "1"/>
</xsd:sequence>
</xsd:complexType>
</xsd:element>
<xsd:element name="OperationDefintion" minOccurs = "1" maxOccurs = "1" >
<xsd:complexType>
<xsd:sequence>
<xsd:element name="Name" type="xsd:string" minOccurs = "1" maxOccurs = "1"/>
<xsd:element name="Revision" type="xsd:string" minOccurs = "0" maxOccurs = "1"/>
<xsd:element name="AttachedDocument" type="xsd:string" minOccurs = "0" maxOccurs = "unbounded"/>
<xsd:element name="Description" type="xsd:string" minOccurs = "0" maxOccurs = "1" />
</xsd:sequence>
</xsd:complexType>
</xsd:element>
<xsd:element name="SetupTime" minOccurs = "0" maxOccurs = "1" >
<xsd:complexType>
<xsd:sequence>
<xsd:element name="SetupTimeString" type="xsd:string" minOccurs = "0" maxOccurs = "1"/>
<xsd:element name="UnitOfMeasure" type="xsd:string" minOccurs = "0" maxOccurs = "1"/>
</xsd:sequence>
</xsd:complexType>
</xsd:element>
<xsd:element name="CollectContainer" type="xsd:boolean" minOccurs = "0" maxOccurs = "1"/>
</xsd:sequence>


</xsd:group>

<xsd:group name = "MaterialRequirement">
<xsd:sequence>
<xsd:element name="LineLocation" type="xsd:string" minOccurs = "0" maxOccurs = "1" />
<xsd:element name="StoreLocation" type="xsd:string" minOccurs = "0" maxOccurs = "1" />
<xsd:element name="Description" type="xsd:string" minOccurs = "0" maxOccurs = "1" />
<xsd:element name="MaterialDefName" type="xsd:string" minOccurs = "1" maxOccurs = "1" />
<xsd:element name="MaterialDefRevision" type="xsd:string" minOccurs = "0" maxOccurs = "1" />
<xsd:element name="UserFields" minOccurs = "0" maxOccurs = "1">
<xsd:complexType>
<xsd:sequence>
<xsd:element name="UserField" type="UserFieldType" minOccurs = "0" maxOccurs = "unbounded"/>
</xsd:sequence>
</xsd:complexType>
</xsd:element>
</xsd:sequence>
</xsd:group>


# Order Management Module Entities

The system properties are not declared (i.e. Id, CreatedOn, LastUpdatedOn, etc.)

## ProductionOrderHeader: NamedObject

```
Property Name Label Property
Type
```
```
Data
Type
```
```
Length Default Natural
Key
```
```
Mandatory Note
```
```
Name Name UniqueName nvarchar
64
```
```
Yes Yes ProductionOrderName will be mapped on the Name Property of
CO NamedObject
ProductName Product Name nvarchar
64
```
```
Yes
```
```
ProductRevision Revision Version nvarchar
16
```
```
Yes Change needed to simplify future life.
```
```
ProductionOrderTyp
eName
```
```
Type Name nvarchar
64
```
```
To be shown only in details
```
```
ProductionOrderQu
antity
```
```
Quantity Quantity decimal Yes
```
```
UOM UOM Unit nvarchar
32
```
```
Yes UOM(Unit of Measure) is usually valid values - MVP we will use
as it is reading the value in B2MML
ProductionOrderPrio
rity
```
```
Priority Number int
```
```
PlannedStart Planned Start DateTime datetime
PlannedEnd Planned End DateTime datetime
LocationName Location Name nvarchar
64
Status Status Status nvarchar
10
LastLaunchingTime <not to be
shown>
```
```
DateTime datetime For the Launch Feature. See API for details.
```
```
AutomaticLaunch <not to be
shown>
```
```
Bool bit For automatic launch of PO
```
```
ImportedInEngineeri
ng
```
```
bool bit For Engineering status of PO
Default value is false
```
## ProductionOrderOperation:NamedSubentity<ProductionOrderHeader>

They will be mapped as a ProductionOrder NamedSubentity (it may change after mvp december 2021)

```
Property
Name
```
```
Label Property
Type
```
```
Data
Type
```
```
Length Default Natural
Key
```
```
Mandatory Note
```
```
Name Name UniqueName nvarchar
64
```
```
Yes Yes OperationName will be mapped on the Name Property of CO SubEntity
```
```
Sequence SequenceNumber int Yes B2MML may not have it, during import it will depend on the position in the
sequence.
If no value is provided, will start from 10-20-30-40...
Unique in the PO, so, we cannot have duplicates.
OperationTypeType Name nvarchar
64
WorkUnitNa
me
```
```
Work
Unit
```
```
Name nvarchar
64
```
```
This a simple string value and will be filled in from Factory Model App
```
```
ProcessTime Process
Time
```
```
Timespan Decimal
In SAP we have a differentiation between Machine time and personal time.
So additional field needed!
ProcessTime
UOM
```
```
<not be
shown>
```
```
Unit nvarchar
32
```
```
UOM is usually valid values - MVP we will use as it is reading the value in
B2MML
```
```
Shown in Process Time
SetupTime Setup
Time
```
```
Timespan Decimal
In SAP we have a differentiation between Machine time and personal time.
So additional field needed!
SetupTimeU
OM
```
```
<not be
shown>
```
```
Unit nvarchar
32
```
```
UOM is usually valid values - MVP we will use as it is reading the value in
B2MML
```
```
Shown in Setup Time
```

```
OperationDef
inition
```
```
<not be
shown>
```
```
Revisioned
Object
Reference
```
```
Yes The reference to revision object - This is a reference to a specific revision,
not to the revision base and implying to the RoR.
```
```
Even if in the B2MML only the Name is specified, so, the B2MML implies that
we need to sue the RoR, here we need to save the actual reference to the
RoR, because, in future the RoR may change BUT we need to remember the
RoR at that point of time.
CollectContai
ner
```
```
<not be
shown>
```
```
bool Enable the Container Validation flow in the rest api and show the proper box
on the UI screen (dafault value = false)
StartDate <not be
shown>
```
```
DateTime dateTime to be renamed PlannedStartDate in 2207
```
```
PlannedEnd
Date
```
```
DateTime date
```
```
StandardColl
ectionQuantity
```
```
<not be
shown>
```
```
decimal? decimal? It is used to fill in the Operator Terminal the collect tp value with a predefined
value
```
ProductionOrderMaterial:NamedSubentity<ProductionOrderOperation>

```
Property
Name
```
```
Label Property
Type
```
```
Data
Type
```
```
Length Default Natural
Key
```
```
Mandatory Note
```
```
Name Name UniqueName nvarchar
64
```
```
Yes Yes It will be mapped on the Name Property of CO SubEntity
```
```
MaterialDefId
<not be
shown>
```
```
InstanceId InstanceId Weak link to MaterialDef It cannot be mandatory because it will return by
Material management once the Allocate Material Flow is completed
(successfully)
MaterialQuan
tity
```
```
Quantity Decimal Decimal Quantity required to produce 1 Parent Assembly / Product
```
```
MaterialUOM <not be
shown>
```
```
Name nvarchar
64
```
```
Basic UOM of material
```
```
shown in Quantity
StoreLocationStore
Location
```
```
Name nvarchar
64
```
```
Location where Material is stored
```
```
LineLocation Line
Location
```
```
Name nvarchar
64
```
```
Work Center - Case Assembly Bottom
```
OperationDefinition:RevisionedObject

```
Property Name Label Property Type Data Type Length Default Natural Key Mandatory Note
```
```
Name Name string string Yes Yes
Revision Revision Revision Yes yes
Description Description string string (to be checked if Common already provide it)
IsRevisionOfRecord Current bool bool
AttachedDocuments<not be shown> list of string list of string they will be shown in a different way
```
ProductionOrderTrace:NamedSubentityObject<ProductionOrderHeader>

```
Property Name Label Semantic meaning Data Type Size Default value Natural Key Mandatory Note
```
```
TransactionType Transaction Name type enum
```
```
(int/string)
```
```
255 yes
```
```
Name Transaction ID unique name id yes yes Guid
Timestamp Timestamp date datetime yes -
UserName User user string 64 ""
Status Status status enum
```
```
(int/string)
```
```
255 yes
```
```
PreviousStatus Previous Status status enum
```
```
(int/string)
```
```
255 yes
```
WorkOrderInfo:NamedObject


```
Property
Name
```
```
Label Property
Type
```
```
Data
Type
```
```
Length Default Natural
Key
```
```
Mandatory Note
```
```
Name Name string string Yes Yes
WorkOrderNa
me
```
```
Name string string Yes yes
```
```
Description Description string string
```
WorkOrderOperationCounters: NamedSubentityOf<WorkOrderInfo>

```
Property Name Label Property Type Data Type Length Default Natural Key Mandatory Note
```
```
Name Name string string Yes Yes
DeltaQueued Quantity decimal decimal this field holds the change in quantity of the workorder
Description Description string string
```
Operation And OperationDefinition Interaction

Note the strong link now. This does not change the fact that OperationDetail may be an object in the semantic model.

```
This entity is introduced for maintaining the work order level information at OrderManagement Module, currently only the work order queueing
quantity is maintained.
```
```
This information helps OrderManagement Module to independently perform some business validation such as a request for PO quantity change
is valid or not.
```
```
This picture contains fields that will be exposed after MVP April 2022 (2204).
```

# Order Manager Status


# Order Management AM Module Entities

## ProductionOrderHeader: NamedObject

```
Property Name Label Property Type Data Type Length Default Natural Key Mandatory Note
```
```
Customer Customer Customer Name nvarchar255 No Extended Property
```
```
ProjectManager ProjectManager nvarchar 250 No Extended Property
```
```
PurchaseOrder PurchaseOrder int No Extended Property
```
```
ResponsibleEngineerResponsibleEngineer nvarchar 250 No Extended Property
```
```
ContractDeliveryDateContractDeliveryDate dateTimeOffSet(7) No Extended Property
```
## GroupDefinition: NamedObject

```
Property Name Label Property Type Data Type Length Default Natural Key Mandatory Note
```
```
Name Name UniqueName nvarchar 64 Yes
PrintFileName Print File Name File Name nvarchar 64 No
```
```
PrintFileUrl Print File Url Url nvarchar 250 No
```
```
GroupQuantity Group Quantity Quantity int No
```
```
GroupDefinitionCoupons <not be shown>Subentity List SubentityObjectOf<GroupDefinition>
```
```
GroupDefinitionProductionOrders<not be shown>Subentity List SubentityObjectOf<GroupDefinition>
```
```
GroupDefinitionProperties
<not be shown>Subentity List SubentityObjectOf<GroupDefinition>
```
```
ProductionGroupTemplate <not be shown> Revisioned Object Reference
```
## GroupDefinitionCoupon : SubentityObjectOf<GroupDefinition>

```
Property Name Label Property Type Data Type Length Default Natural Key Mandatory Note
```
```
ProcessDefinition ProcessDefinitionReference of ProcessDefinition No
CouponQuantity CouponQuantity Quantity int 1 No
```
## GroupDefinitionProductionOrder : SubentityObjectOf<GroupDefinition>

```
Property Name Label Property Type Data
Type
```
```
Length Default Natural
Key
```
```
Mandatory Note
```
```
ProductionOrderHeader ProductionOrderHeader Reference of ProductionOrderHeader No
PartQuantity PartQuantity Quantity int 1 No
```
## GroupDefinitionProperties : SubentityObjectOf<GroupDefinition>

```
Property Name Label Property Type Data Type Length Default Natural Key Mandatory Note
```
```
PropertyName Property Name Name nvarchar 64 No
Value Value Value nvarchar 64 No
```
## ProductionGroupTemplate : NamedObject


```
Property Name Label Property Type Data Type Length Default Natural Key Mandatory Note
```
```
Name Name
TemplateOpDef <not be shown> Subentity List SubentityObjectOf<ProductionGroupTemplate>
GroupNamePrefix Group Name Prefix Group Name nvarchar 64 Yes
```
TemplateOpDefinition : SubentityObjectOf<ProductionGroupTemplate>

```
Property
Name
```
```
Label Property
Type
```
```
Data Type Length Default Natural
Key
```
```
Mandatory Note
```
```
OperationDefinit
ion
```
```
<not be
shown>
```
```
Revision reference object of
OperationDefinition
```
```
No IRevisionedObjectRefField<Operat
ionDefinition>
OperationDefOr
der
```
```
OperationDefOr
der
```
```
Order int No Operation Execution Order
```

# Order Management Module Messages

OM Component publish and consume the following messages:

```
Messages Publisher Service Way Repository
```
```
ProductionOrderCreated OM Publisher OrderManagementMessage
```
```
ProductionOrderQuantityChanged OM Publisher OrderManagementMessage
```
```
CreateWorkOrders OM Publisher OrderMManagementMessage
```
```
WorkOrderCreated TNT Consumer TrackAndTraceMessages
```
```
WorkOrderCompleted TNT Consumer TrackAndTraceMessages
```
```
MaterialCreated MM Consumer MaterialManagementMessages
```
```
ProductionOrderUpdated OM Publisher OrderManagementMessage
```
```
UpdateProductionOrderResponse OM Publisher OrderManagementMessage
```

# CreateWorkOrders

## Summary

Message needed for the Launch operation. It's work as a bridge ebetween Order Management and TnT modules

## Input

```
Name Type Note
```
```
CreateWorkOrderItems ISubentityListField<CreateWorkOrderInputType> The whole Production Order structure List, including materials and
operations
```
## CreateWorkOrderInputType

```
Name Data Type
```
```
Name string
```
```
Description string
```
```
ProductName string
```
```
ProductRevision string
```
```
WorkOrderQuantity decimal
```
```
UOM string
```
```
WorkOrderPriority string
```
```
LocationName string
```
```
ProductionOrderTypeName string
```
```
PlannedStartDate datetime
```
```
PlannedEndDate datetime
```
```
SubentityListField<CreateWorkOrderInputOperationType> CreateWorkOrderInputOperationType
```
## CreateWorkOrderInputOperationType

```
Name Data Type
```
```
Name string
```
```
Description string
```
```
Sequence int
```
```
OperationType string
```
```
WorkUnitName string
```
```
ProcessTime decimal
```
```
ProcessTimeUOM string
```
```
SetupTime decimal
```
```
SetupTimeUOM string
```
```
CollectContainer int
```

```
StandardCollectionQuantity decimal
```
```
ProductionOrderTypeMaterials SubentityListField<CreateWorkOrderInputMaterialType>
```
```
StartDate StringField
```
```
OpdefAttachedDocuments SubentityListField<OpdefAttachement>
```
```
OpdefName string
```
```
OpdefDescription string
```
```
OpdefRevision string
```
CreateWorkOrderInputMaterialType

```
Name Data Type
```
```
Name string
```
```
Description string
```
```
MaterialQuantityRequired decimal
```
```
MaterialDefinitionID decimal
```
```
MaterialUOM string
```
```
StoreLocation string
```
```
LineLocation string
```
OpdefAttachement

```
Name Data Type
```
```
AttachedDoc string
```

# ProductionOrderCreated

## Summary

Message needed to communicate to third party about the change of PO status

```
Name Type Note
```
```
MaterialMessage List<>
```
## MaterialMessage

```
Name Type Note
```
```
MaterialName String
```
```
Revision String
```
```
ProductionOrderHeaderName String
```

# ProductionOrderQuantityChanged

## Summary

Message needed to communicate to third party about the change of PO quantity

## Input

```
Name Type Note
```
```
ProductionOrderName string
```
```
NewQuantity int
```

# ProductionOrderUpdated

## Summary

This message is published from Order Management to propagate the changes of Launched Production Order field's to TNT. So the flow is, whenever there
is a change in any field e.g. priority of a Launched Production Order e.g. PO1. Then via this message the same information will be propagated to TNT.
TNT will update the information in the respective work order and then it publishes a message WorkOrderUpdated to inform Order Management that work
order is updated. Then order management updates the same information in production order (that is PO1 according to e.g.).

## Input

```
Name Type Note
```
```
POName string
```
```
Quantity string
```
```
Priority string
```
```
PlannedStartDate string
```
```
PlannedEndDate string
```
```
UpdateProductionOrderOperations List of UpdateProductionOrderOperationType
```
Note : Currently only priority change is supported and for updating rest of the fields the implementation is in progress.

## UpdateProductionOrderOperationType

```
Name Type
```
```
Name string
```
```
PlannedStartDate string
```
```
PlannedEndDate string
```

# UpdateProductionOrderResponse

## Summary

This message is published from Order Management to propagate the outcome of Production Order update request back to ERP.

## Input

```
Name Type Note
```
```
POName string
```
```
IsSucceeded boolean
```
```
ResponseMessage string
```

# Order Management Module Use Cases


# Modular MOM 2.x UseCase ImportProductionOrder

The first use case we are targeting is the import of production orders.

## Import Production Order Flow

## Request Site by PO



# Modular MOM 2.xNFRs for UseCase ImportProductionOrder

## Customer NFRs

Performance

```
300 calls to ProcessB2MMLProductionSchedule in 16 hours
each "ProductionSchedule" contains 1 "ProductionRequest" with 5 Operations.
300 calls to GET ProdcutionOrderHeader in 1 hour
```
Scalability

```
x concurrent users?
```
## System NFRs

Performance

```
3000 calls to GET ProdcutionOrderHeader in 1 hour
Max 75% CPU utilization
```
Scalability

```
100 concurrent users
```

# Modular MOM 2.x UseCase ImportProductionOrder from

# CSV File

The first use case we are targeting is the import of production orders.

## Import Production Order Flow

## Request Site by PO



# Modular MOM 2.x NFRs for UseCase

# ImportProductionOrder from CSV

## Customer NFRs

Performance

```
10 POs with 10 operations and 4 Materials each operation When I run Import PO Then the system should import POs in < 15 sec
```
Scalability

```
x concurrent users?
```
## System NFRs

Performance

```
10 POs with 10 operations and 4 Materials each operation When I run Import PO Then the system should import POs in < 15 sec
Max 75% CPU utilization
```
Scalability

```
100 concurrent users
```

# Modular MOM 2.x UseCase ImportProductionOrder with

# AutomaticLaunch true

The first use case we are targeting is the import of production orders and launching them. If the AutomaticLaunch flag is not specified or set to true, it will
be launched. If user want to skipped the launching of PO he/she needs to specifically set the AutomaticLaunch flag to false. Please see ProcessProduction
Schedule B2MML / Import Production Order from CSV to know more about AutomaticLaunch flag.

## Import Production Order Flow with AutomaticLaunch flag true


# Modular MOM 2.x NFRs UseCase ImportProductionOrder

# with AutomaticLaunch true


# Order Management Module Permissions

```
Name Effect Note Home Card Relevant
```
```
View Production Orders Allows to visualize Production Orders on the main view, and Visualize their details
View Production Order Tile
Order Management Card
```
```
Import Production Orders Allow to Import a Production Order (via CSV or B2MML)
```
```
Change Production Order Priority Allow to change Production Order Priority
```
```
Change Production Order Quantity Allow to change Production Order Quantity
```
```
Launch Production Order Allow to manually launch a Production Order
```
```
View Process Definition Allow to view Process Definition
Process Tile
```
```
Import Process Definition Allow to import Process Definition
```
```
Release Production Order Allow to move the Production order
```
```
Associate Process Definition Allow to associate a Production order to a Process Definition
```
## AM

```
Name Effect Note Home Card Relevant
```
```
Create Group Template Allows to create a group template
```
```
Create Group Definition Allow to create a group definition
```
```
View Group Definition Allow to view a group definition
```

# AM Order Management Module Messages


# CreateWorkOrders@AM

## Summary

This is specific to AM. To support fields which are very specific to the AM.

This message carries data from Order Management to Track and Trace after production order is launch & below are the AM specific fields added in
message and all are optional.

## Input

```
Name Type Note
```
```
Customer string Customer who ordered the Product.
```
```
PurchaseOrder string Similar to PO name.
```
```
ProjectManager string User
```
```
ResponsibleEngineer string User
```
```
ContractDeliveryDate datetime Contract Delivery Date
```

# TrackAndTrace Module


# TrackAndTrace Module APIs


# AddComments

## Summary

This Endpoint provides the functionality to add a comment under a WorkOrder or WorkOrderOperation entity.

## Endpoint

```
Name Verb Endpoint
```
```
AddComments POST /api/AddComments/Execute
```
## Input

```
Name Type Mandatory Note
```
```
WorkOrderName String yes Name of the workorder under which a comment needs to be added
```
```
OperationName String no
```
```
Comment String yes
```
## Output

No output required

## Errors

For error cases such as invalid WorkOrder reference or missing mandatory parameters, platform behavior is leveraged.

## API usage example

Request body example

{
"Input":
{
"WorkOrderName": "WO1",
"Comment":"test comment",
"OperationName":"OP1"
}

}


# Collect (2207 version)

## Summary

Perform a collect throughput according with the input parameter.

Note: This first release performs only the collect throughput from queuing to produced.

## Behavior

```
Check if the requested collect can be executed
If the operation needs the container validation:
store the collect parameters
send the request
exit (the collect will be executed when a positive response is incoming only if the response is incoming in 2 minutes)
Otherwise:
execute the requested collect
exit
```
The steps of a collect throughput are:

```
Update the counters of the operation and work order, that is: Insert a new row in the WorkOrderOperationCounters with the throughput
quantity.
Update the consumed materials
Update the operations statuses and the work order status
Update the queue counter of the next operation (Insert a new row in the WorkOrderOperationCounters with the throughput quantity) and its
status
in case of the collection mode is 'full' the queue counter and the status of the next operation is updated once that the current status
operation becomes completed
Trace this collect execution on the current operation and also on the next operation and on the work order if theirs statuses are changed.
If the work order is completed, then send a message of 'Completed Work Order' to Order Management
Send the 'backflush message' to ERP
```
## Api

```
form Name Verb Endpoint
```
```
Collect POST /api/Collect/Execute
```
## Input

```
Name Type Mandatory Note
```
```
WorkOrderN
ame
```
```
string yes
```
```
OperationN
ame
```
```
string In case of the operation name is not specified, the collect will be executed on the last operation with queuing > 0 (in case of full
mode it is the only operation with queuing > 0)
```
```
Quantity decimal yes
```
```
From string default=queuing
```
```
To string default=produced
```
```
Description string
```
```
ContainerN
ame
```
```
string it is mandatory only if the operation has the flag CollectContainer set to true
```
## Output

```
Name Type Note
```
```
TransactionID string Unique id (Guid) to identify this transaction
```
```
Timestamp DateTime Timestamp of this transaction
```

```
State string
"Executed": the collect is executed with success;
"ContainerValidationInProgress": the collect is not executed because it's waiting the container validation
```
Errors

```
Code Message Note
```
```
100 The input quantity <value> is less than 0!
```
```
101 The input From '<value>' is not known!
```
```
102 The input To '<value>' is not known!
```
```
110 The work order <value> doesn't exist!
```
```
111 The operation <value> of the work order <value> doesn't exist!
```
```
112 The Workflow associated to the product <value> version <value>doesn't exist!
```
```
113 In the Workflow associated to the product <value> version <value>, the step <value> doesn't exist!
```
```
114 The Work Order Quantity <value> is invalid: the backflush cannot be computed!
```
```
115 The work order <value> is already completed: no more collect can be executed!
```
```
116 The operation <value> of the work order <value> is already completed: no more collect can be executed!
```
```
117 The operation is waiting the validation for a previous collect: for now this collect cannot be executed!
```
```
120 The operation <value> of the work order <value> has only <value> pieces available: the collect throughput of <value> pieces from
'Queuing' cannot be executed!
```
```
121 The operation <value> of the work order <value> needs the container validation: the container parameter cannot be empty!
```
```
131 The 'ContainerValidationRequest' message cannot be launched.
```
```
132 The 'WorkOrderCompleted' message cannot be launched.
```
```
133 The 'BackflushMessage' message cannot be launched.
```
When the collect is executed after the container validation, the following errors can be logged:

```
Code Message Note
```
- The validation response is late: the collect will not be executed
- The collect mode is changed: the collect cannot be executed
- The operation <value> of the work order <value> has only <value> pieces available: the collect throughput of <value> pieces from
    'Queuing' cannot be executed!
- The work order<value> doesn't exist!
- The operation <value> of the work order <value> doesn't exist!
- TrackAndTrace service cannot get the collect parameters related to the validation response
- The work order <value> is already completed: no more collect can be executed!
- The operation <value> of the work order <value> is not waiting a validation response: the collect cannot be executed!
- The next operation <value> is not present!
- The previous operation <value> is not present!
- The 'WorkOrderCompleted' message cannot be launched!
- The 'BackflushMessage' message cannot be launched.

Messages


```
Type Destination Note
```
```
WorkOrderCompleted Order Management
```
```
BackflushMessage ERP
```
```
ContainerValidationRequest ERP
```
```
ContainerValidationResult TnT This message is shown here because the real owner is an external component
```
API usage example

<tnt-module-url>/api/Collect/Execute

```
Collect
```
### {

```
"Input": {
"WorkOrderName": "WO-Name",
"OperationName": "0010",
"Quantity": 10.0,
"From": "Queuing",
"To": "Produced",
"Container": "Container-Name"
},
"__RequestData": {
"TransactionID": {},
"Timestamp": {},
"State": {}
}
}
```
Quality-Test (editing of this section is in progress)

In order to test the collect in case of container validation, the validator is a 'fake' module (IntegrationModule). This response depends on the container
name. The rule is:

<name>_<OK|NOTOK>_<delay in ms>.

So if the name is Container_OK_10000, the response from IntegrationModule will income after 10 seconds and it will be positive.

By default (for example, if the name of container is 'ContainerA'), the response will income after 500 ms and it will be positive.

If the delay is 0 (for example, the name of container is 'ContB_OK_0'), the response will not income.

In order to test the collect service, the used BDD API steps are the following (their code is in OrderManagement.Test):

Given I have a work order named: "(.*)" with (.*) operations

Prepare data for import: define a name of a production order and the number of its operations.

Given I have (.*) work orders named "(.*)" with (.*) operations

Prepare data for import: define the number of production orders and theirs names and the number of theirs operations.

By default

Given the operation "(.*)" of the work order "(.*)" needs validation

The collect on this operation needs the container name as parameter and it will be executed after validation.


Given the target quantity of the work order "(.*)" is (.*)

The target quantity is different from the default.

Then in TracKAndTrace.Test is implemented:

Given I start this test

Create the B2MML file, import it in OM and launch it to create the WO(s) in TnT.


# Filter Work Order Operations view

## Summary

It allows Operator to filter the work order operation view data based on work order operation properties.

Inquiry Services:

```
WorkOrderOperationInquiryService
FillComboWorkOrderOperationInquiryService
```
Filterable Columns:

```
WorkCenterName
ProductName
WorkOrderName
Status
```
## WorkOrderOperationInquiryService

## Endpoint

```
Name Verb Endpoint
```
```
WorkOrderOperationInquiryService POST api/WorkOrderOperationInquiryService/Execute
```
## Input

```
Name Type Mandatory Note
```
```
WorkCenterName string no
```
```
ProductName string no
```
```
WorkOrderName int no
```
```
Status string no
```
```
Skip int no
```
```
Limit int no
```
## Output

"DataLst" : ”[ { "Operation" : { } , "WorkOrder" : { } , "Priority" : { } , "Status" : { } , "Product" : { } , "Revision" : { } , "Queuing" : {}, "Produced" : {}, "Sequence"
: {}, "WorkCenterName" : { } ]”

## API usage example

<tnt-module-url>/api/WorkOrderOperationInquiryService/Execute


```
Request Body
```
### {

```
"Input": {
"WorkCenterName": "Xyz",
"WorkOrderName":"",
"ProductName":"",
"Status":""
},
"__RequestData": {
"WorkOrderOperationLst": {
"Operation": {},
"WorkOrder": {},
"Priority": {},
"Sequence": {},
"Status": {},
"Product": {},
"Revision": {},
"Queuing": {},
"Produced": {},
"WorkCenterName": {}
}
}
}
```
FillComboWorkOrderOperationInquiryService

Endpoint

```
Name Verb Endpoint
```
```
FillComboWorkOrderOperationInquiryService POST api/FillComboWorkOrderOperationInquiryService/Execute
```
Input

```
Name Type Mandatory Note
```
```
ParamName string no
```
```
WorkCenterName string no
```
Output

"dataLst":{}

API usage example

<tnt-module-url>/api/FillComboWorkOrderOperationInquiryService/Execute


Request Body

### {

"input": {
"ParamName":"WorkCenterName",
"WorkCenterName": ""
},
"__RequestData": {
"DataLst": {
"Name": {}
}
}

### }


# Filter Work Order view

## Summary

It returns the following WorkOrderHeader information: Name, Description, ProductionOrderName, ProductionOrderTypeName, Status, ProductName,
ProductRevision, WorkOrderQuantity, QueuingQuantity, InProcessQuantity, ProducedQuantity, WorkOrderPriority, LocationName, ActualStartTime,
ActualEndTime, Uom. ChangeDetails: CreatedBy, CreatedOn, LastChangedBy, LastChangedOn". Data can be filtered by specifying a filter value in the
corresponding parameter (wild cards '%' and '_' are supported).

The default sorting applied to the retrieved data is by WorkOrderPriority (ascending) and CreatedOn (ascending).

## Endpoint

```
Name Verb Endpoint
```
```
WorkOrderInquiryService POST api/WorkOrderInquiryService/Execute
```
## Input

```
Name Type Mandatory Note
```
```
WorkOrderName string no
```
```
ProductName string no
```
```
Status string no
```
```
LocationName string no
```
```
Skip int yes
```
```
Limit int yes
```
```
QueryOptions object yes
```
QueryOptions

```
Name Type Mandatory Note
```
```
SortingLst list of SortingOptions no
```
```
Skip int no
```
```
Limit int no
```
SortingOptions

```
Name Type Mandatory Note
```
```
SortProperty string no
```
```
SortDirection string no "ASC"|"DESC"
```
## Output

```
Name Type Mandatory Note
```
```
name string no
```
```
description string no
```
```
productionOrderName string no
```
```
productionOrderTypeName string no
```
```
Status string no
```
```
productName string no
```
```
productRevision string no
```
```
workOrderQuantity decimal no
```
```
queuingQuantity decimal no
```

```
inProcessQuantity decimal no
```
```
producedQuantity decimal no
```
```
workOrderPriority decimal no
```
```
locationName string no
```
```
actualStartTime datetime no
```
```
actualEndTime datetime no
```
```
uom string no
```
```
createdBy (ChangeDetails) string no
```
```
createdOn (ChangeDetails) datetime no
```
```
lastChangedBy (ChangeDetails) string no
```
```
LastChangedOn (ChangeDetails) datetime no
```
```
Skip int yes
```
```
Limit int yes
```
{"DataLst" : "__Value": ”[ { "name" : { "__Value": "" } , "description" : { } , "productionOrderName" : { } , "productionOrderTypeName" : { } , "Status" :
{ } , "productName" : { } , "productRevision" : {}, "workOrderQuantity" : {}, "queuingQuantity" : {}, "inProcessQuantity" : { }, "producedQuantity" : { }, "
workOrderPriority" : { }, "locationName" : { }, "actualStartTime" : { }, "actualEndTime" : { }, "uom" : { }, "createdBy" : { }, "createdOn" : { }, "lastChange
dBy" : { }, "LastChangedOn" : { } }, {...}, {...} ]” }

API usage example

<om-module-url>/api/WorkOrderInquiryService/Execute


```
Request Body
```
### {

```
"Input": {
"WorkOrderName": "BPACK%",
"ProductName": "",
"Status": "",
"LocationName": "",
"Skip": 0,
"Limit": "40",
"DefaultSortOrder": "Y",
"QueryOptions": {
"SortingLst": [],
"skip": 0,
"limit": 40
}
},
"__RequestData": {
"DataLst": {
"name": {},
"description": {},
"productionOrderName": {},
"productionOrderTypeName": {},
"status": {},
"productName": {},
"productRevision": {},
"workOrderQuantity": {},
"queuingQuantity": {},
"inProcessQuantity": {},
"producedQuantity": {},
"workOrderPriority": {},
"locationName": {},
"actualStartTime": {},
"actualEndTime": {},
"uom": {},
"createdBy": {},
"createdOn": {},
"lastChangedBy": {},
"lastChangedOn": {}
}
}
}
```
Remark: returned data temporarily mime the GetAll format this because the TNT UI use both api to fill the WorkOrder grid and SWF framework allows only
one parser for the data binding. Will be removed as soon as the GetAll call will be no more necesary


Response Body

### {

"dataLst": {
"__Value": "[ { \"name\" : { \"__Value\" : \"PO_D918958A-2490-42D4-B5AC-
001F9984D20A_#1\"} , \"description\" : { \"__Value\" : \"PO_D918958A-2490-42D4-B5AC-001F9984D20A_#1
Description\"} , \"productionOrderName\" : { \"__Value\" : \"PO_D918958A-2490-42D4-B5AC-001F9984D20A_#1\"}
, \"productionOrderTypeName\" : { \"__Value\" : \"PO_D918958A-2490-42D4-B5AC-001F9984D20A_#1 \"} , \"
status\" : { \"__Value\" : \"Ready\"} , \"productName\" : { \"__Value\" : \"P_D918958A-2490-42D4-B5AC-
001F9984D20A\"} , \"productRevision\" : { \"__Value\" : \"1.0\"} , \"workOrderQuantity\" : { \"__Value\" :
\"100\"} , \"queuingQuantity\" : { \"__Value\" : \"100\"} , \"inProcessQuantity\" : { \"__Value\" : \"0\"}
, \"producedQuantity\" : { \"__Value\" : \"0\"} , \"workOrderPriority\" : { \"__Value\" : \"2\"} , \"
locationName\" : { \"__Value\" : \"PO_D918958A-2490-42D4-B5AC-001F9984D20A_#1 Location\"} , \"
actualStartTime\" : { \"__Value\" : \"1/1/0001 12:00:00 AM +00:00\"} , \"actualEndTime\" : { \"__Value\" : \"1
/1/0001 12:00:00 AM +00:00\"} , \"uom\" : { \"__Value\" : \"pcs\"} , \"createdBy\" : { \"__Value\" : \"
Default\"} , \"createdOn\" : { \"__Value\" : \"12/23/2021 7:34:49 PM +01:00\"} , \"lastChangedBy\" : { \"
__Value\" : \"Default\"} , \"LastChangedOn\" : { \"__Value\" : \"12/23/2021 7:34:49 PM +01:00\"} } , { \"
name\" : { \"__Value\" : \"PO_D918958A-2490-42D4-B5AC-001F9984D20A_#3\"} , \"description\" : { \"__Value\" :
\"PO_D918958A-2490-42D4-B5AC-001F9984D20A_#3 Description\"} , \"productionOrderName\" : { \"__Value\" : \"
PO_D918958A-2490-42D4-B5AC-001F9984D20A_#3\"} , \"productionOrderTypeName\" : { \"__Value\" : \"PO_D918958A-
2490-42D4-B5AC-001F9984D20A_#3 \"} , \"status\" : { \"__Value\" : \"Ready\"} , \"productName\" : { \"
__Value\" : \"P_D918958A-2490-42D4-B5AC-001F9984D20A\"} , \"productRevision\" : { \"__Value\" : \"1.1\"} ,
\"workOrderQuantity\" : { \"__Value\" : \"100\"} , \"queuingQuantity\" : { \"__Value\" : \"100\"} , \"
inProcessQuantity\" : { \"__Value\" : \"0\"} , \"producedQuantity\" : { \"__Value\" : \"0\"} , \"
workOrderPriority\" : { \"__Value\" : \"3\"} , \"locationName\" : { \"__Value\" : \"PO_D918958A-2490-42D4-
B5AC-001F9984D20A_#3 Location\"} , \"actualStartTime\" : { \"__Value\" : \"1/1/0001 12:00:00 AM +00:00\"} ,
\"actualEndTime\" : { \"__Value\" : \"1/1/0001 12:00:00 AM +00:00\"} , \"uom\" : { \"__Value\" : \"pcs\"} ,
\"createdBy\" : { \"__Value\" : \"Default\"} , \"createdOn\" : { \"__Value\" : \"12/23/2021 7:35:01 PM +01:
00\"} , \"lastChangedBy\" : { \"__Value\" : \"Default\"} , \"LastChangedOn\" : { \"__Value\" : \"12/23/2021
7:35:01 PM +01:00\"} } , { \"name\" : { \"__Value\" : \"PO_D918958A-2490-42D4-B5AC-001F9984D20A_#4\"} , \"
description\" : { \"__Value\" : \"PO_D918958A-2490-42D4-B5AC-001F9984D20A_#4 Description\"} , \"
productionOrderName\" : { \"__Value\" : \"PO_D918958A-2490-42D4-B5AC-001F9984D20A_#4\"} , \"
productionOrderTypeName\" : { \"__Value\" : \"PO_D918958A-2490-42D4-B5AC-001F9984D20A_#4 \"} , \"status\" : {
\"__Value\" : \"Ready\"} , \"productName\" : { \"__Value\" : \"P_D918958A-2490-42D4-B5AC-001F9984D20A_#1\"}
, \"productRevision\" : { \"__Value\" : \"1.0\"} , \"workOrderQuantity\" : { \"__Value\" : \"100\"} , \"
queuingQuantity\" : { \"__Value\" : \"100\"} , \"inProcessQuantity\" : { \"__Value\" : \"0\"} , \"
producedQuantity\" : { \"__Value\" : \"0\"} , \"workOrderPriority\" : { \"__Value\" : \"3\"} , \"
locationName\" : { \"__Value\" : \"PO_D918958A-2490-42D4-B5AC-001F9984D20A_#4 Location\"} , \"
actualStartTime\" : { \"__Value\" : \"1/1/0001 12:00:00 AM +00:00\"} , \"actualEndTime\" : { \"__Value\" : \"1
/1/0001 12:00:00 AM +00:00\"} , \"uom\" : { \"__Value\" : \"pcs\"} , \"createdBy\" : { \"__Value\" : \"
Default\"} , \"createdOn\" : { \"__Value\" : \"12/23/2021 7:35:04 PM +01:00\"} , \"lastChangedBy\" : { \"
__Value\" : \"Default\"} , \"LastChangedOn\" : { \"__Value\" : \"12/23/2021 7:35:04 PM +01:00\"} } , { \"
name\" : { \"__Value\" : \"PO_D918958A-2490-42D4-B5AC-001F9984D20A_#2\"} , \"description\" : { \"__Value\" :
\"PO_D918958A-2490-42D4-B5AC-001F9984D20A_#2 Description\"} , \"productionOrderName\" : { \"__Value\" : \"
PO_D918958A-2490-42D4-B5AC-001F9984D20A_#2\"} , \"productionOrderTypeName\" : { \"__Value\" : \"PO_D918958A-
2490-42D4-B5AC-001F9984D20A_#2 \"} , \"status\" : { \"__Value\" : \"Ready\"} , \"productName\" : { \"
__Value\" : \"P_D918958A-2490-42D4-B5AC-001F9984D20A\"} , \"productRevision\" : { \"__Value\" : \"1.0\"} ,
\"workOrderQuantity\" : { \"__Value\" : \"100\"} , \"queuingQuantity\" : { \"__Value\" : \"100\"} , \"
inProcessQuantity\" : { \"__Value\" : \"0\"} , \"producedQuantity\" : { \"__Value\" : \"0\"} , \"
workOrderPriority\" : { \"__Value\" : \"3\"} , \"locationName\" : { \"__Value\" : \"PO_D918958A-2490-42D4-
B5AC-001F9984D20A_#2 Location\"} , \"actualStartTime\" : { \"__Value\" : \"1/1/0001 12:00:00 AM +00:00\"} ,
\"actualEndTime\" : { \"__Value\" : \"1/1/0001 12:00:00 AM +00:00\"} , \"uom\" : { \"__Value\" : \"pcs\"} ,
\"createdBy\" : { \"__Value\" : \"Default\"} , \"createdOn\" : { \"__Value\" : \"12/23/2021 7:35:48 PM +01:
00\"} , \"lastChangedBy\" : { \"__Value\" : \"Default\"} , \"LastChangedOn\" : { \"__Value\" : \"12/23/2021
7:35:48 PM +01:00\"} } ] "
},
"id": "0SjSySGiv021iXKHhW"
}


# Get Comments

## Summary

This "WorkOrderAndOperationsCommentInquiryService" inquiry service allows user to retrieve Comments for given "work order name and work order
operation name". This service can return CommentList jsonString fields including OperationName, Comment, UserName and TimeStamp,Name.

## Endpoint

```
Name Verb Endpoint
```
```
WorkOrderAndOperationsCommentInquiryService POST api/WorkOrderAndOperationsCommentInquiryService/Execute
```
## Input

```
Name Type Mandatory Note
```
```
WorkOrderName string Yes Work Order Name
```
```
OperationName string No Work Order Operation Name
```
## Errors

For error, missing mandatory parameters, platform behavior is leveraged.

## Output

```
Response
```
### {

```
"CommentList": {
"__Value": {
```
### }

### }

### }

## API usage example

```
Request Body
```
### {

```
"Input":
{
"WorkOrderName": "Test14",
"OperationName":"0010"
},
```
```
"__RequestData":
{
"CommentList":{}
}
```
### }


Response Body

### {

"commentList": {
"__Value": "[{\"OperationName\":\"0010\",\"Comment\":\"Comments Provided by Arjita\",\"UserName\":\"Admin
Admin\",\"Timestamp\":\"2022-06-03T09:38:46.1427925+00:00\",\"Name\":\"0d77dad1-678d-4352-9a44-2fa3da40b8ba\"},
{\"OperationName\":\"0010\",\"Comment\":\"Test comment from arjita\",\"UserName\":\"Admin Admin\",\"Timestamp\":
\"2022-06-01T19:34:26.7288821+00:00\",\"Name\":\"6b7733f5-d511-4b20-84b5-124cb484f8fd\"}]"
},
"id": "0eg4oGJu6012e2~nrp"
}


# Get Work Order Operations Details

## Summary

This "WOOperationDetailInquiryService" inquiry service allows user to retrieve Work Order Operation for given "work order name and work order operation
name". This service can return Work Order Quantity and Work Order Operation fields including WorkOrderMaterials, WorkOrderOperationTraces and
OperationDefinition( including AttachedDocuments).

## Endpoint

```
Name Verb Endpoint
```
```
WOOperationDetailInquiryService POST api/WOOperationDetailInquiryService/Execute
```
## Input

```
Name Type Mandatory Note
```
```
OperationDetail WorkOrderOperation Yes All the fields from Work Order Operation and Work Order can be added as input
```
```
OperationDetail.__Name string Yes Work Order Operation Name
```
```
OperationDetail.__Name.__Parent.__Name string Yes Work Order Name
```
## Output

```
Response
```
### {

```
"operationDetail": {
"__Value": {
```
### }

### }

### }

## API usage example


Request Body

### {

"Input": {
"OperationDetail": {
"__Name" :"PO_FINAL2_0010",
"__Parent": {
"__Name":"PO_FINAL2_02"
}
}

### },

"__RequestData": {
"WorkOrderQuantity":{},
"OperationDetail": {
"Name":{},
"OperationType":{},
"Sequence":{},
"Description":{},
"OperationDefinition":{
"AttachedDocuments": {}
},
"WorkCenter":{},
"ProcessTime":{},
"WorkOrderMaterials":{},
"WorkOrderOperationTraces": {}
}
}
}

Response Body

### {

"operationDetail": {
"__Value": {
"operationDefinition": {
"__Value": {
"attachedDocuments": {
"__Value": [
"https://web.iiit.ac.in/~pratik.kamble/storage/Algorithms/Cormen_Algorithms_3rd.
pdf",
"https://cezeife.myweb.cs.uwindsor.ca/courses/60-140/notes/140slides.pdf"
]
},
"revisionBaseId": "0RaLO1d2K0u1l5QTs4",
"id": "0RaLO1Ypv0q1ZDlEHO"
}
},
"operationType": {
"__Value": "PO_FINAL2_ - OperationType"
},
"processTime": {
"__Value": 10.00000
},
"sequence": {
"__Value": 1
},
"workOrderMaterials": {
"__Value": [
{
"lineLocation": {
"__Value": "Mat_0010_A - LineLocation"
},
"materialQuantityConsumed": {
"__Value": 0.00000
},


"materialQuantityRequired": {
"__Value": 100.00000
},
"materialUOM": {
"__Value": "kg"
},
"storeLocation": {
"__Value": "Mat_0010_A - StoreLocation"
},
"description": {},
"name": {
"__Value": "Mat_0010_A"
},
"parent": {
"__Value": {
"__Parent": {
"__Name": "PO_FINAL2_02",
"__Id": "0RaLLzMJG0j1YrJ~H6",
"__TypeOfRefDto": 0
},
"__Name": "PO_FINAL2_0010",
"__Id": "0RaLNHAmx0n1YyMshM",
"__TypeOfRefDto": 0
}
},
"id": "0RaLO3Bup0y1YyoFIs"
}
]
},
"workOrderOperationTraces": {
"__Value": []
},
"description": {
"__Value": "PO_FINAL2_-Description"
},
"name": {
"__Value": "PO_FINAL2_0010"
},
"id": "0RaLNHAmx0n1YyMshM"
}
},
"workOrderQuantity": {
"__Value": 100.00000
},
"id": "0RbYJ1l8y0F1kkjp4r"
}


# Provide values for filters in Work Order View

## Summary

It returns all the distinct values of the WorkOrderHeader field specified in the FieldName parameter (allowed fields are: WorkOrderName, ProductName,
Status and LocationName), data can be filtered by specifying a filter value in the corresponding parameter (wild cards '%' and '_' are supported).

Returned data are sorted in alphabetically ascending order for the fields: WorkOrderName, ProductName, and LocationName, while for the Status filed the
data are sorted according to the numeric value of the status ( NotReady = 0, Ready = 1, InProcess = 2, Completed = 3).

## Endpoint

```
Name Verb Endpoint
```
```
FillComboWorkOrderInquiryService POST api/FillComboWorknOrderInquiryService/Execute
```
## Input

```
Name Type Mandatory Note
```
```
FieldName string yes
```
```
WorkOrderName string no
```
```
ProductName string no
```
```
Status string no
```
```
LocationName string no
```
```
Skip int yes
```
```
Limit int yes
```
## Output

```
Name Type Mandatory Note
```
```
DataLst string yes
```
"DataLst" : ”[ { "<requested-fied-name>" : { "__Value" : [ "<field-value-1>", "<field-value-N>" ] } } ]”

## API usage example

<om-module-url>/api/FillComboWorknOrderInquiryService/Execute

```
Request body example
```
### {

```
"Input": {
"fieldName": "WorkOrderName",
"WorkOrderName": "BPACK% ",
"ProductName": "",
"Status": "",
"LocationName": "",
"Skip": 0,
"Limit": 4000
},
"__RequestData": {
"DataLst": {}
}
}
```

Response Body

### {

"dataLst": {
"__Value": [
"BPACK_PO_CSV_Monv01",
"BPACK_PO_CSV_Monv02",
"BPACK_PO_CSV_Monv03",
"BPACK_PO_CSV_Monv04",
"BPACK_PO_CSV_Monv05",
"BPACK_PO_CSV_Monv06",
"BPACK_PO_CSV_Monv07",
"BPACK_PO_CSV_Monv08",
"BPACK_PO_CSV_Monv09",
"BPACK_PO_CSV_Monv10"
]
},
"id": "0SaIdaXj2011ly5mHK"
}


# Send container validation result (preliminary)

## Summary

## API

```
Name Verb Endpoint
```
```
CollectAfterValidationHandler POST /api/CollectAfterValidationHandler/Execute
```
## Input

```
Name Type Optional Notes
```
```
RequestId string no Identifier of the Collect request
```
```
WorkOrderName string no WorkOrder name
```
```
WorkOrderOperati
onName
```
```
string no WorkOrderOperation name
```
```
IsFailed bool no Specify the Container validation result (true = OK, false NOT OK)
```
```
FailureReason string yes In case of validation failure can be used to specify the failure reason. It will be shown in the Operator UI. The value is
temporarily stored in the WorkOrderOperationCollection entity
```
## Output

No output for the moment.

## Errors

```
Code Message Note
```
```
250 ErrorCodes.
TimeoutExpiredInCollectValidat
ion
```
```
"Timeout expired waiting for the collect validation answer. WorkOrder: {WorkOrderName}
WorkOrderOperation: {WorkOrderOperation} RequestId: {RequestId}"
```
```
251 ErrorCodes.
CollectionModeChanged
```
```
"Collect mode is changed during execution. WorkOrder: {WorkOrderName} WorkOrderOperation:
{WorkOrderOperation} RequestId: {RequestId}",
```
```
120 ErrorCodes.
QuantityNotAvailable
```
```
"Requested quantity: {CollectQuantity} is not available. WorkOrder: {WorkOrderName}
WorkOrderOperation: {WorkOrderOperation} available quantity: {AvailableQuantity}"
```
```
210 ErrorCodes.
WorkOrderNotExist
```
```
"The WorkOrder does not exist. WorkOrder: {WorkOrderName} WorkOrderOperation:
{WorkOrderOperation} RequestId: {RequestId}"
```
```
211 ErrorCodes.
WorkOrderOperationNotExist
```
```
"The WorkOrderOperation does not exist. WorkOrder: {WorkOrderName} WorkOrderOperation:
{WorkOrderOperation} RequestId: {RequestId}"
```
```
252 ErrorCodes.
CollectRequestNotExist
```
```
"The RequestId does not exist. WorkOrder: {WorkOrderName} WorkOrderOperation: {WorkOrderOperation}
RequestId: {RequestId}"
```
## API usage example


CollectAfterValidationHandler request body

### {

"Input": {
"RequestId": "078f05f5-c845-4f99-9b06-381fd61884df",
"WorkOrderName": "PO_69BE56AD-3409-4D38-A1B2-C9D651BCC255_002_@[1]",
"WorkOrderOperationName": "0010",
"IsFailed": false,
"FailureReason": ""
}
}


# WorkOrderHeaderService (editing in progress)

## Summary

## Behavior

## Api

```
form Name Verb Endpoint
```
```
Collect POST /api/WorkOrderHeaderService/Execute
```
## Input

```
Name Type Mandatory Note
```
```
Name string yes
```
## Output

```
Name Type Note
```
```
Name string Unique Name: WorkOrderName will be mapped on the Name Property of NamedObject
```
```
Description string
```
```
ProductName string ID/Name in Material Master
```
```
ProductRevision string
```
```
ProductionOrderName string Weak Reference to Production Order
```
```
ProductionOrderTypeNa
me
```
```
string To be shown only in details
```
```
WorkOrderQuantity decimal Quantity to be produced
```
```
UOM string shown in the counter and quantity columns
```
```
WorkOrderPriority int Priority amongst multiple WO - by default order by Priority (WOs with same priority, it should be
displayed in ascending order of creation date)
```
```
Status WorkOrderStatus enum
```
```
QueuingQuantity decimal default value is equal to value of the WorkOrderQuantity
```
```
InProcessQuantity decimal
```
```
ProducedQuantity decimal
```
```
ActualStartTime datetime
```
```
ActualEndTime datetime
```
```
LocationName string
```
```
PlannedStartDate date coming from PO
```
```
PlannedEndDate date coming from PO
```
```
FirstWorkOrderOperati
on
```
```
Reference
```
```
LastWorkOrderOperati
on
```
```
Reference
```

CreatedOn date

CreatedBy string

LastChangedOn date

LastChangedBy string

WorkOrderTraces WorkOrderTrace

WorkOrderOperations WorkOrderOperation

```
WorkOrderOperation
```
```
Name
```
```
Type Note
```
Name string OperationName will be mapped on the Name Property of CO SubEntity

Description string

Sequence int B2MML may not have it, during import it will depend on the position in the sequence.
If no value is provided, will start from 10-20-30-40...
Unique in the PO, so, we cannot have duplicates.

OperationType string

WorkUnitName string This a simple string value and will be filled in from Factory Model App

Status WorkOrderOperation
Status

```
enum
```
QueuingQuantity decimal Default value 0 for all operations, except the first one. In this case the value'll be WorkOrderQuantity

ProducedQuantity decimal

ProcessTime decimal

ProcessTimeUOM string UOM is usually valid values - MVP we will use as it is reading the value in B2MML

```
It's shown in the Setup Time.
```
SetupTime decimal

SetupTimeUOM string UOM is usually valid values - MVP we will use as it is reading the value in B2MML

```
It's shown in the Setup Time.
```
OperationDefinition Reference

CollectContainer bool To be shown in WO Details WOOP Tab

```
Enable the Container Validation flow in the rest api and show the proper box on the UI screen
```
```
During TP Collect, in the flow, we need to check this value and verify if the container is passed as
input send error
```
StandardCollectionQua
ntity

```
decimal? It is used to fill in the Operator Terminal the collect tp value with a predefined value
```
PlannedStartDate coming from PO

PlannedEndDate date coming from PO

Milestone bool Enable the backflush message

ThroghputTag WorOrderOperationT
hroughputTag

```
NotCollectable,Manual,Automatic are the possible values.
```
```
NotCollectable=the operation cannot be collected directly
```
```
Manual=the operation can be collected by the user (UI)
```
```
Automatic=the operation can be collected automatically (not by the user)
```
WorkOrderOperationTra
ces

```
WorkOrderOperation
Trace
```
WorkOrderMaterials WorkOrderMaterial


```
WorkOrderMaterial
```
```
Name
```
```
Type Note
```
```
Name string It will be mapped on the Name Property of CO SubEntity
```
```
MaterialDefId ID Weak link to MaterialDef It cannot be mandatory because it will return by Material management
once the Allocate Material Flow is completed (successfully)
```
```
MaterialQuantityRequired decimal This may need to be updated in future depending upon use case
```
```
MaterialQuantityCons
umed
```
```
decimal This property stores the total quantity of material which we consumed till now.
```
```
MaterialUOM string shown in Quantity Required and Consumed
```
```
StoreLocation string Location where Material is stored
```
```
LineLocation string
```
WorkOrderTrace

```
Name Type Note
```
```
TransactionType TraceWorkOrderTypes Created
```
```
StatusChanged
```
```
PriorityChanged
```
```
Name id Guid
```
```
Timestamp datetime -
```
```
UserName string
```
```
Value string
```
```
PreviousValue string
```
WorkOrderOperationTrace

```
Property Name Data Type Note
```
```
TransactionType TraceOperationTypes Collect
```
```
Changed
```
```
Created
```
```
Name id Guid
```
```
Timestamp datetime
```
```
UserName string
```
```
Quantity decimal
```
```
From CounterTypes Empty
```
```
Queuing
```
```
Produced
```
```
To CounterTypes Empty
```
```
Queuing
```
```
Produced
```
```
Container string
```
Errors


```
Code Message Note
```
Messages

```
Type Destination Note
```
API usage example

<tnt-module-url>/api/WorkOrderHeaderService/Execute

### {

```
"Input": {
"WorkOrderName": "WO-Name"
},
```
```
"__RequestData": {
```
```
"__options": {}, "__paginationList": { "__get": "40", "__startingAt": 0 }, "__sortFields": [ { "__fieldName": "WorkOrderPriority", "__direction": "Ascending"
} ],
```
```
"WorkOrderHeader": { "Name": {}, "ProductName": {}, "ProductVersion": {}, "WorkOrderPriority": {}, "WorkOrderQuantity": {}, "QueuingQuantity": {}, "InPro
cessQuantity": {}, "ProducedQuantity": {}, "Status": {}, "LocationName": {}, "EndTime": {}, "WorkOrderOperations": { "Name": {}, "OperationTypeName":
{}, "WorkUnitName": {}, "QueuingQuantity": {}, "ProducedQuantity": {}, "WorkOrderMaterials": { "MaterialQuantityConsumed": {}, "MaterialQuantityRequire
d": {} } } } }
```
```
}
```

# TrackAndTrace Module Entities (2207 version)

## Model

## WorkOrderHeader:TrackedObject

```
Property
Name
```
```
Label Semantic
meaning
```
```
Data
Type
```
```
Size Persistent Default
value
```
```
Natural
Key
```
```
Mandatory Note
```
```
Name Work Order Unique name string 64 Yes user Unique Name: WorkOrderName will be mapped on the
Name Property of NamedObject
ProductName Product name string 64 user ID/Name in Material Master
ProductRevisi
on
```
```
Revision version string 16 user
```
```
ProductionOrd
erName
```
```
Production
Order
```
```
name string 64 user Weak Reference to Production Order
```
```
ProductionOrd
erTypeName
```
```
Type name string 64 "" To be shown only in details
```
```
WorkOrderQu
antity
```
```
Quantity quantity decimal user Quantity to be produced
```
```
UOM <not to be
shown>
```
```
unit string 32 user shown in the counter and quantity columns
```
```
WorkOrderPrio
rity
```
```
Priority priority int 0 Priority amongst multiple WO - by default order by
Priority (WOs with same priority, it should be displayed
in ascending order of creation date)
Status Status status WorkOrd
erStatus
```
```
255 Ready system
```
```
QueuingQuan
tity
```
```
Queuing quantity decimal NO system default value is equal to value of the WorkOrderQuantity
```
```
InProcessQu
antity
```
```
In-process quantity decimal NO 0
```
```
ProducedQua
ntity
```
```
Produced quantity decimal NO 0
```
```
ActualStartTimeActual Start start datetime 0
```

```
ActualEndTimeActual End end datetime 0
LocationName Location name string 64 ""
PlannedStartD
ate
```
```
Planned
Start Date
```
```
date date coming from PO
```
```
PlannedEndD
ate
```
```
Planned
End Date
```
```
date date coming from PO
```
```
FirstWorkOrd
erOperation
```
```
<not to be
shown>
```
```
Reference Referen
ce
LastWorkOrd
erOperation
```
```
<not to be
shown>
```
```
Reference Referen
ce
CreatedOn Creation
date
```
```
date date ""
```
```
CreatedBy user name string 255
LastChanged
On
```
```
Last
modificatio
n date
```
```
date date NO
```
```
LastChanged
By
```
```
user name string 255 NO
```
```
Workflow <not to be
shown>
```
```
Revisioned
Object
Reference
```
```
Reference 18 YES system The reference to revision object - This is a reference to a
specific revision, not to the revision base and implying
to the RoR.
```
WorkOrderOperation:NamedSubentity<WorkOrderHeader>

```
Property
Name
```
```
Label Semantic
meaning
```
```
Data
Type
```
```
Size Persistent Default
value
```
```
Natural
Key
```
```
Mandatory Note
```
```
Name Operation Unique Name string 64 Yes user OperationName will be mapped on the Name Property
of CO SubEntity
Sequence Sequence int user B2MML may not have it, during import it will depend on
the position in the sequence.
If no value is provided, will start from 10-20-30-40...
Unique in the PO, so, we cannot have duplicates.
OperationType OperationT
ype
```
```
name string 64
```
```
WorkUnitName Work Unit name string 64 This a simple string value and will be filled in from
Factory Model App
Status Status Int? Int? 255 NotReady
/Ready
```
```
system NotReady/ Ready / In Process / completed [ Suspend
feature is not planned but we'll handle with a flag field]
```
```
Default value is Ready for the operation with q>0,
NotReady otherwise.
QueuingQuantityQueuing quantity decimal NO system Default value 0 for all operations, except the first one. In
this case the value'll be WorkOrderQuantity
ProducedQuant
ity
```
```
Produced quantity decimal NO 0
```
```
ProcessTime Setup Timeduration decimal null
ProcessTimeUOM<not to be
shown>
```
```
unit string 32 null UOM is usually valid values - MVP we will use as it is
reading the value in B2MML
```
```
It's shown in the Setup Time.
SetupTime Setup Timeduration decimal null
SetupTimeUOM <not to be
shown>
```
```
unit string 32 null UOM is usually valid values - MVP we will use as it is
reading the value in B2MML
```
```
It's shown in the Setup Time.
OperationDefiniti
on
```
```
<not to be
shown>
```
```
Reference Reference
```
```
CollectContainer Enable
Container
```
```
bool bool false To be shown in WO Details WOOP Tab
```
```
Enable the Container Validation flow in the rest api and
show the proper box on the UI screen
```
```
During TP Collect, in the flow, we need to check this
value and verify if the container is passed as input send
error
StandardCollecti
onQuantity
```
```
<not be
shown>
```
```
decimal? decimal? It is used to fill in the Operator Terminal the collect tp
value with a predefined value
PlannedStartDa
te
```
```
DateTime coming from PO
```
```
PlannedEndDate Planned
End Date
```
```
date date coming from PO
```
```
Milestone Milestone bool bool Enable the backflush message
```

```
ThroghputTag <not to be
shown>
```
```
enable collect enum
```
```
(int
/string)
```
```
MAN NONE,MAN,AUTO are the possible values.
```
```
NONE=the operation cannot be collected directly
```
```
MAN=the operation can be collected by the user (UI)
```
```
AUTO=the operation can be collected automatically (not
by the user)
```
WorkOrderMaterial:NamedSubentity<WorkOrderOperation>

```
Property
Name
```
```
Label Semantic
meaning
```
```
Data
Type
```
```
Size Persistent Default
Value
```
```
Natural
Key
```
```
Mandatory Note
```
```
Name Material Unique Name string 64 Yes user It will be mapped on the Name Property of CO SubEntity
MaterialDefId
<not
shown>
```
```
Instance Id ID Weak link to MaterialDef It cannot be mandatory because
it will return by Material management once the Allocate
Material Flow is completed (successfully)
MaterialQuantit
yRequired
```
```
Required quantity decimal user This may need to be updated in future depending upon use
case
MaterialQuanti
tyConsumed
```
```
Consumedquantity decimal NO 0.00 This property stores the total quantity of material which we
consumed till now.
MaterialUOM <not to
be
shown>
```
```
unit string 32 user shown in Quantity Required and Consumed
```
```
StoreLocation Store
Location
```
```
Name string 64 "" Location where Material is stored
```
```
LineLocation Line
Location
```
```
Name string 64 "" Work Center - Case Assembly Bottom
```
OperationDefinition:RevisionedObject

```
Property Name Label Semantic meaning Data Type Size Default Value Natural Key Mandatory Note
```
```
AttachedDocuments<not be shown> name list of string they will be shown in a different way
Name Operation Definition name string 64 Yes user
Revision Revision version string 128 user
```
WorkFlow:RevisionedObject

```
Property Name Label Semantic meaning Data Type Size Default Value Natural Key Mandatory Note
```
```
Name <not be shown> Product name string 64 Yes
Revision <not be shown> version string 64
FirstStep_id <not be shown> ID id
```
Step:NamedSubentityObject<WorkFlow>

```
Property Name Label Semantic meaning Data Type Size Default Value Natural Key Mandatory Note
```
```
Name <not be shown> name string 64 Yes
```
PathSelector:NamedSubentityObject<Step>

```
Property Name Label Semantic meaning Data Type Size Default Value Natural Key Mandatory Note
```
```
Name <not be shown> name string 64 Yes
PathToUseId <not be shown> id id
```
Path:NamedSubentityObject<Step>

```
This is a copy of current version of operation definition from OM for being able to access quickly the AttachedDocuments in the Operator
Terminal page.
```

```
Property Name Label Semantic meaning Data Type Size Default Value Natural Key Mandatory Note
```
```
Name <not be shown> name string 64 Yes
ToStepId <not be shown> id id
```
WorkOrderTrace:NamedSubentityObject<WorkOrderHeader>

```
Property Name Label Semantic meaning Data Type Size Default value Natural Key Mandatory Note
```
```
TransactionType Transaction Name type enum
```
```
(int/string)
```
```
255 yes
```
```
Name Transaction ID unique name id yes yes Guid
Timestamp Timestamp date datetime yes -
UserName User user string 64 ""
Value Value value string 255 yes (develop in progress for 2207)
PreviousValue Previous Value value string 255 yes (develop in progress for 2207)
```
WorkOrderOperationTrace:NamedSubentityObject<WorkOrderOperation>

```
Property Name Label Semantic meaning Data Type Size Default value Natural Key Mandatory Note
```
```
TransactionType Transaction Name type enum
```
```
(int/string)
```
```
255 yes
```
```
Name Transaction ID unique name id yes yes Guid
Timestamp Timestamp date datetime yes
UserName User user string 64 ""
Quantity Quantity quantity decimal yes
From from name enum
```
```
(int/string)
```
```
255 yes (develop in progress for 2207)
```
```
To to name enum
```
```
(int/string)
```
```
255 yes (develop in progress for 2207)
```
```
Container Container name string 64 2204
```
Settings:NamedObject

```
Property Name Label Semantic meaning Data Type Size Default value Natural Key Mandatory Note
```
```
Name <not be shown> name string 64 yes yes
Value <not be shown> value 128 0
ValueType <not be shown> type enum
```
```
(string/int)
```
```
255
```
WorkOrderStatus:Enum

```
Value Label Flag
```
```
Ready Ready
```
```
NotReady Not Ready
```
```
InProcess In Process
```
```
Completed Completed
```
```
This entity contains the configuration variables.
```
```
ShiftProducedToNextOperationQueue ("Partial" or "Full") to identify the collection mode (default is "Partial")
```
```
ValidationResponseTimeout (in milliseconds) is the maximum waiting time for the container validation (default is 120000)
```

WorkOrderOperationCollection:NamedSubentityObject<WorkOrderOperation>

```
Property Name Label Semantic meaning Data Type Size Default value Natural Key Mandatory Note
```
```
Name <not be shown> unique name string yes yes
Timestamp <not be shown> date datetime yes
OriginalRequest <not be shown> Original Request string nvarchar max The whole original request
ResponseMessage <not be shown> error string 255 2204
```
WorkOrderOperationCounters:NamedSubentityObject<WorkOrderOperation>

```
Property Name Label Semantic meaning Data Type Size Default value Natural Key Mandatory Note
```
```
Name <not be shown> unique name id yes yes Guid
TransactionID <not be shown> Transaction ID id yes
DeltaQueued <not be shown> quantity decimal yes
DeltaProduced <not be shown> quantity decimal yes
DeltaScrapped <not be shown> quantity decimal yes
CreatedOn Creation date date date yes
CreatedBy user name string 255 yes
```
Comments:NamedSubentityObject<WorkOrderHeader>

```
Property Name Label Semantic meaning Data Type Size Default value Natural Key Mandatory Note
```
```
OperationName Operation Name name of operation string 64 no
Comment Comment comment for operation or work order string 1000000 yes
Timestamp Timestamp date datetime yes -
UserName User user string 64 "" yes
Name Name name string 128 yes yes Guid
```
```
It has to save all the Collect TP Request Information when the operation needs the container validation.
```
```
One 1 for each WO OP!!
```

# TrackAndTrace AM Module Entities (Draft Version)

The development is inprogress for these changes and that is why it is marked as draft version.

## WorkOrderHeader: NamedObject

```
Property Name Label Property Type Data Type Length Default Natural Key Mandatory Note
```
```
Customer Customer Customer Name nvarchar255 No Extended Property
```
```
ProjectManager Project Manager Name nvarchar 255 No Extended Property
```
```
PurchaseOrder Purchase Order nvarchar 255 No Extended Property
```
```
ResponsibleEngineerResponsible Engineer Name nvarchar 255 No Extended Property
```
```
ContractDeliveryDateContract Delivery Date dateTimeOffSet(7) No Extended Property
```

# Work Order and Operation Status (after MVP - to be aligned)

Work Order and Operation status are tracked according to the following diagram:

## Work Order Status

```
Status Description Label
```
```
Ready The Work Order has been launched successfully the work order is ready to be traced and tracked. Ready
```
```
InProcess The Work Order is in process, when first operation is set up or TP collected and last Operation not completed. In Process
```
```
Completed The Work Order is completed, all operations are completed. Completed
```
```
Suspended The Work Order is suspended, next operations are not set ready when previous operations are completed.
```
## Work Order Operation Status

```
Status Description Label
```
```
Pending The Work Order Operation is not ready to be traced and tracked.
```
```
When launching a Work Order, the first Work Order Operations are Ready, all the other ones are set Pending.
```
```
Pending
```
```
Ready The Work Order Operation is ready to be traced and tracked.
```
```
Only the first Work Order Operations are Ready when launching a Work Order. The next operations are becoming ready as
soon as a quantity has been collected as produced by the previous operations.
```
```
Ready
```
```
InProcess The Work Order Operation is in process, when at least one throughput has been collected on the operation or operation has
been "started" manually.
```
```
In
Process
```
```
Completed The Work Order Operation is completed, all quantities have been collected. Comple
ted
```
```
CollectingIn
Progress
```
```
The current Work Ortder Operation has a container validation in progress triggered by a collect tp. Collecti
ng
```


# TrackAndTrace Module Messages

TNT Component publish or consume the following messages:

```
Messages Publisher Service Way Repository
```
```
WorkOrderCreated TNT Publisher TrackAndTraceMessage
```
```
WorkOrderCompleted TNT Publisher TrackAndTraceMessage
```
```
CreateWorkOrders OM Consumer OrderManagementMessage
```
```
BackflushMessage TNT Publisher TrackAndTraceMessage
```
```
ContainerValidationRequest TNT Publisher TrackAndTraceMessage
```
```
ContainerValidationResult External - Integration Consumer TrackAndTraceMessage
```
```
WorkOrderUpdated TNT Publisher TrackAndTraceMessage
```

# Backflush Message

## Summary

This message is sent at the end of a collect throughput execution. It will be consumed by ERP.

## Input

```
Name Type Note
```
```
WorkOrderName string The name of the work order
```
```
ProductName string The name of the product
```
```
ProductVersion string The version of the product
```
```
ProductionOrderName string The name of the Production Order (order management)
```
```
OperationName string The name of the operation
```
```
WorkCenterName string The name of the work center
```
```
OperationType string The type of the operation
```
```
From string The counter source of the collect
```
```
To string The counter destination of the collect
```
```
Quantity decimal The moved quantity
```
```
Unit string The UoM related to the quantity
```
```
TransactionTimestamp datetime The timestamp of the collect
```
```
TransactionId string The unique id of the collect (guid)
```
```
UserName string The user that has executed the collect
```
```
MaterialConsumed BackflushMaterial List List of consumed materials
```
## BackflushMaterial:SubentityObjectOf<BackflushMessage>

```
Name Type Note
```
```
MaterialName string material name
```
```
MaterialQuantityConsumed decimal consumed quantity
```
```
MaterialUnit string related UoM
```

# ContainerValidationRequest

## Summary

Request a container validation

## Input

```
Name Type Note
```
```
RequestId string Guid to identify the request (it is the transaction id)
```
```
WorkOrderName string
```
```
WorkOrderOperationName string
```
```
ContainerName string
```
```
Quantity decimal
```
```
UOM string
```
```
WorkCenter string
```
```
ProductName string
```
```
ProductRevision string
```

# ContainerValidationResult

## Summary

Response to ContainerValidationRequest

## Input

```
Name Type Notes
```
```
RequestId string Guid which identifies the validation request
```
```
WorkOrderName string
```
```
WorkOrderOperationName string
```
```
IsFailed bool
```
```
FailureReason string
```

# WorkOrderCompleted

## Summary

Message needed once a WO is completed and need to put the PO in state Finalized. It will be consumed by Order Management

## Input

```
Name Type Note
```
```
ProductionOrderName string The name of the production order destination
```
```
WorkOrderName string The name of the completed work order
```

# WorkOrderCreated

## Summary

Message needed once a WO is created and need to put the PO in state Launched. In addition to that this message is responsible for adding one entry in
WorkOrderInfo entity for the corresponding created WorkOrder. It will be consumed by Order Management

## Input

```
Name Type Note
```
```
ProductionOrderName string
```
```
WorkOrderCreatedDataItems List of WorkOrderCreatedData
```
```
IsFailed bool
```
```
FailureReason string
```
## WorkOrderCreatedData

```
Name Type
```
```
ProductionOrderName string
```
```
WorkOrderName string
```

# WorkOrderUpdated

## Summary

This message is published from TNT whenever work order fields are updated. Then it is consumed by Order Management to update the same set of fields
in Production Order entity.

## Input

```
Name Type Note
```
```
ProductionOrderName string
```
```
Quantity string
```
```
Priority string
```
```
PlannedStartDate string
```
```
PlannedEndDate string
```
```
UpdatedWorkOrderOperations List Of WorkOrderOperationUpdated
```
```
IsFailed bool
```
```
FailureReason string
```
Note : Currently only priority change is supported and for updating rest of the fields the implementation is in progress.

## WorkOrderOperationUpdated

```
Name Type
```
```
Name string
```
```
PlannedStartDate string
```
```
PlannedEndDate string
```

# TrackAndTrace Module Use Cases


# Collection Modes

## Partial Mode



Full Mode

Arch



# TrackAndTrace Launch and Collect Flows

## Launch

## Collect



# TrackAndTrace Module Permissions

```
Name Effect Note Home Card Relevant
```
```
View Work
Orders
```
```
Allows to visualize Work Orders on the main view, and
Visualize their details. Track And Trace Card
View Work Order Tile
```
```
View
Operations
```
```
Allows to visualize Work Order Operations.
```
```
Allow access to Operator Terminal.
```
```
Allow to view Comment Associated to Work Order and
Operations.
```
```
This need to allow access in view BUT also allow access to
collect TP functionality
```
```
User flow
```
```
Track And Trace Card
View Work Order
Operation Tile
```
```
Throughput Col
lection
```
```
Allow to invoke Collect Throughput
```
```
Container
Validation
```
```
Allow to finalize the Container Validation Flow No UI
```
```
Manage
Operations
```
```
Allow to Add\Delete\Edit an Operations
```
```
Add Comment Allow to Add a new Comment
```
## AM

```
Name Effect Note Home Card Relevant
```
```
View Work Orders In addition to the OOB one, it allows to visualize Work Order Group and Grouped Operations
```
```
View Operations In addition to the OOB one, it allows to visualize Grouped Operations
```
```
Start Operation Put an Operation in InProcess State
Put a grouped operation in InProcess State
```

# AM TrackAndTrace Module Messages


# AM TrackAndTrace Module


# AM WorkOrderInquiryService

## Summary

It returns additional properties from existing workorderinquiryservice which are intercepted by AM module.

Fields include: Customer, ContractDeliveryDate, Project Manger, Responsible Engineer, Purchase Order

## Endpoint

```
Name Verb Endpoint
```
```
WorkOrderInquiryService POST api/WorkOrderInquiryService/Execute
```
## Input

QueryOptions

```
Name Type Mandatory Note
```
```
SortingLst list of SortingOptions no
```
```
Skip int no
```
```
Limit int no
```
SortingOptions

```
Name Type Mandatory Note
```
```
SortProperty string no
```
```
SortDirection string no "ASC"|"DESC"
```
## API usage example

<tnt-module-url>/api/WorkOrderInquiryService/Execute

Request body


### {

```
"Input": {
"WorkOrderName": "",
"ProductName": "",
"Status": "",
"LocationName": "",
"Skip": 0 ,
"Limit": 5 ,
"DefaultSortOrder": "Y",
"QueryOptions": {
"SortingLst": [[{ "Value": { "SortProperty": "responsibleengineer", "SortDirection": "DESC" }, "ListItemAction": "Add" }],
"skip": 0 ,
"limit": 9
}
},
"__RequestData": {
```
```
"DataLst": {
"name": {},
"customer":{},
"contractdeliverydate":{},
"description": {},
"productName": {},
"productRevision": {},
"workOrderPriority": {},
"status": {},
"workOrderQuantity": {},
"queuingQuantity": {},
"inProcessQuantity": {},
"producedQuantity": {},
"actualStartTime": {},
"actualEndTime": {},
"locationName": {},
"createdOn": {},
"createdBy": {},
"lastChangedOn": {},
"lastChangedBy": {},
"productionOrderName": {},
"productionOrderTypeName": {},
"uom": {},
"projectmanager":{},
"responsibleengineer":{},
"plannedStartDate": {},
"plannedEndDate": {},
"purchaseorder":{}
}
}
}
```
Responde body example


### {

"dataLst": {
"__Value": "[{\"name\":\"Arjita12345\",\"customer\":\"Customer1\",\"contractdeliverydate\":\"1/1/0001 12:00:00 AM +00:00\",\"description\":null,\"
productionOrderName\":\"Arjita12345\",\"productionOrderTypeName\":\"A\",\"Status\":\"Ready\",\"productName\":\"Samsung\",\"productRevision\":\"rev1.
0\",\"workOrderQuantity\":\"80\",\"queuingQuantity\":\"80.00000\",\"inProcessQuantity\":\"0.00000\",\"producedQuantity\":\"0.00000\",\"workOrderPriority\":
\"1\",\"locationName\":\"Location2\",\"actualStartTime\":\"1/1/0001 12:00:00 AM +00:00\",\"actualEndTime\":\"1/1/0001 12:00:00 AM +00:00\",\"uom\":\"
each\",\"createdBy\":\"service-account-modmom\",\"createdOn\":\"6/22/2022 11:07:35 PM +00:00\",\"lastChangedBy\":\"service-account-modmom\",\"
lastChangedOn\":\"6/22/2022 11:07:35 PM +00:00\",\"Status_Localize\":\"Ready\",\"projectmanager\":\"ProjectManager1\",\"responsibleengineer\":\"
ResponsibleEngineer1\",\"purchaseorder\":\"PurchaseOrder1\",\"plannedstartdate\":\"1/1/0001 12:00:00 AM +00:00\",\"plannedenddate\":\"1/1/0001 12:
00:00 AM +00:00\"},{\"name\":\"ArjitaTesting_12345\",\"customer\":\"Customer1\",\"contractdeliverydate\":\"1/1/0001 12:00:00 AM +00:00\",\"description\":
null,\"productionOrderName\":\"ArjitaTesting_12345\",\"productionOrderTypeName\":\"A\",\"Status\":\"Ready\",\"productName\":\"Samsung\",\"
productRevision\":\"rev1.0\",\"workOrderQuantity\":\"80\",\"queuingQuantity\":\"80.00000\",\"inProcessQuantity\":\"0.00000\",\"producedQuantity\":\"
0.00000\",\"workOrderPriority\":\"1\",\"locationName\":\"Location2\",\"actualStartTime\":\"1/1/0001 12:00:00 AM +00:00\",\"actualEndTime\":\"1/1/0001 12:
00:00 AM +00:00\",\"uom\":\"each\",\"createdBy\":\"service-account-modmom\",\"createdOn\":\"6/23/2022 10:39:02 AM +00:00\",\"lastChangedBy\":\"
service-account-modmom\",\"lastChangedOn\":\"6/23/2022 10:39:02 AM +00:00\",\"Status_Localize\":\"Ready\",\"projectmanager\":\"ProjectManager1\",\"
responsibleengineer\":\"ResponsibleEngineer1\",\"purchaseorder\":\"PurchaseOrder1\",\"plannedstartdate\":\"1/1/0001 12:00:00 AM +00:00\",\"
plannedenddate\":\"1/1/0001 12:00:00 AM +00:00\"},{\"name\":\"Test_12\",\"customer\":\"Customer1\",\"contractdeliverydate\":\"1/1/0001 12:00:
00 AM +00:00\",\"description\":null,\"productionOrderName\":\"Test_12\",\"productionOrderTypeName\":\"A\",\"Status\":\"Ready\",\"productName\":\"
Samsung\",\"productRevision\":\"rev1.0\",\"workOrderQuantity\":\"80\",\"queuingQuantity\":\"80.00000\",\"inProcessQuantity\":\"0.00000\",\"
producedQuantity\":\"0.00000\",\"workOrderPriority\":\"1\",\"locationName\":\"Location2\",\"actualStartTime\":\"1/1/0001 12:00:00 AM +00:00\",\"
actualEndTime\":\"1/1/0001 12:00:00 AM +00:00\",\"uom\":\"each\",\"createdBy\":\"service-account-modmom\",\"createdOn\":\"6/23/2022 3:31:04 AM +00:
00\",\"lastChangedBy\":\"service-account-modmom\",\"lastChangedOn\":\"6/23/2022 3:31:04 AM +00:00\",\"Status_Localize\":\"Ready\",\"
projectmanager\":\"ProjectManager1\",\"responsibleengineer\":\"ResponsibleEngineer1\",\"purchaseorder\":\"PurchaseOrder1\",\"plannedstartdate\":\"1/1
/0001 12:00:00 AM +00:00\",\"plannedenddate\":\"1/1/0001 12:00:00 AM +00:00\"},{\"name\":\"PO_44970\",\"customer\":\"Z1\",\"contractdeliverydate\":
null,\"description\":null,\"productionOrderName\":\"PO_44970\",\"productionOrderTypeName\":\"A\",\"Status\":\"Ready\",\"productName\":\"Samsung006\",
\"productRevision\":\"A\",\"workOrderQuantity\":\"1\",\"queuingQuantity\":\"1.00000\",\"inProcessQuantity\":\"0.00000\",\"producedQuantity\":\"0.00000\",\"
workOrderPriority\":\"2\",\"locationName\":\"Location1_26576\",\"actualStartTime\":\"1/1/0001 12:00:00 AM +00:00\",\"actualEndTime\":\"1/1/0001 12:00:
00 AM +00:00\",\"uom\":\"each\",\"createdBy\":\"service-account-modmom\",\"createdOn\":\"6/8/2022 11:10:42 AM +00:00\",\"lastChangedBy\":\"service-
account-modmom\",\"lastChangedOn\":\"6/8/2022 11:10:42 AM +00:00\",\"Status_Localize\":\"Ready\",\"projectmanager\":\"3M\",\"responsibleengineer\":
\"PO3\",\"purchaseorder\":\"po3\",\"plannedstartdate\":\"1/1/0001 12:00:00 AM +00:00\",\"plannedenddate\":\"1/1/0001 12:00:00 AM +00:00\"},{\"name\":\"
Test14\",\"customer\":\"C1\",\"contractdeliverydate\":\"6/30/2022 9:46:13 AM +00:00\",\"description\":null,\"productionOrderName\":\"Test14\",\"
productionOrderTypeName\":\"A\",\"Status\":\"Ready\",\"productName\":\"Samsung\",\"productRevision\":\"rev1.0\",\"workOrderQuantity\":\"80\",\"
queuingQuantity\":\"80.00000\",\"inProcessQuantity\":\"0.00000\",\"producedQuantity\":\"0.00000\",\"workOrderPriority\":\"1\",\"locationName\":\"
Location2\",\"actualStartTime\":\"5/31/2022 9:46:13 AM +00:00\",\"actualEndTime\":\"6/30/2022 9:46:13 AM +00:00\",\"uom\":\"each\",\"createdBy\":\"
service-account-modmom\",\"createdOn\":\"5/31/2022 9:46:13 AM +00:00\",\"lastChangedBy\":\"service-account-modmom\",\"lastChangedOn\":\"5/31
/2022 9:46:13 AM +00:00\",\"Status_Localize\":\"Ready\",\"projectmanager\":\"M1\",\"responsibleengineer\":\"E1\",\"purchaseorder\":\"o1\",\"
plannedstartdate\":\"5/22/2022 9:46:13 AM +00:00\",\"plannedenddate\":\"6/25/2022 9:46:13 AM +00:00\"},{\"name\":\"Mat-test3a\",\"customer\":\"B1\",\"
contractdeliverydate\":\"6/30/2022 9:46:13 AM +00:00\",\"description\":null,\"productionOrderName\":\"Mat-test3a\",\"productionOrderTypeName\":\"A\",\"
Status\":\"Ready\",\"productName\":\"Samsung\",\"productRevision\":\"rev1.0\",\"workOrderQuantity\":\"80\",\"queuingQuantity\":\"80.00000\",\"
inProcessQuantity\":\"0.00000\",\"producedQuantity\":\"0.00000\",\"workOrderPriority\":\"1\",\"locationName\":\"Location2\",\"actualStartTime\":\"5/31
/2022 9:46:13 AM +00:00\",\"actualEndTime\":\"6/30/2022 9:46:13 AM +00:00\",\"uom\":\"each\",\"createdBy\":\"service-account-modmom\",\"createdOn\":
\"5/31/2022 9:46:13 AM +00:00\",\"lastChangedBy\":\"service-account-modmom\",\"lastChangedOn\":\"6/6/2022 9:40:33 AM +00:00\",\"Status_Localize\":
\"Ready\",\"projectmanager\":\"M2\",\"responsibleengineer\":\"2E\",\"purchaseorder\":\"wo2\",\"plannedstartdate\":\"5/22/2022 9:46:13 AM +00:00\",\"
plannedenddate\":\"6/25/2022 9:46:13 AM +00:00\"}]"
},
"id": "0gJ3c8w7a011iXKHhW"
}


# AM TrackAndTrace Module APIs


# Add Comment From Grouped Operations

## Summary

This Endpoint provides the functionality to add a comment under a grouped operations.

## Endpoint

```
Name Verb Endpoint
```
```
AddCommentFromGroupedOperation POST /api/AddCommentFromGroupedOperation/Execute
```
## Input

```
Name Type Mandatory Note
```
```
WorkOrderGroup String yes
```
```
GroupedOperation String Yes
```
```
Comment String yes
```
```
IsWorkOrderComment boolean Yes If this flat is true then comment will be add under grouped work order else it will be add under grouped operations
```
## Output

No output required

## Errors

For error cases such as invalid WorkOrderGroup, GroupedOperation,Comment reference or missing mandatory parameters, platform behavior is
leveraged.

## API usage example

Request body example

{
"Input":
{
"WorkOrderGroup": "WO1",
"Comment":"test comment",
"GroupedOperation":"OP1",

"IsWorkOrderComment" :"True"

}

}


# Access Management Module


# PermissionGroups file

```
File description
Workspace Management
Merging policies
Consistency checks
```
## File description

Each module must provide permission groups data by means of 'MOMPermissionGroups.json' file.

The file location is the Code directory of the module and must be published (copied) to the target folder specifying the "Copy local, if newer" option in the
Visual Studio properties section.

```
Sample PermissionGroups content
```
### [{

```
"Name": "ViewProductionOrder",
"ShortName": "View Production Order",
"Permissions": [
{
"Resource": "OrderManagement.ProductionOrderHeader",
"ResourceType": "endpoint:model",
"Grants": "Read"
},
{
"Resource": "OrderManagement.FillComboProductionOrderInquiryService",
"ResourceType": "endpoint:inquiry",
"Grants": "Invoke"
},
{
"Resource": "OrderManagement.ProductionOrderInquiryServiceProductionOrderInquiryService",
"ResourceType": "endpoint:inquiry",
"Grants": "Invoke"
}]
},
{
"Name": "ChangeProductionOrderPriority",
"ShortName": "Change Production Order Priority",
"Permissions": [
{
"Resource": "OrderManagement.ChangePOPriorityService",
"ResourceType": "endpoint:service",
"Grants": "Invoke"
}]
}]
```
Resource is composed as: 'ModuleName'.'EndpointName'

Available Resource Types:

```
endpoint:service
endpoint:inquiry
endpoint:model
endpoint:revisionedmodel
```
Available grants (depending on specific resource types):

```
Create
Update
Read
Delete
Invoke
Revise
```
```
Resource Type Possible Grants
```

```
endpoint:service Invoke, Read
```
```
endpoint:inquiry Invoke, Read
```
```
endpoint:model Create, Update, Read, Delete
```
```
endpoint:revisionedmodel Create, Update, Read, Delete, Revise
```
Workspace Management

A part from the default file, we apply the "@" pattern similar to other Work Space files, in the form:

```
MOMPermissionGroups@[*].json
```
Merging policies

During the startup of the module instance, all the MOMPermissionGroups files are loaded from the disk and merged together in a single logical tree,
starting from the workspace files on top of the base MOMPermissionGroups.json and following the dependencies tree specified in the module.settings.json
manifest.

The resulting structure is calculated as the following:

```
The additional permission groups are added as is.
The overridden permission groups are entirely replaced, following the dependencies tree order (the workspace overrides the base one).
```
The permissions, referring to the resources of a single module, are then merged together into the User Management service, using the name of the
permission group as the key. This gives the possibility to define a "logical" permission group across different modules, importing the contributions of each
one. Note that, in this case, resources are defined per module, so no conflict is expected in the resulting permission group. In any case, if a permission is
defined for the same resource coming from different permission groups, the grants are merged together adding the two sets.

Consistency checks

During the startup phase of any modules, the content of the files 'MOMPermissionGroups.json' is published to the UserManagement service following the
'Merging Policies' chapter.
When UserManagement module receives the message, it checks whether the defined permissions match the available securable objects from the model
definition.

UserManagement module shall produce warning logs in the following cases, without giving errors or without stopping the remaining import process:

```
Some permission groups contain permissions that are associated to a not existing securable object in the specific model. (No such securable,
TODO: define the log entry.)
Some existing securable objects are not configured in any permission group. (Orphan securable found, TODO: define the log entry).
Some permissions in the permission group are defined for another model. Behavior: skip with warning.
Some permissions have a resource type that is inconsistent with the resource type of the related endpoint. Behavior: skip with warning.
```
It is due to the module developer, when the MOMPermissionGroups*.json are changed, to check the logs in order to spot any missing, unexpected or
inconsistent configuration.


# Access Management API


# Fetch unassigned User(s) for a specific Group (still in dev)

## Summary

## Endpoint

```
Name Verb Endpoint
```
```
GetUnAssignedUsersForGroup POST api/GetUnAssignedUsersForGroup/Execute
```
## Input

```
Name Type Mandatory Note
```
```
GroupName string yes
```
```
Skip string no
```
```
Take int no
```
## Output

"DataLst":{}

## API usage example

<om-module-url>/api/WorkOrderInquiryService/Execute

```
Request Body
```
### {

```
"Input": {
"GroupName": "Administrators"
},
"__RequestData": {
"DataLst": {}
}
}
```

# Fetch Unassigned User Groups for a specific Role

## Summary

It allows User to view un assigned User Groups for a specific Role. User can also search for specific User Group which is not assigned to this role.

## Endpoint

```
Name Verb Endpoint
```
```
GetUnAssignedUserGroupsForRole POST api/GetUnAssignedUserGroupsForRole/Execute
```
## Input

```
Name Type Mandatory Note
```
```
RoleName string yes
```
```
Skip int no
```
```
Limit int no
```
```
UserGroupName string no
```
## Output

"DataLst":{}

## API usage example

Request body example

For default load

{
"Input": {

"RoleName":"Administrator"

### },

"__RequestData": {
"DataLst": {}
}
}


# Get User Assigned PermissionGroups

## Summary

This api retrieves all the relevant permissiongroups that are mapped to roles to which a given user is assigned.

## Endpoint

```
Name Verb Endpoint
```
```
GetUserAssignedPermissionGroups POST api/GetUserAssignedPermissionGroups/Execute
```
## Input

```
Name Type Mandatory Note
```
```
UserId string yes
```
## Output

"PermissionGroups":{}

## API usage example

Request body example

For default load

{
"Input": {
"UserId": "31b9d021-f2dc-4046-93ac-1787e89e65f6"
},
"__RequestData": {
"PermissionGroups": {}
}
}


# Import Users Service

## Summary

Import users from an external system and persists them as MOMUsers.

## Endpoint

The API is exposed through the endpoint:

```
api/ImportUsers/Execute
```
## Input

The input is general purpose, independent from the Identity Provider that provides users data.

```
Name Type Mandatory Notes
```
```
IdpKind Enum of type IdpKind yes By now the only value is Keycloak. It is defined to allow future extensions of the User JSON Object.
```
```
Users string yes JSON array of User JSON Objects as described in section below.
```
## IdpKind Enum

```
Value
```
```
Keycloak
```
## User JSON Object

```
Name Type Mandatory Notes
```
```
UserId string yes It must be unique, as required by OpenID Connect specification (https://openid.net/specs/openid-connect-core-1_0.
html#IDToken).
If users are imported from Keycloak it will contain a GUID.
```
```
UserName string yes
```
```
UserType string yes Allowed values must match UserTypes enum values ('UserAccount', 'ServiceAccount')
```
```
FirstName string only for
AccountUser
```
```
LastName string only for
AccountUser
```
```
Email string only for
AccountUser
```
## Output

```
Name Type Notes
```
```
NotImportedUsers List<string> Each element is a NotImportedUser JSON Object as described in section below.
```
## NotImportedUser JSON Object

```
Name Type Notes
```
```
User User JSON Object The input User JSON Object that has been rejected.
```
```
ErrorMessage string Reason for rejection. This message is not localized.
```
## Errors


In case the import partially succeeds (ie, some users data is not correct), API result is successful (200) and NotImportedUsers output is populated
accordingly.
In case some users are already imported in the system, then their data will not be considered.

It is highly recommended to request NotImportedUsers as response parameter in order to verify if any user was rejected.

API usage example

```
Import UserAccount user example
```
### {

```
"Input": {
"IdpKind": "Keycloak",
"Users": "[{\"UserId\":\"1\", \"UserName\": \"User1\", \"FirstName\": \"A\", \"LastName\": \"B\", \"
Email\": \"A.B@C.com\", \"UserType\": \"UserAccount\"}]"
},
"__RequestData": {
"NotImportedUsers":{}
}
}
```
```
Import multiple users example
```
### {

```
"Input": {
"IdpKind": "Keycloak",
"Users": "[{\"UserId\":\"1\", \"UserName\": \"User1\", \"FirstName\": \"A\", \"LastName\": \"B\", \"
Email\": \"A.B@C.com\", \"UserType\": \"UserAccount\"}, {\"UserId\":\"2\", \"UserName\": \"User2\", \"
FirstName\": \"D\", \"LastName\": \"E\", \"Email\": \"D.E@C.com\", \"UserType\": \"UserAccount\"}, {\"UserId\":
\"3\", \"UserName\": \"User3\", \"UserType\": \"ServiceAccount\"}]"
},
"__RequestData": {
"NotImportedUsers":{}
}
}
```
```
ImportUsers response in case of complete success
```
### {

```
"notImportedUsers": {},
"id": "0PICNPJao011bXqM6P"
}
```
```
ImportUsers response in case of partial success
```
### {

```
"notImportedUsers": {
"__Value": [
"{\"User\":{\"UserId\":\"1\",\"UserName\":\"User1\",\"UserType\":\"UserAccount\",\"FirstName\":null,
\"LastName\":\"B\",\"Email\":\"A.B@C.com\"},\"ErrorMessage\":\"First Name is mandatory for user account\"}",
"{\"User\":{\"UserId\":\"2\",\"UserName\":\"User2\",\"UserType\":\"FakeAccount\",\"FirstName\":\"
D\",\"LastName\":\"E\",\"Email\":\"D.E@C.com\"},\"ErrorMessage\":\"unknown user type\"}",
"{\"User\":{\"UserId\":\"3\",\"UserName\":null,\"UserType\":\"ServiceAccount\",\"FirstName\":null,\"
LastName\":null,\"Email\":null},\"ErrorMessage\":\"User name is mandatory\"}"
]
},
"id": "0PIDWDArJ011bXqM6P"
}
```
Notes:


The User field in the request json body is a json string as well, for this reason before the " character, it is necessary the escape character \
When we import an user that has the same UserName (but different UserId) of a user already present, the import modifies (only) the UserId field
of the user already present.


# Role Maintenance Service

Basic Role Maintenance Service - Create\Update\Delete


# Role Management

## Summary

This module allows the user to add, view, and delete roles in the system.

## Input

The MOMRole class inherits from NamedObject as described in the wiki: User Model Entities - RevMOM - MES Wiki (siemens.com)

```
Name Type Mandatory Note
```
```
Name String Yes The name of the role.
```
```
Description String No The description of the role.
```
The MOMRole Metamodel utilises the default Platform Maintenance Service for Create\Read\Update\Delete operations.

MOMRole Metamodel has these other properties: MOMUsers, MOMUserGroups and Permissions, which are lists of appropriate entities. Instances of each
entity can be added to the corresponding container properties of the MOMRole entity, based on UI context and security level, interactively.

## Output

None.

## Errors

The default Platform Maintenance Service handles the errors during a failure of any CRUD operation.

## Class Diagram

## Flow



# UserGroup Maintenance Service

Basic UserGroup Maintenance Service - Create\Update\Delete


# User Maintenance Service

Basic User Maintenance Service - Create\Update\Delete


# Access Management Entities

## MOMUser: NamedObject

See also https://www.keycloak.org/docs-api/16.1/rest-api/index.html#_userrepresentation for Keycloak related user information.

```
Property
Name
```
```
Label Property
Type
```
```
Natural
Key
```
```
Inherited Mandatory Note Mappings
```
```
Id Id InstanceId Yes Primary key
Name User string Yes Yes Yes From OIDC spec id Length must not exceed 255 chars,
so current implementation for NamedObject fits well.
```
```
Identity Provider assigned Unique ID.
Ex:
```
```
Keycloak "id": GUID string
```
```
Found in Token: "sub" claim (see Secu
rity Layer-OIDCIDTokendefinition)
Description Descriptionstring Yes no mapping
UserName string Length: 256 KeyCloak "username"
FirstName string Length: 256 Keycloak: "firstName"
```
```
Found in token: "name" claim or in
"given_name"
LastName string Length: 256 Keycloak: "lastName"
```
```
Found in token: "name" claim or in
"family_name"
Email string Length: 256 Keycloak:email
```
```
Found in token: "email" claim
UserType Enum -
UserTypes
```
```
Default value: UserAccount
```
```
MOMPrefere
nce
```
```
MOMPrefe
renceId
```
```
references
to
MOMPrefere
nce
```
## MOMUserGroup: NamedObject

```
Property
Name
```
```
Label Property Type Natural
Key
```
```
Inherited Mandatory Note
```
```
Name User Group string Yes Yes Yes
```
```
Description Description string Yes
```
```
Entries <shown as Users in the
specific tab>
```
```
List Of MOMUser to be developed with the feature "Associate User
To User Group"
```
```
Groups <not to be shown> List of
MOMUserGroup
```
```
Out of scope MVP 2210
```
## MOMRole: NamedObject

```
Property
Name
```
```
Label Property Type Natural
Key
```
```
Inherited Mandatory Note
```
```
Name Role string Yes Yes Yes
```
```
Description Description string Yes
```
```
MOMUsers <shown as Users in the specific
tab>
```
```
List of MOMUser Out of scope for MVP Dec 2021
```
```
It's called MOMUser and not User in order to avoid any possible issue during the table creation (reserved word)
```
```
MOMUserGroup is derived from NamedObject for now. NamedObjectGroups will be developed in the near future. At that time, Some of the
implementation code can be removed from the concrete MOMUserGroup object.
```

```
UserGroups
Lst
```
```
<shown as Groups in the specific
tab>
```
```
List of
MOMUserGroup
```
```
PermissionG
roups
```
```
<shown as Permission Groups in
the specific tab>
```
```
List of
MOMPermissionGro
up
```
```
This is a list of references,
MOMPermissionGroup is not a subentity of
MOMRole
```
MOMPermissionGroup: NamedObject

```
Property
Name
```
```
Label Property Type Natural
Key
```
```
Inherited Mandatory Note
```
```
Name Permission string Yes Yes Yes Standard property being a
NamedObject
```
```
ShortName Short Name string Yes
```
```
MOMPermissions <shown as Permissions in the specific
tab>
```
```
List of
MOMPermission
```
```
Subentity Pattern
```
Example : view PermissionGroups file

MOMPermission: NamedSubentity<MOMPermissionGroup>

```
Property
Name
```
```
Label Property Type Natural
Key
```
```
Inherited Mandatory Note
```
```
(Name) fully qualified name. Example:
```
```
OrderManagement.ImportB2MMLProductionProcess
OrderManagement.POHeader
MaterialManagement.Material
```
```
MOMSecura
ble
```
```
Securable Reference to
MOMSecurable
```
```
When specified, MOMPermission Name is equal to the concatenation
of MOMAppSecurables name and MOMSecurable name
```
```
SecurableTy
pe
```
```
SecurableTy
peType
```
```
Enum -
SecurableTypes
```
```
Yes
```
```
Grants Grants Enum - Actions Yes
```
MOMAppSecurables: NamedObject

```
Property Name Label Property Type Natural Key Inherited Mandatory Note
```
```
MOMSecurables List of MOMSecurable Subentity Pattern
```
```
MOMPermissionGroups List of references to MOMPermissionGroup
```
MOMSecurable: NamedSubentity<MOMAppSecurables>

```
Property Name Label Property Type Natural Key Inherited Mandatory Note
```
```
SecurableType Enum - EndpointSecurableType Yes EndpointSecurableType is defined in platform.
```
```
Possible values:
```
```
Service
Inquiry
Model
RevisionedModel
```
MOMColumnPreference: NamedSubentity<MOMPreferenceTable>

```
Property
Name
```
```
Label Property
Type
```
```
Natural
Key
```
```
Inherited Mandatory Note
```
```
State State string Length: 5000
```

MOMPreferenceTable: NamedSubentity<MOMPreference>

```
Property
Name
```
```
Label Property
Type
```
```
Natural
Key
```
```
Inherited Mandatory Note
```
```
MOMColumnPr
eference
```
```
MOMColumnP
reference
```
```
List of
references to
MOMColumnPr
eference
```
MOMPreference: NamedObject

```
Property
Name
```
```
Label Property
Type
```
```
Natural
Key
```
```
Inherited Mandatory Note
```
```
MOMPreference
Tables
```
```
MOMPreferen
ceTables
```
```
List of
references to
MOMPreference
Table
```
UserTypes

```
Value
```
```
UserAccount
```
```
ServiceAccount
```
Actions: Enum Flags

```
Value
```
```
Read
```
```
Create
```
```
Update
```
```
Delete
```
```
Revise
```
```
Invoke
```
SecurableTypes

```
Value
```
```
endpoint:service,
```
```
endpoint:inquiry
```
```
endpoint:model
```
```
endpoint:revisionedModel
```

# Access Management Use Cases


# Creating AD/LDAP Federation Via Keycloak

In order to federate Keycloak with AD you have to create a User Federation Provider.

For the integration with MOM User Management service the Ldap User Federation Provider is required.

## Configuring the LDAP User Federation Provider

To create a User Federation Provider in Keycloak, from the Administration Panel, navigate to User Federation and then Add User Federation
Provider.

This will open a new page, the below properties are needed to properly configure the LDAP User Federation:

```
Parameter Description
```
```
Enabled Set to ON
```
```
Import User Set to ON
```
```
Edit Mode Set to UNSYNCED
```
```
Vendor Set to Active Directory
```
```
User name
LDAP attribute
```
```
Set to sAMAccountName
```
### RN LDAP

```
Attribute
```
```
Set to cn
```
### UUID LDAP

```
Attribute
```
```
Set to objectGUID
```
```
User Object
Classes
```
```
Set to person, organizationalPerson, user
```
```
Connection URL Specify ldap protocol and IP address.
```
```
Example: ldap://ipaddress
```
```
User DN Full DN of LDAP tree where you users are. This DN is the parent of LDAP users.
```
```
Example: 'ou=users,dc=example,dc=com' assuming that your typical user will have DN like "uid=john,ou=users,dc=example,
dc=com'
```
```
Custom User
LDAP Filter
```
```
Additional LDAP Filter for filtering searched users. Leave this empty if you do not need additional filter. Make sure that it starts with
'(' and ends with ')'
```
```
Search scope Set to Subtree
```
```
Bind Type Set to simple
```
```
Bind DN This is the DN of LDAP admin, which will be used by Keycloak to access the LDAP server.
```
```
Bind Credential Password of LDAP admin. This field is able to obtain its value from vault, use ${vault.ID} format
```
```
Note
```
```
The UNSYNCED parameter allows you to import and synchronize users from Active Directory, but changes on users imported
on Keycloak are not propagated to Active Directory configuration.
```
```
For further info on AD LDAP Syntax Filters, read the following TechNet article:
```
```
Active Directory: LDAP Syntax Filters - TechNet Articles - United States (English) - TechNet Wiki (microsoft.com)
```
```
below an example of LDAP filter for filtering searched user:
```
```
(&(objectCategory=user)(memberOf=CN=OpenShift21_AllowedUsers,OU=CLU12, OU=Groups_OpenShift,OU=Groups,
OU=MYDOM,DC=MYDOM,DC=TST))
```


### 1.

### 2.

### 3.

### 1.

### 2.

# Importing Users in Access Management

This page describes the workflow to import Active Directory users and service accounts from Keycloak to MOM Access Management app.

The users to be imported to MOM Access Management App are the users imported into Keycloak from Active Directory and Keycloak service accounts..

## Prerequisites

The importing scenario must be composed of:

```
A Keycloak instance federated with Active Directory (via ldap user federation provider). A dedicated realm, e.g MOMRealm should be already
created and configured. A OIDC Client (e.g. modmom) should be already configured
```
Reference to other doc

```
A working MOM Service scenario
A Windows 10 machine where the script Import-MomUsers.ps1 will be executed.
```
For further information on Import-MomUsers.ps1 script, see Importing Users in Access Management Via Script

## Step 1: Create a dedicated Realm for MOM

If not already present, create a dedicated realm for MOM

```
Log in to the Keycloak Admin Console as Administrator
Create a Realm for MOM (example: modmom, this is the realm used on our openshift environment, e.g modmom13) if not already present
In the General tab of the newly-created Realm, set the following:
Enabled = ON
Endpoints: OpenID Endpoint Configuration
SAML 2.0 Identity Provider Metadata
```
## Step 2: Create the admin user in the MOM Realm

```
Log in to the Keycloak Admin Console as Administrator
Create the MOM Admin user (admin) in the Realm for MOM and do the following
```
```
In Do
```
```
Details tab Set User Enabled To ON
```
```
Required User Action should be empty
```
```
Role Mappings tab Set for Realm Roles
```
```
Assigned Roles: default-roles-momrealm
```
```
Set for Client Roles = realm-management
```
```
Assigned Roles: view-users
```
```
For more information on Keycloak, see official documentation https://www.keycloak.org/docs/latest/server_admin/#user-management
```
## Step 3: Create the admin in the MOMService

```
Do not use the master realm to manage users and applications in your organization.
```
```
Reserve the usage of master realm for Super Admins in order to create and manage the realms in your system.
```
```
Since the Master Realm will be used to create other Realms and to avoid security issues, we suggest you to create a dedicated Realm for
Opcenter Modular Manufacturing.
```
```
For more information about Keycloak, refer the official documentation: https://www.keycloak.org/docs/latest/server_admin/#admin-console
```

### 1.

### 2.

### 3.

The admin user is created in the MOMService, that is, via an SQL script.

Step 4: Create the OIDC client for the authentication service (modmom)

If not already present, create and configure the OIDC Client for the scenario (e.g modmom). The Client id of the client is the value of the CLIENT_ID
environment variable of the authentication service.

Reference to other doc

Step 5 (optional) Create the OIDC client for importing the user in the MOM realm

```
Log in to the Keycloak Admin Console as Administrator
In the MOM Realm, create the client for importing the MOM users in the MOM application. (Example: client name/client id: um-importer)
Set the following:
```
```
In Do
```
```
Settings tab Set:
```
```
Enabled=ON
Client protocol=openid-connect
access type=confidential
```
```
direct access grants enabled=on
service accounts enabled=on
```
```
Service Account Roles tab Set for Realm Roles :
```
```
Assigned Roles: default-roles-momrealm
Set for Client Roles = realm-management
Assigned Roles: view-users
```
```
The OIDC client just created has a built-in service account
```
```
For more information on Keycloak, see official documentation https://www.keycloak.org/docs/latest/server_admin/#_clients
```
If not already present, create a client scope whose name is the same of the CLIENT_ID environment variable of the authentication service (e.g: modmom)

```
In Do
```
```
Settings tab Set:
```
```
Client protocol=openid-connect
```
```
Display On Consent screen = ON
```
```
Include In Token Scope = ON
```
For the client scope just created, create a mapper (e.g. modmom) of type audience and set the following:

```
In Do
```
```
Settings
tab
```
```
Set:
```
```
Included Client Audience: *The Client id of the client set in the CLIENT_ID environment variable of the authentication service (e.g
modmom)
```
```
Add to ID token = ON
```
```
Add to access token = ON
```
```
This procedure is performed by the KeyCloak Administrator
```

### 1.

### 2.

### 3.

### 1.

### 2.

*prerequisite: the client of the field "Included Client Audience" should be already present and configured in the current realm (e.g modmom)

Get back to the OIDC client (e.g. um-importer)

```
In Do
```
```
Client Scopes tab Add for Default Client Scopes, the client scope just created in the step before
```
Step 6: Further configuration for OpenShift

Perform the following configuration, if not already present.

For the authservice StatefulSet, select Environment Tab and add the following environment variable

```
name:USERID_CLAIM
```
```
value: preferred_username
```
This step is necessary if we want to run the ImportUsers.ps1 script using the client credentials only.

Step 7: Importing Active Directory users in Keycloak

If not already done before, the KeyCloak Administrator logs in to the Keycloak Admin Console as Administrator, performs the federation with Active
Directory and imports the AD Users in the MOMRealm.

Step 8: Run the Import-MOMUsers.ps1 script in Setup mode with user credentials

```
Log in on a Windows machine.
Run the script Import-MOMUsers.ps1 in Setup mode as displayed below:
```
```
Import-MOMUsers -Setup -Type=User
```
```
The following information is required:
```
```
Required information Description
```
```
Keycloak Base Url schema - domain name - port, of the Keycloak endpoint Example.: http://ipaddress:port
```
```
Keycloak Realm Realm name, Example: MOMRealm
```
```
Keycloak User MOM Super User, Example: MOMAdmin
```
```
password Password of the Keycloak user
```
```
ModularMOM - Access
Management ImportUsers endpoint
```
```
https://kubernetescluster:5000/api/ImportUsers/Execute
```
```
Keycloak Client ID OIDC client id: e.g um-importer
```
```
Client Secret The secret related to the Keycloak Client. It can be found in the Clients credentials tab of
Keycloak Admin Console for the related Client
```
Step 9: Add Permissions on MOM Service for the Service account used for further Import

operations

```
The MOM Configurator adds the required set of permissions for the on the MOM Service. Note that at least the permission for Importing users
must be added.
The service account associated to the Keycloak client will be used for subsequent users import from Keycloak to the MOM Service.
```
```
This procedure is performed by a MOM Configurator with MOMAdmin credentials.
```
```
For more information regarding the script, see Importing Users in Access Management Via Script
```

### 1.

### 2.

### 3.

Step 10: Run the Import-MOMUsers.ps1 script in Setup mode with client credentials

```
Log in on a Windows machine.
Run the script Import-MOMUsers.ps1 in Setup mode as displayed below:
```
```
Import-MOMUsers -Setup -Type=Client
```
```
The following information is required:
```
```
Required info Description
```
```
Keycloak Base Url schema - domain name - port, of the Keycloak endpoint Example.: http://ipaddress:port
```
```
Keycloak Realm Realm name, Example: MOMRealm
```
```
Keycloak Client ID OIDC client id: e.g um-importer
```
```
Client Secret The secret related to the Keycloak Client. It can be found in the Clients credentials tab of
Keycloak Admin Console for the related Client
```
```
ModularMOM - Access
Management ImportUsers endpoint
```
```
https://kubernetescluster:5000/api/ImportUsers/Execute
```
Step 11: Run the Import-MOMUsers.ps1 script without parameters

From now on, once the MOM Configurator logged in on the Windows machine with its own Windows account, he/she is able to import the users on the
MOM Services either by running the Import-MOMUsers.ps1 script without parameters or by clicking it.

```
This procedure is performed by a MOM Configurator with a client credentials.
```
```
For more information regarding the script, see Importing Users in Access Management Via Script
```

### 1.

### 2.

# Importing Users in Access Management Via Script

Import-MomUsers.ps1 is a Windows PowerShell script which imports Active Directory (AD) users and Service Accounts from KeyCloak into User
Management app.

The script is self documented so further info can be retrieved with the Get-Help cmdlet as described below.

## Remarks

```
The script has been tested on a Windows 10 environment with Windows PowerShell 5.1.
Import-MomUsers.ps1 is versioned in the M1_UserManagement repository located in: \M1_UserManagement\Scripts
The script will be provided in a secure way, such as via Secufex because it is not included in the Access Management app deployment.
The script stores the service account credentials on a local file in a secure way.
This information can be retrieved by the same user on the same machine. A different user cannot decrypt the sensitive data stored in that file.
```
## Creating the Import-MomUsers.ps1 help

```
Run this script to show the Import-MomUsers.ps1 help:
```
```
Show the script help
```
```
Get-Help .\Import-MomUsers.ps1
```
```
Run this script to show also the detailed help that contains the sections Parameters and Examples:
```
```
Show the scrip help with details
```
```
Get-Help .\Import-MomUsers.ps1 -Detailed
```
## Result

As a result of the procedures above, the following code is an example of printing the complete script documentation including the parameter descriptions
and related examples of usage:

```
Get-Help .\Import-MomUsers.ps1 -Detailed
```
### NAME

```
C:\git_source\M1_UserManagement\Scripts\Import-MomUsers.ps1
```
### SYNOPSIS

```
ModularMOM 2.0 Import-MOMUsers
Siemens (c) 2021
```
### SYNTAX

```
C:\git_source\M1_UserManagement\Scripts\Import-MomUsers.ps1 [-Setup] [[-Type] <String>] [[-KeycloakBaseUrl]
<String>] [[-KeycloakRealm] <String>] [[-ImportUsersUrl] <String>] [[-User]
<String>] [[-Password] <SecureString>] [[-ClientId] <String>] [[-ClientSecret] <SecureString>]
[<CommonParameters>]
```
### DESCRIPTION

```
This script imports the users from a Keycloak realm to the User Management app
The users to be imported are:
a) the Active Directory user accounts federated in Keycloak with a ldap provider
b) the built-in service accounts associated to the OIDC clients
```
```
Recommended usage:
```
```
if the file is moved on another machine, the sensitive data cannot be decrypted.
```

-Step 1
An administrator runs the script in Setup mode using the MOMAdmin credentials where -Type=User. In this
step the Service Accounts are imported in the Access Management app. The client credentials of the client are
also required

Example: the password and the client secret will be asked in Interactive mode
.\Import-MomUsers.ps1 -Setup -Type User -KeycloakBaseUrl https://kchostname:443 -KeycloakRealm
testrealm -ImportUsersUrl https://momumhostname/api/ImportUsers/Execute -User MOMAdmin -ClientId clientname

-Step2
An administrator runs the script in Setup mode using the Service Account imported in the step 1 (-
Type=Client).
All the parameters needed for the command execution are stored in a local file in a secure way.

Example: the client secret will be asked in interactive mode
.\Import-MomUsers.ps1 -Setup -Type Client -KeycloakBaseUrl https://kchostname:443 -KeycloakRealm
testrealm -ImportUsersUrl https://momumhostname/api/ImportUsers/Execute -ClientId clientname

-Step 3
The administrator that has executed the step2 runs the script either without specifying any parameter
or by clicking the script:
The script will import the AD users and the service accounts from KeyCloak in Access Management app
using the values passed in the step 2

Example: all needed parameters of the step 2 are retrieved from the local file previously created in
step 2
.\Import-MomUsers.ps1

Notes:

For security reasons, when the script runs with the pair parameters "-Setup -Type User", the user
credentials and configuration parameters are not stored in the local file.
When the script runs with the '-Setup -Type Client' it stores the configuration parameters in the local
file Import.config.json
The sensitive data are stored encrypted in that file and can be decrypted only by the same user from the
same machine
When the script runs without the -Setup switch parameter, it retrieves the configuration values from Import.
config.json.
In this case the local file is required.

Output:

The script sets the $LASTEXITCODE to one of these values:
0: success
1: partial success (some users has not been imported, in that case the script shows a warning message for
each not imported user)
-1: error (the users cannot be imported)

### PARAMETERS

-Setup [<SwitchParameter>]
In Setup mode all the script parameters should be provided. For any missing parameter, the related
value will be asked to the user
In Setup mode, with Type=Client, all the data will be stored in a local file. Sensitive data will be
stored in a secure way
In not Setup mode, the script parameters will be retrieved from the local file.

-Type <String>
If Type=User, the User Account credentials a read from the parameters $User and $Password. The Client
credentials are read from the parameters $ClientId and $ClientSecret
If Type=Client, the Client credentials are read from the parameters $ClientId and $ClientSecret

-KeycloakBaseUrl <String>
Keycloak end point
Expected format 'http(s)://fqdn:port'

-KeycloakRealm <String>
Keycloak realm name

-ImportUsersUrl <String>


Full URL of the Access Management ImportUsers endpoint
Expected format 'http(s)://fqdn:port/path'

-User <String>
User name
This parameter is required in case of parameter Type=User

-Password <SecureString>
Password
This parameter is required in case of parameter Type=User

-ClientId <String>
The Client name for Client Credential flow

-ClientSecret <SecureString>
The Client Secret for Client Credential flow

<CommonParameters>
This cmdlet supports the common parameters: Verbose, Debug,
ErrorAction, ErrorVariable, WarningAction, WarningVariable,
OutBuffer, PipelineVariable, and OutVariable. For more information, see
about_CommonParameters (https:/go.microsoft.com/fwlink/?LinkID=113216).

### -------------------------- EXAMPLE 1 --------------------------

PS C:\>.\Import-MomUsers.ps1 -Setup -Type User

### -------------------------- EXAMPLE 2 --------------------------

PS C:\>.\Import-MomUsers.ps1 -Setup -Type User -KeycloakBaseUrl https://kchostname:443 -KeycloakRealm
testrealm -ImportUsersUrl https://momumhostname/api/ImportUsers/Execute -User MOMAdmin -ClientId clientname

### -------------------------- EXAMPLE 3 --------------------------

PS C:\>.\Import-MomUsers.ps1 -Setup -Type Client

### -------------------------- EXAMPLE 4 --------------------------

PS C:\>.\Import-MomUsers.ps1 -Setup -Type Client -KeycloakBaseUrl https://kchostname:443 -KeycloakRealm
testrealm -ImportUsersUrl https://momumhostname/api/ImportUsers/Execute -ClientId clientname

### -------------------------- EXAMPLE 5 --------------------------

PS C:\>.\Import-MomUsers.ps1

### REMARKS


To see the examples, type: "get-help C:\git_source\M1_UserManagement\Scripts\Import-MomUsers.ps1 -examples".
For more information, type: "get-help C:\git_source\M1_UserManagement\Scripts\Import-MomUsers.ps1 -
detailed".
For technical information, type: "get-help C:\git_source\M1_UserManagement\Scripts\Import-MomUsers.ps1 -
full".


# Import Users in Access Management

## Non Functional requirements

```
Check the language script provided, given the environment used Bardini, Matteo
Scope for MVP December:
windows environment
it is preferred to use powershell: script is less flexible than console application regarding different environment, but we don't
want to ship more complex things (executables) to the customer
add tooling in Access Management repo, script will be provided to customer as zip and shared with SecuFex and will be
documented
RestAPI ImportUsers Payload is a json string that will be de-serialized into the Access Management backend.
How many users we need to test? > 100 (110)
```
## Functional requirements

Multiple import (calling the script multiple time):

```
For each user already imported the MOMUser is updated? Limitation for MVP Dec 2021: If some relevant field for us changes, do not update
MOMUser will not deleted as a sync mechanism (all the imported users are persistent)
Limitation for MVP Dec 2021, Imported users are AD only, no local keycloak user is automatically imported (see keycloak model details to
distinguish)
Return LASTEXITCODE for the script must be defined and documented: proposal 0: succeeded, -1: error, 1: partially succeeded.
ImportUser RestAPI should return: 200 in case it is succeeded and partially succeeded, 5xx in case of errors.
```
Errors:

```
For each not imported user in the set we need to notify the invoking user the reason. The only thing to check is the required fields needed during
the import batch. In case some fields are not specified for a user the creation of the MOMUser is skipped and added to the list of errors.
```
## Security

Document how to use the script in a secure way. Check needed privilege (least possible) to access keycloak GetUsers API (with Keycloak token). If
possible avoid to require usage of admin user, document the least needed privilege.

How the script can store configuration options?

The proposal is this:

1) The administrator perform a setup on the target machine, starting the script with a special option (-Setup $true) and specifying all the parameters
needed by the script such as:

```
The base URL for Keycloak service (https://fqdn:port)
The Keycloak realm to be used.
The Keycloak user and password to be used to query the list of users
The ClientId and ClientSecret of a registered Keycloak client to get a valid token for ModMom RestAPI call
The full URL for the UM Service (ImportUsers endpoint)
```
2) The script will store the configuration in a local file in the filesystem, encrypting all the secret information handled by SecureString and outputted them in
the file as encrypted strings (DPAPI based).

3) After this configuration phase a more generic User can start the script with a single click (without any parameters to be specified)

4) The script will read and decrypt all the sensitive data from the configuration file and will use them to perform the Import functionality,


Notes:

```
The user permissions on the configuration file can be changed by the Administrator and a security note should be put in the documentation to
warn on this (and to let the administrator properly decides the read permissions on this file for the local machine).
The configuration file is not portable across different machines: it is encrypted for the local user only, this enhance the security albeit it will limit the
usability by design.
```
First time use

In order to import users with a single click script, some conditions must be met:

```
At the beginning, the administrator should set up both Keycloak and UM module to authorize the ImportUsers call
The script has to take the previous point into account in order to let the administrator to perform the first import.
A specific client in Keycloak, with its corresponding service account, can be configured (and imported into UM) to perform the following
ImportUsers calls.
```

Script Interface

Stage 1 (setup):

Import-MOMUsers -Setup -Type=User


Input:

```
Enpoints
User Credentials for MOMAdmin
```
This stage will NOT STORE configurations (with credentials)

Perform the first import

Stage 2 (hardening):

Import-MOMUsers -Setup -Type=Client

Input:

```
Endpoints
Client Credentials
```
This will store the configuration to the local filesystem with encrypted credentials (DPAPI, current user scope).

Perform the import with client credentials

Stage 3 (runtime, oneclick):

Import-MOMUsers

Input: none

This will read the inputs from the configuration file decrypting the credentials (DPAPI, current user scope)

Perform the import (sync between Keycloak and ModularMOM)

Notes:

```
For security reasons and to enforce the hardening stage, the first setup stage will not store the configuration entered by the user using the
MOMAdmin credentials. This sensitive credentials are just asked each time you run the script in this mode using secure strings parameters for the
password (not displayed).
In order to give the MOMAdmin a way to store the configuration and run the script afterwards without entering all the parameters again, the -
Setup -Type=Client has to be performed at least one time, after configuring the Service Account inside UM, manually assigning to it the proper
permissions.
After this hardening stage, the script can be run without additional parameters by the Windows user account which has set up the secrets. Other
Windows accounts needed to run the script (other people) must perform the Stage 2, knowing the configuration parameters such as client_id and
client_secret, this is an additional security measure to restrict the access to the allowed people on the local machine.
```
Keycloak Model Details

Keycloak has its own Rest API on the administration interface.

You can browse users from the users endpoint documented here: https://www.keycloak.org/docs-api/15.0/rest-api/index.html#_users_resource

GET /{realm}/users/

Example of how a federated AD user looks like:


### 1.

### 2.

### [

### {

```
"id": "57cc2bb5-f4b3-4e21-a86c-75446696bb4b",
"createdTimestamp": 1634745727245,
"username": "itr00295",
"enabled": true,
"totp": false,
"emailVerified": false,
"firstName": "Piergiorgio",
"lastName": "Trubini",
"email": "piergiorgio.trubini@siemens.com",
"federationLink": "593a05b1-2ab3-43ae-b421-cddabfab5779",
"attributes": {
"LDAP_ENTRY_DN": [
"CN=Trubini Piergiorgio,OU=Users,OU=SWQA,DC=SWQA,DC=TST"
],
"locale": [
"it-IT"
],
"LDAP_ID": [
"e95c0e59-9631-4138-ae0c-5611a14c8e4d"
],
"createTimestamp": [
"20091012083453.0Z"
],
"modifyTimestamp": [
"20211022000220.0Z"
]
},
"disableableCredentialTypes": [],
"requiredActions": [],
"notBefore": 0,
"access": {
"manageGroupMembership": true,
"view": true,
"mapRoles": true,
"impersonate": true,
"manage": true
}
},
...
]
```
From here you can find:

```
In the attribute section you can see that there is a LDAP_ID entry, this means that the user has been imported from the LDAP User federation
under Keycloak.
Client Crendentials service accounts, on the other hands, are NOT exposed on this endpoint by default.
```
However, for the second point, the service account is indeed a user for the Keycloak model but only exposed knowing the specific Id:

GET /{realm}/users/{id}

for example


### {

```
"id": "dc02b605-5787-4e46-8a76-cf1e9c522fc8",
"createdTimestamp": 1633686237054,
"username": "service-account-erp-importer",
"enabled": true,
"totp": false,
"emailVerified": false,
"disableableCredentialTypes": [],
"requiredActions": [],
"notBefore": 0,
"access": {
"manageGroupMembership": true,
"view": true,
"mapRoles": true,
"impersonate": true,
"manage": true
}
}
```
Service account for the client is just a special kind of user account with username service-account-CLIENT_ID. You can perform user operations on
this account as if it was a regular user.

When you specify the filter username=service-account (each username which contains the substring "service-account):

GET /{realm}/users?username=service-account

All the service accounts (client enabled for client credentials flow) are then returned.

The information mapped from the service accounts to MOMUser entity are:

```
id Name
username UserName
any UserType=ServiceAccount
```
A special validation is then performed on the backend to ignore other properties required for the UserAccount profile information (FirsName,LastName,
Email,etc).

Important: note that the default returned user number is 100: the script has to take this into account using (first, max query option parameters to handle
pagination properly).


# Access Management Permissions

```
Name Effect Note Home Card
Relevant
```
```
View Roles Allows to visualize MOM Roles (and the associated Permissions
and User Groups)
```
```
Add and Remove button has to be disabled if only
this permission is set Access
Management Card
View MOM Role
Tile
```
```
View User
Groups
```
```
Allows to visualize MOM User Groups (and the associated Users) Add and Remove button has to be disabled if only
this permission is set Access
Management Card
View MOM User
Group Tile
```
```
Manage Roles Allows to add and remove MOM Roles
```
```
Allow to attach and detach User Groups to Role - it need to allow to
view User Groups too
```
```
Allow to attach and detach Permissions to Role - it need to allow to
view Permissions too
```
```
Manage User
Groups
```
```
Allows to add and remove MOM User Groups
```
```
Allow to assign and remove Users to User Group - it need to allow
to view Users too
```
```
Manage Users Allow to import users from external system as we as manipulate
them via CRUD REST API
```
```
For the import procedure
```
```
For Adding, Delete, Update User
```
```
Required for user provisioning (SaaS)
```

# Access Management - RBAC Overview

## Summary

This page is to give a first overview on the RBAC model developed for Modular MOM 2.

## Details

Check for Access Management Entities for COs.

Permission Groups are a collection of Permissions, fine grain access control. For now Permissions Group will be provided exclusively by the modules, see
PermissionGroups file.

The only action the User (Admin in the picture) can do is to associate permission groups to a role.

## Practical Example

## Example 1


In this example User A and User B have the capability to do the following tasks:

```
View WO OP
Collect TP
View WO
View PO
```
Example 2

In this example User A has the capability to do the following tasks:

```
View WO OP
Collect TP
View WO
View PO
```
Instead, User B and User C can only perform this two tasks:

```
View WO OP
Collect TP
```

# Sample Data


# Sample data for Modular MOM 2.X - 202112 release




# Acronyms Definitions

## Explanation

```
Acronym Definition
```
```
MOM manufacturing operations management
```
```
MES manufacturing execution system
```
```
DSL domain specific language
```
```
CDO configurable data object
```
```
WO Work Order
```
```
WOOP Work Order Operation
```
```
PO Production Order
```
```
MD Material Definition
```
```
OP Production Order Operation
```
```
API application programming interface
```
```
IaaS infrastructure as a service
```
```
PaaS platform as a service
```
```
SaaS software as a service
```
```
CI continuous integration
```
```
CD continuous deployment
```
```
Module Definition
```
```
OM Order Management
```
```
TnT Track and Trace
```
```
MM Material Modeling
```
```
FM Factory Modeling
```
```
AM Access Management
```

### 1.

```
a.
i.
```
```
ii.
```
```
b.
i.
ii.
iii.
iv.
v.
vi.
2.
a.
i.
ii.
b.
c.
i.
ii.
3.
a.
b.
c.
4.
5.
a.
b.
c.
d.
6.
a.
b.
i.
7.
```
# App Creation Workshop

This page is WIP just to help us defining the first use cases

Participants

```
Daniel
Felix
Jan
```
App

```
App: Factory Modelling
Interface:
Find Factory Site by PO
Clarification of behavior if no or no matching site is found: automatic generation of site?
Find material by PO (material=final material)
Same behavior as mentioned above
Idea:
Out of the box: ISA 95 / IEC 62264 template
Site is mandatory, other hierarchical levels are optional
Levels can be renamed but have static identifier
Hierarchical levels can reference other levels
Later iterations: Definition of new Work Center Types and Work Unit Types should be possible
Provide Configuration Option for Factory Model
App: Order Management
Import Production Order
NFR: Performance - Import Time (massive import?)
NFR: Load
Show production orders to user
Change PO Property
NFR: Performance - Time to Load, Time to Safe
NFR: Load
App: Launch
Create Work Order
Auto Launch per default
Launching Rules, default: PO
App: Dispatch
App: Track & Trace
WO history
Operation History
Material History
Equipment History
App: Material Management
Cr + Fn both instantiate based on master data, concept could be applied in MMOM
Check semantics of container model vs. foundation tracking units
Recursivity of container?
App: Operations Management
```
```
General NFR: Time to install and configure to be productive
```
UI Requirements:

```
For table / list views:
User decides which properties are shown in which order
Admin user can decide which properties are filterable
Admin user can decide which properties are sortable
Admin user can customize property header
```
Questions to be answered:

```
Definition of trackable unit
recursivity of container - mapping
Automatic generation of sites?
```
Appendix:

Factory Model:



# Business Process Flow & Epics

Epics outlining the first Modules can be found on MyPolarion:

https://mypolarion.industrysoftware.automation.siemens.com/polarion/#/project/MOM/workitem?id=MOM-17088

This link will direct to PL Portal and shows the overall business process for the first use cases, within PL Portal you have the chance to drill down on the
tasks to see more details of the process:

https://plportal/#/maa/businessProcessMap/?activityId=1176&snapshotId=1349&navId=20


### 1.

### 2.

### 3.

### 4.

### 5.

### 6.

### 7.

### 8.

# Non Functional Requirements

## Process [to be defined and agreed on between PrM, PO and Q]

```
PrM defines NFRs in Wiki
PO and PrM agreen on maturity of NFRs
PrM transfer mature NFRs to Polarion
PO link NFRs to (new) Features
PO define Test Cases based on NFRs
(Features that are NFR relevant will need a corresponding Tag) Only linking instead of Tag?
Feature Teams are responsible for implementing test cases
NFRs are tracked against Test Cases in a transparent way (Dashboard)
```
Quality Attributes


NFR to be defined at Feature level or need to have separate NFR features?


# NFR Dashboards

Dashboards are maintained by Adams , Michael in sumo logic:

Execution on OpenShift modmom-18 (reference environment for on-prem):

```
Material Management
Order Management
Track and Trace
```
Execution on XCR SaaS Production-like scenario:

tbd

## Material Management

```
ID Title Threshold Loop Sumo Query
```
```
23643 NFR - Import of 2000
materials should be done
< 5 min
```
```
300s 5 _sourceCategory="jmeter/modmom_test" OR _sourceCategory="dev/nfr/modmom"
| parse "*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*" as timestamp,elapsed,label,responseCode,
responseMessage,threadName,dataType,success,failureMessage,bytes,sentBytes,grpThread,allThreads,
URL,latency,idleTime,connect,testcase,build
| where URL contains "<NFR Environment>/materialmodelingapi/api
/ProcessB2MMLMaterialObject/Execute"
```
```
| where timestamp != "timeStamp"
| where testcase == "23643"
| where allThreads == "1"
| where success == "true"
| where responseCode == "200"
| (elapsed/1000/60) as elapsed
```
```
| formatDate(_messageTime, "MM/dd/yyyy HH:mm:ss:SSS") as time
| avg(elapsed) as elapsed
```
## Order Management

```
ID Title Threshold
on UI
```
```
Threshold
on
backend
```
```
Loop Sumo Query
```
```
2639
6
```
```
[NFR] Import of 10 PO with 10 operations 10 Operation
Definition and 4 materials each
```
```
na 40s 10 _sourceCategory="jmeter/modmom_test" OR
_sourceCategory="dev/nfr/modmom"
| parse "*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*" as
timestamp,elapsed,label,responseCode,
responseMessage,threadName,dataType,success,
failureMessage,bytes,sentBytes,grpThread,allThreads,
URL,latency,idleTime,connect,testcase,build
| where URL contains "<NFR Environment>/
api/ProcessB2MMLProductionScheduleObject/Execute"
```
```
| where timestamp != "timeStamp"
| where testcase == "26396"
| where allThreads == "1"
| where success == "true"
| where responseCode == "200"
| (elapsed/1000) as elapsed
```
```
| formatDate(_messageTime, "MM/dd/yyyy
HH:mm:ss:SSS") as time
| avg(elapsed) as elapsed
```

```
2663
0
```
```
[NFR] View Production Orders one request with < 1.5 sec 1.5s 1s 10 _sourceCategory="jmeter/modmom_test" OR
_sourceCategory="dev/nfr/modmom"
| parse "*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*" as
timestamp,elapsed,label,responseCode,
responseMessage,threadName,dataType,success,
failureMessage,bytes,sentBytes,grpThread,allThreads,
URL,latency,idleTime,connect,testcase,build
| where URL contains "<NFR Environment>/
ordermanagementapi/api/ProductionOrderInquiryService
/Execute"
```
```
| where timestamp != "timeStamp"
| where testcase == "26630"
| where allThreads == "1"
| where success == "true"
| where responseCode == "200"
| (elapsed/1000) as elapsed
```
```
| formatDate(_messageTime, "MM/dd/yyyy
HH:mm:ss:SSS") as time
| avg(elapsed) as elapsed
```
```
3102
6
```
```
[NFR] Import 10 PO Header with 10 Operations and 4
Materials each with automatic launch true0 PO Header
with 10 Operations and 4 Materials each with automatic
launch true
```
```
na 60s 10 _sourceCategory="jmeter/modmom_test" OR
_sourceCategory="dev/nfr/modmom"
| parse "*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*" as
timestamp,elapsed,label,responseCode,
responseMessage,threadName,dataType,success,
failureMessage,bytes,sentBytes,grpThread,allThreads,
URL,latency,idleTime,connect,testcase,build
```
```
| where label contains "Transaction: Import
and Launch execution: <NFR Environment>"
| where timestamp != "timeStamp"
| where testcase == "31026"
| where allThreads == "1"
| where success == "true"
| where responseCode == "200"
| (elapsed/1000) as elapsed
```
```
| formatDate(_messageTime, "MM/dd/yyyy
HH:mm:ss:SSS") as time
| avg(elapsed) as elapsed
```
```
29368 [NFR] Search of ProductionOrderEntity for different filter
should return result within 2 sec with given sort order
```
```
2.5s 2s 100 _sourceCategory="jmeter/modmom_test" OR
_sourceCategory="dev/nfr/modmom"
| parse "*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*" as
timestamp,elapsed,label,responseCode,
responseMessage,threadName,dataType,success,
failureMessage,bytes,sentBytes,grpThread,allThreads,
URL,latency,idleTime,connect,testcase,build
| where URL contains "http://<NFR
Environment>/ordermanagementapi/api
/ProductionOrderInquiryService/Execute"
```
```
| where timestamp != "timeStamp"
| where testcase == "29368"
| where allThreads == "1"
| where success == "true"
| where responseCode == "200"
| (elapsed/1000) as elapsed
```
```
| formatDate(_messageTime, "MM/dd/yyyy
HH:mm:ss:SSS") as time
| avg(elapsed) as elapsed
```
TrackAndTrace

```
ID Title Threshold
on UI
```
```
Threshold
on
backend
```
```
Loop Sumo Query
```

27882 [NFR] Collect from queueing to produced on
1 Operation of WorkOrder iterated for 100
times

```
1.5s 1s 100 _sourceCategory="jmeter/modmom_test" OR _sourceCategory="dev
/nfr/modmom"
| parse "*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*" as timestamp,
elapsed,label,responseCode,responseMessage,threadName,
dataType,success,failureMessage,bytes,sentBytes,grpThread,
allThreads,URL,latency,idleTime,connect,testcase,build
| where URL contains "http://<NFR Environment>/trackan
dtraceapi/api/collect/Execute"
```
```
| where timestamp != "timeStamp"
| where testcase == "27882"
| where allThreads == "1"
| where success == "true"
| where responseCode == "200"
| (elapsed/1000) as elapsed
```
```
| formatDate(_messageTime, "MM/dd/yyyy HH:mm:ss:
SSS") as time
| avg(elapsed) as elapsed
```
29586 [NFR] WorkOrderInquiryService -with filter
on WorkOrderName, ProductName, Status,
LocationName and Limit 50

```
2s 1.5s 100 _sourceCategory="jmeter/modmom_test" OR _sourceCategory="dev
/nfr/modmom"
| parse "*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*" as timestamp,
elapsed,label,responseCode,responseMessage,threadName,
dataType,success,failureMessage,bytes,sentBytes,grpThread,
allThreads,URL,latency,idleTime,connect,testcase,build
| where URL contains "http://<NFR Environment>/trackan
dtraceapi/api/WorkOrderInquiryService/Execute"
```
```
| where timestamp != "timeStamp"
| where testcase == "29586"
| where allThreads == "1"
| where success == "true"
| where responseCode == "200"
| (elapsed/1000) as elapsed
```
```
| formatDate(_messageTime, "MM/dd/yyyy HH:mm:ss:
SSS") as time
| avg(elapsed) as elapsed
```
29939 [NFR] FillComboWorkOrderInquiryService -
with filter on WorkOrderName,
ProductName, Status, LocationName and
Limit 50

```
2s 1.5s 100 _sourceCategory="jmeter/modmom_test" OR _sourceCategory="dev
/nfr/modmom"
| parse "*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*" as timestamp,
elapsed,label,responseCode,responseMessage,threadName,
dataType,success,failureMessage,bytes,sentBytes,grpThread,
allThreads,URL,latency,idleTime,connect,testcase,build
| where URL contains "http://<NFR Environment>/trackan
dtraceapi/api/FillComboWorkOrderInquiryService/Execute"
```
```
| where timestamp != "timeStamp"
| where testcase == "29939"
| where allThreads == "1"
| where success == "true"
| where responseCode == "200"
| (elapsed/1000) as elapsed
```
```
| formatDate(_messageTime, "MM/dd/yyyy HH:mm:ss:
SSS") as time
| avg(elapsed) as elapsed
```
35464 [NFR] 5 Users invoking Collect TP on 5
different WorkOrders

```
1.5s 1s 100 _sourceCategory="jmeter/modmom_test" OR _sourceCategory="dev
/nfr/modmom"
| parse "*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*" as timestamp,
elapsed,label,responseCode,responseMessage,threadName,
dataType,success,failureMessage,bytes,sentBytes,grpThread,
allThreads,URL,latency,idleTime,connect,testcase,build
| where URL contains "http://<NFR Environment>/trackan
dtraceapi/api/collect/Execute"
```
```
| where timestamp != "timeStamp"
| where testcase == "35464"
| where allThreads == "5"
| where success == "true"
| where responseCode == "200"
| (elapsed/1000) as elapsed
```
```
| formatDate(_messageTime, "MM/dd/yyyy HH:mm:ss:
SSS") as time
| avg(elapsed) as elapsed
```

35466 [NFR] 5 Users invoking Collect TP on 5
different operations of the same WorkOrder

```
1.5s 1s 100 _sourceCategory="jmeter/modmom_test" OR _sourceCategory="dev
/nfr/modmom"
| parse "*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*" as timestamp,
elapsed,label,responseCode,responseMessage,threadName,
dataType,success,failureMessage,bytes,sentBytes,grpThread,
allThreads,URL,latency,idleTime,connect,testcase,build
| where URL contains "http://<NFR Environment>/trackan
dtraceapi/api/collect/Execute"
```
```
| where timestamp != "timeStamp"
| where testcase == "35466"
| where allThreads == "5"
| where success == "true"
| where responseCode == "200"
| (elapsed/1000) as elapsed
```
```
| formatDate(_messageTime, "MM/dd/yyyy HH:mm:ss:
SSS") as time
| avg(elapsed) as elapsed
```
42832 [NFR] 10 Users invoking Collect TP on 10
different operations of the same WorkOrder

```
2s 1.5s 100 _sourceCategory="jmeter/modmom_test" OR _sourceCategory="dev
/nfr/modmom"
| parse "*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*" as timestamp,
elapsed,label,responseCode,responseMessage,threadName,
dataType,success,failureMessage,bytes,sentBytes,grpThread,
allThreads,URL,latency,idleTime,connect,testcase,build
| where URL contains "http://<NFR Environment>/trackan
dtraceapi/api/collect/Execute"
```
```
| where timestamp != "timeStamp"
| where testcase == "42832"
| where allThreads == "10"
| where success == "true"
| where responseCode == "200"
| (elapsed/1000) as elapsed
```
```
| formatDate(_messageTime, "MM/dd/yyyy HH:mm:ss:
SSS") as time
| avg(elapsed) as elapsed
```
28539 [NFR] View 3000 work orders 1.5s 1s 100 _sourceCategory="jmeter/modmom_test" OR _sourceCategory="dev
/nfr/modmom"
| parse "*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*" as timestamp,
elapsed,label,responseCode,responseMessage,threadName,
dataType,success,failureMessage,bytes,sentBytes,grpThread,
allThreads,URL,latency,idleTime,connect,testcase,build
| where URL contains "http://<NFR Environment>/trackan
dtraceapi/api/WorkOrderOperationInquiryService/Execute"

```
| where timestamp != "timeStamp"
| where testcase == "28539"
| where allThreads == "1"
| where success == "true"
| where responseCode == "200"
| (elapsed/1000) as elapsed
```
```
| formatDate(_messageTime, "MM/dd/yyyy HH:mm:ss:
SSS") as time
| avg(elapsed) as elapsed
```
44329 25 Users invoking Collect TP on 25 different
WorkOrders

```
2s 1.5s 100 _sourceCategory="jmeter/modmom_test" OR _sourceCategory="dev
/nfr/modmom"
| parse "*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*" as timestamp,
elapsed,label,responseCode,responseMessage,threadName,
dataType,success,failureMessage,bytes,sentBytes,grpThread,
allThreads,URL,latency,idleTime,connect,testcase,build
| where URL contains "http://<NFR Environment>/trackan
dtraceapi/api/collect/Execute"
```
```
| where timestamp != "timeStamp"
| where testcase == "44329"
| where success == "true"
| where responseCode == "200"
| (elapsed/1000) as elapsed
```
```
| formatDate(_messageTime, "MM/dd/yyyy HH:mm:ss:
SSS") as time
| avg(elapsed) as elapsed
```

# NFR Environment

## Reference Environment on Openshift - modmom-18

Cluster

```
Node CPU Type tbd
```
Services

```
Material Modeling Order Management Factory Track&Trace Access Management
```
```
agreed de facto agreed de facto agreedde facto agreedde facto agreed de facto
CPU request500m 50m 500m 50m 500m 50m 500m 50m 500m 50m
limit 500m 200m 500m 200m 500m 200m 500m 200m 500m 200m
RAM request2Gi 256Mi 2Gi 256Mi 2Gi 256Mi 2Gi 256Mi 2Gi 256Mi
limit 2Gi 1Gi 2Gi 1Gi 2Gi 1Gi 2Gi 1Gi 2Gi 1Gi
#Pods 3 3 3 3 3 3 5 5 3 3
```
## Production-like Environment on XCR

```
Node CPU Type tbd
```
Services

```
Material Modeling Order Management Factory Track&Trace Access Management
```
```
agreed de facto agreed de facto agreedde facto agreedde facto agreed de facto
CPU request500m 50m 500m 50m 500m 50m 500m 50m 500m 50m
limit 500m 200m 500m 200m 500m 200m 500m 200m 500m 200m
RAM request2Gi 256Mi 2Gi 256Mi 2Gi 256Mi 2Gi 256Mi 2Gi 256Mi
limit 2Gi 1Gi 2Gi 1Gi 2Gi 1Gi 2Gi 1Gi 2Gi 1Gi
#Pods 3 3 3 3 3 3 5 5 3 3
```
## Production Environment on XCR / AWS

```
Description 2204 2207 2210 Avg Max Comments
```
```
Entities
```
```
Production Orders 2.500 3000 2.500 3.000
```
```
Word Orders 2.500 3000 2.500 3.000
```
```
Number of Products 50 50 50 100
```
```
Operations / PO 10 10 5 20
```
```
Total Operation Definitions in system 200 200 200 500
```
```
Work Order Operations in TnT 12.500 12-
500
```
```
12.500 60.000
```
```
Material / Operation 4 4 5 10
```
```
Total Material Definition in system 1.000 1.000 1.000 1.000
```
```
Factory entities 100 100 100 200 Factory data - like Line / WC / WU only
```
```
Users
```
```
Number of Users 100 100 100 100
```

Number of concurrent users 5 25 25 40

Transactions

Import of PO / Day 60 100

Launch of PO to WO / Day (^6060100)
B2MML Imports / Day 20 20 50 B2MML having multiple PO (60/20 = 3 PO / B2MML)
B2MML Update 10 10 20
Collect Throughput Transactions /
day
30.000 30.000 60.000 Number of Throughput Collections via UI or external system (both utilize the same
API)
60 in process PO's with 5 Ops each = 300 Operations
300 Operations with 100pc ea to be produced = 30.000 pc.
30000 pc to be produced in 1 day
1 day = 86000sec
86000/30000= ~3sec/pc
Every 3 sec 1 Collect Throughput
1 Throughput = 1 pc


# NFR Definitions

The diagram depicts the software product qualities from standard ISO 25010.


# General NFRs for all Modules

## Functional Suitability

```
When selecting and opening an item from a list the back button should direct the user to the list item he has selected before.
In general the UI should remain reactive (for import, launch, collect throughput, scrolling, filtering, ...) when a background task is being performed
```
## Performance Efficiency

```
All pages (incl. content) must be visible in less than 1,5s
When scrolling a list, loading of new items should take less than 1,5s
Task must be performed within 2s after the user triggered an action and result must be clearly visible
If user has not received a feedback in less than 2s, he must be shown that a task is being performed
Time it needs after the User clicked "Login" until he can start working with the system must be less or equal to 2s
```
## Compatibility

```
TBD: versioning, deployment and testing
After every Release of a single Module independent of the Start-Up (SU) with either a minor or major update, there need to be mechanism in
place that ensure compatibility between Modules across SU in the production environment for our customers. As we provide single Modules with
different kind of functionality which at the end the customer decides what module composition he wants, we need to make you that the solution is
compatible considering new versions. The mechanism mentioned to ensure the required compatibility needs to be worked out from the
architectural side.
```
## Portability

## Reliability

```
Operation in general for Modular Manufacturing is 24/7
Ramp-Up Phases:
2204 (Early Access): 90%, Scheduled maintenance window of 2h every Sunday is acceptable, max. outage time per downtime: 2h
2207 (Limited Preview Production Ready for Selected Customers): 95%, Scheduled maintenance window of 2h every Sunday is
acceptable, max. outage time per downtime: 1h
2210 (SaaS Private Cloud (On-Premise?)): 99%, Scheduled maintenance window of 1h every Sunday is acceptable, max. outage time
per downtime: 5m (if stricter requirements apply per module, this is defined in the module specific NFRs)
End Goal for 2301:
No maintenance windows (0s)
max. outage per downtime: 1m
System must be available 99.9%
```
## Security

## Maintainability

## Usability

```
All pages must be visible in any kinds of Industrial device
Minimal Resolution Supported: TBD
```
## Measurable NFRs

```
NFR 2204 2207 2210 Goal Comment
```
```
Performance Efficiency
```
```
Page Loading Times 1,5s 1,5s 1,5s 1,5s All pages (incl. content) must be visible in less than 1,5sec
```
```
Table Data Reload 1,5s 1,5s 1,5s 1,5s When scrolling a list, loading of new items should take less than 1,5sec
```
```
Login Performance 2s 2s 2s 1,5s
```

Task Execution 2,0s 2,0s 2,0s 2,0s User must get feedback in less than 2 seconds after triggering a task

User Feedback 2,0s 2,0s 2,0s 2,0s If user has not received a feedback in less than 2 seconds, he must be shown that a task is being
performed

Reliability

Availability (24/7) 90% 95% 99% 99,9%

Max. Outage Time 2h 1h 5m 1m

Maintenance
Windows

```
2h 2h 1h 0s Only acceptable on Sundays after customer uses system in production environment
```

# Order Management NFRs

## Draft:

```
System must be available 90% by 2204 release
```
Production Order Import:

```
90% (with enough iterations) of PO's import is <4s for one PO with 10 Operation and 4 Materials / Operation
Parallel Import of PO must be possible – e.g. 50 PO's will be simultaneously imported by ERP nightly import.
```
```
Under no circumstances we can loose a already imported production order from the system
If the import fails, we need to persist the failed production order and error message for further failure analysis why the import failed.
```
Work Order Launch:

```
<4s for one PO with 10 Operation and 4 Materials / Operation
Under no circumstances we can loose a already imported work order from the system
Under no circumstances we can loose a work order during launch. If the launch fails, we need to persist the failed work order and error message
for further failure analysis
System must be capable of launching 5 POs in parallel with 5 concurrent Users and UI remains reactive (for import, launch, collect throughput,
scrolling, filtering, ...)
```
## Functional Suitability

## Performance Efficiency

## Compatibility

## Usability

## Reliability

## Security


Maintainability

Portability

Measurable NFRs

```
NFR 2204 2207 2210 Goal Comment
```
```
Performance Efficiency
```

# Track and Trace NFRs

## Draft

```
System must be available 90% by 2204 release
System must be available 95% (to receiving throughput collection) by 2207 release. Open Question: this module is close to the shop floor and
collects very frequently transactions, how can we make sure loosing as less as possible transactions if TnT is down for any reason.
System must be available 99.9% (to receiving throughput collection) by 2210 release
```
Operation Collect Throughput:

```
Reliability: No through-put message shall be lost if it has been successfully retrieved. From the moment we receive a collect TP message either
from our UI or from an external system, these shopfloor transaction are very much important to represent the current state of the shop floor
execution. Therefore we should make sure we these transaction are not being lost at any moment.
System must be capable to retrieve 40 concurrent throughput collections by 40 users with 90% not taking longer than 1 second and 99% under 2
seconds. How many times does this happen?
User must get feedback of successful or unsuccessful throughput collection in less than 2 seconds
```
## Functional Suitability

## Performance Efficiency

## Compatibility

## Usability

## Reliability

## Security

## Maintainability

## Portability

## Measurable NFRs

```
NFR 2204 2207 2210 Goal Comment
```
```
Performance Efficiency
```


# Home Page NFRs

## Draft

```
System must be available 90% by 2204 release
```
## Functional Suitability

## Performance Efficiency

## Compatibility

## Usability

## Reliability

## Security

## Maintainability

## Portability

## Measurable NFRs

```
NFR 2204 2207 2210 Goal Comment
```
```
Performance Efficiency
```

# Access Management NFRs

## Draft

```
System must be available 99.99% (otherwise none of the other modules will be able to work)
```
## Functional Suitability

## Performance Efficiency

## Compatibility

## Usability

## Reliability

## Security

## Maintainability

## Portability

## Measurable NFRs

```
NFR 2204 2207 2210 Goal Comment
```
```
Performance Efficiency
```

# Factory Modeling NFRs

## Draft

```
System must be available 90% by 2204 release
System should hold 150 Factory Entities UI remains reactive (for import, launch, collect throughput, scrolling, filtering, ...)
Inserting new factory resources after importing a Production Order should not take longer then 2 sec for 90% of the cases, 99% should not take
longer than 4 sec
```
## Functional Suitability

## Performance Efficiency

## Compatibility

## Usability

## Reliability

## Security

## Maintainability

## Portability

## Measurable NFRs

```
NFR 2204 2207 2210 Goal Comment
```
```
Performance Efficiency
```

# Material Modeling NFRs

## Draft

```
System must be available 90% by 2204 release
System should hold 1000 Material Definitions and UI remains reactive (for import, launch, collect throughput, scrolling, filtering, ...)
Inserting new Material Definitions after importing a Production Order should not take longer then 2 sec for 90% of the cases, 99% should not take
longer than 4 sec
```
## Functional Suitability

## Performance Efficiency

## Compatibility

## Usability

## Reliability

## Security

## Maintainability

## Portability

## Measurable NFRs

```
NFR 2204 2207 2210 Goal Comment
```
```
Performance Efficiency
```

# NFR Parking Lot

## Documentation:

```
All NFRs need to be in Polarion
Performance NFR should always be part of the business requirement
All other Requirements should be managed as separate features.
```
To be discussed:

```
How to handle maintenance windows? How do we communicate it to the customer?
```
```
How are we ramping up after 2210?
```
```
What are typical reason why the system goes down? How should we test these?
Schema upgrade?
```
```
Redundancy of Infrastructure?
```
```
Automatic downtime detection?
```
```
How do we measure availability to proof to customer? Audit capability
```

# Architectural Qualities

## Microservice Architecture Characteristics

## Componentization via Services

Applying single responsibility principle at the architectural level, components are units of software that are:

```
independently deployable, replaceable, upgradable.
Rewriting a component without affecting its collaborator is possible.
independently testable.
built around business capability
```
Components can:

```
be owned by small team.
have different technology stack.
```
Assessment for current state of MMOM

In metadata-driven architecture of MMOM, practically only "one" microservice is being developed. This microservice which is named runtime platform show
s different behavior at runtime based on the given library of the business functionality. In other words, at runtime different services like TnT or OM are, in
fact, different running instances of the same microservice. For simplicity, we refer to the libraries containing business functionality which can change the
behavior of platform microservice at runtime as "business modules" (or for short "module") in the rest of the text. Here are the items that may affect
negatively true componentization:

```
(independently deployable, upgradable, replaceable) Business modules share dependency on:
same set of libraries of the platform. Any change to platform should be propagated to all modules. Moreover, any bug or instability of
platform affects all modules. (in contrast to fault isolation)
same messaging libraries. Due to the generic nature of platform, different Grpc endpoints are identified by the message content or
message payload itself. For example, WorkOrderCreated is the message class which is shared by TnT and OM.
For example, once it happened that TnT and OM used different incompatible versions of the messaging library, therefore their
connection was broken. (difficult to debug!)
(support for different technology stack) Only one technology stack is supported: the one used by the platform.
(support for small teams) Effectively, there exists currently one large development team spread over the globe (of course there are arranged in
several teams but the question is whether they can independently work?)
(clear business capability) It seems there are strong overlaps or dependencies between some business modules in terms of functionality. In other
words, no clear boundaries between the business scope of different modules
For example, OM handles POs and TnT handles WOs. However, it seems that PO and WO should be always synchronized while WO is
in progress (after being launched and before being completed). One simple failure scenario: when WO is in launched state, we lose the
connection between TnT and OM. Therefore, PO and WO become unsynchronized or inconsistent until the connection gets established
again. Now the question is whether the window of inconsistency is acceptable at all? If so what can be the maximum length of the
window? From the business point of view, how should it be handled if the window length exceeds the limit? PO gets "invalid" or
"unknown" state, TnT should not proceed with WO when no response from OM, ... Once it happened that PO stayed in "Launching"
state while WO was completed.
(independently testable) It seems currently integration tests are shared among different modules. (moreover, for example, for testing collect
throughput from TnT, import PO and launch PO should be done by OM.)
```
Here are the related development qualities which can be negatively affected:

```
Extensibility
Maintainability
Flexibility
Evolution
```
Here are the related runtime qualities which can be negatively affected:

```
Compatibility
Interoperability
Co-Existence
```
How can we test/evaluate?

Integration tests as well as contract testing should reveal strong coupling, dependency and compatibility issues.

*Contract testing is a technique for testing an integration point by checking each module in isolation to ensure the messages it sends or receives conform
to a shared understanding that is documented in a "contract". There are tools, methods to manage and document the contracts.

*Integration tests verify the correctness of a service by focusing on the interaction points and making them very explicit.

### DRAFT!!


Decentralized Data Management

```
Managing updates across services
Transactionless coordination between services
Eventual consistency (problems are dealt with by compensating operations)
Event-driven data management
Independent data schema changes
Providing independently-evolving schemas
Polyglot persistence
```
Assessment for current state of MMOM

```
Polyglot persistence is not supported since all modules share the same data layer of platform.
It seems that there is a strong correlation between data schemas of different modules (dependent schemas). For example, when PO gets
modified in OM, WO needs to be modified in TnT as well.
Managing of updates across modules:
Currently, only one WO is created per PO. It is not clear how consistency is handled when one PO results in several "CreateWO"
messages from OM to TnT? (cross-microservice business transactions)
```
```
An example for inconsistency state - Lost "WO Created" message:
```
```
No clear specification about windows of inconsistency (eventual consistency) for different scenarios of updates across services.
Strong consistency example: WO and PO status should always be consistent. The user of the system should never see PO
status "Launching" and WO status "Ready".
Eventual consistency example: WO and PO status can be inconsistent temporary but the acceptable length of inconsistency
window should be specified.
```
Here are the related runtime qualities which can be negatively affected:

```
Reliability
Recoverability
Fault Tolerance
Fault Isolation
```
How can we test/evaluate?


Integration tests should reveal inconsistency issues when services get unsynchronized.

Inter-Process Communication

```
Smart endpoints and dumb pipes
Common mechanisms:
HTTP request-response
Lightweight messaging
"Reliable" asynchronous messaging
```
In MMOM, inter-process communication is via asynchronous Grpc communication.

Assessment for current state of MMOM

```
Late message delivery
For example, TnT does not handle late delivery of "WOCreated". Once it happened that the state of PO remained "Launching"
(forever...) while the state of WO was "Complete" due to broken communication from TnT to OM. (TnT on edge device scenario)
Late message delivery (1:N): this happens when one app sends updates to multiple apps.
For example, If a “Location”, “Site”, ... is changed in “Factory”, it needs to be communicated to OM and TNT since they have the field
“Location” in their PO and WO.
Out of order message delivery
For example, "WO Created" can be received after "WO Completed" and results in inconsistency.
```
```
Duplicated messages
For example, while TnT handles duplicate "CreateWO" message, OM does not handle duplicate "WOCreated".
Point-to-point communication (direct link) between the modules can become quite complex as the number of modules grows. (inside every
module, the address of destination modules should be configured)
```
Here are the related runtime qualities which can be negatively affected:

```
Reliability
Recoverability
Fault Tolerance
Fault Isolation
```
How can we test/evaluate?

Integration tests should reveal issues with duplication delivery, out of order delivery and late delivery.

Design for Failure


Since in MSA, a business request spans multiple services (components) separated by network partitions, it is important to consider the possible failure
modes in the system and resilient.

```
Tolerate to the failure of services
Quick failure detection
Automatic restore of the service
```
In MMOM, the infrastructure handles redeployment of a crashed service.

Assessment for current state of MMOM

```
It seems currently it is not resilient to communication failures.
For example, once it happened that the communication from TnT (deployed on edge device) to OM was broken. But OM did not show
any error messages to the user and kept the state of PO at "Launching" forever.
In current setup, if DB server instance crashes, all services become unavailable.
Single point of failure (other than DB do we have other single point of failures like caching server?)
It is not clear if we have currently the problem of cascading failures?
How cascading failure problem can be evaluated?
How are the values for timeouts (in retry) mechanisms configured? (are they based on some business requirements?)
```
Here are the related runtime qualities which can be negatively affected:

```
Availability
Resiliency
Reliability
Recoverability
Fault Tolerance
Fault Isolation
```
How can we test/evaluate?

Integration tests, component tests, and especially end-to-end tests (exercise the fully deployed system and manipulate it through public interfaces such as
GUIs and service APIs.) should verify resiliency of the system in case of failures. The tests should consider different failure scenarios such as:

```
Random outages:
Crash of individual service
Crash of sql server
Crash of keycloak service
Communication failures: broken link, network latency
```
For every scenario, the consistency of data should be verified as well as how services handle failures, for example, by error messages, default values,
cache, ...

One famous approach is Chaos Monkey: https://netflix.github.io/chaosmonkey/

Evolutionary Design

```
Granular release planning
Fast release process
Each service is tolerant to changes of other services
Versioning is used as the last resort to handle breaking changes between service or as integration approach
```
In MMOM, versioning is used vastly to handle breaking changes like

```
Versions of business modules
Versions of platform
Versions of messaging libraries (for n modules, there exists O(n^2 ) messaging libraries)
```
Moreover, in MMOM, release is for the entire set of modules constituting an app.

Here are the related development qualities which can be negatively affected:

```
Extensibility
Maintainability
Flexibility
Evolution
```
Here are the related runtime qualities which can be negatively affected:

```
Compatibility
Interoperability
Co-Existence
```
How can we test/evaluate?

Contract testing should reveal incompatibility between modules.


Other aspects

Due to the complexity of microservice architecture in which end-user requests flow through several services which are connected in a network, measuring
the performance of services at runtime is essential. There are various variables which affect the performance like current traffic load, state of supporting
services like caching, slow services, etc.

Here are the related runtime qualities which should be measured via NFR tests:

```
Throughput
Response Times
Resource Utilization
Costs
```
Moreover, load tests should expose the services which can't scale in response to high traffic loads.

Conclusion

In summary, it seems that the issues discussed may be handled in the design or in the implementation of the specific modules. For example, processing of
messages which are correlated like one PO generates several CreateWOs should be addressed probably at the design level. However, for example,
handling of late response can be done for individual messages depending on the business use-case in the implementation of business modules.

Moreover, as the examples discussed show, it seems that current tests do not necessarily catch them. Therefore, in the design of tests, in addition to input
and output data, the dynamic nature of the MSA should be simulated.

Test Cases

Scalability

Horizontal Scaling Test:

```
Goal: with more hardware nodes, more throughput is reached and response times remain constant
Test 1: Track&Trace module is deployed on one pod.
Result: get max throughput with a single pod (e.g. collect WO)
Test 2: Track&Trace module is deployed on two pods (+load balancer)
Result: get max throughput with two pods
Test Passes:
Throughput: throughput(2 pods) * 1.5 > throupghput(1pod) (have at least a 50% higher throughput)
Response Times: avg response times (2 pods) <= avg response times (1pod) (+-10%)
Business Impact if test fails:
Not able to handle growing customer demand by additional resources.
```
Reliability

```
Test 1
Goal: identifying inconsistencies that can happen in Launch scenario
Steps:
OM launches a PO
TnT receives the CreateWO message and creates a WO with "Ready" state.
TnT sends back "WOCreated" to OM
Before OM receives "WOCreated", "one" of the following failure happens:
Communication link between OM and TnT gets broken.
Communication link between OM and TnT gets slow.
OM crashes.
Duration of failure should be greater than the retry timespan for CreateWO message.
Meanwhile TnT completes the WO. State of WO is now "Completed".
Failure disappears:
Communication goes back to normal.
OM gets restarted.
Result: PO state is "Launching" and WO state is "Completed". (inconsistent)
Test Passes:
OM and TnT handles the failure appropriately via error messages, error status, ...
Test 2
Goal: identifying inconsistencies that can happen during OM and MM interaction.
Steps:
OM sends "ProductionOrderCreated" to MM
MM receives it successfully.
MM crashes before consuming the message.
MM gets restarted.
Test Passes:
If MM consumes the message after restart and replies back to OM.
If the message has been stored at MM before crash, the error is handled appropriately by MM and OM.
```

# Supportability - Diagnostics

```
Macro
Area
```
```
Topic Description P
ri
o
rity
```
```
Notes Co
m
me
nt
```
```
Diagn
ostic
Tools
```
```
Log
Collec
tion
```
```
Need to have a tool that collect automatically all needed log files
(included the windows logs and so on) one shot. It should be
configurable for the macro area
```
```
1
```
```
Auto
matic
Dump
```
```
The Product has to create a collection of logs in case of "crash" of a
compoment for “post-mortem” analysis by R&D
```
```
2 the dump has to contain also the logs thirdy-part: e.g IIS or
operative system logs, ect ect
```
```
Expor
t
Scena
rio
```
```
Need a tool that collect all customer configurations in order to import
them in the support scenarions one shot
```
```
3 A quick configuration export/import allow to have scenarios very
close to the real cases nd it helps to reproduce an issue with the
same customer context
```

# Supportability - Documentation

```
Topic Description Pri
ority
```
```
Notes Com
ment
```
```
User
Documentation
```
```
Need to have Product documentation very simple and clear for final user (partner
/customer).
Readable without several links to other documents
```
```
1
```
```
Troubleshooting
documentation
```
```
Need to have a documentation for final user (partner/customer) that collects the common
known issues (specially regarding the starting configuration)
```
```
2 It can be integrated with KB article and
videos published on Support Center
```

# Supportability - Logging

```
Topic Description P
ri
o
rity
```
```
Notes Comment
```
```
Logs
for final
user (
partner
/custom
er)
```
```
Need to have very detailed and readable logs.
The message errors should allow the end users
to understand by themselves the area of the
problems and if possible propose the solution
```
```
1 e.g an entry cannot run because
there is another one in running for
the same order, need specify in
the log also which is the running
entry.
```
```
Definition Log fields and enhancing based on requirements.
Ensure Log levels are defined and used correctly. Mapping Log
level and fields should be structured. When we change log level -
do we need to restart service? or it is dynamic? ( check with
Matteo )
```
```
Message end user gets when using system needs localization
and messages are comprehendable by end user
```
```
Log
Perciste
ncy
/Archivi
ng
```
```
Need to have logs percistent and not overwritten
in few time
```
```
1 All Logs are collected in Sumo. Stretegy for persistance need to
discussed with Sumo team ( talk to Mauro )
```
```
Log size Need to have logs not heave in term of disk
space and impact on system performance
```
```
1 All depends on storage allocation in Sumo. ( it may cost storage
space cost )
Storage should be configurable
```
```
Log for
Product
support
```
```
Need to have very detailed and readable logs
with all trace of events logged for data and user.
The trace should allow the Product Support to
understand the root cause and propose a
solution
```
```
2 the logs should track the history
of an event in order to know what
happened before the error
occurred
```
```
Is it getting previous log entries based on some event ( criteria )
```
```
Logs
for R&D
```
```
Need to have very specialistic logs in order to
solve complex issues and real product defects.
```
```
3
```

# Supportability - Tools

```
Topic Description Pri
ority
```
```
Notes Com
ment
```
```
Check
Configuration
Tool
```
```
Need to have a tool to check all the prerequisites hardware and
software before installing or making an upgrade.
It has to create a readable report shareable for example with a
customer IT.
```
```
1 Already available for OpCenter Foundaation
```
```
Performance
Tool
```
```
Need to have a dedicated tool for performance analysis and DB
maintence plan
```
```
2 requirements could also be collected within APA team
```
```
Data
Archiving
```
```
Need to have a simple tool that allow the customer to archive in order
to avoid performance degradation on data reading
```
```
3 The data archived has to be easy to reach (e.g. for reporting) and
readable from a version to another one in order
```

# NFR Coverage

## Module & Platform Coverage

Coverage ~ 3 out of 5 "components" = 60%

## Business Feature Coverage

Business Features already closed.

```
NFR Test
```
```
20042 Feature Import Production Order with Materials x
```
```
20247 Feature Import Material Definition x
```
```
20252 Feature View Production Orders x
```
```
20253 Feature Support filtering in grid ( Production Order )
```
```
21825 Feature Factory Modeling - Allocate Site
```
```
22225 Feature View Material Grid for Material Module
```
```
23412 Feature View Work Order Details
```
```
23413 Feature View Work Orders
```
```
23414 Feature View Operations in Operator Terminal
```
```
23466 Feature Perform Collect TP From Operator Terminal
```
```
24386 Feature Perform Collect TP From Operator - API x
```
```
26479 Feature Support filter on WO Page
```
```
26856 Feature Sequence Operations by customer specific sequence logic
```
```
27020 Feature View PO Sort By Status and Create Date/Import Date
```
```
27502 Feature Automatic Launch PO after Import x
```
```
28519 Feature Support Sorting via Inquery Service on WO Page
```
Coverage ~ 5 of 16 = 30%

## Quality Coverage (based on ISO 25010)


Important = Robustness , Availability, Recoverability, Fault Tolerance, Scalability, Interoperability,

Coverage ~ 2 of 7 = 30%

API Coverage

2022-05-03 used in jmx files


materialmodelingapi/api/Material
materialmodelingapi/api/ProcessB2MMLMaterialObject/Execute
ordermanagementapi/api/LaunchProductionOrder/Execute
ordermanagementapi/api/ProcessB2MMLProductionScheduleObject/Execute
ordermanagementapi/api/ProductionOrderHeader
ordermanagementapi/api/ProductionOrderInquiryService/Execute
ordermanagementapi/api/operationdefinition
trackandtraceapi/api/FillComboWorkOrderInquiryService/Execute
trackandtraceapi/api/WorkOrderHeader
trackandtraceapi/api/WorkOrderHeader/Get
trackandtraceapi/api/WorkOrderInquiryService/Execute
trackandtraceapi/api/WorkOrderOperationInquiryService/Execute
trackandtraceapi/api/collect/Execute
trackandtraceapi/api/operationdefinition
trackandtraceapi/api/workflow

Caution: this is a superset of all HTTP requests (also used for test preperation)

Coverage ~ 15 out of ??

Architecture Sensitivity Points

```
Sensitivity Point Requirement Tests
```
```
Communication Resilience 0
```
```
Communication Security 0
```
```
Distributed Business Data Consistency 0
```
```
ORM performance 0
```
```
Data Properties as C# Objects may induce performance overheads 0
```
```
Hybrid Deployment & Orchestration 0
```
```
Kubernetes Operator Development 0
```
```
Binary dependency between Apps sharing the same Message Model 0
```
Coverage ~ 0%

Risk Coverage

Risk approach currently not followed.

Future Releases

```
2112 2204 2207 2210 2301 2304 beyond
```
```
Requirements 11 8???
NFR Test? 9
```
Currently not addressed.

Coverage of NFRs for Startups

Currently not addressed.


# Before You Start

Welcome on board OCMOD users!

## Prerequisites

You need to have:

```
Access to MS Team "Modular MOM 2.x" (all SM)
```
```
Access to team-specific MS Team and Chat (all SM)
```
```
Access to internal Confluence site wiki02: Modular MOM 2 (contact Khairnar, Prashant -Platform- and Napelyani , Amir - MES -)
```
```
Access to user Documentation on Confluence site wiki01: Opcenter Modular Manufacturing Documentation Home (contact Alvarez Villanueva,
Beatriz )
```
```
Access to TFS Azure DevOps (contact De Pascale, Mauro)
```
```
VDI with Template Opcenter EX and/or Linux: Open ticket to MOMIS Ticket Service
```
```
Access to OpenShift: Open ticket to MOMIS Ticket Service
```
```
Email inserted in dedicated distribution list (@Distribution List owner)
```
```
Scrum events and CoPs in your Outlook calendar
```
## First Steps

The following pages help you to better understand Opcenter Modular Manufacturing:

```
OCMOD Concepts And First Steps To Use. It contains introductory videos regarding Metadata concepts and one operational video explaining
how to create a Module.
Quick Start to Developing with Opcenter Modular Manufacturing: It describes how to perform basic operations to start working OCMOD.
```

# First Steps for Startup Users

Welcome on board Startup users!

Please check if you meet the requirements listed in the prerequisites section of Before You Start,


# OCMOD Concepts And First Steps To Use

The following videos describe Opcenter Modular Manufacturing concepts and the procedure to create a Module:

Intro to Modular MOM Metadata concepts

Introduction To Metadata Concept

Modular MOM First Session: How to create a Module using Metadata


# Introduction of OCMOD Documentation

## Opcenter Modular Manufacturing documentation spaces

There are 2 documentation spaces of Modular MOM:

```
momwiki02 - Internal Documentation: This space is populated by all Feature team members. Internal documentation is meant to be used as
knowledge exchange among teams and cannot be published.
(Structure and organization are in progress).
momwiki01 - User Documentation: This space is updated by the Documentation referent with the collaboration of feature team members, and
contains the following official Opcenter Modeler Manufacturing manuals currently released for the version 2204:
```
```
Release Notes: It is the first manual a user should read because it briefly describe what we provide, the limitations, the available
documentation
Security Concept: Provides a set of concepts, best practices, and practical configuration settings to address security risks and threats.
User Manual: It describes how to use the UI and to guide the user to completely use the product following the production process.
Integration Guide: Describes how to integrate external systems with Opcenter Modular Manufacturing and how to perform specific
manufacturing operations by using the systems.
```
## Documentation process

For quality reasons, any internal or user documentation page must meet the requirements explained in User Story - Definition of Done (DoD):

```
Internal Documentation (wiki02) is created/updated with the correct details (by Development Teams). In particular, the following points should be
taken into account
For B2MML schema changes: the default values and the mapping should be also provided (also with an example)
For data model changes: the datatype along with the size, the default values, and optional/mandatory options should be provided
For the API: the behavior, the input and the output parameters, and the error codes should be provided (also with and example)
Messages
User Documentation (wiki01):
Create/Update documentation with correct details (by Development Teams)
Review documentation (by Documentation Referent)
Validate documentation :
by Product Managers, Product Owners, and/or Architects Community in case of new documents or existing documents that
have been consistently updated.
by Development Team members in case of specific document updates.
```
## How to manage TFS Documentation items

## User Documentation

During the sprint phase 2, the User Stories which require user documentation must have the tag Documentation.

```
You need to have access in both doc spaces, just check if the Edit button is visible.
```
```
If not, contact;
```
```
for momwiki01 space: Bea Álvarez (beatriz.alvarez@siemens.com)
for momwiki02 space:
Platfom: Khairnar, Prashant
MES: Napelyani , Amir
```
```
Tip
```
```
Any US that contains the label UX must also contain the label Documentation: interface updates correspond to documentation updates.
```

Each US contains :

```
1 task Draft user documentation assigned to one development team member.
1 task Review documentation assigned to the technical writer (beatriz.alvarez@siemens.com).
```
In order to close the task, the page must be reviewed:

```
by the Product Owner in case of new pages or existing ones that have been consistently updated.
by the development team member in case of specific updates.
```
To better track the documentation tasks and filter the Review documentation tasks regardless of the teams ,the following TFS items are assigned to the
OCMOD technical writer:

```
1 tag Doc-activities ,
If necessary, the User Story Improve usability of user documentation to collect doc activities not linked to a certain TFS item that implies
documentation updates such as proofreading, align terminology , update code, update images or diagrams, and fix broken links.
```
Internal Documentation

During the sprint phase 2, the User Stories which require internal documentation must have a task with title Internal Documentation assigned to one
team member.

```
For more information regarding Confluence procedures and how-to pages, see Working with Documentation.
```
```
Be aware that technical writers cannot review internal documentation.
```

# Quick Start to Developing with Opcenter Modular

# Manufacturing

This manual describes how to perform basic operations to start working with Opcenter Modular Manufacturing:

Configuring the Development Environment : a mandatory step to configure a Visual Studio environment for Metadata Runtime Solutions and Projects.

Architecture Overview: a list of documents written and maintained by the Architect group to understand the architectural concepts.

Siemens Web Framework Fundamentals: a list of video repositories and tutorials to start working with SWF.

How to Create a Configurable Object: initial procedure to create a Configurable Object, the starting point of Metadata development.

Creating a Metadata Model: procedure to create a Metadata Model and define the Metadata for Opcenter Modular Manufacturing.

Developing an App / Metadata Module: initial procedure to create and develop a Metadata Module that will be used to configure the business objects and
processes used by an OCMOD application.

Other important concepts & reference topics:

```
Platform & Model Versioning
Metadata/Metamodel topics
```

### 1.

### 2.

### 3.

### 4.

### 1.

### 2.

### 3.

# Configuring the Development Environment

This document describes how to configure a Visual Studio Environment for Metadata Runtime Solutions and Projects.

## Prerequisites

Required Software

```
Visual Studio 2019 Professional (16.8.3) or newer
DotNetCore SDKs : 5.0+, (we should not need 3.1.300+, 2.1.805+)
Microsoft SQL Server 14.0 Express or greater
Microsoft SQL Server Management Studio 18.5 or greater
```
Required Authentication/Authorization

```
An SWQA domain account to access both this wiki as well as the git repos. So if you're reading this on the wiki, you already have an account.
Permissions to be configured for both of these systems for appropriate access, e.g. edit for wiki.
```
Enable LongPath Support in Windows.

## Step 1: Install MS SQL database server and SQL Management Studio

These products are available with MSDN licenses or available for evaluation purposes. Follow standard procedures to install the Default instance,
otherwise change the environment configuration in case of an instance-based installation

## Step 2: Validate the database server is running and you can connect

Procedure

```
Connect to localhost server using Windows Authentication and SWQA credentials.
Verify your login has the sysadmin role.
In your Windows system environment variables, define the hostname and then restart Visual Studio to apply environment changes:
Set the variable MODMOM_DBHOSTNAME to the desired name of your database host. Defaults to localhost.
Set the variable MODMOM_DBNAME to the desired name of your database. Defaults to ModMomDb.
Save and apply variable.
You can validate the variable is set by starting a new console window.
Create an empty database by using the following script :
```
```
-- Drop if exists and create database--
-- Note: The database must match your MODMOM_DBNAME environment variable--
```
```
USE master
IF EXISTS(select * from sys.databases where name='ModMomDb')
BEGIN
ALTER DATABASE ModMomDb SET SINGLE_USER WITH ROLLBACK IMMEDIATE;
DROP DATABASE ModMomDb
END
```
```
CREATE DATABASE ModMomDb
ALTER DATABASE ModMomDb SET ALLOW_SNAPSHOT_ISOLATION ON
```
## Step 3: Clone Git Repositories

Procedure

```
Prepare an empty directory for source code. Preferably a short path, for example c:\source. You may rename the default directory MetadataRunti
me if desired.
Clone one of the listed repositories into your empty source directory by using either ‘git clone’ or any SCM tool.
```
```
Azure Devops Repo Name Repo URL
```
```
MetadataRuntime https://tfs05mom.industrysoftware.automation.siemens.com/MOM/ModularMOM/_git/MetadataRuntime
```
```
For the purposes of this document command line tools are used. Perform equivalent functions from within Visual Studio or any other Git tool.
```

### 3.

### 4.

### 1.

### 2.

### 3.

### 4.

### 5.

### 6.

### 7.

```
Change into the git repository and verify master branch.
Create a feature branch using checkout -b (git checkout -b branch_name) and start coding.
```
Source Solutions and Projects

To build MetadataRuntime you can run this PowerShell script:

Here below you find a description of each solutions, listed in their build/run order:

```
Camstar AOP Aspects Solution
Automation\ILWeavers\Metadata\Camstar.Metadata.Aspects.sln
It provides additional tests on metadata weaver to validate the weaver nuget package
(Optional) If you work on metadata weaver, then build also: Automation\ILWeavers\Metadata\Camstar.Metadata.Aspects.Tests.sln
```
```
Observability Solution
Automation\ILWeavers\Observability\ObservabilityAspects.sln
It provides additional tests on observability weavers to validate the weavers nuget packages
(Optional) If you work on observability weavers, then build also: Automation\ILWeavers\Observability\ObservabilityAspects.Tests.sln
```
```
Main Solution
Platform\Platform.sln
Requires nuget.org http://nuget.orgpackage source feed until we determine internal server.
```
```
Sample Model Solution – Sample Metamodel
Platform\InputMetadataModel\SampleModel\Camstar.Core.SampleModel.sln
```
```
DTO Generator Project – Generates Input / Read, Write and Message models from Sample Model
This is a consolidated generator and covers all code generation of (Sample)
MetaModel.Platform\Siemens.MOM.Model.DtoGenerator\Siemens.MOM.Model.Generator.csproj
Run/Debug this project
```
```
Generated Models Solution – Where Generators write models, maps and messages are written
Platform\GeneratedModels\GeneratedModels.sln
Ensure Models are generated from the previous step with recent datetimes
```
```
Build GeneratedModels.sln: 3 projects should build successfully
```
Deploy Database Schema

2021-04-13 update: The environment variable MODMOM_DBUSERNAME determines how the governance / database will operate

```
When set to Memory_Database the environment will use a SQL in memory only database - No password is needed.
When set to Trusted_Connection the environment will assume a Windows trusted authentication connection to a MS Sql Server
database - Current credentials are used, no password is needed.
When set to anything else, the password is also expected to be set and these two values form the database credentials.
```
```
Branch names are arbitrary and should convey some meaning as they will proliferate, one simple approach to organizing them is to
prefix with initials/branch_name. Creating branches in this form allows UI tools to expand/collapse the list of branches with a tree
control.
```

### 1.

### 1.

[Matteo: I fear this is not up-to-dateAccorsi, Carlo Nagamalli, Ramesh could you please verify it?]

Database Connection Modes

```
Runtime – Schema is read-only, this is used under normal operation of the system
Upgrade – Schema is compared with the logical model and differences are deployed to the database.
Limits apply, most adds/updates occur without intervention
Data is preserved using this method
Install – Schema is dropped if exists and re-created, all data is lost using this method
Connection String
For development purposes, Windows Authentication is used for connectivity and requires only a hostname and database name.
The class Platform\Siemens.MOM.Model.WriteBase\Session\SessionHelper.cs contains the hard-coded hostname and
database references.
Connectivity will be parameterized / externalized in the future
From the Integration Tests Solution, run the test WriteModelNhTests.DeployDatabase()
This connects to the database with the Upgrade Mode
Since no tables exist, the result is equivalent to using Install Mode
This process deploys tables representing the logical model to the empty database
Database is ready for use
```
API Configuration

```
The API needs the location and namespaces of the Read/Write assemblies
These values are defined in the Manifest.XML file located in the root of the project Siemens.MOM.Platform.Api
By default, the Sample model values are set. If using another model, make the appropriate updates to paths and namespaces.
```
Launch API

```
Launch the project Siemens.MOM.Platform.Api: The server should start on localhost:5000.
```
Invoke API

[Matteo: I fear this is not up-to-date Nickenig, Gero and Nagamalli, Ramesh could you please verify it?]

Procedure

```
Do either of the following:
Use http://localhost:5000 to see the available API endpoints and invoke them here
Use Postman or cURL:
```
```
$curl --location --request POST 'http://localhost:5000/api/Factory' \
--header 'Content-Type: application/json' \
--data-raw '{
"tenantId": "string",
"entity": {
"defaultQty": 90,
"description": "string",
"myProp": "prop1",
"name": "f1"
}
}'
```
Sample Inputs

```
For latest and correct API post, please refer to this repo:
```
```
https://tfs05mom.industrysoftware.automation.siemens.com/MOM/ModularMOM/_git/RestAPISamples
```
```
it contains a set of Postman collections that you can use to check if your local service is running
```

Examples of API inputs for various endpoints

```
Create Factory & locations added implicitly from the service logic
POST /api/Factory
```
### {

```
"tenantId": "string",
"entity": {
"defaultQty": 90,
"description": "string",
"myProp": "prop1",
"name": "f1"
}
}
```
```
Create Factory with locations added explicitly & initialize productionstatus
POST /api/Factory
```
### {

```
"tenantId": "string",
"entity": {
"name": "f2",
"description": "added thru api",
"defaultQty": 900,
"myProp": "f2-prop1",
"locations": [
{
"name": "f2-loc1",
"description": "added thru api",
"status": 1
},
{
"name": "f2-loc2",
"description": "added thru api",
"status": 2
},
{
"name": "f2-loc3",
"description": "added thru api",
"status": 3
}
],
"productionStatus": {
"status": 100
}
}
}
```
```
Create Employee - Set Factory reference by Id
POST /api/Employee
```
```
Due to the frequency of the update in the platform some of this examples may be out of date.
Please refer to this repo for updated collections:
```
```
https://tfs05mom.industrysoftware.automation.siemens.com/MOM/ModularMOM/_git/RestAPISamples
```
```
As of 2021-03-21, the API's are being redone and the input bodies will change.
```

### {

```
"tenantId": "string",
"entity": {
"department": "dept-1",
"description": "blah...",
"email": "no-email",
"factory": {
"guidBasedId": "12EF7ED5-1D68-432D-A515-AC40018140E0"
},
"name": "e0"
}
}
```
Create Employee - Set Factory reference by Name
POST /api/Employee

### {

```
"tenantId": "string",
"entity": {
"department": "dept-1",
"description": "blah...",
"email": "no-email",
"factory": {
"refName": "f1"
},
"name": "e1"
}
}
```
Create Employee - Set Factory reference by Name & set AdministratorLocation reference by Name & Parent (subentity reference field)
POST /api/Employee

### {

```
"tenantId": "string",
"entity": {
"department": "dept-1",
"description": "blah...",
"email": "no-email",
"factory": {
"refName": "f2"
},
"name": "e2",
"administratorLocation": {
"refName": "MainLoc",
"refParentObject": {
"refName": "f2"
}
}
}
}
```
Update Employee - Modify Dept, Desc, Factory, AdministratorLocation
PATCH /api/Employee


### {

```
"tenantId": "string",
"entityKey" : {
"refName": "e1"
},
"entity": {
"department": "dept-1-modified",
"description": "blah...modified",
"email": "no-email",
"factory": {
"refName": "f2"
},
"name": "e1",
"administratorLocation": {
"refName": "MainLoc",
"refParentObject": {
"refName": "f2"
}
}
}
}
# "refParentObject": {
# "guidBasedId": "12EF7ED5-1D68-432D-A515-AC40018140E0"
# }
# StartTrackedObject
{
"tenantId": "string",
"entity": {
"intValue": 90,
"name": "t0-1"
}
}
```
Delete entity
DELETE /api/Employee

### {

```
"refName": "e2"
}
```
Create Product (first rev)
POST /api/Product

### {

```
"tenantId": "string",
"entity": {
"base": {
"refName": "p1"
},
"defaultStartQty": 900,
"isRevOfRcd": true,
"revision": "r1"
}
}
```
Create Product revision (new rev)
POST /api/Product/Revise


### 1.

### 2.

### 1.

### 2.

### {

```
"tenantId": "string",
"entityKey": {
"refName": "p1"
},
"entity": {
"defaultStartQty": 900,
"isRevOfRcd": false,
"revision": "r2"
}
}
```
Troubleshooting Build of Projects and Solutions

```
Verify Required SDK is installed
```
```
Make sure you have at least a 5.X version of the Netcore SDK installed and the required Visual Studio version (see section Required Software a
bove). If not, stop and go do that.
```
```
Verify NuGet package cache is clean
```
```
Open c:\users\<username>\.nuget\packages and delete the folders siemens.* and camstar.*
Similarly, delete all local nuget sources from the Platform/nugets folder.
```
```
Verify Visual Studio version
```
```
Make sure you have Visual Studio version 16.8.5 or greater.
Make sure you have access to this feed. You will need a valid SWQA account.
```
```
This artifact repository contains the published (pre-release) nuget package dependencies. Those with versions < 1.0.0 ( for example 1.0.0-CM-
XXXXX-YYYYY)
```
```
You will not be able to build without access to this feed:
```
```
https://tfs05mom.industrysoftware.automation.siemens.com/MOM/_packaging/ModMOM_Platform/nuget/v3/index.json
```
```
Note
```
```
Visual Studio Enterprise 2019 version 16.10.3 also tested
```

### 1.

### 2.

### 3.

### 4.

### 5.

### 6.

### 7.

Building a Platform solution

```
Open Master branch of MetadataRuntime Platform.sln
```
```
Manage the Nuget sources and ensure you have the 3 package source feeds displayed below:
```
```
Access the modmom2_feed by entering the SWQA account credentials.
Restore the packages.
Upon completion, you will find this package in your nuget cache:
```
```
Build the Platform.sln. It should succeed without errors.
In addition, you should see the following NuGet packages created in your local source:
```
Building Camstar AOP Aspects Solution

```
Visual Studio Enterprise 2019 16.10.3 also tested.
```
```
It is important that you have the following packages here at this point. Other stages of the build process depend on them.
```

### 1.

### 2.

### 1.

### 2.

### 1.

### 2.

```
Open the Camstar AOP Aspects Solution.
```
```
Build the solution.
```
Building Observability Solution

```
Open the Observability Solution.
```
```
Build the solution
```
Building Sample Model

```
Open the Sample Model Project
```
```
Build the solution
```
Running Code Generator

```
Result
```
```
At this point, ALL non-generated code, frameworks and the SampleModel are compiled and built.
```

### 1.

### 2.

### 3.

### 4.

### 5.

### 1.

### 2.

### 3.

### 4.

### 1.

### 2.

### 3.

```
Back in the Platform solution, run the Dto Generator against the Sample Model.
```
```
Run the Generator (DTO Generator)
Open the Generated Models solution.
```
```
Build the solution.
Create or ensure you have an empty database created in SQL Server named ModMomDb or the name that's assigned in the env variable MODM
OM_DBNAME: the database should exist without tables.
```
Deploying Sample Model persistence configuration to the database

```
Open the Integration Tests Solution
```
```
Build the solution
Run all tests, everything should be marked as passed.
When the test succeeds, you have successfully deployed the database. Validate tables are deployed
```
Result

The database and logical models are now synchronized.

Configuring API

```
To ensure the HTTPS redirection works correctly, from a command window, issue this command:
```
```
c:\>dotnet dev-certs https --trust
```
```
The Security Warning dialog box will appear, click Yes.
```

### 3.

### 4.

### 5.

### 6.

### 7.

### 1.

### 2.

```
Back in the Platform solution, start the API.
The server will start on an arbitrary port. Check the console for the value
```
```
Open http://localhost:<port>/swagger to will see the various API endpoints.
Post the input below to your is input to your http://localhost:<port>/api/Factory Endpoint
```
### {

```
"tenantId": "first",
"entity": {
"defaultQty": 90,
"description": "First Factory",
"myProp": "prop1",
"name": "f1"
}
}
```
```
Validate the object is created the API.
```
Resolving Error: ERR_CONNECTION_REFUSED

```
Symptom: ERR_CONNECTION_REFUSED error
Cause: https redirection stops working when the browser tries to redirect from http://localhost:5000 to https://localhost:5001
Resolution/Workaround: Replace local launchSettings.json using this file from the repo.
```
Troubleshooting Build Issues

Resolving References

Projects will not build because dependencies cannot be resolved despite having proper references to either projects or DLLs.

The example below depicts the error that occurs. The underlined message suggests restoring via the command line.

Resolution/Workaround

```
Open a CMD window and get into the directory of the project that cannot build, highlighted above.
In this case, the directory is: C:\Work\ModMOM\MetadataRuntime\Platform\Tests\Siemens.MOM.Platform.ApiIntegrationTests
Issue the command: dotnet restore
```

# Siemens Web Framework Fundamentals

As described in the Siemens page https://viewer.docs.sws.siemens.com/en-US/product/224565084/sws/PL20181210134853460.apollo_topicSet/html
/web_framework_overview, "Siemens Web Framework (SWF) is a configuration first framework that lets you rapidly develop web applications with
significantly less code to write, test, maintain and download at runtime. It follows top trends in the front-end development like component-oriented design,
declarative coding, consolidation of state management, SPA and client-side routing, extensions via JS code to manage application complexity."

## Basic Operations: Video Repository of Modular MOM Meetings

The link below goes to the repository of videos recorded during the Knowledge Transfer meetings of Modular MOM 2.x. The shared folder contains
introductory videos of UI Development using SWF and BDD (Behavior Driven Development) UI Tests with Mars Automation Project (a Cucumber-JS
/Puppeteer automation framework).

Modular MOM 2.x - UI Development - All Documents (sharepoint.com)

To improve your knowledge regarding SWF, click the link to see basic SWF exercises :

https://internal.viewer.docs.sws.siemens.com/en-US/product/224565084/sws/PL20181210134853460.apollo_topicSet/html/hello_world_application

## Tutorials

GitLab tutorial

https://gitlab.industrysoftware.automation.siemens.com/Apollo/swf/-/tree/master/tutorials

SWF -react wiki documentation

https://gitlab.industrysoftware.automation.siemens.com/Apollo/swf/-/wikis/home

Showcase

[http://swf/](http://swf/)

Sample react application

[http://swf/react-sample-app](http://swf/react-sample-app)

Mars automation link

https://gitlab.industrysoftware.automation.siemens.com/LCSDevops/mars

Mars automation wiki

https://gitlab.industrysoftware.automation.siemens.com/LCSDevops/mars/mars-automation/-/wikis/home

## Siemens Web Framework Home Page

Navigate in the SWF home page to get more information from documentation to code quality matrix, and more:

https://gitlab.industrysoftware.automation.siemens.com/Apollo/afx/-/wikis/home

## SWF Support

Click the Microsoft Teams link to get help on SWF :

https://teams.microsoft.com/l/channel/19%3a7c4963399786464fb392511a27c793e4%40thread.tacv2/SWF%2520office%2520hours?groupId=b537b7d5-
8459-4ef5-afc2-164f6c01349d&tenantId=6b5bd02b-92d2-40b2-9ffd-c9c94280c757

### 1.

### 2.

### 3.

### 4.

### 5.

### 6.

### 7.

```
Please follow the tutorial in this order:
```
```
Hello world application
Consuming common app frame
URL Routing and Page Layout
Lists with Data Provider
Working with Commands
Working with Table | 6.Property Widgets | 6. Edit in summary
Edit in Table | | 7. Localization
```

### 1.

### 2.

### 3.

### 4.

### 5.

### 6.

### 7.

### 8.

### 9.

### 10.

### 11.

### 12.

### 13.

### 14.

### 15.

### 16.

### 17.

### 18.

### 19.

### 20.

# How to Create a Configurable Object

A Configurable Object contains a collection of Fields and can be represented in the C# class of the Object. It is the core of the Metamodel design because
it enables the creation of classes or object types to describe an object instance, the central idea of the Metadata.

## Prerequisites

The following software is required to create a new CO for Opcenter Modular Manufacturing:

```
Visual Studio version 16.8.3 or newer as well as the following Visual Studio workloads :
ASP.NET and web development
.NET desktop development
Latest version of .NET 5.0 SDK
SQL Server Management Studio version 14.0 Express or newer.
SQL Server version 14.0 Express or newer. Follow standard procedures to install the Default instance, otherwise change the environment
configuration in case of an instance-based installation. Then open SQL Server Management Studio and verify that you can connect to the server
by using the Windows authentication.
A tool to clone Git repositories. It is recommended to use either Sourcetree or Git Bash.
```
## Workflow

```
Prepare a database.
Create an empty folder for source code with a short name. For convenience the path to the created folder should contain no spaces.
Clone the following Git repositories into your folder for source code:
```
```
Git repository name Git repository URL
```
```
MetadataRuntime https://tfs05mom.industrysoftware.automation.siemens.com/MOM/ModularMOM/_git/MetadataRuntime
```
```
M1_Template https://tfs05mom.industrysoftware.automation.siemens.com/MOM/ModularMOM/_git/M1_Template
```
```
M1_Template.Tests https://tfs05mom.industrysoftware.automation.siemens.com/MOM/ModularMOM/_git/M1_Template.Tests
Update NuGet.Config.
Empty the folder C:\Users\<your user folder>\.nuget\packages.
Install System supplied tools and scripts
Prepare the MetadataRuntime Platform.
Set up a new model from the template.
Create a new CO.
Optionally, perform unit test on the new model.
Publish, deploy, and test End-to-End
Set the system variables.
Generate the new model.
Build the solutions.
Publish the Platform API.
Publish the data model.
Configure the Platform API.
Launch the generated API.
Cleaning MetadataRuntime Platform
Enter http://localhost:5000 in the address bar of your browser.
```
## Result

The new Configurable Object is created in a new module and works correctly.


### 1.

### 2.

### 3.

### 1.

### 2.

### 3.

# Preparing a Database

The following procedure explains how to prepare the database where a new Configurable Object will be created.

## Procedure

```
Open SQL Server Management Studio.
Make sure your login has the sysadmin role.
Do either of the following:
To use any existing database, drop it by executing the query for dropping existing database.
To create a new database, execute the query for creation of a new database.
```
Query for Dropping Existing Database

Before executing this query, replace <database name> with the name of the database you want to drop:

```
USE master
IF EXISTS(select * from sys.databases where name='<database name>')
BEGIN
ALTER DATABASE <database name> SET SINGLE_USER WITH ROLLBACK IMMEDIATE;
DROP DATABASE <database name>
END
```
```
CREATE DATABASE <database name>
ALTER DATABASE <database name> SET ALLOW_SNAPSHOT_ISOLATION ON
```
Query for Creation of a New Database

Before executing this query, replace <database name> with the name of your new database:

```
CREATE DATABASE <database name>
ALTER DATABASE <database name> SET ALLOW_SNAPSHOT_ISOLATION ON
```
## Next Steps

```
Create an empty folder for source code with a short name. For convenience the path to the created folder should contain no spaces.
Clone Git repositories into your folder for source code listed in the step 3 of How to Create a Configurable Object.
Update NuGet.Config
```

### 1.

### 2.

### 3.

### 4.

### 5.

### 6.

### 1.

### 2.

# Updating NuGet.Config

The models use nuget packages as references to access the MetadataRuntime binaries. These nuget packages are located when the system searches
the nuget.config files. The MetadataRuntime Plaform nuget packages are created as part of the Platform build pipeline process.

Follow this procedure to update the NuGet.Config file.

## Procedure

```
Open File Explorer.
Open the folder: C:\Users\<your username>\AppData\Roaming\NuGet.
Open the NuGet.Config file in any text editor.
Replace all in the <configuration> section with the following:
```
```
<packageSources>
<add key="nuget.org" value="https://api.nuget.org/v3/index.json" protocolVersion="3" />
<add key="nuget_global" value="C:\Users\<your user folder>\.nuget\packages" />
<add key="Microsoft Visual Studio Offline Packages" value="C:\Program Files (x86)\Microsoft
SDKs\NuGetPackages\" />
<add key="modmom2_feed" value="https://tfs05mom.industrysoftware.automation.siemens.com/MOM
/_packaging/ModMOM_Platform/nuget/v3/index.json" />
</packageSources>
<packageRestore>
<add key="enabled" value="True" />
<add key="automatic" value="True" />
</packageRestore>
<bindingRedirects>
<add key="skip" value="False" />
</bindingRedirects>
<packageManagement>
<add key="format" value="0" />
<add key="disabled" value="False" />
</packageManagement>
```
```
Replace <your user folder> with your user folder name.
Save the changes to the file.
```
## Next Steps

```
Empty the folder C:\Users\<your user folder>\.nuget\packages.
Prepare MetadataRuntime Platform
```

### 1.

# Install System Supplied Tools and Scripts

```
Install the idgenerator tool, selecting the current version
```
```
Install IdGenerator
```
```
dotnet tool install Siemens.MOM.Platform.Tools.IdGenerator --global
```

### 1.

### 2.

### 3.

### 4.

### 5.

### 6.

### 1.

### 2.

### 3.

### 4.

### 5.

# Preparing MetadataRuntime Platform

To prepare the MetadataRuntime Platform, you will need to build the required solutions located in the MetadataRuntime repository in the right sequence.
You can do it in one of the following ways:

```
Using Visual Studio where you will need to open and build each solution.
Running the PowerShell script that will build the required solutions in the right sequence automatically.
```
## Preparing MetadataRuntime Platform in Visual Studio

```
Open Visual Studio.
Build ObservabilityAspects.sln located in <path of your folder for source code>\MetadataRuntime\Automation\ILWeavers\Observability.
Build ObservabilityAspects.Tests.sln located in the same folder.
Build Camstar.AOP.Aspects.sln located in <path of your folder for source code>\MetadataRuntime\Automation\ILWeavers\Metadata.
Build Platform.sln located in <path of your folder for source code>\MetadataRuntime\Platform\InputMetadataModel\SampleModel.
Build Camstar.Core.SampleModel.sln located in <path of your folder for source code>\MetadataRuntime\Platform.
```
## Preparing MetadataRuntime Platform Using PowerShell Script

```
Run PowerShell as administrator.
Go to <path of your folder for source code>\MetadataRuntime.
Execute this command:
```
```
.\BuildAll.ps1
```
```
If an error occurs, execute the command again.
Close the appeared Command Prompt window where Siemens.MOM.Platform.Api.exe was run after executing the script.
```
## Next Step

Set Up a New Model from Template


### 1.

### 2.

### 3.

### 4.

```
a.
```
```
b.
5.
6.
```
# Setting Up a New Model from Template

Modular MOM provides the M1_Template and M1_Template.Tests repos as a tool used to create a starting point for a new model. Its usage is described
below.

## Procedure

```
Prepare repositories for your new model
Contact the Configuration Manager to create two repositories for your model with the names
M1_<your model name>
M1_<your model name>.Tests
Note that a short name without spaces for the model facilitates the management.
The repos can be at https://tfs05mom.industrysoftware.automation.siemens.com/MOM/ModularMOM/_git/ or on a different server (e.g.
gitlab).
Ensure you have cloned the M1_Template and M1_Template.Tests repos.
Open PowerShell window as administrator.
Go to the folder <path of your folder for source code>\M1_Template and execute the following command where <your model name> is the name
of the model created above.
```
```
.\Copy-From-Template.ps1 <your model name>
```
```
If your repos are stored in the same server as the M1_Template, you can accept the default for the model repo by pressing Enter when
you see this message:
```
```
If your repos were created on a different server, enter the URL for that server at the prompt.
Open Visual Studio
Build the solution M1_<your model name>.sln located in <path of your folder for source code>\M1_<your model name>.
```
## Next Step

Create a Configurable Object


# Creating a New CO

The Configurable Object contains a collection of Fields and can be represented in the C# class of the Object. It also enables the creation of classes or
object types to describe an object instance, the central idea of the Metadata.

As the example of creation of a new CO the following procedures are available:

```
Creating a New Named Object
Creating a New Service Object
```

### 1.

### 2.

### 3.

### 4.

### 5.

### 6.

### 7.

### 8.

# Creating a New Named Object

In this page, the model that was set up using M1_Template, will be modified by adding a new named object named NDO. Follow this procedure to create
a new named object.

## Procedure

```
Open Visual Studio.
Open M1_<your model name>.sln located in <path of your folder for source code>\M1_<your model name>.
Right-click M1_<your model name>.csproj.
Select Add then New Folder. (you can rename the new folder as you want.)
Right-click the new folder.
Select Add and then New Item.
Add a new C# class and name it as you want.
Replace all in the new C# class with your code following the code example and its description below.
```
```
This code adds a new inherited named object named NDO with three fields named F_Boolean, F_String, and F_Integer:
```
```
using Siemens.MOM.MetaModel.Framework;
using Siemens.MOM.MetaModel.Framework.AOP_Aspects;
using Siemens.MOM.Metadata.Common;
using System.Collections.Generic;
```
```
namespace M1_<your model name>.Model
{
[COTypeId("0kTZhcN")]
public partial class NDO : NamedObject
{
protected override void _RegisterFieldTypes()
{
base._RegisterFieldTypes();
RegisterField(FieldInitializer.BooleanField<NDO>.Get("F_Boolean"));
RegisterField(FieldInitializer.StringField<NDO>.Get("F_String"));
RegisterField(FieldInitializer.IntegerField<NDO>.Get("F_Integer"));
}
```
```
public BooleanField F_BooleanField = null;
public bool F_Boolean
{
get { return F_BooleanField.Value; }
set { F_BooleanField.Value = value; }
}
```
```
public StringField F_StringField = null;
public string F_String
{
get { return F_StringField.Value; }
set { F_StringField.Value = value; }
}
```
```
public IntegerField F_IntegerField = null;
public int F_Integer
{
get { return F_IntegerField.Value; }
set { F_IntegerField.Value = value; }
}
}
}
```
```
Code block or code line Description
```
```
namespace M1_<your
model name>.Model
```
```
Specifies the model namespace.
```
```
Replace <your model name> with the name of your model.
```

### 8.

```
[COTypeId
("0kTZhcN")]
```
```
COTypeId is a unique ID that must be assigned to each object in the model. If the same COTypeId is used in
different object, an error is generated while building the model.
```
```
To generate a new COTypeId (assumes the prerequisite of the IdGenerator tool is installed):
```
```
Create new COTypeId
```
```
IdGenerator COTypeId
```
```
This will generate and output a unique id which can be copy/pasted into the new class
```
```
public partial
class NDO :
NamedObject
```
```
Defines a new inherited class named NDO. partial means that this class can be extended by using different
workspaces. If a customer using the Platform wants to extend this class, partial will let the customer to create
the same class with the same name, make it partial too, and add either necessary fields or functionalities.
```
```
This class is inherited from NamedObject. The new named object NDO will have all the functionalities from Na
medObject and its parent BaseObject.
```
```
protected override
void
_RegisterFieldTypes
()
```
```
RegisterFieldTypes is a method that is present in BasedObject. This method is used to register all fields.
```
```
To register new fields in NDO, override is used. It used to override the fields of NamedObject and to execute a
new function instead of the function in NamedObject.
```
```
base.
_RegisterFieldTypes
();
```
```
The base class is not executed if override is used. base is used for execution of base class RegisterFieldTypes
first. It used to register the fields from NamedObject – Name and Description.
```
```
RegisterField
(FieldInitializer.
BooleanField<NDO>.
Get("F_Boolean"));
RegisterField
(FieldInitializer.
StringField<NDO>.
Get("F_String"));
RegisterField
(FieldInitializer.
IntegerField<NDO>.
Get("F_Integer"));
```
```
RegisterField is a method of Platform that is used to register a new field. It is required to specify field type and
a class the field is registered in, and property name.
```
```
To register a new field, specify field type and class name the field is registered in.
```
```
public
BooleanField
F_BooleanField =
null;
public bool
F_Boolean
{
```
```
public BooleanField F_BooleanField = null is a field. Field is something of a container that keeps data.
Properties are methods of access to this container.
```
```
The F_Boolean field has two methods :
```
```
get is requested to get data form the container.
```
```
To see the inherited functionalities, you can right-click NamedObject and select Go to Definition. In
the opened NamedObject.cs, you can see the functionalities that NDO will have. In particular, there
are two fields – Name and Description. Likewise, you can check the functionalities that NamedObje
ct inherits from BaseObject.
```

### 8.

### 9.

### 10.

### 11.

### 12.

### 13.

```
get { return
F_BooleanField.
Value; }
set {
F_BooleanField.
Value = value; }
}
```
```
set is requested to load data into the container.
```
```
In this code block, the Value field is requested and either a value is loaded into this field or data is loaded from
this field and then it is returned.
```
```
Save your changes to the C# class file.
Right-click the folder you created in step 4.
Select Add and then New Item.
Add a new JSON file.
```
```
Replace all in the new JSON file with your code following this code example and its description:
```
```
For each class, apart from a C# file, there is a JSON file with the same name for specifying additional properties The properties specified in the
JSON file will be united with the C# file and a new class will be created (class uniting the properties)
```
### {

```
"Description": "Named object",
"Label": "NDO_NamedObject",
"AssociatedServiceName": "Siemens.MOM.Metadata.Common.NamedDataService`1",
"Persistence": {
"TableName": "NDO"
},
"Fields": {
"F_Boolean": {
"Description": "BooleanField type property",
"Label": "F_Boolean",
"CreateOnInitialize": true,
"Persistence": {
"ColumnName": "F_Boolean"
}
},
"F_String": {
"Description": "StringField type property",
"Label": "F_String",
"Required": "UserInputRequired",
"Persistence": {
"ColumnName": "F_String"
}
},
"F_Integer": {
"Description": "IntegerField type property",
"Label": "F_Integer",
"Persistence": {
"ColumnName": "F_Integer"
}
}
}
}
```
```
Code block or code line Description
```
```
"Description": "Named object",
```
```
Each named object has description.
```
```
"Label": "NDO_NamedObject",
```
```
Each named object has a label. The label is required for future use of a new class. Also, the
label is used for internationalization.
```
```
The name of the JSON file must be the same as the C# file name. If your C# class is a generic one, the JSON file should contain the `1
postfix (for example, NDO`1.json).
```

### 13.

### 14.

### 15.

```
"AssociatedServiceName":
"Siemens.MOM.Metadata.Common.
NamedDataService`1",
```
```
Specifies service. If one works with a Modeling object externally, one works with the object
by using, for example, an API request or external system but we cannot get data from the
object directly.
```
```
"Persistence": {
"TableName": "NDO"
```
```
Persistence defines if data of this object will be saved to database. A persistent object is an
object that is saved to database. If an object is not saved to database, it is saved in RAM.
```
```
"TableName": "NDO" means that for NDO there will be a table with the same name in
database.
```
```
"Fields": {
```
```
Used to add additional parameters for the fields that were defined in the C# class.
```
```
"F_Boolean": {
"Description": "BooleanField
type property",
"Label": "F_Boolean",
"CreateOnInitialize": true,
"Persistence": {
"ColumnName": "F_Boolean"
```
```
"F_Boolean": – field name.
"Description": – each field has description.
"Label": – each field has a label.
"CreateOnInitialize": – if there is a request from a user without the F_Boolean field
and CreateOnInitialize is set to true, the field will be created automatically.
"Persistence": – used to define a table column in database.
```
```
"Required": "UserInputRequired",
```
```
By default, if Required is not used, it is set to Required: None automatically.
```
```
If UserInputRequired is used, a input is required from a user.
```
```
If just Required is used, when a user make no input, the system will create it.
Save your changes to the JSON file.
Build M1_<your model name>.sln.
```
Next Step

Perform Unit Tests on New Model


### 1.

### 2.

### 3.

### 4.

### 5.

### 6.

### 7.

### 8.

# Creating a New Service Object

In this page, a new service object named NamedService will be created for NDO that was created in How to Create a Configurable Object. Follow this
procedure to create a new service object

## Procedure

```
Open Visual Studio.
Open M1_<your model name>.sln located in <path of your folder for source code>\M1_<your model name>.
Right-click M1_<your model name>.csproj.
Select Add then New Folder (you can rename the new folder as you want.)
Right-click the new folder.
Select Add , then New Item.
Add a new C# class and name it as you want.
Replace all in the new C# class with your code following the code example and its description below.
```
```
This code adds a new inherited service object named NamedService that will allow you to add, update, and delete NDO:
```
```
using System.Collections.Generic;
using Siemens.MOM.MetaModel.Framework;
using Siemens.MOM.MetaModel.Framework.AOP_Aspects;
using Siemens.MOM.MetaModel.Interfaces;
using Siemens.MOM.Platform;
```
```
namespace M1_<your model name>.Services
{
[COTypeId("0x2rLbn")]
public partial class NamedService<ClassType> : ServiceObject, INamedModelingService<ClassType> where
ClassType : class, INamed
{
public ClassType MyData { get; protected set; }
IBaseObject IModelingService.GetModelingObject() => MyData;
IBaseObject IModelingService.Create() => Add() as IBaseObject;
bool IModelingService.Update(IBaseObject obj) => Update(obj as ClassType);
bool IModelingService.Delete(IBaseObject obj) => Delete(obj as ClassType);
```
```
public ClassType Add()
{
MyData = Create<ClassType>();
return MyData;
}
```
```
public virtual ClassType Add(string name)
{
MyData = Create<ClassType>();
MyData.ObjectName = name;
return MyData;
}
```
```
public virtual bool Update(ClassType newData)
{
MyData = newData;
return Update_User();
}
public virtual bool Update_User()
{ return true; }
```
```
public virtual bool Delete(ClassType data)
{
MyData = null;
return (data as BaseObject).Delete();
}
```
```
public virtual bool Delete(string name)
{
return (Get(name) as BaseObject).Delete();
}
```

### 8.

```
public IBaseObject Get(InstanceId id)
{
MyData = _ExecutionContext.GetById<ClassType>(id);
return MyData;
}
public virtual ClassType Get(string name)
{
MyData = _ExecutionContext.GetByName<ClassType>(name);
return MyData;
}
```
```
public IEnumerable<IBaseObject> GetAll(QueryOption queryOption)
{
return _ExecutionContext.GetAll<ClassType>(queryOption);
}
```
```
INamed INamedModelingService.Get(string name) => Get(name);
}
}
```
```
Code block or code line Description
```
```
namespace M1_<your model
name>.Services
```
```
Specifies the model namespace.
```
```
Replace <your model name> with the name of your model.
```
```
[COTypeId("0kTZhcN")]
```
```
COTypeId is a unique ID that must be assigned to each object in the model. If the same COTypeId
is used in different object, an error is generated while building the model.
```
```
To generate a new COTypeId (assumes the prerequisite of the IdGenerator tool is installed):
```
```
Create new COTypeId
```
```
IdGenerator COTypeId
```
```
This will generate and output a unique id which can be copy/pasted into the new class
```
```
public partial class
NamedService<ClassType> :
ServiceObject,
INamedModelingService<Class
Type> where ClassType :
class, INamed
```
```
Defines a new inherited class named NamedService. partial means that this class can be
extended by using different workspaces. If a customer using the Platform wants to extend this class,
partial will let the customer to create the same class with the same name, make it partial too, and
add either necessary fields or functionalities.
```
```
NamedService is inherited from ServiceObject. Base functionalities of NamedService are realized
in ServiceObject. So, all services that we want to implement must be inherited either from ServiceOb
ject or its successors.
```
```
INamedModelingService is an extended version of the IModeling interface. This interface is
designed to interact with the Modeling objects.
```
```
where ClassType : class, INamed – specifies that ClassType must realize the methods from the
INamed interface.
```
```
You can right-click INamedModelingService and select Go to Definition. You will see
what interfaces must be realized. The following methods must be created: Add, Delete,
and Update and the CRUD operations. They are basic operations that allow you to
create, remove, and update objects.
```

### 8.

```
public ClassType MyData {
get; protected set; }
IBaseObject
IModelingService.
GetModelingObject() =>
MyData;
IBaseObject
IModelingService.Create()
=> Add() as IBaseObject;
bool IModelingService.
Update(IBaseObject obj) =>
Update(obj as ClassType);
bool IModelingService.
Delete(IBaseObject obj) =>
Delete(obj as ClassType);
```
```
The type is specified by using ClassType as this a generic class. Apart from the class, ClassType
is also submitted. In this case, ClassType is NDO that is created in Creating a New Named Object.
```
```
public ClassType Add()
{
MyData =
Create<ClassType>();
return MyData;
}
```
```
public virtual ClassType
Add(string name)
{
MyData =
Create<ClassType>();
MyData.ObjectName =
name;
return MyData;
}
```
```
An object of the ClassType type is created and added to MyData.
```
```
public virtual bool Update
(ClassType newData)
{
MyData = newData;
return Update_User();
}
public virtual bool
Update_User()
{ return true; }
```
```
public virtual bool Delete
(ClassType data)
{
MyData = null;
return (data as
BaseObject).Delete();
}
```
```
public virtual bool Delete
(string name)
{
return (Get(name) as
BaseObject).Delete();
}
```
```
This is the Delete method. MyData is set to null. Whatever the information, MyData is set to null and
the base Delete method of BaseObject is requested.
```
```
public IBaseObject Get
(InstanceId id)
{
```
```
Get is an additional method. It returns a specific object after receiving ID. If we need to receive NDO
and we know the ID, we can use the Get method.
```

### 8.

### 9.

### 10.

```
MyData =
_ExecutionContext.
GetById<ClassType>(id);
return MyData;
}
public virtual ClassType
Get(string name)
{
MyData =
_ExecutionContext.
GetByName<ClassType>(name);
return MyData;
}
```
```
public
IEnumerable<IBaseObject>
GetAll(QueryOption
queryOption)
{
return
_ExecutionContext.
GetAll<ClassType>
(queryOption);
}
```
```
INamed
INamedModelingService.Get
(string name) => Get(name);
```
```
GetAll is an additional method. It returns all objects of the type that satisfies parameters of
QueryOption. If the parameters are not defined, all the objects of NDO are returned.
```
```
Save your changes to the C# class file.
Build M1_<your model name>.sln.
```
Next Step

Perform Unit Tests on New Model


### 1.

### 2.

### 3.

### 4.

### 5.

# Performing Unit Tests on New Model

The model that was set up following the steps in Setting Up a New Model from Template with the named object and service object created following the
steps in Creating a New Named Object and Creating a New Service Object respectively is used here as the example of how to unit test a new model.
Follow these steps to unit test your new model.

## Procedure

```
Open Visual Studio.
Open M1_<your model name>.sln located in <path of your folder for source code>\M1_<your model name>.
In Solution Explorer, expand M1_<your model name>.Tests.
Select M1_<your model name>_UnitTests.cs.
The selected C# file contains the test class that was generated to perform tests. By default, the test class name is <your model name>_UnitTest.
The test class contains predefined fields that are designed to help with testing. Follow description of the test class below.
```
```
Code line Description
```
```
readonly BLEngineConfig
blConfig = new
BLEngineConfig("TestModel",
System.IO.Path.GetFullPath
(@".\M1_MyModel.dll"));
```
```
blConfig is a configuration file that contains the name of your model and the path to the .dll file that
was generated while setting up the new model. The path is relative.
```
```
readonly
BizLogicEngineFactory
blEngineFactory = new
BizLogicEngineFactory();
```
```
Contains Factory. Factory is the factory for creation of Engine. Engine is a basic object providing
the capability to perform tests. Engine creates Meta-Metamodel.
```
```
Mock<ILogger> mockLogger;
```
```
Logger is used for logging. It will not be used in this topic.
```
```
Assert.IsNotNull(blEngine,
"blEngine is null");
```
```
Checks blEngine to be created and not to be null.
```
```
Add your test method following the code example and its description below:
```
```
This test method is designed to test NDO that was created in Creating a New Named Object.
```
```
[TestMethod]
public void TestMetaMetadata()
{
// Make sure Configurable 'Types' are initialized
var configTypes = blEngine.ConfigurableTypes;
Assert.IsTrue(configTypes.Any());
```
```
System.Type type = blEngine.GetType("M1_<your model name>.Model.NDO");
Assert.IsNotNull(type);
```
```
var metaType = blEngine.GetMetaType(type);
Assert.IsNotNull(metaType);
Assert.IsFalse(metaType.IsService());
Assert.IsTrue(metaType.IsPersistent());
Assert.IsTrue(metaType.IsNamed());
Assert.IsFalse(metaType.IsFrameworkDefinedType());
Assert.IsFalse(metaType.IsSystemObject());
```
```
foreach (var fmKVP in metaType.Fields)
```
```
The new Metamodel named <M1_<your model name> was created following the steps
in Setting Up a New Model From Template and Creating a New CO. The new
Metamodel contains the NDO object (the NDO objects contain the model). The Meta-
Metamodel contains a class that describes your model. Your model has different
attributes that will be used for testing.
```

### 5.

### {

```
var fm = fmKVP.Value;
_ = fm.FieldType;
_ = fm.DataType;
_ = fm.IsList();
_ = fm.IsPersistent();
_ = fm.TypeCategory;
bool val;
// scalar types
val = fm.IsNamedRef();
val = fm.IsRevisionedRef();
val = fm.IsSubentityRef();
val = fm.IsNamedSubentityRef();
val = fm.IsSubentity();
val = fm.IsNamedSubentity();
// list types
val = fm.IsNamedRefList();
val = fm.IsRevisionedRefList();
val = fm.IsSubentityRefList();
val = fm.IsNamedSubentityRefList();
val = fm.IsSubentityList();
val = fm.IsNamedSubentityList();
}
```
```
var fields = metaType.Fields;
var fld = fields["F_Boolean"];
Assert.IsNotNull(fld);
Assert.IsTrue(fld.CreateOnInitialize);
Assert.AreEqual(fld.Required, RequiredFieldFlag.None);
```
```
fld = fields["F_String"];
Assert.IsNotNull(fld);
Assert.IsFalse(fld.CreateOnInitialize);
Assert.AreEqual(fld.Required, RequiredFieldFlag.UserInputRequired);
Assert.AreEqual(fld.TypeCategory, FieldTypeCategory.Native);
}
```
### }

### }

```
Code block/Code line Description
```
```
var configTypes = blEngine.
ConfigurableTypes;
```
```
Checks blEngine to initialize Configurable Types.
```
```
Assert.IsTrue(configTypes.Any());
```
```
configTypes.Any lets to make sure the Configurable Types are not empty.
```
```
System.Type type = blEngine.GetType
("M1_<your model name>.Model.NDO");
```
```
Gets the type by the type name. The type variable gets the NDO class.
```
```
Assert.IsNotNull(type);
```
```
Checks the type to be found and not to be null.
```
```
var metaType = blEngine.GetMetaType
(type);
```
```
Gets the meta-metamodel of the NDO. GetMetaType gets description of the NDO. met
aType lets to check properties of the NDO class.
```
```
This line finds the NDO class with the three fields that was defined while
following the steps in Creating a New CO.
```
```
The type contains different properties. In particular, these three fields were
defined while following the steps in Creating a New Named Object:
```

### 5.

```
Assert.IsNotNull(metaType);
```
```
Checks the NDO to be got and its name has no mistypes.
```
```
Assert.IsFalse(metaType.IsService());
```
```
Checks the NDO class not be a service.
```
```
Assert.IsTrue(metaType.
IsPersistent());
```
```
Checks the NDO class to be persistent.
```
```
Assert.IsTrue(metaType.IsNamed());
```
```
Checks the class to realize the INamed interface.
```
```
Assert.IsFalse(metaType.
IsFrameworkDefinedType());
```
```
Checks the metaType not be a framework type.
```
```
Assert.IsFalse(metaType.
IsFrameworkDefinedType());
```
```
Checks the metaType not be a system object.
```
```
foreach (var fmKVP in metaType.
Fields)
```
```
metaTypeFields contains a collection of all fields including fields in the parent NDO.
```
```
var fm = fmKVP.Value;
_ = fm.FieldType;
_ = fm.DataType;
_ = fm.IsList();
_ = fm.IsPersistent();
_ = fm.TypeCategory;
```
```
Fields for each field that can be useful. For example, to check that it is a list, field a is
persistent one.
```
```
// scalar types
val = fm.IsNamedRef();
val = fm.IsRevisionedRef();
val = fm.IsSubentityRef();
val = fm.IsNamedSubentityRef();
val = fm.IsSubentity();
val = fm.IsNamedSubentity();
```
```
Checks that each field realizes a particular interface.
```
```
F_Boolean
F_Integer
F_String
```
```
In the test method, after getting the type, the NDO class is inserted in the
variable. Then, we get metaType of this type.
```
```
While following the steps in Creating a New Named Object, persistence was
defined in the .json file.
```
```
This line checks that the NDO class we get is a custom class, not a class in
the framework.
```
```
This line checks that the NDO class we get is not a system object.
```

### 5.

### 6.

```
// list types
val = fm.IsNamedRefList();
val = fm.IsRevisionedRefList();
val = fm.IsSubentityRefList();
val = fm.IsNamedSubentityRefList();
val = fm.IsSubentityList();
val = fm.IsNamedSubentityList();
```
```
Checks that fields are lists of different types. We check nothing here, but provide a list
of useful methods for metType that can be needed for the user in the future.
```
```
var fields = metaType.Fields;
```
```
Records all the fields of our metaType in the Fields variable.
```
```
var fld = fields["F_Boolean"];
```
```
Gets access to a specific field and starts working with it. In this line, get access to the
F_Boolean field.
```
```
Assert.IsNotNull(fld);
```
```
Checks the field to be found, that it exists, and there is no mistype.
```
```
Assert.IsTrue(fld.
CreateOnInitialize);
```
```
Checks that CreateOnInitialize is set to true.
```
```
Assert.AreEqual(fld.Required,
RequiredFieldFlag.None);
```
```
Checks that the Required value of our field is set to none.
```
```
In the JSON, we don't have it. By default, if we did not define it, it is set to none.
```
```
fld = fields["F_String"];
```
```
Gets access to a specific field and starts working with it. In this line, get access to the
F_String field.
```
```
Assert.IsNotNull(fld);
```
```
Checks the field to be found, that it exists, and there is no mistype.
```
```
Assert.IsFalse(fld.
CreateOnInitialize);
```
```
Checks that CreateOnInitialize is set to false.
```
```
Assert.AreEqual(fld.Required,
RequiredFieldFlag.UserInputRequired);
```
```
Checks that the Required value of our field is set to UserInputRequired.
```
```
This was defined in the .js. Tihis field was defined there clearly, if it was not, it would be
none.
```
```
Assert.AreEqual(fld.TypeCategory,
FieldTypeCategory.Native);
```
```
Checks TypeCategory of native and that the field is of prime and scalar value.
```
```
Scalar are all prime values like Boolean String Decimal DataTime and so on.
```
```
Depending on your model, add another test methods following the code example and its description below:
```
```
This test method is designed to test NamedService that was created in Creating a New Service Object.
```
```
[TestMethod]
public void TestNamedService()
{
```

### 6.

```
var svcType = blEngine.GetActualTypeFromGeneric("M1_<your model name>.Services.
NamedService`1", "M1_<your model name>.Model.NDO");
Assert.IsNotNull(svcType);
```
```
var metaType = blEngine.GetMetaType(svcType);
Assert.IsNotNull(metaType);
```
```
Assert.IsTrue(metaType.IsService());
Assert.IsFalse(metaType.IsPersistent());
```
```
Assert.IsFalse(metaType.IsFrameworkDefinedType());
Assert.IsFalse(metaType.IsSystemObject());
```
```
IService service = blEngine.CreateService(svcType);
string entityName = service + "_NdoTestEntity";
```
```
// Test Add(name)
dynamic instance = (service as dynamic).Add(entityName);
Assert.IsNotNull(instance);
Assert.AreEqual<string>(entityName, instance.ObjectName);
```
```
// Test Get(name)
instance = (service as dynamic).Get(entityName);
Assert.IsNotNull(instance);
Assert.AreEqual<string>(entityName, instance.ObjectName);
```
```
// Test Delete(name)
dynamic bIsDeleted = (service as dynamic).Delete(entityName);
Assert.IsTrue(bIsDeleted);
instance = (service as dynamic).Get(entityName);
Assert.IsNull(instance);
```
```
service.ExecuteService();
}
}
}
```
```
Code block/Code line Description
```
```
var svcType = blEngine.GetActualTypeFromGeneric("M1_<your
model name>.Services.NamedService`1", "M1_<your model name>.
Model.NDO");
```
```
Type of the service is received. The type consists
of the two parameters:
```
```
generic type that will be used (NamedService)
class type (NDO).
```
```
Assert.IsNotNull(svcType);
```
```
Type of the service is received.
```
```
var metaType = blEngine.GetMetaType(svcType);
Assert.IsNotNull(metaType);
```
```
Meta type is received. The meta type is a
combination .cs file and .json file.
```
```
Assert.IsTrue(metaType.IsService());
```
```
Checks the created object to be a service.
```
```
Assert.IsFalse(metaType.IsPersistent());
```
```
Checks the created service not to be a persistent
one.
```
```
Assert.IsFalse(metaType.IsFrameworkDefinedType());
```
```
Checks the created object not be a framework type.
```
```
Checks the created object not be a system object.
```

### 6.

### 7.

### 8.

### 9.

### 10.

### 11.

```
Assert.IsFalse(metaType.IsSystemObject());
```
```
IService service = blEngine.CreateService(svcType);
string entityName = service + "_NdoTestEntity";
```
```
The service is created. Then, a variable is created.
The variable is a combination of the service and
NDO.
```
```
// Test Add(name)
dynamic instance = (service as dynamic).Add(entityName);
Assert.IsNotNull(instance);
Assert.AreEqual<string>(entityName, instance.ObjectName);
```
```
Tests the Add method.
```
```
// Test Get(name)
instance = (service as dynamic).Get(entityName);
Assert.IsNotNull(instance);
Assert.AreEqual<string>(entityName, instance.ObjectName);
```
```
Tests the Get method.
```
```
// Test Delete(name)
dynamic bIsDeleted = (service as dynamic).Delete(entityName);
Assert.IsTrue(bIsDeleted);
instance = (service as dynamic).Get(entityName);
Assert.IsNull(instance);
```
```
Tests the Delete method.
```
```
Add the following usings to the header:
```
```
using System.Linq;
using Siemens.MOM.MetaModel.Interfaces.Metadata;
using Siemens.MOM.MetaModel.Interfaces;
```
```
Replace M1_<your model name> with your model name.
Save the changes to the file.
Open Test Explorer.
Run the test.
```
Next Step

Publishing, deploying, and performing test End-to-End


# Publishing, Deploying, and Performing Test End-to-End

Link to initial discussion with Mauro: https://splm-my.sharepoint.com/:v:/g/personal/xb3vwy_splm_siemens_com
/EZzCO6wya1dEhAK5UW6SgbkB8VkMbrKQxjhdEQNRJ4nHRA.

```
To be filled with content. It is required to clarify how to do it.
```

### 1.

### 2.

### 3.

### 4.

# Setting System Variables

Follow this procedure to set the system variables.

## Procedure

```
Open System Properties.
Select the Advanced tab.
Click Environment Variables.
Add the following system variables:
```
```
Variable name Variable value
```
```
MODMOM_DBNAME Enter the name of the database you defined while preparing the database.
```
```
MODMOM_DBHOSTNA
ME
```
```
Enter your server name. For example, VM-VDIP41-019.
```
### MODMOM_DBUSERNA

### ME

```
Trusted_Connection
```
```
MODMOM_MODULE Enter the full name of your model that was generated while setting up a new model from the template (M1_<your
model name>).
```
## Next Step

Generate New Models


### 1.

### 2.

### 3.

### 4.

### 5.

# Generating New Models

Follow this procedure to generate the new models.

## Procedure

```
Open PowerShell as administrator.
In File Explorer, create a new folder named GeneratedModel_<your model name> in <path of your folder for source code>\MetadataRuntime\Plat
form.
Go to this folder: <path of your folder for source code>MetadataRuntime\Platform\Siemens.MOM.Model.DtoGenerator\bin\Debug\net5.0\.
Run the script:
```
```
.\Siemens.MOM.Model.Generator.exe <path of your folder for source code>\M1_<your model
name>\bin\Debug\net5.0\M1_<your model name>.dll <path of your folder for source
code>\MetadataRuntime\Platform\GeneratedModel_<your model name> <path of your folder for source
code>\MetadataRuntime\Platform\Siemens.MOM.Model.DtoGenerator\Templates M1_<your model name>
```
```
Check <path of your folder for source code>\MetadataRuntime\Platform\GeneratedModel_<your model name> to contain these folders:
M1_<your model name>.Dto
M1_<your model name>.Proto
M1_<your model name>.Write
```
## Next Step

Build Solutions


### 1.

### 2.

### 3.

### 4.

### 5.

### 6.

### 1.

### 2.

### 3.

### 4.

# Building Solutions

You can build the solutions in one of the following ways:

```
Using Visual Studio where you can build all the required solutions at once.
Using PowerShell where you will need to build the solutions one by one:
```
## Using Visual Studio

```
Open Visual Studio.
Build GeneratedModels.sln located in <path of your folder for source code>\MetadataRuntime\Platform\GeneratedModel_<your model name>.
Open File Explorer.
Open this folder: <path of your folder for source code>\MetadataRuntime\Platform\GeneratedModel_<your model name>\bin\Debug\net5.0.
Copy these files:
M1_<your model name>.Common.deps.json
M1_<your model name>.Common.dll
M1_<your model name>.Common.pdb
M1_<your model name>.Dto.deps.json
M1_<your model name>.Dto.dll
M1_<your model name>.Dto.pdb
M1_<your model name>.Proto.deps.json
M1_<your model name>.Proto.dll
M1_<your model name>.pdb
M1_<your model name>.Write.deps.json
M1_<your model name>.Write.dll
M1_<your model name>.pdb
M1_<your model name>.Write.runtimeconfig.dev.json
M1_<your model name>.Write.runtimeconfig.json
```
```
Paste the copied files into this folder: <path of your folder for source code>\MetadataRuntime\Platform\GeneratedModel_<your model name>\bin.
```
## Using PowerShell

```
Open PowerShell as administrator.
Run this script:
```
```
dotnet build <path of your folder for source code>\MetadataRuntime\Platform\GeneratedModel_<your model
name>\M1_<your model name>.Dto\M1_<your model name>.Dto.csproj -c Debug -o <path of your folder for
source code>\MetadataRuntime\Platform\GeneratedModel_<your model name>\bin
```
```
Run this script:
```
```
dotnet build <path of your folder for source code>\MetadataRuntime\Platform\GeneratedModel_<your model
name>\M1_<your model name>.Proto\M1_<your model name>.Proto.csproj -c Debug -o <path of your folder for
source code>\MetadataRuntime\Platform\GeneratedModel_<your model name>\bin
```
```
Run this script:
```
```
dotnet build <path of your folder for source code>\MetadataRuntime\Platform\GeneratedModel_<your model
name>\M1_<your model name>.Write\M1_<your model name>.Write.csproj -c Debug -o <path of your folder for
source code>\MetadataRuntime\Platform\GeneratedModel_<your model name>\bin
```
## Next Step

```
Publish Platform API
```

### 1.

### 2.

# Publishing Platform API

Follow this procedure to publish the PlatformAPI:

## Procedure

```
Open PowerShell as administrator.
Run this command:
```
```
dotnet publish <path of your folder for source code>\MetadataRuntime\Platform\Siemens.MOM.Platform.
Api\Siemens.MOM.Platform.Api.csproj -f net5.0 -c Debug -o <path of your folder for source
code>\MetadataRuntime\Platform\GeneratedModel_<your model name>\bin
```
## Next Step

Publish Data Model


### 1.

### 2.

# Publishing Data Model

Follow this procedure to publish the data model.

## Procedure

```
Open PowerShell as administrator.
Run this command:
```
```
dotnet publish <path of your folder for source code>\M1_<your model name>\M1_<your model name>.sln -f
net5.0 -c Debug -o <path of your folder for source code>\MetadataRuntime\Platform\GeneratedModel_<your
model name>\bin
```
## Next Step

Configure Platform API


### 1.

### 2.

### 3.

### 4.

# Configuring Platform API

Follow this procedure to configure the Platform API.

## Procedure

```
Open modelsettings.json located in <path of your folder for source code>\MetadataRuntime\Platform\GeneratedModel_<your model name>\bin
in any text editor.
Replace all with the following:
```
### {

```
"ModularMOM": {
"Metadata": {
"domainmodellocation": "M1_<your model name>.dll",
"inputmodellocation": "M1_<your model name>.Dto.dll",
"inputmodelnamespace": "M1_<your model name>.Dto",
"requestmodelnamespace": "M1_<your model name>.Request",
"responsemodelnamespace": "M1_<your model name>.Response",
"writemodellocation": "M1_<your model name>.Write.dll",
"writemodelnamespace": "M1_<your model name>.Write",
"messagemodellocation": "M1_<your model name>.Proto.dll",
"messagemodelnamespace": "Siemens.MOM.MessageModel"
},
"ModelInstrumentation": {
"Providers": [
"M1_<your model name>"
]
}
}
}
```
```
Replace <your model name> with the name of your model.
Save the file.
```
## Next Step

Launch Generated API


### 1.

### 2.

### 3.

# Launching Generated API

Follow this procedure to launch the generated API.

## Procedure

```
Open PowerShell as administrator.
Go to this folder: <path of your folder for source code>\MetadataRuntime\Platform\GeneratedModel_<your model name>\bin.
Run this command:
```
```
dotnet Siemens.MOM.Platform.API.dll
```
## Next Step

Clean MetadataRuntime Platform


### 1.

### 2.

### 3.

### 4.

### 5.

### 6.

# Cleaning MetadataRuntime Platform

Follow this procedure to clean the MetadataRuntime Platform.

## Procedure

```
Run PowerShell as administrator.
Go to <path of your folder for source code>\MetadataRuntime.
Execute this command:
```
```
.\CleanAll.ps1
```
```
Open File Explorer.
Open this folder: C:\Users\<your user folder>\.nuget.
Create a folder named packages.
```
## Next Step

Enter [http://localhost:5000](http://localhost:5000) in the address bar of your browser.

## Result

The new Configurable Object is created.


### 1.

### 2.

### 3.

### 4.

### 5.

### 6.

### 7.

### 8.

### 9.

### 10.

# Creating a Metadata Model

Opcenter Modular Manufacturing is based on the Metadata-driven architecture so the Metamodel is used to define the Metadata for Opcenter Modular
Manufacturing, that is, to configure the business objects and processes used by Opcenter Modular Manufacturing applications.

To create a Metamodel you have to create the related Solution and populate it with entities. To better configure the model, a default template is provided
and its folder structure (the same as the template repository) is organized to incorporate repositories as submodules in order to store separately the Model
dependencies in their own git repos. These submodules can be shared among other modules to automatically update the submodules code when the
related repository is updated.

## Folder Structure

The folder structure of the sample model M1_Model is the following:

```
Model project: M1_Model
props
config
.gitmodules
Submodule
source code for the submodule
Test project: M1_Model.Tests
Integration test project: M1_Model.IntegrationTests
NuGets folder
Docker file
Generate commands
```
## Procedure

```
Click the link https://tfs05mom.industrysoftware.automation.siemens.com/MOM/ModularMOM/_git/WS_M1_Template
In the TFS breadcrumb displayed on top select M1_Template as the repository for create the Metamodel solution and select master as branch in
the drop-down list box below: the M1_Template is displayed.
At the top right corner, select Clone and then the Visual Studio option to clone the repository.
Browse for the repository location and rename the template folder with the name of the module that will contain the configured solution and its
defined structure.
In Visual Studio, check out the local branch.
Perform the following operations in order to be aligned with the template name:
Update props.
Update credentials and NuGet paths in conf.
Update the URL of desired submodule repository in the .gitmodule
Check that the NuGet folder is created in the repository to manage all NuGet packages.
In Visual Studio, open the solution and add the Metamodel classes inherited from the framework ones. In order to be persisted in the database,
the information regarding any class is written in the following files:
entityname.cs: contains the business logic and any other logic to be written in this C# class, it can be inherited from the following base
classes:
NamedObject if it has to be named object
NameSubentityOf<T> in case of SubEntities
entityname.json: specifies any configuration and composition on this C# class and allows to configure persistence table and column
information for the defined field.
entityService.cs: contains logic which can be exposed as API to external system. Available framework defined templates:
NamedDataService for CRUD operations on entities
ShopFloorService for implement operations performed in MES system.
Define the corresponding class properties.
Update the dll path in the batch script to generate and build models for APIs
Create the Docker Image and put it in the root directory.
```
## Result

The Metadata Model is created Note that you can repeat this procedure to create as many Models as you want in order to compose Apps to finally create
the Product.

```
Note that in case of new Metamodel class or service, each configurable object must be decorated with the attribute COTypeId(“<CO Type Id
value>)” and the value of COTypeId has to be uniquely generated by using the IdGenerator.Tool present in MetadataRuntime solution.
```

# Developing an App / Metadata Module

A Metadata Module is an isolated and independently deployable unit that can be composed with other loosely couple modules in order to create a
customized Product. A metadata model is used to configure the business objects and processes used by a Modular MOM application. A metadata model
will have dependencies on many other models that are maintained separately and stored in their own git repos.

## Directory Structure

Because the metadata is persisted as C# files and must be compiled into a single binary, a module and all of its dependencies must all be accessible to
the project, both during development (for intelligence to work properly) and during the build. However, nested dependencies will cause compilation issues if
they are all included. To address this, each module will specify a flat list of all of the dependencies required as you can see in the examples below.

Example of nested dependencies and the obvious duplicates

```
MESApp
Common
Dispatch
Common
ESigs
Common
TrackAndTrace
Common
DataCollection
Common
ESigs
Common
ESigs
Common
DataCollection
Common
```
Example of the module dependencies without the duplication

```
MESApp
Common
Dispatch
TrackAndTrace
ESigs
DataCollection
```
In order to create a consistent Module directory structure, Modular MOM uses the submodules feature of git, so each model must observe a consistent
directory structure.

The complete repos for each dependency will be cloned into a directory under the Code folder, excluding any deeper dependencies.

Each module will have an associated .Tests repo managed as a dependency to the module.

## Module Directory Layout

To fulfill this requirement, Modular MOM is utilizing the submodules feature of git. To more easily manage these dependency submodules, each model will
conform to a consistent directory structure. The structure without any dependencies added will have the following structure:

MESApp
\Code
\Model
\Services
\Tracking
dependencies.props
nuget.config
.csproj
\Tests
MESApp.sln
README.md
.gitmodules
Copy-From-MESApp.ps1

The complete repos for each dependency will be cloned into a directory under the Code folder:

MESApp
\Code
\M1_Common
\M1_DataCollection


### 1.

### 2.

### 3.

### 4.

### 5.

### 6.

### 1.

### 2.

### 3.

### 4.

### 5.

### 1.

\M1_Dispatch
\M1_ESigs
\M1_TrackAndTrace

One effect of this change is that each module will have an associated .Tests repo, and this repo is treated as a dependency to the module.

MESApp.Tests
\IntegrationTests\MyApp.IntegrationTests
\Unit.Tests

MESApp
\Tests
\MyApp.Tests

Creating a New Module

To create a new module you have to modify a copy of the module M1_Template.

Prerequisites

Git version 2.3 or higher to run the script Copy-From-Template.ps1. It is highly recommended to install the latest version.

Procedure

The procedure below explains how to create the MeSApp module. module. Follow the steps below to create the MESApp module:

```
Create the "remote" repos. This requires the assistance of someone who has the permission to add repos. These repos should be empty (i.e.
without history).
Module: M1_MESApp
Module tests: M1_MESApp.Tests
Open a Powershell command prompt window
Navigate to the directory containing the repos CD \Repos
If you have not yet, get or update the Template repos
git clone https://tfs05mom.industrysoftware.automation.siemens.com/MOM/ModularMOM/_git/M1_Template M1_Template
git clone https://tfs05mom.industrysoftware.automation.siemens.com/MOM/ModularMOM/_git/M1_Template.Tests M1_Template.Tests
Copy the template as a starting point for the new model:
CD M1_Template
Execute the script ./Copy-From-Template.ps1 with the following parameters:
```
```
Parameter Description
```
```
newName The name of the repo to create. In this example: MESApp
```
```
newURL The URL of the remote git server where the model will be created. It defaults to the same repo as the M1_Template. If
you're using a separate git server, e.g. https://gitlab.com/username/
```
### -

```
replaceRepo
```
```
This switch will replace any data in the remote repo.
```
```
Example: ./Copy-From-Template.ps1 MESApp -replaceRepo
Using your IDE, search for TODO and manually make the changes described such as the module description.
```
Modifying a Module Open a Powershell command prompt window

```
Go to the directory that contains the repos: CD \Repos
Get the module code: git clone https://tfs05mom.industrysoftware.automation.siemens.com/MOM/ModularMOM/_git/M1_MESApp M1_MESApp
Enter the module directory: CD M1_MESApp
Pull the code for existing dependencies: git submodule update --init --depth=1
Put the Tests submodule onto the main branch:
cd Tests/M1_MESApp.Tests
git checkout main
```
Adding a Submodule to an existing module

```
Add the new module e.g. ESigs: git submodule add https://tfs05mom.industrysoftware.automation.siemens.com/MOM/ModularMOM/_git
/M1_ESigs Code/M1_ESigs
```
```
DO NOT clone the new git repo to your local machine. The /Copy-From-Template.ps1 MESApp will create the repo and push it to the remote
repo.
```

### 1.

### 2.

### 1.

### 2.

### 3.

```
a.
b.
```
### 1.

### 2.

### 3.

### 4.

### 5.

### 1.

### 2.

### 3.

Building the Module

```
Open the module solution.
Build the module project.
```
Updating Test Code

```
Open the module solution.
Make the changes to the test project.
Commit and push the changes from the test submodule and then:
From the Tests/MESApp.Tests directory, this will update the reference in the module to the new commit point.
From the M1_MESApp directory, this will save the updated commit point for the next time it's cloned
```
Updating a Submodule to a new Commit Point

Once changes have been made to a submodule repo, the other repos that depend on the submodule must be updated to reference the new commit point.

```
Navigate to the submodule directory : cd /repos/M1_MESApp/Code/ESigs
Get potential commit points: git fetch
Update the commit point: git checkout \[main | branchName]
cd ../..
Commit and push the changes to the M1_MESApp module
```
Removing a Submodule from an Existing Module

```
Remove the code: git submodule deinit Code/M1_ESigs
Remove the reference: git rm Code/M1_ESigs
Cleanup git: Remove-Item -LiteralPath ".git/modules/M1_ESigs" -Force -Recurse
```
Troubleshooting

Symptom:

remote: TF401035: The object '0bfc9ad18ee19be277937ff25a8f70ea0acab2d2' does not exist.
fatal: protocol error: bad pack header

Cause:

A submodule was updated in place (e.g. changes made to M1_POC\Code\M1_Common instead of \M1_Common) and committed but not pushed. and the
submodule reference in the parent module was committed and pushed. This causes the remote repo to reference a commit identifier from the local repo
that hasn't been pushed to the remote repo.

Fix:

Go to the submodule and checkout main.

```
Notes for Troubleshooting
```
```
Building submodules is not needed and will cause problems in compiling the containing module.
In case of an error indicating that something cannot be found in Comon.Tests, check if the "--depth=1" was missing from the git submodule
update command
```

# Common Guidelines

Ideas ...

```
Development Guidelines
Sonar Qube
...
```

# Glossary

```
Module Naming Convention:
Apps
Business Related
Software Related
```
Please add when required:

## Module Naming Convention:

```
Name Short Description
```
```
Solution Collection of Apps servicing a specific purpose/industry
```
```
App/Module Purposeful software that can be used independently providing monetized value
```
```
Tool Configure and administer solutions and apps
```
```
Common Service Non-independent capabilities available in more than one app
```
## Apps

```
App Short Description
```
```
Order Management Import Production Order
Create Work Order (might be done in Scheduling)
Preparation and Launch of Work Order
Supervision and Control of Work Order Status (Suspend/Resume)
```
```
Track and Trace Collect and Trace manufacturing activities,
Compute and Track,
Work In Process, Resources and Work Alert,
Send performance feedback,
Record “as built” data
```
```
Equipment Management Maintain factory layout and resources of an enterprise to route processes & resources
```
```
Material Management Maintain catalogue of materials, BOM, routings, formulas and product defects
```
```
Administration Maintains and manages users, roles, and permissions
```
## Business Related

```
Term Description
```
```
Production
Order
```
```
the order from the customer to manufacture some product
```
```
Product
/Revision
```
```
the item ordered by the customer
```
```
Order
Quantity
```
```
the number of items ordered by the customer
```
```
Planned Start
/End Date
```
```
dates and timestamps planned internally by planning and scheduling application
```
```
Actual Start
/End Date
```
```
dates and timestamps tracking actual start and end
```
```
Material the items that go into the making of the product as well as the product itself
```
```
Operation
Sequence
```
```
order in which the operations are performed
```
```
Quantity
Fields
```
```
the quantity indicating what is queuing, what is in process, and what is produced
```
```
Container used to transport the finished product from one location to another location
```

```
Active
Directory
User
```
```
Windows domain user.
```
```
B2MML The B2MML (Business to Manufacturing Markup Language) standard is the form of XML implementation used for communication
between the external system and Modular MOM.
```
```
BOM The BOM is a list of all materials required to produce a product showing the quantity of each required. These may be raw materials,
intermediate materials, subassemblies, parts, and consumables.
```
```
Priority The priority of work orders or production orders defined at the time of order creation. This priority is of importance for a supervisor or
operator for the day-to-day operations.
```
```
Throughput The recording of the outcome of a manufacturing operation based on milestone or operation which results in the backflush of the
inventory. A notification is triggered on successful completion for other components.
```
```
Operation The action in order fulfill a Work Order.
```
```
External
Operation
```
```
Operation which is performed by a vendor or supplier outside the enterprise
```
```
Line Location The location where materials are consumed.
```
```
Storage
Location
```
```
The location where finished products/materials are stored.
```
```
Permissions Authorization for accessing a command, UI element or API.
```
```
Production
Schedule
Schema
```
```
A data model that maps to B2MML schema for importing production orders in to Modular MOM.
```
```
Production
Orders
```
```
Orders for production of specific product. It is composed by operations assigned to work centers.
```
```
SSO - Single
Sign-On
```
```
Single Sign-on behavior allows a user to traverse through the 3 Modular MOM Apps seamlessly without having to authenticate repeatedl
y. Once the user has been authenticated on any one of Apps, they can navigate across the other Apps without having to log in again.
```
```
Token A valid pass for accessing the APIs, generated using the token service with a prescribed time limit of validity.
```
```
User Fields These are special placeholders in B2MML elements where the user can set any value of their choice which should be available in the
system. These values are not processed for any logical operation.
```
```
Product
Version
```
```
The version of the product to be manufactured.
```
```
Work Center Location where manufacturing activities are performed to produce a material or product.
```
```
Work Orders Orders for manufacturing in-house material for the final product.
```
Software Related

```
Term Description
```
```
Roles defines what permissions are granted to which user groups
```
```
User Groups group of users that is used to assign permissions
```
```
Permissions defines what actions a user can perform in the system
```
```
Metadata Data about data
```
```
Metamodel Data about metadata
```
```
Configurable Object
(CO)
```
```
A core concept and logical representation of the Modular MOM’s Metamodel. CO has attributes to configure its behavior and is
composed of fields, methods, and event handlers
```
```
Scalar single valued
```
```
List multi valued
```
```
CDO Map maps information between any two CDOs
```
```
User Story a description of a change in the system from the perspective of a user who wants to accomplish something
```
```
Association (object
relation)
```
```
weak relationship with no owner
```

Composition (object
relation)

```
strong relationship with a parent or owner
```
Aggregation (object
relation)

```
represents a collection or one to many relationship
```
App/Module an isolated and independently deployable unit that can be composed with other loosely coupled modules in order to create a
customized product

Distributed System a collection of independent software/hardware components called nodes that cooperate by exchanging messages over a
communication network to achieve one common goal

Platform Agnostic when software is not programmed to run on one platform but can be used on many platforms

Containerization when software is split up into lightweight containers that are packaged with everything they need to be run on their own in the
environment

Workspace A logical concept that facilitates an isolated area for customization and extension of the Metadata Models and to make the
customized models easy to upgrade

Multitenancy when multiple instances of apps operate in a shared environment and each instance is logically isolated but physically integrated

MOM on cloud cloud readiness for opcenter cloud applications

Modular when the functional capabilities of the system are decomposed into a set of granular parts where each part represents a piece of
the overall application


# Working with Documentation

Siemens’ documentation follows the Topic-based authoring documentation, a modular approach to content creation where content is structured
around topics that can be mixed and reused in different contexts. It is defined in contrast with book-oriented or narrative content, written in the linear
structure of written books.

In addition to the end-user documentation updated by technical writers, you can handle Confluence pages even if you have never created a single page ,
just follow these procedures:

Creating Confluence pages

Reordering pages

Updating pages

Inserting Info Boxes

Inserting Images

See the Page History


### 1.

### 2.

### 3.

### 4.

### 1.

### 2.

### 3.

# Creating Documentation Pages pages in Confluence

## Procedure

```
Click the ellipsis button located on top of the page
Insert the title on top of the page.
Populate the page
Click Save.
```
## Example

## Reordering pages

```
Click the ellipsis on the upper right-hand corner of the page and then select View in Hierarchy.
.Drag and drop the page of interest to the proper position.
Click the page link to return to the original page.
```
## Example


### 1.

### 2.

### 3.

### 4.

### 5.

### 6.

# Updating Pages

## Procedure

```
Select the page to update.
Click Edit or press the E key on the keyboard.
Update the page.
Optionally, enter a comment in the edit box on the bottom of the page and select whether notify watchers or not by selecting the related checkbox.
```
```
Click Preview to check the page layout and then Edit to return to the original page.
Click Save.
```
## Result

The updated page is displayed and shows author name and time of the last modification below the page title.

The main page of the doc space displays the author name, date and time of the last modification and link to the updated page.

The History page of the updated one displays the author name, date and time of the last modification, eventual comment and version number



### 1.

### 2.

### 3.

### 4.

### 5.

### 6.

### 7.

# Inserting Images

## Procedure

```
Click the ellipsis button located on top of the page and select Attachments.
In the Attach Files section, click Choose File to browse for the file to insert and select it.
Click Attach. and then View Page to return to the original page.
Click Insert files and images in the command bar above the page.
Click Attached to this page on the left, select the file of interest and click Open.
Optionally, add a comment in the box below.
Click Insert.
```
## Example

## Available operations

```
Deleting an image: In the Attachments page, click Delete and then OK to confirm the deletion.
```
```
Resizing an image: Open the page in Edit mode, select the image and modify its dimensions by either entering the desired pixels or selecting the
buttons close to the image.
```
```
Please remove any unused or replaced image in order to reduce the page dimension.
```

Inserting effects to an image: Open the page in Edit mode, click Properties, select Effects on the left, then Snapshot and click Save.


### 1.

### 2.

### 3.

### 4.

# Inserting Info Boxes

Info boxes provide useful information and can be added to Confluence pages using the corresponding macros.

## Procedure

```
Click the button and select the Other macros command.
Do either of the following:
select Formatting from the list on the left and then the desired note.
enter the type of note to be inserted in the Search box on the upper right corner
Optionally, enter a title and click the checkbox to show or hide the information icon.
Click Insert: the info box is displayed and ready to be filled with the desired information.
```
All info boxes can be modified and removed by selecting the box and then the corresponding command.

## Types of info boxes

Tip: A note containing entirely optional and non-essential information that can however help the reader accomplish their tasks.

```
Examples
```
```
An alternative way of doing something, which may be faster for some readers or in certain situations
An additional operation that may not be obvious, but that can speed up the current task
```
Info: A note containing neutral or positive information, that is used to clarify the current context or describe a special case.

```
Examples
```
```
A non-critical operation that must be performed only in certain conditions.
A clarification on something mentioned beforehand.
```
Note: A note providing information that is essential to perform a task or to understand a concept. If the reader disregards an important note, they may not
be able to perform the task or understand the concept.
Usage of must/must not and imperatives is recommended in important notes.

```
Examples
```
```
A restriction that must be taken into account when performing a task.
An unwanted consequence caused by not following a documented procedure.
```
Warning : A note providing information that is critical to perform a task. If the reader disregards a warning note, the system may become unstable and/or
data loss can occur.
Usage of must/must not and imperatives is mandatory in caution notes.

```
Examples
```
```
A warning that data loss can occur
A warning that a program may become unresponsive or may terminate abruptly
```


# Inserting Links


# Reordering Pages


# See the Page History


### 1.

```
a.
```
```
b.
2.
3.
4.
```
```
5.
```
```
a.
```
### 1.

### 2.

### 3.

### 1.

### 2.

### 3.

### 4.

```
a.
b.
c.
d.
e.
5.
```
# User Story - Definition of Ready (DoR)

```
Definition of Ready for User Stories
General Considerations
Factors to be Considered for Writing the Acceptance Criteria
```
The Definition of Ready (DoR) for User Stories embraces the list of considerations that we have to think about in order to properly consider a User Story
ready for development. A dedicated Tag, named "Ready", has been created for this purpose.

## Definition of Ready for User Stories

```
User Stories are properly refined
Vertical slicing is the preferred way to design a User Story (see Feature, User Story, Task, Epic - RevMOM - MES Wiki (siemens.com)
for details)
Good User Stories satisfy the INVEST criterion (see INVEST in Good Stories, and SMART Tasks - XP123 for details)
User Stories have a proper Acceptance Criteria, Description, Story Points, and Tags (if required)
User Stories are sprint-able
User Stories contain Gherkin Test Cases to follow the BDD approach (Behavior Driven Development - Behavior-driven development - Wikipedia) (
pseudo BBD code is also acceptable)
If there is any security impact, assign a tag (Security) to User Stories and update the security tab at Feature level (see the following figure)
```
```
To be checked if additional security tests are needed. From security point of view, it is important to check that both positive and negative
use cases are defined
```
## General Considerations

```
User Stories should be as small as possible by following vertical slicing
Consider using a Story Template (e.g., As a <user> I want <functionality> so that <benefit>) in order to better understand the requirements (that
is, to answer to the WHO and WHY questions)
Write the User Stories in a way that they are understandable to both the Feature Teams and the Stakeholders
```
## Factors to be Considered for Writing the Acceptance Criteria

```
User Story requirements
Documentation
Security
Testing scope
Unit Testing
Integration Testing
API Automation
UI Automation
NFR
Deployment
```

# User Story - Definition of Done (DoD)

```
Business/Technical Debt/Architectural
Spike
```
TheDefinition of Doneincludes all activities required to get a User Story intoClosedstate.

## Business/Technical Debt/Architectural

```
Development
Code is checked in Master branch and deployed in an integration environment (modmom-10)
(According to Overall Retrospectives, code should be reviewed before checked in and deployed. Code Review should be considered as
a best practice. In order to improve this practice, each team can consider working in pair and mob)
The Postman Collections, used to provide examples of requests, should be updated in the following Repo
Review
UX
Review with UX expert for design, icon, styles, messages, behavior (e.g. Number of clicks) etc.
Localization and Translation need to be completed, if applicable
Security
Critical and high code smells, vulnerability, SonarQube bugs, and security hot spots match the targets
Security tab at Feature level is updated completely
Testing
Make sure the involved repos have SonarQube, branch policy, and measurement of code coverage configured
All new tests, including NFR and Security tests (see Test Strategy, Naming & Tagging Conventions for Test Cases and NFR Test
Guideline) are completed and successfully executed.
Regression tests are successfully executed
KPIs of NFR Tests are visualized in the NFR Dashboard on Sumo Logic (instructions here).
Code coverage satisfies the targets defined (see Code Coverage)
SonarQube criteria are met
Test Plan:
Create the corresponding Test Cases in TFS and associate them to the US using the "Tested By" relation
Associate test methods to the TFS Test Cases
For NFR Tests:
Create TFS Test Cases and provide in the "Summary" tab the link to JMeter project file and the reference to the
specific test inside JMeter
For BDD Tests:
Write the tests in the feature files
In the TFS TC "Summary" tab provide a link to the feature file (Example please see here).
Insert the Test Cases in the Sprint Test Plan, in a Query-Based Suite for the committed US
Insert the Test Cases in the Master Test Plan, in a Static Suite
Set Test Cases to Ready state
Documentation
Internal Documentation (wiki02) is created/updated with the correct details (by Development Teams). In particular, the following points
should be taken into account
For B2MML schema changes: the default values and the mapping should be also provided (also with an example)
For data model changes: the datatype along with the size, the default values, and optional/mandatory options should be
provided
For the API: the behavior, the input and the output parameters, and the error codes should be provided (also with and example)
Messages
User Documentation (wiki01):
Create/Update documentation with correct details (by Development Teams)
Review documentation (by Documentation Referent)
Validate documentation :
by Product Managers, Product Owners, and/or Architects Community in case of new documents or existing documents
that have been consistently updated.
by Development Team members in case of specific document updates.
```
```
General
All child tasks are completed and closed
All Acceptance Criteria are satisfied
Logging and Tracing are included in the code
Third-party software clearance page is updated, if required
Release-Tag is set (or Release field is filled in)
```
## Spike

```
Only if all children User Stories are closed, the Feature can be put into Resolved State and assigned to PO.
```
```
As soon as PO puts the feature into Closed State, the teams must publish a stable build of the relevant module.
```

Development
Code is checked in Master branch (Spike repo)
Review
Outcomes of a Spike is reviewed by Product Owner and/or Architects Community
Documentation
Internal documentation (wiki02) is created/updated with the correct details
General
All child tasks are completed and closed
All Acceptance Criteria are satisfied
Third-party software clearance page is updated, if required


# User Story - Definition of Done (checklist)

This page has been created with the goal to provide the Definition of Done as a checklist. An wider description is provided here

```
Code in Master branch and deployed in an integration environment (modmom-10)
```
```
Postman Collections
```
```
Review with UX expert
```
```
Localization and Translation
```
```
Critical and High code smells
```
```
Vulnerability
```
```
SonarQube bugs
```
```
Security hot spots and evaluation
```
```
All necessary tests, including NFR and Security tests are completed and successfully executed
```
```
No regressions
```
```
Code coverage
```
```
Create the Test Case in TFS (Test-US relation "Tested By")
```
```
For NFR Tests create TFS Test Case in TFS (in “Summary" tab link JMeter project file and reference to test inside JMeter)
```
```
BDD Tests in feature files
```
```
Insert Test Cases in the Sprint Test Plan
```
```
Insert Test Cases in the Master Test Plan (Static Suite)
```

Set Test Cases to Ready state

Internal and user documentation is created/updated with the correct details

Review documentation

Third-party software clearance page is updated, if required

All Acceptance Criteria are satisfied

Release-Tag is set (or Release field is filled in)


```
a.
```
```
i.
```
# Product Security - Definition of Done for Feature and

# Release

## Security Process during Product development

```
The security process for User Stories is defined inside the DoR and DoD for the user story:
```
```
User Story - Definition of Ready (DoR)
User Story - Definition of Done (DoD)
```
```
In particular for each User Story it is necessary to evaluate if there is a security impact and if there is any security impact fill the security tab at
Feature level.
```
```
The security process for the Feature (DoD) requires that:
The Feature can be closed if Security tab is completed.
Dev Security Risk Assessment cannot remain in "To be evaluated" state.
If according to dev the Feature does not have security Impact, Dev Security Risk Assessment must be put to "No Security
Impact". In this case, please add a short explanation in the "Security Assessment Explanation" field.
If according to dev the Feature has a security impact, Dev Security Risk Assessment must be put to "Security Impact". The
field "Security Assessment Explanation" can be used for adding useful details. In case of Security Impact all the other fields
must be filled before the feature is closed.
If TRA status is set to «Necessary» a dedicated User Story is opened by PSE in order to track TRA activities. These User Stories are
linked to a security dedicated Feature assigned to PSE. The security dedicated Feature must be planned in the same version of the
development ones.
```
```
The security process for the Release (DoD) requires that:
the security dedicated Feature is closed.
```
## Details of the different phases

## Phase 1: Get US ready for development

During Backlog Refinements the requirements are analyzed and user stories are created. Usually also Supporting Roles (like Security Experts and QA) are
present in these meetings.

In case any security impact is detected, tag "Security" is assigned to the User Stories and the security tab at feature level is updated.

https://splm.sharepoint.com/sites/SecurityExpertsMOM/SitePages/PSS-i.aspx?OR=Teams-HL&CT=1632845892345

```
Dev Security Risk Assessment: This field responds to the question, if the feature has a security impact in the team's opinion. The team
must change the default value. The following values are available:
```

```
a.
```
```
i.
ii.
iii.
b.
c.
```
```
i.
ii.
d.
```
```
i.
ii.
iii.
e.
```
```
i.
ii.
iii.
```
```
To be evaluated (default)
No Security Impact
Security Impact
PSE (Product Security Expert) Assigned: The team inserts the Product Security Expert (Roncagliolo, Isabella or Turolla, Andrea)
PSS (Product and Solution Security) Security Risk Evaluation: This field is filled in by the PSE, who can chose among the following
values:
No Security Impact
Security Impact
TRA (Threat and Risk Analysis) Status: This field gives information about the current status of the Threat and Risk Analysis. The team
together with the PSE chose among the following values
Necessary
Not necessary
Completed
Security Test: This field specifies, if additional security tests are needed. From a security point of view, it is important to check that both
positive and negative use cases are defined; this is highlighted in the titel of the test case, i.e. "[API]-NEG Import..." The team together
with the PSE/QA chose among the following values
Defined
In review
Not necessary
```
Currently the Security tab is only available at Feature level.

If more security-relevant user stories belong to one feature, take care updating it, considering always the worst case.
I.e. the team defines US1 as security-relevant and updates the Security fields in the feature accordingly; US2 instead is not security-relevant. In that case
do NOT change the " Security Impact" value to "No Security Impact".

Phase 2: Closure of US

The US can be closed, if the Security tab at Feature level is updated correctly and completely.

Phase 3: Closure of Business-Feature

The Feature can be closed, if the Security tab is updated correctly and completely.

If TRA status is set to «Necessary» a dedicated User Story is opened by PSE in order to track TRA activities. These User Stories are linked to separate s
ecurity dedicated Feature. This security dedicated Feature is assigned to PSE and must be closed in the same version of the development ones.

Phase 4: Closure of Security-Feature

The Security-Feature can be closed, if the TRA activities are completed.

Only then, the version can be released and shipped.


# Test Strategy

ModMOM2 Test Strategy


# Interfacing with ModularMOM Adopter Startups

```
Adopter Startups should pull the latest version of the platform on Mondays after the sprint review
Until the first official release, breaking changes, which force the adopters to refactor their code, are admissible
Adopters can open bugs in tfs05, specifying:
the exact version of the platform which causes the problem, in the "Found in Build" field
the name of the startup who has opened the bug, in a tag of the form "OpenedByXYZ"
a description of the problem
In order to prevent, or at least detect as early as possible, problems caused by platform updates, adopter startups can provide their models and
automated BDD test cases, which ModularMOM startup will execute frequently in their pipeline
```

### 1.

### 2.

### 3.

### 4.

### 5.

### 6.

```
a.
b.
c.
```
```
d.
i.
ii.
```
# Procedure for Pull Request

```
Work in progress
```
```
Commit to master branch only by means of Pull Request (and not directly)
```
```
When you want to modify master branch with changes regarding
personal branch: use user/<Initials of developer or short nick name>/<meaningful name of branch>
(i.e.: user/MB/governance_refactoring)
work items: use <work item shortcut>_<work item ID>, i.e. US, TSK, BUG
(i.e.: US_12345)
Merge From... master
Clean solutions and run script Build all
Run all Unit and Integration tests and check if they are succeeded successfully
Check with SonarLint Visual Studio integration, if no new findings emerged from the analysis
Create Pull Request from your branch to master:
put meaningful descriptions to all change sets
associate all TFS work items (understand if US/Bug or task)
define reviewers to approve changes and to inform about changes (to define how to apply reviewers concept)
(Note: pull request must be completed quickly)
complete pull request if change is approved
if branch is personal, delete it at each pull request
if branch is shared between developers, delete it at the end of the development (at latest: closure of the associated work item)
```
Frequency: Perform a Pull Request whenever a Changeset is consistent and respects all quality requirements, even if the functionality is partial, so that
all other teams can integrate their changes in a continuous way.


# Best practices for Git Branching and Merging

```
Introduction
Definitions
Git Tools
Getting Started
Creating a Branch
Branch Naming Conventions
Keeping master up to date
Updating Feature branch with updates from Master branch
Merge Conflicts
Pushing local branch to remote
Raising a Pull Request
Stale / Aging branches
Rebasing
Merging back features from long separated branch
```
## Introduction

Git is an open source distributed source control system used to manage software projects large and small. Its efficient use of branches to isolate changes
provides a powerful mechanism for managing complex code bases. Git offers several types of branching strategies which allow teams of developers to
work on multiple features simultaneously and reliably combine them into a single code base.

## Definitions

Repository - The physical storage facility for organizing source controlled files. Metadata in the repository records each file's history along with differences
between each revision of the file.

Master branch - The root or default branch in a Git repository. This is sometimes called 'master' or 'main'.

Feature branch - Or simply 'branch' a logical area to store isolated changes while work is in progress. When a branch is complete, it's 'merged' back to
the master.

Commit - An atomic set of changes. It can be a single character in one file or hundreds of file. A commit represents an immutable entry in the repository.
Along with file changes, the author and timestamp are recorded.

Merge - The process of combining differences between two branches into one. Merging is a one way operation that accepts changes from one branch into
the currently checked out branch.

Pull Request - A more formal merge that requires review or approval and often times code quality gates such as code coverage and passing tests.

Conflict - A situation that arises when two developers modify the same region of a file with different changes. A can sometimes be auto reconciled by Git
however in most cases it requires manual intervention.

## Git Tools

There are a myriad of tools for interacting with Git repositories and there are too many to cover. While modern versions of Visual Studio have good
integration with Git, many examples in this document are shown using the Git Bash console or Atlassian SourceTree to demonstrate a concept. Each
respective tool will have an equivalent function not documented here.

## Getting Started

There are other guides that show how to clone or create a repository.

For example: Create a Metadata Module

This guide expects that the repository is already cloned to your workstation and the repository is 'clean' or free of any changes.

Issue the command below to list the state of the repository

```
$git status
```

Creating a Branch

Before you begin coding, you need to create a branch.

To create a branch, issue the command below:

```
$git checkout -b <branch-name>
```
Below, we create the 'Feature1' branch.

With the 'Feature1' branch created, again check status. The branch is clean with no changes.


Branch Naming Conventions

While there is no perfect approach to naming branches, however following a common nomenclature helps keep the repository organized. Since branch
names can be arbitrary, it can be difficult to manage without some order.

One way to think about repository Branches is like files on a file system. While nothing technically prevents anyone from creating all files in a single root
folder, that's not usually done. Typically files are given some structure using folders to help organize them. Likewise in git, a branch name may include a
forward slash / and when it does, Git UI client tools will display the branches as a tree structure.

In the example above the branch 'Feature1' was created on the root folder along with master.

```
Branch should convey some meaning as they will proliferate
One simple approach to organizing them is to prefix with initials/branch_name as shown below
Format is 2 or 3 characters of your initials followed by a slash, then your branch name
The example below creates the branch ca/Feature2
```
```
Creating branches in this form allows UI tools to expand/collapse the list of branches with a tree control.
```
Expanded Collapsed

Many developers are already doing this and you can see how the branches are organized here.

https://tfs05mom.industrysoftware.automation.siemens.com/MOM/ModularMOM/_git/MetadataRuntime/branches?_a=all

Keeping master up to date

While you are developing your feature, others are making changes and merging them into the master. It's important to keep your local master branch
current so that you can keep adding updates to your feature branch. By doing so you can be confident that your feature changes are compatible with the
current state of the code base. In this example, we'll switch back to the master and there will be new changes we'll 'pull' down.


To receive these changes, perform the following command:

```
$git pull
```
Now I'll switch back to the Feature1 branch


Updating Feature branch with updates from Master branch

In the previous section, you pulled the current changes for the master branch. Now we need to add them into our feature branch.

Note: your feature branch must be checked out before you're able to merge from the master. This is a one way operation: From master To feature
(current checked out branch)

While on the desired destination branch, in this case Feature1, issue the following command.

```
$git merge master
```

The updates previously pulled down from the master are now present in your feature branch.

Merge Conflicts

Merge conflicts occur when the same region of one or more files have incompatible or overlapping changes. When this happens the branches cannot be
automatically merged and require manual corrections to fix.

There are several approaches to resolving merge conflicts.

1.) Correct prior to merge - preserves history

2.) Correct during a merge - changes history because a commit is added.

TODO

Pushing local branch to remote

After you've created your local feature branch, it's a good idea to push it to the server where is is backed up.

In this case, we're on the branch named ca/WriteModelUpdate

To push the branch to remote use the command below

```
$git push
```
However if no remote branch exists, you'll receive the error below.


In which case you simply reissue the command with the additional parameters.

```
$ git push --set-upstream origin <branch_name>
```
And once you've done this, the branch now exists both locally and remote.

When your branch is complete, you can issue 'git push' one last time to sync your changes.

Raising a Pull Request

TODO

Stale / Aging branches

We should regularly prune the branches that show in this view.

https://tfs05mom.industrysoftware.automation.siemens.com/MOM/ModularMOM/_git/MetadataRuntime/branches?_a=stale

Rebasing

Rebasing is a process of resetting the branch point (from the master) to your feature branch.


Merging back features from long separated branch

Sometimes several people work around a single feature branch, sometimes creating branches from it.. While it's always best to stay in sync with the
master regularly, there are times when the features are outdated from the current master. In this case, here is one approach to consolidating the changes
back to the master without conflicts.


# TFS Azure DevOps

```
Work Items & Tracking
Information mapping
Available Tags
Backlog Refinement Information
Release Information
Product Area
Stakeholders Information (in progress)
Test Information
CI/CD Information
Documentation
UX Information
Security Information
Misc Information
```
## Work Items & Tracking

```
Category Work item type Controls backlogs/boards
```
```
Epic Epic Epic portfolio backlogs and boards
```
```
Feature Feature Feature portfolio backlogs and boards
```
```
Requirement
User Story (Agile Template)
Product Backlog Item (Scrum Template)
Issue (Basic Template)
Requirement (CMMI Template)
```
```
Product backlogs and boards and Sprints backlog
```
```
Task Task Sprints Taskboards
```
```
Bug Bug Dependent on how bugs are tracked
```
## Information mapping

4 concepts are applied to product backlog items that must be decoupled in TFS:

```
Concept Description Mapped in TFS
```
```
What
```
```
It represents the functional area of the product in which the change is going to be done. Tags
```
```
When
```
```
It represents the time slot in which the activity is planned to be executed. Iteration Path (Sprint number)
```
```
Who
```
```
It represents the team who is in charge to complete the activity. Area (Path)
```
```
Where
```
```
It represents the release in which the change is going to be performed. Tags
```
These concepts are independent, e.g.:

```
the same team work on one or more functional areas at the same time
different teams work on the same functional area during the same sprint
the same team could work on two different releases during the same sprint
```

Available Tags

Backlog Refinement Information

To identify the refinement status of work items, we use the following tags:

```
Tag
Name
```
```
Used
for
```
```
Description Notes for future
```
```
BR0 Feature The feature is in discussion between PM,
PO and Architects.
```
```
What happens in BR0 meetings:
```
```
Alignment between PM, PO and
Architects to decide the scope of
next sprint
High level analysis of requirements
Refinement of features
```
```
Prerequisite:
```
```
PM is aware about the stakeholder's
priority for next upcoming sprint(s).
```
```
An agile PO (our couple PM+PO) is a cooperative person to create well functioning Scrum Teams. He
acts as a facilitator bringing together the right people at the right moment.
```
```
Also team referents and stakeholders could be involved in BR0 meetings, so that they can establish a
relationship and discuss the requirements directly among each other, without having the need of a
"translator".
```
```
BR1 Feature Feature is completely discussed in BR0
among PM, PO and Architects. The
feature is now ready for the next level of
discussion in BR1.
```
```
What happens in BR1 meetings:
```
```
POs together with PMs and
Architects present the feature to the
team representatives.
The team representatives get a
clear understanding of the
requirement.
Who, what and why questions are
answered.
Team representatives decide the
driver(Not Owner for
implementation) of the feature for
further refinements.
```
```
Prerequisite:
```
```
Topics of BR1 are defined in advance to
give the chance to the team members to
decide who participates
```
```
Product backlog should be ranked as per priority, so that Teams can understand the content of BR1.
```
```
As Product Backlog Items are no written requirement specifications, but placeholder for conversation,
it could be a team's exercise to split the requirement into smaller features, thinking about the benefit,
acceptance criteria and NFRs.
```
```
The teams continue the discussion started with stakeholders (in BR0) and PO helps that the
conversation is effective. Already here US can be drafted, no decision about feature driver.
```
```
BR2 Feature,
User
Story
```
```
Feature is completely discussed in BR1
with team referents. The feature is now
ready to be split into smaller user-facing
items in BR2 with the entire team that
acts as a driver.
```
```
What happens in BR2 meetings:
```
```
The teams create sprint-able User
Stories from the perspective of User(user-
facing), READY for planning, considering
the Definition of Ready.
```
```
Prerequisite:
```
```
All the participants should be well
prepared on the topic.
```
```
Other team member's can participate
based on demand.
```
```
When creating sprint-able user stories from the perspective of User(user-facing) ,we could work
together as members of our entire project Modular MOM2. All members able to contribute in analyzing
the requirements participate and create together the user-stories, independently to which team they
belong to. This ensures that the requirement is analyzed from various perspectives and promotes
knowledge sharing.
```
```
In doing so, the decision who takes what can be moved to the latest possible moment, remaining
flexible and able to respond to market requirement.
```
```
Ready User
Story
```
```
The User Story is created and discussed
in BR2 and the Definition of Ready is
followed. The US is ready for Sprint
Planning.
```
```
Note: US and Feature can be active,
even if the entire feature is not flagged
with tag "Ready".
```
```
Feature All User Stories are completely refined
matching the entire Feature scope.
```

Release Information

To identify in which version the implemented functionality is released, we can put the following tags on the TFS work items:

```
Tag
Name
```
```
Used
for
```
```
Description Notes
for
future
```
```
FPEG_ToB
eAnalyzed
```
```
Bug Tag assigned during Bug Review meetings; the Bug Review members decided that the bug must be analyzed by a team member.
Then, based on the team member's inputs, the Bug Review members decide again, if it will be fixed for the current version or
deferred to future versions.
```
```
FPEG_ToB
eFixed
```
```
Bug Tag assigned during Bug Review meetings (once called "FPEG" meetings, it's a german acronym for "Product Decision Group",
mainly POs, Quality Manager, PrMs, LALs); the Bug Review Meeting members decided to fix the bug (and merge to master) in
current version.
```
```
FPEGDefer
redFromYY
MM
```
```
Bug Tag assigned during Bug Review meetings; the Bug Review members decided to defer the bug to a future version.
I.e. FPEGDeferredFrom2112 means that Bug Review members decided not to fix the bug for December (12) release in year 21.
```
```
NotRelevan
tForYYMM
```
```
Bug Tag to identify that bug is not relevant for a certain release.
```
```
NotRelevan
tForProduct
```
```
Feature,
User
Story,
Bug
```
```
Tag to identify that the item is not relevant for the product.
```
```
In case of backlog item, it means that the tagged implementation doesn't need to be released officially. It is the opposite tag of
"ReleaseYYMM". It is i.e. used for POC or Spike backlog items.
```
```
In case of a Bug, it means that the bug has no impact on the product release. It is i.e. used for bugs on a code that is not delivered.
```
```
ReleaseYY
MM
```
```
Feature,
User
Story,
Bug
```
```
Tag to identify the release in which the functionality is released, i.e. Release2112 (December MVP 2021).
```
```
In addition to Features and Bugs, we need to tag the User Stories as well, because
```
```
we need to certify that all US are tested by test cases
we (will) have only vertically-sliced, auto-consistent, user-facing product backlog items (user stories)
we will be able to release after each sprint
```
Product Area

To identify which product area is impacted by the implemented functionality, we can put the following tags on the TFS work items:

```
Tag Name Used for Descripton
```
```
AM All WI Tag to identify product area "Additive Manufacturing"
```
```
Common All WI Tag to identify product area "Common"
```
```
Communications All WI Tag to identify product area "Communication"
```
```
Configurator All WI Tag to identify product area "Configurator"
```
```
Ext App Comms. All WI Tag to identify product area "External App Communications"
```
```
FM All WI Tag to identify product area "Factory Management"
```
```
Home All WI Tag to identify product area "Homepage"
```
```
Int. App Comms. All WI Tag to identify product area "Internal App Communications"
```
```
Metadata All WI Tag to identify product area "Metadata"
```
```
MetadataRuntime All WI Tag to identify product area "Metadata Runtime"
```
```
Metamodel All WI Tag to identify product area "Metamodel"
```
```
MM All WI Tag to identify product area "Material Management"
```
```
OM All WI Tag to identify product area "Order Management"
```
```
Platform All WI Tag to identify product area "Platform"
```
```
SaaS All WI Tag to identify product area "SaaS"
```
```
TNT All WI Tag to identify product area "Track and Trace"
```
Stakeholders Information (in progress)

```
Tag Used Description
```

```
for
```
```
APS-SU All WI Tag to identify work items related to an external Startup organization, in this case "Advanced Planning and Scheduling".
```
```
IPL-SU All WI future use: Tag to identify work items related to an external Startup organization, in this case "Intra Plant Logistics".
```
```
IPS-SU All WI Tag to identify work items related to an external Startup organization, in this case "Inspection Planning Shopfloor".
```
```
Q-SU All WI Tag to identify work items related to an external Startup organization, in this case "Quality".
```
```
RDL-SU Bug future use: Tag to identify work items related to an external Startup organization, in this case "Research, Development and
Laboratory"
```
```
EWA All WI Tag to identify work items related to a final customer, in this case "Elektronikwerk Amberg", a Siemens Factory in Amberg (town
in Germany).
```
```
FD-SI
Zug
```
```
All WI Tag to identify work items related to a final customer, in this case "Siemens Smart Infrastructure" in Zug (town in Switzerland).
```
```
Morf3D All WI Tag to identify work items related to a final customer, in this case "Morf3D", a company in US for Additive Manufacturing.
```
Test Information

```
Tag Used
for
```
```
Description
```
```
BDD_UI Test
Case,
Bug
```
```
Tag to identify that work items are related to BDD-UI-testing.
```
```
FoundIn2112
FoundIn2204
```
```
Starting from Release2207, this tag is substituted by
FoundIn field.
```
```
Bug Tag to identify when bug was found.
```
```
FromExploratory Bug Tag assigned by the person who creates the bug. It helps to better
understand if exploratory testing was useful and how many bugs were
created in total during this activity.
```
```
FtrInt User
Story
```
```
Shortcut for "Feature Integration".
```
```
Tag to identify the User Story that tests the integration in the entire system.
```
```
At least one User Story of a Feature must be tagged with "FtrInt"
In case of vertically sliced User Stories, all User Stories could be
tagged with "FtrInt", because each US represents a small
functionality and the integration in the entire system can be tested.
The "FtrInt" User Story doesn't necessarily have to be a dedicated
US, i.e. there is a US for the frontend which can be used also as
"FtrInt" US, because if it works from the UI it means that the
Feature as a whole works.
```
```
Functional Test
Case
```
```
Test Category "Functional"
```
```
IntegrationBroken Bug Tag to identify issues found by Nightly Test Runs which cannot be fixed
immediately.
```
```
NFR User
Story
```
```
Tag to identify that the work item describes a non functional requirement.
```
```
NoNFR User
Story
```
```
Tag to identify that the work item does not need any NFR tests.
```
```
NoTCRequired
```
```
This tag is substituted by custom filed "Test Details". All
User Stories (except Architectural) must have a linked
Testcase, or the Alternative Verification must be provided.
```
```
User
Story
```
```
Shortcut for "No Testcase required".
```
```
Tag to identify that the User Story must not have linked Test Cases.
```
```
In case of vertically sliced User Stories, there are only few cases of
not-tested User Stories.
If Spike User Stories have been created for investigations and
learning purposes, tag them as "NoTCRequired", when no
acceptance tests are executed.
If Architectural User Stories are tested with already existing
acceptance tests (in Release Test Plan), tag them as
"NoTCRequired".
```

```
Avoid creating "fake" Business User Stories which must be tagged
with "NoTCRequired"; if User Stories are not user-facing, consider
creating only one single User Story with various tasks that refer to
the Feature scope.
```
```
NotProductBug Bug Tag to identify a misbehavior of a test or something else, that does not
infects the system and is not visible to users. These bugs do not need to
have the "Release"-Tag/field.
```
```
Obsolete Test
Case
```
```
Tag to identify that test case is obsolete
```
```
Performance Test
Case
```
```
Test Category "Performance"
```
```
QA All WI Tag to identify that the work items are managed by the QA (Test
Infrastructure) team.
```
```
ShouldHaveNFRTests User
Story
```
```
Tag to identify that the work item should have NFR tests.
```
CI/CD Information

```
Tag Used for Description
```
```
CI/CD All WI Tag to identify that the work items are managed by the CI/CD team.
```
```
Deployments All WI Tag to identify that the work items are related to Deployment activities.
```
```
DevOps All WI Tag to identify that the work items are managed by the DevOps team.
```
Documentation

```
Tag Used for Description
```
```
Doc-Activities Task Tag to identify all work for documentation referent (used for query-purposes)
```
```
Documentation Feature,
User Story
```
```
Tag to identify that the user documentation must be modified.
```
```
Draft user documentation Task Tag to identify that the user documentation is in draft mode.
```
UX Information

```
Tag Used for Description
```
```
Icon User Story UX internally usage
```
```
Icon (last) User Story UX internally usage
```
```
SWF Feature,
User Story,
Bug
```
```
Tag to identify that SWF is involved.
```
```
UX User Story Tag is set by UX experts; it identifies that the work items are UX-relevant.
```
```
UX done User Story Tag is set by UX experts; it identifies that all UX related work is done by UX experts.
```
Security Information

```
Tag Used
for
```
```
Description
```
### ISO27

### 001

```
All WI Tag to identify that work item is related to ISO27001 activities
```
```
Securi
ty
```
```
All WI Tag to identify that the TFS work item has security aspects, and therefore security activities must be taken into account (such as the
definition of security tests).
```
```
(Independently from this tag, tab "Security" must be filled for all Features, because not all work items tagged with "Security" might
have a security impact and need further analysis)
```

Misc Information

```
Tag Used for Description
```
```
Clearing for future use?
```
```
DEVBLOCK Bug Tag to identify that the bug is blocking current development activities.
```
```
Module Epic
```
```
OnHold Features,
User Story,
Task
```
```
Tag to identify that the work item is put on hold, waiting to be reactivated.
```
```
Refactoring Feature Tag to identify that the work item contains refactoring activities.
```
```
Sub-Module Epic
```

# Communication Rules

```
Don't broadcast message via Teams, if you want to address a single person. Use personal
email instead.
Use "Important" or "Urgent" to highlight the priority of the message.
Give feedback, if message is read using "Thumb up".
Use tags to address specific people/groups.
```

# Procedure to set up first user in XCR

----------------------------------------------------------1st phase-------------------------------------------------------------------------------

Automated: (On board manager)
1) Creation of sam auth app from template(This would persist the info in registry)
2) Store the computed urls (using fqdn) into db
3) Delete message fron queue

Manual: Pre req- Tenant Namespace is present
1) Retrieve below tenant info based on accountId from OBM db
a. Tenant Name
b. Tenant Id
c. DataPrefix
d. DomainPrefix
e. Product
f. Sam auth details
f.1 ClientId
f.2 Client seceret
f.3 Base url
g. Keycloak Secrets
g.1 Onboard manager
g.2 modmom

2) Install keycloak from template
3) Update Keycloak for samAuth integration(sam clientId and Secret)

4) Update keycloak onboardmanager seceret (Secret received from step1 Keycloak properties )
5) Update rancher k8 secret of obm
6) Update keycloak modmom seceret (Secret received from step1 Keycloak properties)
7) Update auth service and api gateway with modmom Secret
8) Install AM - TBD (manual or automated)
9) Install rest of modules
10) Update OBM with status as "INVOKEOPERATOR" for same tenant (TBD: provide update query)
11) Push the same message into AWS queue (TBD: provide aws credntials and queue detail)

----------------------------------------------------------2nd phase-------------------------------------------------------------------------------

Automated (On Board manager):
1) Pick the message again
2) It will fetch the status from db as "INVOKEOPERATOR".


3) Create modmom service user in AM
4) Create first business user in keycloak and AM and add to admin group
5) Update ARM
6) Send Notification
7) Delete the message from queue

Manual:
1) Update auth service and api gateway with modmom clientId and Secret

Additional Set up for BDD data feed

1. Go to rancher
2. go to cluster-> download the document -> kubctl.config
3. install kubectl on machine
4. paste the content of downloaded config file to your config file
>kube >$ ls config
cat config
mv downloadfile config
5. change to name space with
kubectl config set-context --current --namespace=mom-integration
6. kubctl get pods - To checkif we are connected to correct context.
7. Now to connect to SQL instance we have to do port-forward
kubctl port-forward pod/sqlserver-0 -n mom-integration 1433: 1433
8. Follow the script written by Pier
1. Importing user from keycloak to AM
2. Create the UG
3. Create permisssions
9. Go to AM, and add users to UG for UG_BDDTest
Assign Role RL_BDDAPI_Test necessary permission.

1) Assign admin group to modmom in AM using first business user login (only for mom-integration)
2) Assign api automation related group to modmom in AM using first business user login (only for mom-integration)


# Manual configuration of User with Permission

## Webkey User set up in AccessManagement

This method is to set-up user with Admin permission to access modules using POSTMAN as tool.

Prerequisite:

```
Tenant namespace is present and all the infrastructural component such as, ApiGateway, keycloak and all the business modules are deployed
and Onboardmanager service client is configured in keycloak.
AM ConfigureFirstUser script has been executed during AM installation
A valid Webkey User
```
Set up postman authentication configuration with below information specific to that namespace:

## Configuration Steps

```
Add Webkey user to Keycloak User
```
```
Url: https://modularmom.mom-integration.preprod01.prod.us-east-1.kaas.sws.siemens.com/auth/admin/realms/modmom/users
```
```
Example of Request
```
### {

```
"username":"Ocmodtest.siemens@gmail.com",
"firstName":"OcMod",
"lastName":"Test",
"email":"Ocmodtest.siemens@gmail.comm",
"enabled": true
}
```

```
Link User to IdentityProviderUrl: https://modularmom.mom-integration.preprod01.prod.us-east-1.kaas.sws.siemens.com/auth/admin
/realms/modmom/users/<Replace Keycloak userId>/federated-identity/samAuth
```
```
Example of Request
```
### {

```
"userId":"default-webkey|Ocmodtest.siemens@gmail.com|<Sam Account Id>|<Sam User Id>",
"userName":"Ocmodtest.siemens@gmail.com"
}
```
```
Add user to AM
```
Url: https://modularmom.mom-integration.preprod01.prod.us-east-1.kaas.sws.siemens.com/accessmanagementapi/api/MOMUser

```
Example of Request
```
### {

```
"Input": {
"Name": "<KeycloakUserId>",
"FirstName": "",
"LastName": "",
"UserName": "Ocmodtest.siemens@gmail.com",
"Email": "Ocmodtest.siemens@gmail.com"
}
}
```
```
Add user to Admin group in AMUrl: https://modularmom.mom-integration.preprod01.prod.us-east-1.kaas.sws.siemens.com
/accessmanagementapi/api/MOMUserGroup
```
```
Example of Request
```
### {

```
"Input": {
"Name": "",
"Entries": [
{
"value": {
"__TypeOfRefDto": "NamedRefDto",
"__Name": "<Keycloak UserId>"
},
"ListItemAction": "Add"
}
]
},
"__RequestData": {
"__Defaults": {
"__Label": true,
"__Value": true,
"__Type": true
}
},
"entityKey": {
"__Name": "Admin Group",
"__TypeOfRefDto": "NamedRefDto"
}
}
```


### 1.

### 2.

### 3.

# OBM Flow

## 1. AWS account – SQS queue configuration

Create a AWS account and SQS queue as per the standard.

CloudFormation Template: LatestSQSTemplate

```
PFA, CloudFormation Template used for the Configuration parameter in Modular MOM
```
## 2. Service account registration

Steps

1.Take approval from line manager

Mail Template: Re SAM Account request form.msg

2. SAM account request

Template: Zeus_Account_Request.xlsx

Please send request to Kunal Kishor(kunal.kishore@siemens.com) to integrate ModularMOM apps with Zeus platform services.

3. Once the account is created we check the account in SamConsol - https://samconsole.preprod.teamcenterwebservices.com
4. Assign at least any 3 Policies from the below policy

```
ModMomArmPolicies
ModMomEventNotificationPolicy
ModMomNotifyPolicy
ModMomProvisioningPolicy
ModMomRegisterApp
ModMomRegisterProduct
ModMomSamPolicies
ModMomUnregisterProduct
```
5. Permissions.txt - This file is helpful when we need for each endpoint which policy is needed

For more details you can refer the below link
[http://civ6w178:3000/services/samauth/landing/landing_page.html](http://civ6w178:3000/services/samauth/landing/landing_page.html)

## 3. Product and App registration

Prerequisite:

```
Create AWS account
Set up a Simple Queue Service to receive data
After creating Sam Account use that service user in postman for configuration
```
```
https://samconsole.preprod.teamcenterwebservices.com/ Click on Users Access keys click on plus button(+)
```
Steps to register

```
Register the product
```
Url: https://arm.us-east-1.preprod.teamcenterwebservices.com/product/registerProduct

Request Body:

{
"name": "OpcenterModularManufacturing_Test_Integration",
"description": "A rich collection of purpose full cloud-first modules able to run standalone or interconnected to deliver customer Apps
/ solutions which are easy to configure and extend",
"tags": {
"purpose": "pre-production",
"env": "preprod"
}
}


### 1.

### 2.

```
add new product name
add description as per the product
```
```
Link to a add product the tier
```
Url: https://arm.us-east-1.preprod.teamcenterwebservices.com/product/<RegisterProductId>/addProductTier

Request Body:

{
"name": "OpcenterModularManufacturing_Basic_Integration",
"description":"Offers access to basic OpcenterModularManufacturing",
"sku": "MOD2000",
"provisionSteps": [
{
"type": "CreateTenantAccount"
},
{
"type": "ProvisionEvent",
"stepData": {
"notifyOnly": false,
"publishTopicId": "https://sqs.us-east-1.amazonaws.com/12346789/xxxx-Queue"
}
}
],
"provisionUserSteps": [
{
"type": "CreateTenantAccount"
},
{
"type": "ProvisionEvent",
"stepData": {
"notifyOnly": true,
"publishTopicId": "https://sqs.us-east-1.amazonaws.com/12346789/xxxx-Queue"
}
}
],
"unprovisionSteps": [
{
"type": "UnprovisionEvent",
"stepData": {
"notifyOnly":true,
"publishTopicId": "https://sqs.us-east-1.amazonaws.com/12346789/xxxx-Queue"
}
}
],
"unporivionUserSteps": [
{
"type": "UnprovisionEvent",
"stepData": {
"notifyOnly":true,
"publishTopicId": "https://sqs.us-east-1.amazonaws.com/12346789/xxxx-Queue"
}
}
],
"cleanUpSteps": [
{
"type": "CleanUpEvent",
"stepData": {
"notifyOnly":true,
"publishTopicId": "https://sqs.us-east-1.amazonaws.com/12346789/xxxx-Queue"
}
},
{
"type": "CleanUpTenantAccount"
}
],
"updateSteps": [
{
"type": "UpdateEvent",
"stepData": {
"notifyOnly":true,
"publishTopicId": "https://sqs.us-east-1.amazonaws.com/12346789/xxxx-Queue"
}
}
]
}


### 1.

### 2.

### 3.

### 4.

### 1.

```
add register ProductId in URL
Update the product name
Update the SKU
Update PublishTopic
```
```
Add/update new register queue in Rancher
```
use https://k8s.prod.us-east-1.kaas.sws.siemens.com/dashboard Select Specific namespace(ocmod-integration-2204) Storage ConfigMaps onboardmgr-
vars

4. Provider role creation

Prerequisite:

Refer the below link for the product registration

Product and App registration

Steps:

```
ModMom: ProviderRole with actAsRole trust policy
```
Please send request to zeus /PLpass team [Brendan Brolly (brendan.brolly@siemens.com)] for provider role to the new product.

Example:

{

"name": "ProviderRoleForOpcenterAppCreation_Integration",

"description": "The role used by Opcenter to delegate samAuth App creation",

"trustPolicy": "{\"version\": \"2019-09-30\",\"rules\":[{\"effect\":\"Permit\",\"actions\":[\"sam:actAsRole\"],\"subjects\":[\"sws:::ocmod\"]}]}",

"targetProvisionedResources": [

"sws:::arm:::product:productName/OpcenterModularManufacturing_Basic_Integration"

],

"rolePolicy": "{\"version\":\"2019-09-30\",\"rules\":[{\"effect\":\"Permit\",\"actions\":[\"samauth:registerAppFromTemplate\"],\"resources\":[\"sws:::samauth::
template::*\"]},{\"effect\":\"Permit\",\"actions\":[\"samauth:unregisterApp\",\"samauth:createClient\",\"samauth:getAppInfo\",\"samauth:updateApp\",\"samauth:
listApps\",\"samauth:addAppScopes\"],\"resources\":[\"*\"]}]}",

"tags": {

"AuthAppCreatorRole": "samauth app creator role",

"Owner": "OpcenterModularManufacturing"

}

}

ModMom has requirement to register new app in samAuth using approved template into provisioned tenant's sam account programmatically during tenant
resource provisioning step.
Our requirement fits more of "ProviderRole" functionality exposed by PlPaas platform.
ProviderRole allows Siemens Services to perform activities on behalf of the tenant.

More details about ProviderRole for reference : [http://civ6w178:3000/services/sam/Developer_API_Reference/providerRole/createProviderRole.html](http://civ6w178:3000/services/sam/Developer_API_Reference/providerRole/createProviderRole.html)


### 1.

5. Provision user and product

Prerequisite:

Postman configuration for a given environment

Url: https://tfs05mom.industrysoftware.automation.siemens.com/MOM/ModularMOM/_git/RestAPISamples

Steps

```
Link to add new product
```
Provision a product for a new tenant user. A new account is created as the given tenant name.

Url: https://arm.us-east-1.preprod.teamcenterwebservices.com/product/provisionProduct

RequestBody:

{
"tenantName": "Pratapgad_1",
"product": {
"name": "OpcenterModularManufacturing",
"tier": "OpcenterModularManufacturing",
"sku": "MOD1000"
},
"productUser": {
"firstName": "test",
"lastName": "test",
"email": "test.test@siemens.com"
},
"tags": {
"purpose": "testing"
},
"description": "OcMod Application Account"
}

ResponseBody:

{
"accountId": "bbcf8b71485d48a993b3f075cafa7080",
"userId": "3c435f59a9f748f184bdce72aa8f89a2",
"status": "ProvisionInitiated",
"jobId": "OpcenterModularManufacturing-OpcenterModularManufacturing-c5c396f4ffcb42bf8c9050a95773f9ce"
}

More details about Provision Product for reference : [http://civ6w178:3000/services/arm/Developer_API_Reference/products/provisionProduct.html](http://civ6w178:3000/services/arm/Developer_API_Reference/products/provisionProduct.html)

2. Process Product Provisioning Event

Provision of a product, ARM publishes 'provision-event' to product owners, providing them with an opportunity to setup resources and other artifacts
required to provision the product.

Url: https://arm.us-east-1.preprod.teamcenterwebservices.com/product/processProductProvisioningEvent

RequestBody:

{
"eventId": "adeb7e38cccf45cf978a592c0db9ca08:pe0:OpcenterModularManufacturing_Test:Basic1_OpcenterModularManufacturing_Test",
"eventType": "arm-product-provision-event",
"statusCode": "Success"
}

More details about Product Provisioning Event Product for reference : [http://civ6w178:3000/services/arm/Developer_API_Reference/products](http://civ6w178:3000/services/arm/Developer_API_Reference/products)
/processProductProvisioningEvent.html

3. Link to add new User

Provision additional users in a tenant where the product was already provisioned. To add new user is added to the tenant account, if not already existing in
the account.

Url: https://arm.us-east-1.preprod.teamcenterwebservices.com/product/provisionProductUser

RequestBody:


### {

"tenantAccountId": "bbcf8b71485d48a993b3f075cafa7080",
"product": {
"name": "OpcenterModularManufacturing",
"tier": "OpcenterModularManufacturing",
"sku": "MOD1000"
},
"productUser": {
"firstName": "test",
"lastName": "test",
"email": "test.test7@siemens.com"
},
"tags": {
"purpose": "testing"
},
"description": "OcMod app"
}

ResponseBody:

{
"accountId": "bbcf8b71485d48a993b3f075cafa7080",
"userId": "cf7aa2b03d6445cea89a5d1d07107c4e",
"status": "ProvisionInitiated",
"jobId": "OpcenterModularManufacturing-OpcenterModularManufacturing-f24b0a16052e450984ceb335caba5983"
}

More details about new provision user for reference : [http://civ6w178:3000/services/arm/Developer_API_Reference/products/provisionProductUser.html](http://civ6w178:3000/services/arm/Developer_API_Reference/products/provisionProductUser.html)

4. Link to get list of provision product

Get for the list of provisioned products.

Url: https://arm.us-east-1.preprod.teamcenterwebservices.com/product/listProvisionedProducts

ResponseBody:

{
"provisionedProducts": [
{
"tenantAccountId": "5cb406a8e9e44fbd80d2597761fc039c",
"tenantUserId": "26abebf9eddb414a8908c94e36a7eec9",
"productName": "OcMod-Test-10",
"tierName": "Basic",
"sku": "MOD10010",
"status": "ProvisionSuccess",
"creationDate": "02/18/2022 10:10:39",
"markedForDelete": false,
"activeState": "Active",
"provisionOptions": {},
"allowanceOptions": {},
"provisionActionData": {
"requestId": "arm-48b399fe-35a3-4c53-9b30-37110291af1e",
"workflowId": "da52b98849a148d2ac9205b6c7e188fe",
"eventIds": [
"32a8e466260740c8a9c4cc9fb3ca9934:pe0:OcMod-Test-10:Basic"
]
}
}

}

More details about list of provision Product for reference : [http://civ6w178:3000/services/arm/Developer_API_Reference/products/listProvisionedProducts.](http://civ6w178:3000/services/arm/Developer_API_Reference/products/listProvisionedProducts.)
html

5. Link to UnProvision User

Unprovision the product for a tenant user within the given account. The process shall remove the user's ability to work with the given product tier and
initiate the cleanup of any resources that were previously setup for the user.


Url: https://arm.us-east-1.preprod.teamcenterwebservices.com/product/unprovisionProductUser

RequestBody:

{
"tenantAccountId": "bbcf8b71485d48a993b3f075cafa7080",
"tenantUserId": "cf7aa2b03d6445cea89a5d1d07107c4e",
"product": {
"name": "OpcenterModularManufacturing",
"tier": "OpcenterModularManufacturing",
"sku": "MOD1000"
},
"excludeSteps": ["NotifyEmail"],
"tags": {
"purpose": "testing"
}
}

ResponseBody:

{
"accountId": "bbcf8b71485d48a993b3f075cafa7080",
"userId": "cf7aa2b03d6445cea89a5d1d07107c4e",
"status": "UnprovisionInitiated",
"jobId": "OpcenterModularManufacturing-OpcenterModularManufacturing-f24b0a16052e450984ceb335caba5983"
}

More details about UnProvision user for reference: [http://civ6w178:3000/services/arm/Developer_API_Reference/products/unprovisionProductUser.html](http://civ6w178:3000/services/arm/Developer_API_Reference/products/unprovisionProductUser.html)

6. Link to UnProvision Product

Unprovision the product for a tenant user. The process shall remove the user's ability to work with the given product tier and initiate the cleanup of any
resources that were previously setup for the user.

Url: https://arm.us-east-1.preprod.teamcenterwebservices.com/product/unprovisionProduct

RequestBody:

{
"tenantName": "Pratapgad_1",
"tenantUserEmail": "test.test@siemens.com",
"product": {
"name": "OpcenterModularManufacturing",
"tier": "OpcenterModularManufacturing",
"sku": "MOD1000"
},
"tags": {
"purpose": "testing"
}
}

ResponseBody:

{
"accountId": "bbcf8b71485d48a993b3f075cafa7080",
"userId": "3c435f59a9f748f184bdce72aa8f89a2",
"status": "UnprovisionInitiated",
"jobId": "OpcenterModularManufacturing-OpcenterModularManufacturing-c5c396f4ffcb42bf8c9050a95773f9ce"
}

More details about UnProvision Product for reference : [http://civ6w178:3000/services/arm/Developer_API_Reference/products/unprovisionProduct.html](http://civ6w178:3000/services/arm/Developer_API_Reference/products/unprovisionProduct.html)

6. Setting up AM user and user groups

AM User and User Groups refer : Manual configuration of User with Permission


# Service account registration

Steps

```
Take approval from manager
```
Re SAM Account request form.msg

Attach template for the approval mail

```
SAM account request
```
Zeus_Account_Request.xlsx

Please send request to Kunal Kishor(kunal.kishore@siemens.com) to integrate ModularMOM apps with Zeus platform services.

For more details you can refer the below link
[http://civ6w178:3000/services/samauth/landing/landing_page.html](http://civ6w178:3000/services/samauth/landing/landing_page.html)


### 1.

### 2.

### 1.

### 2.

# Product and App registration

Prerequisite:

```
Create AWS account
Set up a Simple Queue Service to receive data (AWS account – SQS queue configuration)
```
Postman configuration for a given environment

Url: https://tfs05mom.industrysoftware.automation.siemens.com/MOM/ModularMOM/_git/RestAPISamples

Set up postman authentication configuration with below information specific to that namespace

Steps to register

```
Register the product
```
Url: https://arm.us-east-1.preprod.teamcenterwebservices.com/product/registerProduct

Request Body:

{
"name": "OpcenterModularManufacturing_Test_Integration",
"description": "A rich collection of purpose full cloud-first modules able to run standalone or interconnected to deliver customer Apps
/ solutions which are easy to configure and extend",
"tags": {
"purpose": "pre-production",
"env": "preprod"
}
}

```
add new product name
add description as per the product
```
```
Link to a add product the tier
```
Url: https://arm.us-east-1.preprod.teamcenterwebservices.com/product/<RegisterProductId>/addProductTier

Request Body:

{
"name": "OpcenterModularManufacturing_Basic_Integration",
"description":"Offers access to basic OpcenterModularManufacturing",
"sku": "MOD2000",
"provisionSteps": [
{
"type": "CreateTenantAccount"
},
{
"type": "ProvisionEvent",
"stepData": {
"notifyOnly": false,
"publishTopicId": "https://sqs.us-east-1.amazonaws.com/778546410073/OcMod-Test-Integration-Queue"


### 1.

### 2.

### 3.

### 4.

### }

### }

### ],

"provisionUserSteps": [
{
"type": "CreateTenantAccount"
},
{
"type": "ProvisionEvent",
"stepData": {
"notifyOnly": true,
"publishTopicId": "https://sqs.us-east-1.amazonaws.com/778546410073/OcMod-Test-Integration-Queue"
}
}
],
"unprovisionSteps": [
{
"type": "UnprovisionEvent",
"stepData": {
"notifyOnly":true,
"publishTopicId": "https://sqs.us-east-1.amazonaws.com/778546410073/OcMod-Test-Integration-Queue"
}
}
],
"unporivionUserSteps": [
{
"type": "UnprovisionEvent",
"stepData": {
"notifyOnly":true,
"publishTopicId": "https://sqs.us-east-1.amazonaws.com/778546410073/OcMod-Test-Integration-Queue"
}
}
],
"cleanUpSteps": [
{
"type": "CleanUpEvent",
"stepData": {
"notifyOnly":true,
"publishTopicId": "https://sqs.us-east-1.amazonaws.com/778546410073/OcMod-Test-Integration-Queue"
}
},
{
"type": "CleanUpTenantAccount"
}
],
"updateSteps": [
{
"type": "UpdateEvent",
"stepData": {
"notifyOnly":true,
"publishTopicId": "https://sqs.us-east-1.amazonaws.com/778546410073/OcMod-Test-Integration-Queue"
}
}
]
}

```
add register ProductId in URL
Update the product name
Update the SKU
Update PublishTopic
```
```
Add/update new register queue in Rancher
```
use https://k8s.prod.us-east-1.kaas.sws.siemens.com/dashboard Select Specific namespace(ocmod-integration-2204) Storage ConfigMaps onboardmgr-
vars


### 1.

# Provider role creation

Steps:

```
Refer the below link for the product registration
```
Product and App registration

2. ModMom: ProviderRole with actAsRole trust policy

ModMom has requirement to register new app in samAuth using approved template into provisioned tenant's sam account programmatically during tenant
resource provisioning step.
Our requirement fits more of "ProviderRole" functionality exposed by PlPaas platform.
ProviderRole allows Siemens Services to perform activities on behalf of the tenant.

Please send request to PLPass team [Brendan Brolly (brendan.brolly@siemens.com)] for provider role to the new product.

Example:

### {

"name": "ProviderRoleForOpcenterAppCreation_Integration",

"description": "The role used by Opcenter to delegate samAuth App creation",

"trustPolicy": "{\"version\": \"2019-09-30\",\"rules\":[{\"effect\":\"Permit\",\"actions\":[\"sam:actAsRole\"],\"subjects\":[\"sws:::ocmod\"]}]}",

"targetProvisionedResources": [

"sws:::arm:::product:productName/OpcenterModularManufacturing_Basic_Integration"

],

"rolePolicy": "{\"version\":\"2019-09-30\",\"rules\":[{\"effect\":\"Permit\",\"actions\":[\"samauth:registerAppFromTemplate\"],\"resources\":[\"sws:::samauth::
template::*\"]},{\"effect\":\"Permit\",\"actions\":[\"samauth:unregisterApp\",\"samauth:createClient\",\"samauth:getAppInfo\",\"samauth:updateApp\",\"samauth:
listApps\",\"samauth:addAppScopes\"],\"resources\":[\"*\"]}]}",

"tags": {

"AuthAppCreatorRole": "samauth app creator role",

"Owner": "OpcenterModularManufacturing"

}

}

More details about ProviderRole for reference : [http://civ6w178:3000/services/sam/Developer_API_Reference/providerRole/createProviderRole.html](http://civ6w178:3000/services/sam/Developer_API_Reference/providerRole/createProviderRole.html)


### 1.

# Provision Product and User

Postman configuration for a given environment

Url: https://tfs05mom.industrysoftware.automation.siemens.com/MOM/ModularMOM/_git/RestAPISamples

Steps

```
Link to add new product
```
Url: https://arm.us-east-1.preprod.teamcenterwebservices.com/product/provisionProduct

RequestBody:

{
"tenantName": "Pratapgad_1",
"product": {
"name": "OpcenterModularManufacturing",
"tier": "OpcenterModularManufacturing",
"sku": "MOD1000"
},
"productUser": {
"firstName": "test",
"lastName": "test",
"email": "test.test@siemens.com"
},
"tags": {
"purpose": "testing"
},
"description": "OcMod Application Account"
}

ResponseBody:

{
"accountId": "bbcf8b71485d48a993b3f075cafa7080",
"userId": "3c435f59a9f748f184bdce72aa8f89a2",
"status": "ProvisionInitiated",
"jobId": "OpcenterModularManufacturing-OpcenterModularManufacturing-c5c396f4ffcb42bf8c9050a95773f9ce"
}

More details about Provision Product for reference : [http://civ6w178:3000/services/arm/Developer_API_Reference/products/provisionProduct.html](http://civ6w178:3000/services/arm/Developer_API_Reference/products/provisionProduct.html)

2. Link to add new User

Url: https://arm.us-east-1.preprod.teamcenterwebservices.com/product/provisionProductUser

RequestBody:

{
"tenantAccountId": "bbcf8b71485d48a993b3f075cafa7080",
"product": {
"name": "OpcenterModularManufacturing",
"tier": "OpcenterModularManufacturing",
"sku": "MOD1000"
},
"productUser": {
"firstName": "test",
"lastName": "test",
"email": "test.test7@siemens.com"
},
"tags": {
"purpose": "testing"
},
"description": "OcMod app"
}

ResponseBody:

{
"accountId": "bbcf8b71485d48a993b3f075cafa7080",
"userId": "cf7aa2b03d6445cea89a5d1d07107c4e",


"status": "ProvisionInitiated",
"jobId": "OpcenterModularManufacturing-OpcenterModularManufacturing-f24b0a16052e450984ceb335caba5983"
}

More details about new provision user for reference : [http://civ6w178:3000/services/arm/Developer_API_Reference/products/provisionProductUser.html](http://civ6w178:3000/services/arm/Developer_API_Reference/products/provisionProductUser.html)

3. Link to get list of provision product

Url: https://arm.us-east-1.preprod.teamcenterwebservices.com/product/listProvisionedProducts

ResponseBody:

{
"provisionedProducts": [
{
"tenantAccountId": "5cb406a8e9e44fbd80d2597761fc039c",
"tenantUserId": "26abebf9eddb414a8908c94e36a7eec9",
"productName": "OcMod-Test-10",
"tierName": "Basic",
"sku": "MOD10010",
"status": "ProvisionSuccess",
"creationDate": "02/18/2022 10:10:39",
"markedForDelete": false,
"activeState": "Active",
"provisionOptions": {},
"allowanceOptions": {},
"provisionActionData": {
"requestId": "arm-48b399fe-35a3-4c53-9b30-37110291af1e",
"workflowId": "da52b98849a148d2ac9205b6c7e188fe",
"eventIds": [
"32a8e466260740c8a9c4cc9fb3ca9934:pe0:OcMod-Test-10:Basic"
]
}
}

}

More details about list of provision Product for reference : [http://civ6w178:3000/services/arm/Developer_API_Reference/products/listProvisionedProducts.](http://civ6w178:3000/services/arm/Developer_API_Reference/products/listProvisionedProducts.)
html

4. Link to UnProvision User

Url: https://arm.us-east-1.preprod.teamcenterwebservices.com/product/unprovisionProductUser

RequestBody:

{
"tenantAccountId": "bbcf8b71485d48a993b3f075cafa7080",
"tenantUserId": "cf7aa2b03d6445cea89a5d1d07107c4e",
"product": {
"name": "OpcenterModularManufacturing",
"tier": "OpcenterModularManufacturing",
"sku": "MOD1000"
},
"excludeSteps": ["NotifyEmail"],
"tags": {
"purpose": "testing"
}
}

ResponseBody:

{
"accountId": "bbcf8b71485d48a993b3f075cafa7080",
"userId": "cf7aa2b03d6445cea89a5d1d07107c4e",
"status": "UnprovisionInitiated",
"jobId": "OpcenterModularManufacturing-OpcenterModularManufacturing-f24b0a16052e450984ceb335caba5983"
}

More details about UnProvision user for reference: [http://civ6w178:3000/services/arm/Developer_API_Reference/products/unprovisionAllUsers.html](http://civ6w178:3000/services/arm/Developer_API_Reference/products/unprovisionAllUsers.html)


5. Link to UnProvision Product

Url: https://arm.us-east-1.preprod.teamcenterwebservices.com/product/unprovisionProduct

RequestBody:

{
"tenantName": "Pratapgad_1",
"tenantUserEmail": "test.test@siemens.com",
"product": {
"name": "OpcenterModularManufacturing",
"tier": "OpcenterModularManufacturing",
"sku": "MOD1000"
},
"tags": {
"purpose": "testing"
}
}

ResponseBody:

{
"accountId": "bbcf8b71485d48a993b3f075cafa7080",
"userId": "3c435f59a9f748f184bdce72aa8f89a2",
"status": "UnprovisionInitiated",
"jobId": "OpcenterModularManufacturing-OpcenterModularManufacturing-c5c396f4ffcb42bf8c9050a95773f9ce"
}

More details about UnProvision Product for reference : [http://civ6w178:3000/services/arm/Developer_API_Reference/products/unprovisionProduct.html](http://civ6w178:3000/services/arm/Developer_API_Reference/products/unprovisionProduct.html)


# Agile Scrum Practice Modular MOM2

```
Agile
What is Agile?
How should we use Agile Values and principles?
4 Agile Values
12 Agile Principles
Scrum
Scrum Values
Why Scrum is Awesome
Biggest Scrum Benefits
```
## Agile

## What is Agile?

```
Agile is a set of values and principles.
```
```
It is an approach to software development
under which requirements and solutions evolve
through the collaborative effort of
self-organizing and cross-functional teams
and their customers/end users.
```
## How should we use Agile

## Values and principles?

```
The teams make decisions based on agile
values and principles.
```
## 4 Agile Values 12 Agile Principles

## Scrum

## Scrum Values

```
Scrum is an empirical-process-control development framework
in which a cross-functional self-managing Team develops a product in an iterative incremental
manner.
```
```
Iterative: with each iteration the software is improved through the addition of greater detail.
Incremental: the software is built and delivered in pieces; each increment is fully coded and
tested.
```
```
Each timeboxed Sprint, a potentially shippable product increment is delivered and, ideally, shipped.
```
```
Value-Driven Culture is Essential:
```
```
Courage: Scrum Team members have courage to do the right thing and work on though
problems
Focus: Everyone focuses on the work of the Sprint and the goals of the Scrum Team
```

```
Commitment: People personally commit to achieving the goals of the Scrum Team
Respect: Scrum Team members respect each other to be capable, independent people
Openness: The Scrum Team and its stakeholders agree to be open about all the work and the
challenges with performing the work
```
Why Scrum is Awesome

```
Simple Framework
Empirical
Short Cycles (Delivery,
Learning, Feedback)
```
Biggest Scrum Benefits

```
Early Delivery of High Value
Adaptability
Transparency
Focus on Quality
High Velocity
Sustainable Pace
Continuous Improvement
Continuous Feedback
High Trust Environment
Motivation
```

# Artifacts

1. Product Backlog
2. Sprint Backlog
3. Product Increment
Artifacts in Modular MOM 2

## 1. Product Backlog

```
It contains all the work which is required to add value to the product, e.g.: requirements, use cases, bugs, technical debt, infrastructural or
architectural work, PoCs.
```
```
It is ranked (highest priority on top) and continuously refined by PO, Dev Team, and (if possible) stakeholders.
```
```
It gives a global view and mid/long-term forecasting.
```
## 2. Sprint Backlog

```
It contains work to be done in the sprint (with details).
```
```
It is defined at sprint start (sprint planning) and monitored during the sprint.
```
```
It gives teams a detailed, short term view of required work.
```
## 3. Product Increment

```
Each timeboxed Sprint, a potentially shippable product increment (PSI) is delivered and, ideally, shipped.
```
## Artifacts in Modular MOM 2

```
Product Backlog in Modular MOM2:
Features
User Stories
Sprint backlog: In the dedicated team section, chose the backlog of the current sprint
Product Increment: For information on the Increment, please see this dashboard.
```

# Feature, User Story, Task, Epic

```
User Story
Feature
Tasks
Epic
Something more about User-facing
Product Backlog Items (Business
User Stories)
Important Aspects
Templates
Invest in good User Stories
Vertical Slicing
Splitting Techniques
```
```
TFS Configuration
```
```
In our TFS configuration we chose the "Agile"
TFS template that uses the "User Story" term
representing the Product Backlog Item. This
term "User Story" might be misleading,
because not everything in a product backlog is
a "story of a user" (even if it is convenient that
the majority of product backlog items are "user
stories" in an agile organization).
```
```
Instead, on the newer "Scrum" TFS template
the level of PBIs is not called "User Stories"
but "Product Backlog Items".
```
## User Story

```
It is a description of a change in the system behavior from the perspective of a user, who wants to accomplish something. A real User Story is
mapped with our TFS Item “User Story” and Value area BUSINESS.
```
```
A US is a work item which is user-facing, but it is not necessarily useful for the market. So, a US is useable, but not necessarily useful alone.
```
```
In an agile organization it is convenient that most product backlog items are real user stories, so that the organization earns money. But sometimes
there are also other activities to do, like Architectural work, Technical Debt reduction or research investigations. They are mapped with TFS Item “User
Story” and Value areas NOT BUSINESS. Please see here the different Value Areas.
```
```
Configuration in TFS
```
```
A user story is assigned to a dedicated feature team (Area + Assigned To) that is responsible to deliver the functionality.
```
```
Ideally it is small enough to be completed within a sprint; the Iteration Path indicates when the item is processed or closed.
```
```
User Stories contain detailed Description and Acceptance Criteria (including test cases and NFR, if needed).
```
```
If a User Story creates further work, modifications or enhancements, we create a new User Story to trace the new implementations and leave the
existing User Story with the initial scope.
```
```
State Definition
```
```
New Candidates for Sprint Backlog are in New state.
```
```
Active As soon as at least one activity (task) starts, the backlog item is moved in Active state.
```
```
Resolved optional: If development tasks are closed, the backlog item can be moved in Resolved state.
```
```
Closed If all activities (including doc) are closed and tests are executed successfully on the integration environment, the backlog item is
moved in Closed state.
```
```
Removed The backlog item is not needed anymore for the product.
```
```
Incomplete backlog items remain Active (or Resolved) and
```
```
are moved to the next sprint
are moved back to New state if the item is temporarily on hold
```
## Feature

```
A feature is basically the same as a User Story, but with a different granular level; it might be useful for grouping in order to facilitate planning.
```
```
E.g.:
With a User Story we implement a user-facing functionality, but without having other user stories implemented, this US alone does not bring the entire
requested value to the customer. So, we could use Features to package different US.
```
```
A feature typically represents a user-facing, shippable component of software.
```
```
Configuration in TFS
```
```
A feature is assigned to a dedicated feature team (Area + Assigned To) that acts as a driver for the entire functionality. A feature driver is responsible
for organizing effective backlog refinement meetings and for coordinating eventual cross-team collaboration. If the priority requires it, the single child
user stories can be distributed among all feature teams and SMEs.
```
```
It can span more than a sprint; the Iteration Path indicates when the item is processed or closed.
```

### 1.

### 2.

### 3.

```
Definition
```
```
New The requirement is inserted in product backlog, but work has not started yet.
```
```
Active At least one of the child User Stories is in active.
```
```
Closed The requirement is fulfilled and all child User Stories are closed.
```
```
Removed The feature is not needed anymore for the product.
```
Tasks

They represent every work which is planned for completing a product backlog item, like requirement analysis, development, testing, documentation,
etc.

Configuration in TFS

Tasks are assigned to a dedicated feature team member (Area Path + Assigned To) who is responsible for fulfilling the task.

It lasts approximately 1 to 2 days.

```
State Definition
```
```
New The activity has not started now.
```
```
Active The feature team is working on the task.
```
```
Closed The activity has been finished.
```
Epic

Epics are used by product management.

Something more about User-facing Product Backlog Items (Business User Stories)

Important Aspects

There are 3 important aspects that must be considered for each User

Story. These aspects are called the 3C’s:

```
C (Card)
a short written description of the change introduced to the entire system from the
perspective of a user
used for planning and as a reminder
could be mapped in TFS field Title + Description
C (Conversations)
highlights the details of the story
could be mapped in TFS field Discussion
C (Confirmation)
tests used to determine if expectations are fulfilled and if story is complete
could be mapped in TFS field Acceptance Criteria + Linked Test Cases
```
Templates

It makes sense to use Templates for user-facing items, in order to describe

the system change from a user's perspective.

An example for a Template is the following: As a <user> I want <goal / capability> so that <benefit>

Start using a template, because:

```
a template helps to improve the understanding of the story, as it helps to remember who is the
user and what is the benefit of developing the story.
the teams learn how to ask correct questions to understand the requirement: WHO, WHAT,
WHY
```

As prerequisite, the teams need a good understanding about the users of the system (stakeholder map,
develop profiles with photos, meet stakeholders).

Skip using a template:

```
if there is no user interaction (e.g. Architectural US, Spikes), because there might be no
advantage.
if the team is experienced enough in writing stories, so that they are used to ask the correct
questions.
```
In any case, if you use a template or not, stay aware that the item is written in a way that can be
understood by the whole team, including customers.

Invest in good User Stories

Finally, after splitting, our user-facing items are INVEST. This is an

acronym for

```
Independent
Avoid introducing dependencies between stories, because this leads to prioritization and
estimation problems, but try to create stories that can be implemented, tested and closed
without any dependency of another work item. There might be a logical dependency, but not a
technical one.
Negotiable
Stories are no written contracts or detailed requirement specifications that must be
implemented. They are descriptions of functionality used as reminders to hold the conversation.
Story details must be negotiated in conversations between customer/PO and the team (e.g. in
Backlog Refinements) and become acceptance criteria and tests. There is no need to include all
relevant details, but just enough! We need to encourage ourselves to write stories in a way, that
ideally the conversation can be resumed by any developer and customer.
Valuable
Avoid writing stories only valued by developers, because the benefits should be apparent to
customers and purchasers, so that the items can be prioritized. Valuable means that we create
user-facing items.
Estimable
It is important to estimate the size of a story, because a story cannot be prioritized without
considering the "costs". Sometimes estimation seems not to be possible, but there are ways to
let them become estimable: If developers lack domain knowledge, do further discussions with
PO/stakeholders, if developers might lack technological knowledge, create a spike, if the story is
too big, split it.
Small
Size matters, because too short stories leads to much bureaucracy (> combine) and too large
stories cannot be used in planning (> split). "Size the story to the horizon" means to have small
sized stories on top of our backlog, while large stories are at the bottom.
Testable
Tests prove that the story has been successfully developed. So we write down Acceptance
Criteria / Acceptance Tests to capture the user expectations. Ideally we write tests early, before
coding, in 2 steps (notes, test cases). They should be written whenever someone talks about
the story and wants to capture details, i.e. at the start of the sprint or whenever tests are
discovered during and after programming. Whenever possible, tests should be automated!
```
Vertical Slicing

Think about the whole story as a multi-layer cake, e.g., information layer,

integration layer, and configuration layer.

When we create items along technical lines, we are just serving a part of the cake which has little value
to the user. But we want to give the user the essence of the entire cake! The best way is here to slice
the cake/requirement vertically through all layers.

```
Consequence When teams work in
vertical slices, they...
```
```
When teams work in horizontal slices,
they...
```
```
Return of
Investment
```
```
make value explicit in their
backlog, have more
conversations about value and
lower the risk to accidentally
build low-value changes.
```
```
risk to develop functionalities which do not
meet the real need, or do not really produce
return of investment.
```
```
get value sooner. delay return of investment.
```
```
get earlier, higher quality
feedback.
```
```
delay the feedback from stakeholders,
because multiple slices must be completed in
```

```
order to deliver an increment of value.
```
```
Efficiency
Productiveness
```
```
prevent multi-tasking and
reduce need for cross-team
coordination.
```
```
tend to originate multi-tasking and need for
increased cross-team coordination.
```
```
Predictability become more predictable in
delivering value, because
working software becomes the
primary measure of progress.
```
```
become less predictable (WIP tends to go out
of control), exposing themselves to requests
for long term commitment.
```
```
Flexibility can provide adequate flexibility
to the business.
```
```
tend to accumulate work before it become
visible to stakeholders, reducing the flexibility
provided to the business and exposing
themselves to "Step 2" risk (creating business
debt in product).
```
```
Quality typically work in a sustainable
pace, and can easily guarantee
the quality of the product.
```
```
have more difficulties in controlling the quality
of the product and tend to postpone
integration tests.
```
```
Knowledge
Sharing
```
```
break down complexity more
easily, reducing the feature
teams learning curve.
```
```
create a context which promotes competence
specialization instead of knowledge sharing.
```
Splitting Techniques

There exist many different Splitting Patters that offer «cheat sheets» that

help to create user-facing vertical-sliced backlog items.

```
SPIDR
Zero-One-Many
Hamburger technique
Work Guide to splitting User Stories
21 splitting patterns and related questions to ask during refinement
Operations, Data, Cross-cutting, Performances, Priority
Splitting mistakes and how to avoid them
```

# Bugs, Issues

```
Bugs
Who
When
How
Assign Bugs
Fix Bugs
Postpone Bugs
Close Bugs
Issue
```
```
TFS Configuration
```
```
Regarding Bugs Management, in our TFS
configuration we configured that "Bugs are
managed with Requirements". So Bugs
appear at requirements level, similar to User
Stories. They are estimated with Story Points
and might have child-tasks. Bugs are not
children of User Stories.
```
## Bugs

```
A bug is any problem in existing functionality. They can be found in any moment and can refer to platform/architecture or business functionality.
```
```
Note:
A missing functionality is traced with backlog item "User Story".
```
```
State Definition
```
```
New When a bug is found, a TFS item "Bug" must be created. Its status is automatically set
to New.
```
```
Active As soon as bug fixing starts, the item is moved in^ Active^ state.
```
```
Resolved Resolved^ state indicates that the bug is fixed and needs to be verified.
```
```
Preferably a bug should be verified by a different person than the person who has fixed
it. It is not required to assign it to the person who has found it; it can also be verified by
every other team member having competences.
```
```
Closed If the bug is tested successfully on integrated scenario, the item is moved in^ Closed^ stat
e.
```
## Create Bugs

```
Who
```
```
Bugs can be created by anyone who finds an error in the system (platform/architecture or business bug) and who has the proper rights in TFS to do so.
```
```
Defects raised by external startup organizations are also traced with TFS item Bug. As the external startup organizations do not have the proper rights
to create items in TFS, a Modular MOM 2.x member needs to create the item according to their input. To identify bugs created on behalf of external
startups, tags are used (please see chapter "Test Information" in wiki page TFS Azure DevOps).
```
```
The created item is automatically set to New state.
```
```
Starting from Sprint 19, Nightly builds are up and running. There is a specific team (sprint-wise round-robin schedule) in charge of analyzing the
outcome. For issues which cannot be fixed immediately, a bug is created. Please have a look here for the round-robin procedure.
```
```
When
```
```
# Situation To Do
```
```
1 User Story is active and Test Case fails
No creation of Bug item
Problem is fixed immediately
```
```
2 User Story is closed and Test Case fails
During test execution, create a new work item Bug
```

```
TFS creates a "Tested By" relation between Bug e Test Case
Test Case is already linked to the User Story
```
```
3 Test Case does not fail
(usability bug or logical bug, explorative
tests)
```
```
Create a new work item Bug
If required, create a Test Case and link it to the Bug (relation: Tested by)
This can lead to a situation where a bug is neither linked to a Test Case nor to a User
Story
```
In general:

```
Bugs are linked to Test Cases
In case there is a bug without test case, Repro Steps must be self-explained
```
How

Fields to be filled in:

```
Field Description
```
```
Area
Team with proper knowledge and competencies to solve the bug
Default value "ModularMOM", in case no team can be identified
```
```
Iteration
Default value "ModularMOM"
Current Sprint, in case the bug is found by a failing Nightly build
```
```
Assigned to Unassigned
```
```
Repro steps
Description how the bug can be reproduce
Pay particular attention, if no Test Case is linked to the Bug.
```
```
Priority Identification of impact for the client:
```
```
1 : Product shouldn't ship without the successful resolution of the work item. The bug should be addressed soon.
2 : Product shouldn't ship without the successful resolution of the work item. The bug doesn't need to be addressed
immediately.
3 : Resolution of the work item is optional based on resources, time, and risk.
4: lowest
```
```
Severity
Identification of impact for the project or software from critical to low (see table below)
In case of blocking "platform bugs" chose the highest severity possible
```
```
Activity
Identification of the context
I.e. Deployment, Testing, Development, Requirements, Design, Documentation
```
```
Deployment For future use only
```
```
Development List useful information related to the released fix, such as the Commit, the Pull Request, the Build, the Release, and the
Merge activities
```
```
System Info > Found In
Build
```
```
Use the release date information to identify when bug is found, i.e. 2204
```
```
Tag Ext Identification tag, if bug was found by other external startup organizations
```
```
add Related work items I.e. Test Case, User Stories
```
Please see Microsoft Azure DevOps documentation for fields specific to bugs.

```
Bug
Severity
```
```
Description Comments Consequences on shipment
```
```
Critical
```

### 1.

### 2.

### 3.

### 4.

```
Critical function
defect
```
```
Product generally not usable
Important function not usable
Faulty computing result (calculation logic), if critical
Significant damage at the customers’ installation possible
Operational safety endangered
Security issue
```
```
Bug fix,
no release
```
```
(In case of a security bug, the Security
Expert must be involved)
```
```
High Significant
functional
limitation
```
```
Deviation of one or more system characteristics
Important function only usable with restrictions
Workaround by the customer is possible
```
```
Bug fix,
no release
```
```
Medium Minor functional
limitation
```
```
The defect does not cause failure, does not impair usability and the desired processing
results are obtained by working around the defect.
```
```
Bug fix if possible,
shipment allowed if % respected
```
```
Low Cosmetic error The defect is related to the aesthetics. Bug fix if possible,
shipment allowed if % respected
```
Bugs % must be in range of Quality Targets. Please see the corresponding dashboard.

Assign Bugs

```
# Situation To Do
```
```
1 Bug can be assigned to a team of
competence/knowledge Put corresponding Team name in Area Path
Assign the item to the appropriate team member, or put the Scrum Master's name or
leave the field Unassigned.
```
```
2 Bug cannot be assigned to a team of
competence/knowledge Assign item to PO, so that Severity and Priority can be discussed and the item can be
ranked in product backlog accordingly
Assign the item to a team in Sprint Planning Meetings
```
Usually the Iteration is set to default value "ModularMOM". If a product bug is found by a failing Nightly build, set the Iteration to current sprint.

Fix Bugs

Once a bug has been fixed, and it has been verified locally by its assignee, it has to be released (with the aim to be verified) by following these steps:

```
The fix must be committed (Commit operation)
A Pull Request must be done against Master Branch (see [Procedure for Pull Request - RevMOM - MES Wiki (siemens.com)] for additional
details about Pull Request)
In case of an automatic system, once a Pull Request has been requested, a Build is automatically scheduled in order to provide a new
Release which embraces the fix
When a new Release is created, an automatic installation is performed and linked Test Cases are also automatically executed
```
Note:
If a bug is linked to a Pull Request (see [Procedure for Pull Request - RevMOM - MES Wiki (siemens.com)] for additional details about Pull Request),
the links in the Development section are automatically created

Postpone Bugs

When a bug is postponed, it is going to be lowered in product backlog ranking.

Close Bugs

After bug fixing, the item is put in Resolved state to verify if the problem disappeared. Field System Info > Integrated In Build must be set. Use the
release date, i.e. 2207.

Ideally, this verification should be done by a person different from the one who fixed it. It is not required that the bug is tested by the person who has
found it. It can also be verified by every member within the team who fixed the bug.

After successful testing, the item is put in Closed state.

Issue

Issues can be used to track various inputs coming from users, i.e. questions, bugs, enhancements. In Modular MOM 2.x we use this work item to track
internal Pipeline defects.

Configuration in TFS


In case of Pipeline issues, assign the work item to the members of the CI/CD team.

The item should be processed as soon as possible.

```
State Definition
```
```
Active The issue has been created and the assignee is working on it.
```
```
Closed The issue has been resolved.
```

# Test Plan, Test Suite, Test Case

```
Test Plan
Test Suite
Test Case
```
```
Link to: ModMOM2 Test Strategy wiki page
```
```
Lint to: ModMOM 2.x Test Strategy
presentation including KPIs
```
## Test Plan

```
Test Plans are used to track testing for sprints or releases, to see when the testing is complete.
```
```
There are two kinds of Test Plans in Modular MOM 2.x, the Sprint Test Plan and the Master Test Plan. The feature teams have to insert the Test
Cases in both Test Plans.
```
```
Sprint Test Plan
There is a separate Test Plan for each single sprint. It is organized by User Stories and it contains only the new acceptance tests for this specific
sprint.
```
```
Master Test Plan
It contains all the Test Cases of all sprints in a release. It is organized by logical criteria (e.g. modules such as User Management, Factory, Material
Management, and Order Management) in order to better understand which are the test results of the different modules.
```
```
State Machine
```
```
State Definition
```
```
Active The Test Plan in this state represents the active work.
```
```
Inactive The Test Plan in this state represents the work that has finished.
```
```
Note:
The acceptance tests of the current sprint (the ones inserted in the Sprint Test Plan) become regression tests when a new sprint is started. They are
executed frequently along with the Nightly Build.
```
## Test Suite

```
The Test Suites are used to structure the Test Plan. Please, find the examples for Modular MOM 2.x Test Suites in the table below:
```
```
Master Test Plan Sprint Test Plan
```
```
static Test Suite query-based Test Suite
```

### 1.

```
a.
b.
```
```
c.
```
```
2.
a.
b.
```
```
c.
d.
```
```
3.
```
```
4.
```
```
5.
6.
a.
b.
```
State Machine

```
State Definition
```
```
In Planning The Test Suite in this state represents the work that has been recently added and it is in definition state.
```
```
In Progress The Test Suite in this state represents the active work.
```
```
Completed The Test Suite is this state represents the work that has finished.
```
Test Case

A Test Case is a set of actions executed to verify that a particular functionality is working properly. An automatic test is mapped to a TFS Item named
"Test Case".

During our Backlog Refinements we split the feature scope into one or more USs.
Splitting should possibly be vertical: this results into user-facing, auto-consistent, and testable User Stories. If the Feature is already fine-grained and
represents a small user-facing functionality, the single child-US refers to the feature scope.

Applying the vertical slicing concept to User Stories helps to improve the Quality, because the integration in the entire system is tested immediately
with each single US belonging to the Feature (the Quality is always under control).

Configuration in TFS

Test Cases are associated to User Stories, not to Features, because it is the (Business) User Story that represents the small user-facing
functionality to be tested. For further explanations, please see here: Feature, User Story, Task, Epic.

Anyhow, for process reasons, currently we also need to make sure that the entire feature in correctly integrated in the entire system and covered by
acceptance tests. To identify this, we use different tags.

```
Tag "FtrInt" (Feature Integration)
At least one User Story must be tagged with "FtrInt"
In case of vertically sliced User Stories, all User Stories could be tagged with "FtrInt", because each US represents a small
functionality and the integration in the entire system can be tested.
The "FtrInt" User Story doesn't necessarily have to be a dedicated US, i.e. there is a US for the frontend which can be used also as
"FtrInt" US, because if it works from the UI it means that the Feature as a whole works.
Tag "NoTCRequired" (No Testcase required)
In case of vertically sliced User Stories, there are only few cases of not-tested User Stories.
If Spike User Stories have been created for investigations and learning purposes, tag them as "NoTCRequired", when no
acceptance tests are executed.
If Architectural User Stories are tested with already existing acceptance tests (in Release Test Plan), tag them as "NoTCRequired".
Avoid creating "fake" Business User Stories which must be tagged with "NoTCRequired"; if User Stories are not user-facing,
consider creating only one single User Story with various tasks that refer to the Feature scope.
Some Gherkin Test Cases are listed in the Feature's Acceptance Criteria. The various child-USs of the feature refer to all of them. A
placeholder Test Case is created in TFS and associated to the "FtrInt" US(s).
The Feature Team decides on which test framework the Test Cases are implemented (BDD API, BDD UI, JMeter, ..). Therefore, use the
"Summary" tab in the TFS Test Case.
Associate the "test method" with the TFS Test Cases, if technically possible.
Test Cases associated to the User Stories must be:
Inserted into the Sprint Test Plan, grouped into Test Suites according to their User Stories.
Inserted into the Master Test Plan, grouped into the dedicated Test Suites.
```
State Machine


```
State Definition
```
```
Design The Test Case in this state represents the work that is recently added and it is in definition state.
```
```
Ready The Test Case in this state is ready for execution.
```
```
Closed The Test Case is this state represents the work that has finished.
```
Link between Gherkin Test Cases and TFS work items


# TFS Items - Creation Workflow

```
Event Activities
```
```
Backlog
Refinement
```
```
In BR0 the participants create the upcoming Features.
```
```
In further Backlog Refinements we split big Features, so that they are representing a shippable component of the software (i.e. a use-
case).
```
```
We brainstorm the requirements together, extend the Feature's Description and create all needed User Stories. The single Business
User Stories are less complex, but still user-facing.
```
```
We refine the requirement, until the user story reaches the "Ready" state (Definition of Ready).
```
```
Contrarily to Scrum, in Backlog Refinements the teams already commit to the work items they will work on and they create their team
backlogs.
```
```
Sprint
Planning
```
```
The teams break down the user story into tasks considering the following stages to be executed:
```
```
Evaluation of the Architecture and Design
Implementation steps
Test activities (Unit, Integration, ..., Security)
Documentation activities (draft user documentation, review documentation, ...)
Other activities related to tags put on the Story
```
```
Example:
```
```
Sprint If the work on dedicated items reveals the need of new activities, create new Product Backlog Items.
```

# MomValueArea

User Stories are classified with the following Value Areas:

```
Value Area Description Examples Tests
required
```
```
Test
associated
to TFS
Test Cases
```
```
Architectural Provide technical services to implement business
functionalities.
```
```
Note:
Consider that these activities are not directly exposed to
stakeholders; there is the risk that we find out late the
implementation does not exactly fit their need.
```
```
Infrastructural work required in one or
more components, as a pre-requisite to
implement one or more business
functionalities
Realization of infrastructural services which
are required to complete business
functionalities which were left incomplete or
too "poor" in the previous increment.
```
```
Yes No
```
```
Business Improve the product to directly fulfill customers or
stakeholders needs, both from the functional and non-
functional point of view.
```
```
New functionalities
Completion of functionalities which were left
too "poor" in the previous release
Non-functional aspects (e.g. improvement in
performances, usability or configurability of
the system)
```
```
Yes Yes
```
```
Spike Short, time-boxed activity used to gain the knowledge
necessary to reduce the risk of a technical approach,
better understand a requirement, or increase the
reliability of a story estimate.
```
```
Research and POC activities
Knowledge transfer activities
Analysisi activities
```
```
No
```
```
(if no code
is
produced)
```
```
No
```
```
Technical Debt Improve the codebase in order to mitigate risks of low
quality and to ensure that development effort remains
always proportional to feature complexity.
```
```
All activities that don't result in changes to the exposed
functionalities but in the way in which they are executed.
```
```
Refactoring working parts
Improvements on automatic test coverage
Improvement of code readability
Removing code duplication
Improvement in the continuous integration
and delivery pipeline
Improvement in internal documentation
```
```
Yes No
```

# Estimating

```
Story Points
Velocity
Capacity
```
## Story Points

For estimating we need a method that

```
allows to change mind whenever there is new information
is quick
is tolerant to imprecisions
works for large and small stories
```
So in Agile, we are estimating with Story Points.

Story points are numbers (0, 1, 2, 3, 5, 8, 13, 21) that are defined by each team during Backlog Refinement 2 and/or Sprint Planning 2.

The story point calculation responds to the question "how much work is it to fully complete the requirement". So we estimate the quantity of work.

Usually it contains effort, risks and complexity, but also other implementation factors could be within the story points calculation depending on the Defin
ition of Done (i.e. refactoring and test automation).

We can use the Story Points estimation in order to:

```
trigger discussions if all implementation aspects are considered (at least in case of disagreement within the team/area).
do a predictive planning and define the sprint backlog, based on
the approach of relative estimation
the team’s velocity (so story points evaluation is regarding the team only)
```
## Velocity

Velocity correlates to how the team estimates requirements.

If external circumstances are equal (team members, no holiday period), velocity should be stable. This shows the maturity of the team, because they are
predictable. It can happen that velocity changes, but it is a signal to understand why and improve.

## Capacity

Capacity correlates to actual task time. It takes into consideration the variation in work hours by team members as well as holidays, vacation days, and
non-working days.

We specify capacity in terms of hours.

The Capacity per day for each team member tracks the time dedicated to backlog work. Other activities like CoP participation, Backlog Refinements,
Retrospectives are not considered in the capacity.

We can use Capacity estimation in Sprint Planning 2 so that the team can understand if it is over or under committed for the sprint. Checking it daily, the
team is able to understand if they are on track.


# Events

1. Sprint
2. Sprint Planning
3. Daily Scrum
4. Sprint Review
5. Retrospective
Ongoing Activity: Product
Backlog Refinement
Events in Modular MOM 2

## 1. Sprint

```
The sprint is the container for the events.
```
## 2. Sprint Planning

```
It consists of two parts that boil down to what and how.
```
```
Sprint Planning One (WHAT - Global) focuses on the selection of ready items from those offered by the Product Owner, wrapping up questions, and
definition of the Sprint Goal. It is a meeting for all teams together where they decide which team will work on which items.
```
```
PO presents the highest priority items to the teams.
The teams distribute the items amongst themselves.
Any open questions related to the items (that weren’t resolved in earlier PBR) are discussed.
Discuss about the need for coordination between teams during the upcoming sprint and plan correspondent meetings if needed.
```
```
Sprint Planning Two (HOW - Team) focuses on creating the sprint backlog. It is a separate meeting per team where each team creates the plan for
getting the items to ‘done’ during the Sprint.
```
```
The team collectively decides which items they will work on.
The team creates its plan for getting the items to ‘done’.
```
## 3. Daily Scrum

```
15 minutes spent daily together at team level to exchange the information the team needs to take a shared responsibility and to manage themselves
in order to achieve their sprint goal.
```

### 1.

### 2.

### 3.

The Daily Scrum is a meeting for the team itself.

4. Sprint Review

The Sprint Review is an inspect-adapt point at the end of the sprint. It is the occasion for all teams to review one Potentially Shippable Product
Increment (PSI) together. The focus is on the whole product.

```
Customers and stakeholders examine what the teams built and discuss changes and new ideas.
The teams, Product Owner and users/customers/stakeholders decide the direction of the product.
```
5. Retrospective

The Sprint Retrospective is a meeting at the end of the sprint, where the teams are looking back to the past sprint, every team on its own.

```
Brainstorming about obstacles / impediments
Finding solutions / actions for improvement (and experiment / adopt them afterwards)
```
The Overall Retrospective is above the level of single team. It has the purpose to discuss cross-team, organizational and systemic problems within
the area and organization.

Ongoing Activity: Product Backlog Refinement

Generally, ongoing Product Backlog Refinement (PBR) is needed within each sprint to refine items to be ready for future sprints, so that

```
Sprint Planning meetings will run smoothly
Backlog remains populated with relevant, detailed and estimated items
Teams increase shared understanding and adaptiveness across them.
```
Backlog Refinement is done regularly and occupies about 5-10% of the sprint.

What do we do:

```
Understand WHY we need to implement the requirement
Understand WHO asks for the implementation
Split big items
Do detailed item clarification until items are "ready" for implementation (don't do problem solving; clarify until you have enough information for
planning)
Share questions and uncertainties
Identify strongly-related items that suggest shared work, common work, or coordination.
Estimate
```
```
Role Duties
```
```
Product
Owner
```
```
Presents ideas and the likely vision for the upcoming sprints.
```
```
Feature
Team Break-down the Product Backlog Items into smaller pieces, through collaboration with the Product Owner and Subject Matters
Experts.
Communicate to the Product Owner any consequence relating to what is being asked for and its rank-order for delivery,
including any technical, design or business debt that may be incurred as a result.
```
```
Scrum
Master Encourages participants to use established patterns to break-down Product Backlog items into fine grained requirements (see
Chapter "Splitting Techniques" here: Feature, User Story, Task, Epic)
If required, he/she can also facilitate the meeting, introducing the agenda, clarifying the rationale and the rules of the meeting
and indicating the time-box for the meeting.
```
```
create a level of granularity that ensures the Product Backlog contains fine grained requirements.
```
```
Subject
Matter
Experts
```
```
Participate in estimation and discussion to assist with increasing transparency of upcoming Product Backlog Refinements.
Observe.
```
Events in Modular MOM 2

Please refer to the following page: Scrum Events


# Accountabilities (Roles)

```
Scrum Team
Subject Matter Experts
```
## Scrum Team

```
A Scrum Team consist of Product Owner, Scrum Master and Developers.
```
```
Role Responsibilities Notes
```
```
Product Manager &
Technical Product Owner
```
```
In Modular MOM 2.x we adopted the Coexistence
model.
```
```
So, Product Manager(s) and Technical Product
Owner(s) are treated as a conjoined team that is
collectively responsible and accountable for their
success and that of their organizations to deliver a
high-quality product, meeting roadmap demands.
```
```
Product Manager
```
```
The Product Manager is business-focused on
product vision, product strategy, product
roadmap, and product lifecycle management.
```
```
Responsibilities
```
```
Together with Technical Product Owner,
responsible for ranking the backlog,
considering input from Business and
Development.
Responsible for driving business decisions.
```
```
Activities
```
```
Manages and communicates Product
Roadmap.
Collaborates with Technical Product Owner
to set business vision and agree on scope to
be delivered at estimated cost in regular
intervals.
Together with Technical Product Owners,
manages and orders Product Backlog items
to achieve goals and missions.
Tightens every feedback loop regarding the
product (e.g. release often, release early,
talk to customers,Minimal Marketable
Product, etc.).
Learns and plays with product to understand
its features, capabilities, and limitations.
Together with Technical Product Owners,
demos work deliverables to stakeholders.
```
```
Currently we have the following situation (called "Contract
Game")
```
```
Product Manager ("Business") is responsible for product
definition, not taken into account product realization.
```
```
Product Owner ("Development") is responsible for
technical execution, not taken into account product
definition.
```
```
Possible consequences: Dysfunction, Conflicts, Not
cooperative, Blaming
```
```
Scrum has a solution for this: The Product Owner who
creates a balance between authoritative power and
responsibility. The Product Owner
```
```
makes all strategically necessary decisions in order
to achieve best results
solves the Contract Game
```

```
Decides on activities to perform, gate, and
delegate to find the right balance between
doing and leading without becoming a
bottleneck.
```
He is not responsible for Product Backlog or for
driving process decisions.

Technical Product Owner

The Technical Product Owner is product
development-focused on translating product
roadmap into Product Backlog. In addition, he
drives Feature Teams' efforts during Sprints and
has responsibility for planning technical and
architectural efforts to reduce technical debt.

Responsibilities

```
Together with Product Manager, responsible
for ranking the backlog, considering input
from Business and Development
Responsible for maximizing value of work of
Feature Teams.
Responsible for driving technical product
roadmap.
```
Activities

```
Together with Product Managers, manages
and orders Product Backlog items to achieve
goals and missions.
Ensures visibility, transparency, and clarity of
Product Backlog and its items.
Processes input from organization to
determine order of Product Backlog items.
Exclusively provides work to Feature Teams.
Collaborates with Feature Teams to provide
domain information.
Monitors and communicates work in
progress status.
Is public face of Agile team and facilitates
access to stakeholders.
May participate in acceptance testing of work
deliverables.
Together with Product Managers, demos
work deliverables to stakeholders.
Decides on activities to perform, gate, and
delegate to find the right balance between
doing and leading without becoming a
bottleneck.
```
He is not a team lead, manager or project
manager; he is not responsible for driving process
decisions.

Summary

```
Product Roadmap Product Manager
```
```
Business Model and
Financials
```
```
Product Manager
```
```
Strategy and Market
Research
```
```
Product Manager
```
```
Vision and Leadership Product Manager
```
```
Product Lifecycle
Management
```
```
Product Manager
```
```
User Experience and
Product Backlog
```
```
Technical
Product Owner
```

```
For a more detailed list, please refer here (Agile
Practice Framework)
```
```
Developers
```
```
("Development" is not referring
only to software development,
but to the development of the
entire product as a whole.)
```
```
Ideally, they are composed of software
developers, testers, analysts, architects, UX
designers, documentation writers, etc. They are
capable to take over various tasks, i.e. a
programmer can do testing, an analyst can
program, a tester can help to define UX. The
single members are so called T-shaped members,
having expert knowledge in a specific technical
field, but having also general knowledge in the
whole system.
```
```
The developers
```
```
do all necessary work for transforming an
idea into a shippable product
work cross-functional
are self-managing to achieve the sprint goal
are flexible and adaptable
```
```
Sometimes we hear the term "Development Team" which
was used in former scrum guide. In order to emphasize
that the "Scrum team" is the only team, the word was
substituted by the word "Developers".
```
```
Sometimes we also hear the term "Feature Team". This
expresses the fact that the team is capable to implement
an entire functionality (requirement/feature) on its own. It
doesn't refer to the TFS item "Feature", as this item might
only be a container for many small functionalities of the
same topic.
```
```
Scrum Master A sports team usually has a trainer or coach, a
person who is not actively involved in the game
on the playing field but nevertheless is a key
element for the success of the team. There are of
course teams trying to do without a coach, but
their success normally is limited.
```
```
What is such a sports coach doing? In general, he
is responsible for making it possible, both, for the
individual team members and the team as such,
to perform in the best possible way by
harmonizing the interactions between individuals
and ensuring that they act as a true team, not as
a group of individual persons.
```
```
The scrum master can be seen a bit like such a
coach: his/her goal is the same: a well-performing
team.
```
```
The scrum master:
```
```
facilitates and coaches the Scrum Team
supports Product Owner and Organization in
implementing better Scrum
helps everyone understand Scrum theory,
practices, rules, and values
ensures goals, scope, and product domain
are understood by everyone on the Scrum
Team
helps to remove impediments
```
```
Contrarily to sports, he/she is not responsible for
implementing methods and rules, but he/she
supports the team by providing help in the
process of solving questions the team is
confronted with. He/She monitors the team from
outside, analyses interactions, tries to understand
and detect possibilities for improvements.
```
Subject Matter Experts

In Modular MOM 2.x we have also defined some other roles.

```
Role Responsibilities Notes
```
```
Subje
ct
Matter
Experts
```
```
The Subject Matter Experts have authority in a
particular area or topic. They can be i.e.
Architects, User Experience referents, Product
Security Experts, and so on.
```
```
Please see the description of the Modular MOM
2.x Subject Matter Experts below. All of them
(except Documentation Manager and SF
```
```
If Subject Matter Experts become Development Team members, they will be directly
involved in the day-to-day work of the team: this allows them to directly feel and
understand the difficulties of the team and to spread their knowledge and transfer it to
the team members. This helps to arrive at our target that the Development Team
becomes capable to do all necessary work for transforming an idea into a shippable
product.
```
```
Reasons why it is convenient to have real Feature Teams and not Single Experts:
```

### 1.

### 2.

### 3.

### 4.

### 5.

### 6.

### 7.

```
Experts) are not part of the Development
Teams, but they are there to support the teams
(Supporting Roles).
```
```
Flexibility & Value focus
A feature team has the knowledge and the skills to complete an entire end-to-end
customer-centric feature; so the teams are flexible enough to respond
immediately to the market and concentrate on customer value (and so we earn
money).
Stability
Even if one of the feature team members is absent (vacation, illness), the feature
can be completed thanks to the other team members.
Quality
Specialists are loosing focus on the system as a whole. There is the risk that
specialists are not aware anymore if their software changes impact the system in
a wrong manner; unconsciously specialists might create technical debt; they
might not understand anymore if i.e. an integration test is missing. So a feature
team stands for quality as all members together are able to focus on the system
as a whole.
Familiar team
Feature team members know each other very well. They work in a familiar and
safe atmosphere; they consolidate a work modality; the team members do not
need to decide and work alone.
Continuous improvement
Only a team that works together and knows each other and can find actions for
continuous improvement.
Stable relation with stakeholders
...
```
Archit
ect
Owner
(and
Archit
ects)

```
The Architect
```
```
plans and designs the structure of
technology systems and discusses it with
clients
analyzes the needs of large systems and
breaks them down into smaller manageable
parts
works alongside managers or clients to
agree on their IT requirements
meets the developers to discuss the
system software needs
troubleshoots system issues as they arise.
helps to resolve technical problems as and
when they arise
coordinates with the development team to
ensure the system runs smoothly
trains on system procedures
is involved in technical strategies along
with TPO
Suggests and discusses future IT or
architectural development changes to TPO
or clients.
drives Architect CoP
```
Qualit
y
Assur
ance
Owner

```
The Quality Assurance Owner
```
```
implements methods to inspect, test and
evaluate products and production
equipment
ensures that products adhere to quality
standards
prepare reports by collecting, analyzing and
summarizing data
works according to deadlines for the
delivery of products
trains production practices and quality
assessment of goods
tracks products through the manufacturing
process to guarantee that each part of the
process is correct
performs detailed and recorded inspections
of final products so the products are up to
industry and company standards
```
Relea
se
Mana
ger

```
The Release Manager
```
```
takes responsibility for the transition of
enterprise software products from
development to release to manufacturing.
guides and oversees the release process
ensures that critical milestones are reached
ensures that important activities are
completed, such as
adherence to open and inner source
policies and protocols
mitigation or closure of product
release risks
distribution and staging of product
release among others.
```
### SF

Expert

```
The Software Factory Expert
```
```
supports the Architect in his activities
is a link between the Architect and the
Development Team
brings the architectural knowledge into the
Development Team
```
Confi
gurati
on

```
The Configuration Manager
```
```
develops CI/CD principles
```

Mana
ger

```
reviews and modifies CI/CD principles,
iteratively
maintains CI/CD tools and platforms
develops and maintains pipeline
configurations
automates processes
develops automation tools and scripts
necessary for building, integrating, and
deploying software releases
```
Docu
menta
tion
Mana
ger

```
The Documentation Manager
```
```
develops and improves techniques helping
to cover PBIs (bugs, user stories, and
features) in the documentation timely
develops and improves techniques helping
to control readiness of the documentation
for specific PBIs
organizes and improves the review process
for the documentation
ensures readiness of the documentation for
release
assists with the Q-Gate process to provide
prepared documentation, if the Q-Gate
process is in use
manages publication of the documentation,
if the documentation is published to
Support Center
manages the availability of the
documentation internally
contributes during inspection visits of
anybody who wants to check the
documentation
collaborates with POs, development teams
and others to help in addressing
documentation gaps and deficiencies
Manages documentation translation and
localization process
drives continuous improvement of
documentation
```
### UX

Refere
nts

```
The User Experience Referent is
```
```
responsible to create an effective and
satisfying User Journey.
responsible to make sure the product
adheres to the UX Policy of DI SW and the
Style of MOM products.
```
```
Activity:
```
```
collaborates with PRM and POs to
understand WHO the users are, WHAT
they try to achieve and what they NEED to
do so
designs workflows and UIs to be simple,
clear, efficient, satisfying and consistent
designs them to be consistent with DI SW
UX and MOM UX in Style, layouts,
iconography, typography
evaluates different solution variants
aligns solution concepts with development,
architecture, POs, PRM and MOM UX
communicates the idea, the user journey,
the UI
collaborates with the different roles to find a
practicable solution
evaluates the implementation result and
submits improvement requests
evaluates the product with customers and
collect customer feedback
is a contact person for development for any
question related to the product's UX or UI
rules / specifics
spreads the knowledge of UX/UI rules to
the development team and related roles
```

Produ
ct
Securi
ty
Expert
PSE

```
tbd
```

# Scrum Events

## A sprint in Modular MOM lasts 3 weeks. The sprint starts on a Monday and ends on a Friday. For details please see TFS.

```
Event Driver Attendees Cadence PST
(Ramesh,
Nick)
```
```
EST
(Anthony,
Carlo)
```
```
CET
(Durga,
Matteo,
Mitra,
Mauna
Kea,
Monviso)
```
```
IST
(Heggi,
Pratapgad,
Shivneri)
```
```
Notes
& Link
to
meeting
```
```
Global
Planning
```
```
PO Mandatory:
```
```
Team repr.
SM *)
```
```
Optional:
```
```
PM
Architects
SF Experts
Supporting Roles
SF Owners
```
```
1h
```
```
Monday at sprint start
```
```
05:00-06:00
am
```
```
08:00-09:00
am
```
```
02:00-03:00
pm
```
```
05:30-06:30 pm Click here
to join the
meeting
```
```
Team
Planning
```
```
Team
Pratapgad
```
```
Mandatory:
```
```
SM *)
```
```
All other roles are
optional
```
```
max. 2h
```
```
Tuesday at sprint start, after Global Planning
```
```
Join
meeting Pra
tapgad
Team
Shivneri
```
```
Join
meeting Shi
vneri
Team
Sinhagad
```
```
Join
meeting Sin
hagad
Team
Mauna
Kea
```
```
max. 2h
```
```
Monday at sprint start, after Global Planning
```
```
04:00-06:00
am
```
```
07:00-09:00
am
```
```
03:00-05:00
pm
```
```
07:30-09:30 pm Click here
to join the
meeting
Team
Monviso
```
```
04:00-06:00
am
```
```
07:00-09:00
am
```
```
03:00-05:00
pm
```
```
07:30-09:30 pm Join
meeting Mo
nviso
Daily
Standup
```
```
Team
Pratapgad
```
```
Mandatory:
```
```
SM *)
All other roles are
optional
```
```
15/30 min
```
```
Every day
```
```
09:30-10:00
pm
```
```
00:30-01:00
am
```
```
06:30-07:00
am
```
```
11:00-11:30
am
```
```
Join
meeting Pra
tapgad
Team
Shivneri
```
```
Click here
to join the
meeting
Team
Sinhagad
```
```
Join
meeting Sin
hagad
Team
Mauna
Kea
```
```
00:30-01:00
am
```
```
03:30-04:00
am
```
```
09:30-10:00
am
```
```
02:00-02:30 pm Click here
to join the
meeting
Team
Monviso
```
```
00:30-01:00
am
```
```
03:30-04:00
am
```
```
09:30-10:00
am
```
```
02:00-02:30 pm Join
meeting Mo
nviso
Team
Retrospect
ive
```
```
SM
Pratapgad
```
```
Mandatory:
```
```
Team
```
```
Monday at sprint start, before Global Planning 09:30-10:00
pm
```
```
00:30-01:00
am
```
```
06:30-07:00
am
```
```
11:00-11:30
am
```
```
Join
meeting Pra
tapgad
SM
Shivneri
```
```
Join
meeting Shi
vneri
```

```
All other roles are
optional; please contact
the teams before joining
```
```
SM
Sinhagad
```
```
Join
meeting Sin
hagad
SM
Mauna
Kea
```
```
Tuesday at sprint start 00:30-02:00
am
```
```
03:30-05:00
am
```
```
09:30-10:00
am
```
```
02:30-04:00 pm Click here
to join the
meeting
SM
Monviso
```
```
Monday at the sprint start 02:30-03:30
am
```
```
05:30-06:30
am
```
```
11:30-12:30
am
```
```
04:00-05:00 pm Join
meeting Mo
nviso
Overall
Retrospect
ive
```
```
SM Mandatory:
```
```
PM
PO
Architects repr.
Supporting Roles
Team repr.
```
```
Optional:
```
```
SF Owners
Others
```
```
1.5 h
```
```
First Tuesday of sprint
```
```
04:30-06:00
am
```
```
07:30-09:00
am
```
```
01:30-03:00
pm
```
```
06:00-07:30 pm Click here
to join the
meeting
```
```
Sprint
Review
```
```
Team
repr.
```
```
Mandatory:
```
```
Stakeholders
PM
PO
Architects repr.
Supporting Roles
SM *)
```
```
Optional:
```
```
SF Owners
Others
```
```
2 h
```
```
Last Friday of sprint
```
```
05:30-07:30
am
```
```
08:30-10:30
am
```
```
02:30-04:30
pm
```
```
07:00-09:00 pm Click here
to join the
meeting
```
```
Backlog
Refinemen
t 0
```
```
PM
PO
Arc
hite
cts
SF
Exp
erts
```
```
no optional attendees 1.5 h
```
```
Mondays, Wednesdays, Fridays
```
```
(Monday-BR0 in first week starts half an hour
later due to Global Planning; Friday-BR0 in last
week is skipped due to Sprint Review)
```
```
06:00-07:30
am
```
```
09:00-10:30
am
```
```
03:00-04:30
pm
```
```
07:30-09:00 pm Click here
to join the
meeting
```
```
Global
Backlog
Refinemen
t 1
```
```
PO Mandatory:
```
```
PM
Architects repr.
Supporting Roles
Team repr.
SF Experts
SM *)
```
```
Optional:
```
```
other Team
members
SF Owners
```
```
2 h
```
```
Wednesdays
```
```
01:30-03:30
am
```
```
04:30-06:30
am
```
```
10:30-12:30
pm
```
```
03:00-05:00 pm Click here
to join the
meeting
```
```
Team
Backlog
Refinemen
t 2
```
```
Team
Pratapgad
```
```
Mandatory:
```
```
PO
SF Experts
SM *)
On demand:
```
```
Architects repr. **)
PM **)
Supporting Roles
required other
Team members
```
```
1 h
```
```
Mondays and Thursdays
```
```
01:30-02:30
am
```
```
04:30-05:30
am
```
```
10:30-11:30
am
```
```
03:00-04:00
pm
Team
Shivneri
```
```
01:00-02:00
am
```
```
04:00-05:00
am
```
```
10:00-11:00
am
```
```
02:30-03:30
pm
```
```
Click here
to join the
meeting
Team
Sinhagad
```
```
02:00-03:00
am
```
```
05:00-06.00
am
```
```
11:00-12:00
am
```
```
03:30-04:30
pm
```
```
Click here
to join the
meeting
Team
Monviso
```
```
1 h
```
```
Tuesdays and Fridays
```
```
01:00-02:00
am
```
```
04:00-05:00
am
```
```
10:00-11:00
am
```
```
01:30-02:30 pm Join the
Meeting
(1st week)
```
```
Join the
Meeting
(2nd week)
Team
Mauna
Kea
```
```
02:00-03:00
am
```
```
05:00-06:00
am
```
```
11:00-12:00
am
```
```
02:30-03:30 pm Click here
to join the
meeting
```
Legend:

```
bold: teams and date/time of their dedicated meetings
*) can become optional, if team is mature enough
**) strongly recommended to be present starting from Sept '21
```
```
Abbreviation Meaning
```

```
PM Product Manager
```
```
PO Product Owner
```
```
repr. representatives
```
```
SM Scrum Master
```
```
SF Software Factory
```
Further information:

```
Accountabilities (Roles)
Modular MOM2 Organization
```

# MM MES Retrospectives (from Sprint 24)

```
Open Action Items
Global Retrospective 24 May 2022 - Sprint 24
Closed Action Items
```
## Open Action Items

## Global Retrospective 24 May 2022 - Sprint 24

```
Status Topic Action Assigned To Notes
```
```
Mes team members spend a bit more
effort on user documentation
(if information is missing, involve actively
everyone who can help on getting the
information)
```
```
MES Team members
```
```
Mes Team members define in BR that
feature needs internal and external
documentation
(use dedicated tag, create tasks, decide
where to put the pages).
```
```
MES Team members
```
```
Mes SM/PO discuss with Amir how to
optimize BDD-UI development.
```
```
Bardini, Matteo , Hegd
e Heggi, Ganesh
```
```
Montaldo, Ernesto ,
Malhotra, Parul , Girnar
, Yuvraj , Hingse,
Sharad
```
```
Mes SM/PO involve Peter in discussion
how to optimize BDD-UI development.
```
```
Bardini, Matteo , Hegd
e Heggi, Ganesh
```
```
Montaldo, Ernesto ,
Malhotra, Parul , Girnar
, Yuvraj , Hingse,
Sharad
```
```
Mes SM/PO define a procedure to
replicate bug-data, so that bug fix can be
tested.
```
```
Bardini, Matteo , Hegd
e Heggi, Ganesh
```
```
Montaldo, Ernesto ,
Malhotra, Parul , Girnar
, Yuvraj , Hingse,
Sharad
```

```
UX team contact MES PO if specific data
is needed.
```
```
Thombare, Sagar
```
Closed Action Items


# MM PLT Retrospectives (from Sprint 25)

```
Open Action Items
Global Retrospective 05 Jul 2022 - Sprint 25
Closed Action Items
```
## Open Action Items

## Global Retrospective 05 Jul 2022 - Sprint 25

```
Status Topic Action Assigned To Notes
```
## Closed Action Items


# MM MES Sprint Review Agenda

Start Time: PST: 5:00 AM - EST: 8.00 AM - CET: 2.00 PM - IST: 5.30 PM

Duration: 1.15 hour

### DURATION TOPIC PRESENTER

```
15 min Feature 30688: Create Update and Display Non-Conformance Process for Operation - Block Operation Flow
```
```
Feature 44793: Enhance WOOP View in WO Details
```
```
Feature 46067: Group Definition - Capture Print Properties and visualize in Operator Terminal
```
```
Sinhagad
```
```
10 min Feature 27213: Comment Work Order and Work Order Operation
```
```
Feature 42029: AM- Enrich Production Order and Work Order with AM Common Properties
```
```
Rajgad
```
```
15 min Feature 42823: AM Specific Launch
```
```
Feature 42040: AM Customization - Operator Terminal and Grouped Operation Execution
```
```
Feature 42041: Additive Manufacturing - View GroupedOperation and Customization of View WOOP
```
```
Sahyadri
```
```
15 min User Story 42456: Collect by Milestone definition and management
```
```
User Story 47775: The completed operations can be shown
```
```
User Story 46935: [Core] - Work Instruction and Collect throughput Task
```
```
Monviso
```
```
10 min Feature 21862: NFR - Automatically restore reference database before test execution
User Story 34468: Create backup of reference database in modmom-18 to be used for NFR Tests
User Story 34469: Automatically restore reference database from backup before NFR tests execution
```
```
User Story 37571: Automate AM database configuration
```
```
Feature 28480: Extract Code Coverage from BDD API Tests
```
```
Feature 44184: Onboarding suppport for other startups
User Story 44207: Onboarding support APS
```
```
QA Infrastructure Team
```
Link to access MM MES Sprint Review: Click here to join the meeting


# MM PLT Sprint Review Agenda

Start Time: PST: 6:15 AM - EST: 9.15 AM - CET: 3.15 PM - IST: 6:45 PM

Duration: 1 hour

### DURATION TOPIC PRESENTER

```
15 min No demo
```
```
Pratapgad
```
```
15 min User Story 47301: Create UI for delete Configurable Objects
```
```
User Story 36296: API: Override an inherited field
```
```
Shivneri
```
```
15 min
```
```
Mauna Kea
```
```
5 min
```
```
CI/CD Community
```
```
5 min
```
```
Arch Community
```
Link to access MM Platform Sprint Review: Click here to join the meeting


# Scrum Events Preparation

The following sections summarize relevant actions that the Scrum Master(s) volunteering for each meeting should perform in order to properly prepare,
execute and close it.

## Sprint Planning 1

```
Some days in advance:
Check with POs if Bugs are prioritized
```
```
During the meeting:
Remind meeting goal (understand content of sprint - WHAT)
```
```
Be aware, if Bugs have been taken in consideration
```
## Sprint Planning 2

```
During the meeting:
```
```
Take care if there is the need of external trainings (if so, contact SF Leaders) - Global Retro action 11)
```
```
Be aware, if Documentation tasks have been written (draft documentation task to team; review documentation task to Bea)
```
## Sprint Review

```
Some days in advance: moving towards the approach that teams will organize themselves with creating slides and agenda
Check if previous Parking Lots are answered
```
```
Prepare Parking Lot in Teams (Channel Sprint Review)
```
```
Remind everyone to modify agenda in wiki
```
```
Contact Architect and Supporting Roles to understand if they present their sprint outcome
During the meeting:
Remind meeting goal (inspect&adopt)
```
```
Start recording
```
```
Persist feedback (i.e. use parking lot) (should be done by PO)
```
## Global Retrospective

```
Some days in advance:
Define Facilitator
```
```
Facilitator defines the Retro format and organizes accordingly (inform audience)
```
```
Remind all people to check action items and update the state.
```
```
During the meeting:
Introduction
Highlight that topics are related to overall project, no team-specific issues
```
```
Try to create a safe atmosphere: What is said in retro remains among retro participants (only action items are carried outside)
```
```
(for future: Perform an icebreaker)
```
```
Discussion of old action items
Limit the discussion of old action items to max 20 min
```
```
Consider a time-limit for each action: after 5 min discussion of each point we go ahead or we call a new meeting
Creating Action Items
Define only max 2 action points for each of the most voted topics (first three topics)
```
```
Define SMART action items (small, measurable, achievable, realistic, timely)
```
```
Closure:
Read aloud the agreed actions to make sure they are clear for everybody
```
```
Perform a session ROTI
After the meeting:
Copy defined actions into Confluence
```
```
Put actions into correct tubes (like DoD, DoR, Procedures, Checklists, ecc...)
```

```
Send summary email
```
```
Post an announcement in Teams to celebrate positives notes
```
```
Mark tasks "as completed" in Retrium
```
Backlog Refinement 1

```
Before the meeting:
Check if meeting is needed (or if team backlog is already full)
```
```
Check in backlog if the features are equally assigned to teams; if not, notice it to POs that there might be a bottleneck
```

# Overall Retrospectives (Sprint 1 to Sprint 23)

Global Retrospective 22 Mar 2022 - Sprint 20

```
Status Topic Action Assigned
To
```
```
Notes
```
```
Timeboxed Retro Before Retro: Remind all people to
check action items and update the
state.
```
```
Facilitator Update from Retro 23:
```
```
The action items are
inserted in the scrum
master checklist how to
facilitate retrospectives.
It will be the Facilitator's
duty to follow stuck to
this list.
```
```
Limit the discussion of old action
items to max 20 min.
```
```
Facilitator
```
```
During discussion of old action
items: Consider a time-limit for each
action: after 5 min discussion of
each point we go ahead or we call a
new meeting
```
```
Facilitator
```
```
Define only max 2 action points for
each of the most voted topics (first
three topics)
```
```
Facilitator
```
```
Define SMART action items (small,
measurable, achievable, realistic,
timely)
```
```
Retro-
Participants
```
```
Adopt a list of rules and conventions
for communicating more effectively.
```
```
All Team
Members
```
```
See here: Communicatio
n Rules
```
Global Retrospective 01 Mar 2022 - Sprint 19

```
Status Topic Action Assigned
To
```
```
Notes
```
```
Automate
Garbage
Collection activity
for clean up the
database
```
```
CoP QA
Balzer, Peter
```
```
Update from Retro 20:
There is a ongoing discussion in CoP QA, if each test cleans
database after execution.
Activity is assigned from "All Feature Teams" to CoP QA.
```
```
Check if we can
fix failed BDD
test cases on
priority. Should
be discuss further
on possible
solution in QA
CoP.
```
```
QA CoP Update from Retro 20:
This has not been discussed in CoP QA, because we need to
involve people who are responsible for backlog priority, like POs,
PMs. Together they have to decide with which priority the failing test
cases should be considered.
```
```
Can we assign this activity to POs and PMs who get in touch with
Peter Balzer?
Update from Retro 23:
We agreed that we want to follow our current process: team who is
in charge to check the Nightly runs, analyzes the issues and open
bugs in case of failing tests which cannot be fixed immediately. The
bugs are assigned to the team who owns the failing tests. The team
takes care about the bug as soon as possible in sprint, involving also
PO, in case the sprint goal cannot be fulfilled and some already
planned activities must move out of sprint backlog.
Check possible
solution with PO's
for more time to
stabilization if
there is any failed
integration
scenario.
```
```
All Feature
Teams
```
```
Update from Retro 20:
There was no opportunity to go through this action item. We can try
to address this topic together with the topic below in a separate
meeting.
```
```
Update from Overall Retrospective Follow-Up
```
```
On 01 Apr 2022 the Feature Teams has a follow-up about this topic
(see also email sent on 01 Apr 2022 at 3:08 PM by Montaldo,
Ernesto
It has been decided "What if we have something wrong in Integration
environment that is preventing the teams working (not a stable
situation)? Can we think of a stabilization time-window where a
whole team can dedicate to solve the issues?
```

```
It has been raised that we should move from a three months release
to a monthly release because of SaaS and a stabilization period
does not fit with this mindset. No actions required for now, we just
have to change our mentality, in particular we have to fix regressions
as soon as they show up."
```
In BR meetings
this kind of
conflicts should
be discussed.

```
Bardini, Matteo
Hegde ,
Ganesh
```
```
Update from Retro 20:
We stepped into a deeper discussion :
The problem does not occur, if feature/team is independent.
The problem of integration might come up, if the closure of
one feature depends on the closure of another activity
(maybe done by another team).
It is also difficult to know who to ask/involve.
POs told us that they work on the following:
Backlog items present an entire user requirement that will be
sliced by the Feature teams
Requirements should be sliced vertically by the teams; each
one presenting a small user functionality
So typically features are not divided anymore in backend
/frontend part.
POs asked why the teams postpone integration to last week of
sprint? Why they do not integrate earlier?
=> we defined the following detailed action items regarding this issue
(and original action will be closed)
```
```
Update from Overall Retrospective Follow-Up
On 01 Apr 2022 the Feature Teams has a follow-up about this topic
(see also email sent on 01 Apr 2022 at 3:08 PM by Montaldo,
Ernesto
It has been decided that "We should ask for the global picture during
the BR1. We have to get the general use case in order to highlight
potential relations between Features and User Stories.
```
```
In case some dependencies are present (and this can always
happen), the first action could be having a joined BR2 where the
teams, together with POs, Archs, and PMs can analyze the relations
in details."
```
In case Backend
and Frontend
Features are
created by PO/PM

```
teams
help POs
to
highlight
this and
address it.
teams
organize
themselves
that both
features
are done
by the
same
team.
```
```
All teams
```
Merge frequently
and integrate as
early as possible.

```
All teams
```
Highlight (i.e
during PBR), if
there is a feature
dependency from
business point of
view.

```
Bardini, Matteo
Hegde Heggi,
Ganesh
Capponi, Diego
```
```
Note:
PO cannot highlight the fact, that same code is manipulated by
different teams.
```
Organize a
separate meeting
to further discuss
this issue.

```
Nitsche,
Andrea
```
Maintain nightly
build separately

```
QA Infra
Team, Balzer,
Peter
```
```
Update from Retro 20:
Currently UI tests are running on modmom-18, API tests on
modmom-10;
this action can only be closed after teams have closed action item
below.
```
```
=> We agreed that the action item below is closed; as consequence
we close also this item.
```
Identify test
cases which
need a clean
database

```
All Feature
team
```
```
Update from Retro 20:
Q: How do we divide the work?
A: Author-wise, this means each team takes care about the tests
they have written.
Q: Does the teams have prepared a list of test cases which need
clean database?
A:
```
- no for: Mauna Kea, Sinhagad, Monviso, Pratapgad, Shivneri
- not applicable for Rajgad
To identify these test cases we use a tag "NeedsCleanDb" in feature
file.

```
Update from Retro 23:
```
```
We discussed this action in Mauna on the 28th of March:
We are no owners of BDD-UI tests and our BDD-API tests are
written in a way that database is cleaned automatically. For Mauna
this activity is closed.
```

```
Do we need to distribute Zorkas Test cases among the teams?
```
```
=> We agreed that this activity is done for Release 2204, as all tests
are fixed.
```
Global Retrospective 08 Feb 2022 - Sprint 18

```
Status Topic Action Assigned
To
```
```
Notes
```
```
All Feature Teams will update
the postman collection repo.
```
```
All Feature
Teams
```
```
The source of truth will be the folder repo
, not the wiki anymore.
```
```
According to Bardini, Matteo comments,
the folder is precisely a repo (link to the re
po). Please SM share this information
with the Feature Teams.
```
```
Definition of Done is updated
respectively. So in future the postman
collections will be updated, but ...
```
```
Update from Retro 20:
Teams need to check if postman
collection is also updated for already
existing APIs?
```
```
not applicable for: Rajgad
ok for: Sinhagad
checks to be done by:
Shivneri, Pratapgad, Mauna
Kea, Monviso
```
```
Update for Retro 23:
We discussed this action in Mauna on the
28th of March and identified the postman
collections to be updated. We created the
following User Story 37040: Update
Postman Collections which is not
scheduled yet. For Mauna this action item
can be closed.
```
```
Shivneri started on postman collection,
still work must be completed.
```
```
Update from Retro 23:
```
```
agreed all to close it as it is "old"
```
```
The Wiki page (Functional
Specification > xxx Module >
xxx Module APIs) will be
updated accordingly with a
link to the repo.
```
```
All Feature
Teams
```
```
The Definition of Done should
be updated.
```
```
Scrum
Masters
```
```
Peter will create a Wiki page
for Guideline on how to
develop tests steps to avoid
duplicated code.
```
```
Peter will be responsible for
maintaining the page.
```
```
Balzer, Peter A link to the recording about how to setup
step tests is available here.
```
```
Update from Retro 20:
It is an ongoing activity; Peter is
reorganizing how tests steps are shared
between modules.
```
```
Update from Retro 23:
Peter created a dedicated feature for this
activity:
Feature 46074: Wiki page for Guideline
on how to develop tests steps to avoid
duplicated code. Peter will be responsible
for maintaining the page
activity cannot be forgotten, item is closed
All Feature Teams, when it
will be required, will provide
support (as Evangelists) to
other Features Teams when a
lack of knowledge is
highlighted.
```
```
It will be one team member
who is going to work together,
for a specific amount of time,
with other Feature Teams'
members.
```
```
All Reminder for future
```
```
SM's can discuss with the team for
defining strategy for KT.
```
Global Retrospective 18 Jan 2022 - Sprint 16 , 17 Combined

```
Status Topic Action Assigned
To
```
```
Notes
```
```
Team members
will discuss
internally on how
```
```
All Team
members
```
```
Mauna:
```
```
We think that code review is a good technique which
facilitates knowledge sharing across the code base
and across the team. It helps developers to learn
```

```
to decide policy
/practice about
code review.
```
```
new techniques, to learn the code base and to grow
the skill sets so that they can write better code.
```
```
One way to do Code Review almost 100% of the
working time is the technique of Pair (or Mob)
Programming, as you have a constant dialog among
the team members how to best achieve the goal.
Without this continuous dialog it is difficult for any
reviewer to understand the written code.
```
```
So Code Review yes, but do not block PR without
Code Review, as we create bottlenecks, try to shift
the responsibility of PR to other people and might
tend to NOT check-in code frequently.
```
```
The Definition of Done has been updated by adding
a note on Code Review as best practice
```
```
Add Code review
as a 'best practice'
in DoD
```
```
Montaldo,
Ernesto
```
```
Scrum master will
check with Peter
for conducting a
knowledge sharing
session for BDD
UI.
```
```
SMs, Balzer,
Peter
```
```
Some basic knowledge of BDD-UI testing can be
found in "Team Onboarding Learning Path" in Teams.
```
```
For specific questions or topics, the Scrum Masters
are getting in touch with the teams and provide this
information to Peter (ongoing activity)
```
```
Update from Retro 18:
Feature Teams provided feedback to Peter before
and during Retrospective. After Retro, Scrum
Masters have checked with outstanding teams; no
additional topics to add.
=> Peter can proceed organizing a session
accordingly.
```
```
Update from Retro 20:
```
```
Sharma, Pallavi will perform a basic session on this
topic on Wednesday, 23rd March. Invitation will be
sent.
In CoP QA was decided that next CoP (on 24th
March) is dedicated to BDD-UI testing. All members
interested can participate.
=> Let's keep action item open until the sessions are
done.
Team members
will keep track of
maintaining old
test cases with
new feature
development
```
```
All Team
members, SM
```
```
Comment from Balzer, Peter: we must pay attention
to compatibility. So in theory, test cases must not be
modified once a release is out. Only due to the fact
that we don't have customers yet, we may still
change something which breaks compatibility.
```
```
SM's can talk with team and respond to Peter
comment on maintaining test cases compatibility.
```
```
Update from Retro 18:
Keep this action item open until test maintenance
becomes a common practice for all feature teams.
```
```
If Definition of Done is followed, the maintenance of
test cases is guaranteed and comes for free:
```
```
Regression tests are successfully executed
```
```
=> if we all agree on following the DoD, this action
item can be closed
```
```
Update from Retro 20:
Also Nightly Test runs help ourselves to understand if
previous test cases are failing. Nightly Test Runs are
checked daily by Teams; in addition the test
execution is checked also by a dedicated team. If
failing tests are found, fix of tests is performed as
soon as possible.
```
Global Retrospective 07 Dec 2021 - Sprint 15

```
Status Topic Action Assigned
To
```
```
Notes
```
```
Need more time (3 week of
sprint) for bug fixing/
integration test cases
execution to maintain the
quality - Team representatives
```
```
Team
representativ
es
```
```
Lodha, Vishal
```
```
Lodha, Vishal Will discuss
with the leadership team to
identify the time frame
allocated to bug fixing
/integration test cases.
```
```
Update from Retro 18:
The teams who brought up
this topic and voted for it, are
aware of the fact that with
shorter release cycles it is
```

```
difficult/(impossible) to have
an entire sprint for
stabilization.
SF Owners were not present
in Retro, so we are waiting for
Leadership feedback to
understand, if action item can
be closed.
```
```
Update from Retro 19:
Vishal - We can keep last
sprint of each release for bug
fixing activity along with other
activities also.
```
Team representatives may
participate in SWF CoP on
MOM level which happend
once per week - SM's help on
this point. Can get help on
meetings invite from Hitesh.

```
Team
representativ
es
```
```
SMs will manage multiple
SWF meeting invites, and
sync up with all feature
teams.
```
```
Please see here the overview
of all SWF and UX related
meetings:
Communities of Practice
(CoPs)
```
Need to take care about
Openshift environment for any
glitches that cause unstable
and impact sprint review
activity.

```
De Pascale,
Mauro
```
```
Sengupta, Ankita will check
on how to manage Openshift
environment on the review
day.
```
Any common module
(including CI/CD work) related
changes needs to be notified
to the all teams in advance
through Teams channel->
General /Developments.

```
All Team
members
```
Need to define strategy for
maitaining the all BDD test
cases - QA CoP can discuss
the same in CoP.

```
Balzer, Peter Moholkar, Sunetra will check
on strategies to maintaining
BDD test cases in QA CoP
```
```
We talked about this activity
in an Agile CoP together with
Peter:
```
```
Thanks to the Nightly Tests
Runs, we are immediately
informed about the tests
which need maintenance.
Until now we do not have any
customer, so we do not need
to maintain versions for TFS
items "Test Case" yet. But as
soon as customers are
```

```
involved, this topic will be
discussed in CoP QA and
shared among the teams.
```
```
See also Retro 16+17:
"Update old test cases"?
```
```
Proposal: close this action
item as not relevant now
```
Global Retrospective 16 Nov 2021 - Sprint 14

```
Status Topic Action Assigned
To
```
```
Notes
```
```
Platform Versioning Team
can
subscribe
to wiki
page to
be
notified
about
the
changes.
```
```
All interested
members of
the
Development
Teams
```
```
Documentation can be found here: Platform & Model Versioning
```
```
Whats New in Platform can be found here: Internal Release Notes
```
```
How to subscribe:
```
```
Share/collect architectural documentation Add link
to
Teams
folder in
wiki
```
```
Nitsche,
Andrea
```
```
Architecture Overview: a list of documents written and maintained by the architect group to understand the
architectural concepts.
```
```
This information is inserted in Quick Start to Developing with Opcenter Modular Manufacturing.
```

```
Involve UX before sprint review Create a
UX
review
task
(with link
for
scenario)
for UI
features
at
planning,
so that
UX
expert
can
check
what has
been
implemen
ted
```
```
all
Development
Teams
```
```
Weekly basis UX team is communicating with each team on UX review work
```
```
As soon
as UI
feature
gets into
presenta
ble state,
team
informs
UX
expert as
early as
possible
before
sprint
review
```
```
all
Development
Teams
```
Global Retrospective 26 Oct 2021 - Sprint 13

```
Status Topic Action Assigned
To
```
```
Notes
```
```
"Indian"-Excel
needs to be
updated regarding
UI
```
```
Sheth, Hemal
```
```
Malhotra,
Parul
```
```
It is an Excel sheet to understand which
KT session is needed for each team.
```
```
Share Link to basic
SWF exercises
```
```
Joshi,
Deepak
```
```
The following page has been created on
momwiki02:
Siemens Web Framework
Fundamentals - RevMOM - MES Wiki
```
```
In that page you can find useful links to
the SWF and MARS documentation
along with the link to the Video
Repository of Modular MOM Meetings
related to the UI Knowledge Transfer:
Modular MOM 2.x > General > Files >
Team Onboarding Learning Path > UI
Development
```
```
Everyone was informed by email about
this resolved action.
```
```
Clarify if KT session
between Mauna and
Hitash can be
shared; if so, share
in Teams
```
```
Immordino,
Giovanni
```
```
Update internal wiki
"How to start"
regarding KT
sessions for UI
```
```
Alvarez
Villanueva,
Beatriz
```
```
Share recorded
session about Test
UI/API in Teams
```
```
Malhotra,
Parul
```

```
QA team
implements existing
Build Strategy
```
```
Balzer, Peter,
QA team
```
```
Strategy is defined here.
```
```
QA team has items in backlog and
works on it.
```
```
Original action
"Make proposal for
release strategy"
assigned to Peter
/Mauro/Architects is
changed to
following action
```
```
Inform team about
Release Strategy
agreed by PM
department
```
```
SF Owners
```
```
Trifoglio,
Giuliano
```
```
Lodha, Vishal
```
```
During the retrospective we assigned
this action to Peter, Mauro and the
Architects. But they do not have the
strategical background to be able to
make a proposal. But they (and the
entire team) need to know that proposal
in order to adopt tests, pipeline and
architecture.
```
```
SF Owners and PM are responsible to
provide a release strategy and inform
the teams accordingly.
```
```
Update from Retro 18:
SF Owners are not present in Retro; we
are waiting for their feedback
```
```
Update from Retro 19:
Vishal: - has shared strategy for the
same action point.
```
Global Retrospective 12 Oct 2021 - Sprint 12

```
Status Topic Action Assigned
To
```
```
Notes
```
```
On bugs management can
help the recent activity for
Knowledge Sharing/Transfer
where teams' representatives
are in contact with the
Software Architects.
```
```
@Software
Architects
```
```
Kirve, Ajit , Montaldo, Ernesto
, Nitsche, Andrea , Sheth,
Hemal talk to our SF Experts
in team and ask them to talk
about this action in Architect
group (because American
were not present in Retro).
```
```
Montaldo, Ernesto : Prepare a
TFS query that facilitates in
this action
```
```
Team connects to sw
architects to address bugs;
bugs discussion happens in
arch community (which is 2x a
week) in exchange meeting.
```
```
There is also an ongoing
activity driven by the SMs on
bugs management that can
help.
```
```
@Scrum
Masters
```
```
Nitsche, Andrea , Montaldo,
Ernesto
```
```
Wiki page Bugs, Issues is
created. It is currently (16th
November 21) in review state
by PO, QA and Teams.
```
```
NFR should be harmonized
across all the business
Features. E.g., Import
Material and Import
Production Order with
Materials
```
```
Hegde ,
Ganesh Bard
ini, Matteo
```
```
As starting point, there should
be a feature for NFR env and
there are general guidelines
for all features.
```
```
Feature 21824:
Definition of
Performance Test
Environment
NFR Environment
```
```
If any inconsistencies are
noticed, please inform Heggi.
```
```
Heggi will highlight all Features
/Functionalies that need
benchmarks.
```
```
Heggi have documented all
needed NFR
```
Global Retrospective 15 Sep 2021 - Sprint 10 and 11

```
Status Topic Action Assigned
To
```
```
Notes
```
```
The Schedule of external @all Teams
```

training should be balanced
with respect to assigned
work.

```
Teams/SMs can take care about
this action in Planning Phase2
and in case of need of external
training, contact Lodha, Vishal
or Trifoglio, Giuliano
```
```
MOM-UI kit not ready yet Bardini,
Matteo
```
```
The MOM UI Kit is already
available but it has not been
integrated in our apps yet. This
item can be closed, but we have
to keep tracking this activity
```
```
Backlog Architecture
features need to be more
self-contained/finite. We
need refinement well
prepared
```
```
Nagamalli,
Ramesh Potti
gar,
Durgaprasad
Anand
```
```
Sheth, Hemal will take follow up
with Ramesh and Durga.
```
```
Need guidance on
modeling business objects
and new services,
maintenance services.
```
```
Nagamalli,
RameshAgha
zarian, Nick
```
```
We can get guidance in S/W
factory KT session.
```
```
At least 1-2 members from
each team should start
working and learning SWF
6.0 Fx. Training can be
arranged.
```
```
@all Teams Feature team will check if they
are comfortable to work on SWF
6.0
```
```
During development, we
need to use tags in bugs
like blockers.
```
```
@all Teams SMs prepare a process how to
handle bugs; proposal will be
shared and discussed with
```
```
Hegde , Ganesh Bardini, Matteo
```
```
and among entire ModularMOM
2.x team.
```
```
Consider with teams:
```
```
usage of severity instead
of tags?
usage of "Platform" tag for
platform specific bug
```
```
Wiki page Bugs, Issues is
created. It is currently (16th
November 21) in review state by
PO, QA and Teams.
```

```
We can also check any
blockers the same in the
Global sprint planning meet
```
```
Hegde ,
Ganesh Bard
ini, Matteo
```
```
Montaldo, Ernesto talked to POs
about the possibility to assign
bugs in sprint planning (and
therefore use the product
backlog view)
```
```
We need to analyze and
address any risk in
infrastructure or platform
bugs before starting any
respective development.
```
```
Pottigar,
Durgaprasad
Anand
```
```
Action is closed, because there
is the same action defined in
sprint 12 retro (architecture
exchange)
```
Global Retrospective 04 Aug 2021 - Sprint 9

```
Status Topic Action Assigned
To
```
```
Notes
```
```
Consider this
point as a good
practice while
preparing DoR
```
```
Sheth, Hemal Note is inserted in our current Definition of
Ready (User Story - Definition of Ready
(DoR) - RevMOM - MES Wiki (siemens.
com)) specifying it as an activity for BR2.
```
```
SMs are currently reviewing the DoR for
our Business phase where this point will
be considered as well.
```
```
Prepare a wiki on
story slicing
technique and
share with team.
```
```
Nitsche,
Andrea
```
```
Please find the information here:
Feature, User Story, Task, Epic
```
```
Information is shared with everyone via
email. Feedback is welcome!
```
```
Action can be closed, if everyone agrees.
```
```
Check with PO
and Peter on
prioritizing the
bugs and process
to manage issues
```
```
CoP SM Inserted in our internal Scrum Master's
Improvement Backlog and processed. We
will change TFS Bug Configuration from
"as Tasks" to "as Requirement", so that
Bugs are on level of US and can be
prioritized similar to US.
```
```
Share a wiki link
of "Scrum
Events" to all
team members.
```
```
Nitsche,
Andrea
```
```
Please find the information here:
Scrum Events
```
```
Information is shared with PM, PO,
Architects, SMEs, Supporting Roles and
SMs (who will discuss it within the
teams). Feedback is welcome!
```
```
SMs can
encourage QA
representative to
participate in QA
CoP.
```
```
all SM Inserted in our internal Scrum Master's
Improvement Backlog. Every SM is talking
to his/her team and then we exchange
feedback.
```
Global Retrospective 14 Jul 2021 - Sprint 8

```
Status Topic Action Assigned
To
```
```
Notes
```
```
Reject pull requests if
they fail integration tests.
```
```
All FTs
```
```
Remove stale / obsolete
or unmerged branches
(they can be kept locally).
```
```
All FTs De Pascale, Mauro :
Setup a meeting with limited
people to finalize the strategy on
this (consider updating the Best
Practices, if required).
```
```
We already have a point into the
best practices page:
```

```
Best practices for Git Branching
and Merging - RevMOM - MES
Wiki (siemens.com)
```
```
Update from Retro 10:
However actually we have tons of
stale branches and this is
something that we need to
address.
```
```
Update from Retro 17:
@SMs/Team Referents: Remind
teams in Scrum call about this
activity
```
```
Update Retro Retro 18:
For many teams it is already a
common practice.
After Retro: Check with Monviso
team; they follow this practice.
```
```
=> action item can be closed
```
```
Create POC US in
Backlog.
```
```
Balzer, Peter US is created under
Feature 20476: Infrastructure &
Training for BDD UI Tests
```
```
First draft of testing
scenarios should be
defined for a US during
BR.
```
```
All FTs Note is inserted in our current
Definition of Ready (Modular
MOM2's DoR) specifying it as an
activity for BR2.
```
```
SMs are currently reviewing the
DoR for our Business phase
where this point will be considered
as well (Sheth, Hemal).
```
```
Agenda is sent before the
BR meetings. PO/architect
/SM send the agenda.
```
```
PO, SMs,
Architect
```
```
We have already started following
this and kept it open just monitor
for few sprints.
```
Global Retrospective 23 Jun 2021 - Sprint 7

```
Status Topic Action Assigned
To
```
```
Notes
```
```
Create a separate channel (Development)
to notify team about any major changes in
code.
```
```
Nitsche,
Andrea
```
```
Done, channel is
maintained by
the team
members.
```
```
Setup a meeting to finalize the approach
on how to prevent breaking modules post
metadata runtime changes.
```
```
Sheth, Hemal Sheth, Hemal Kir
ve, Ajit
```
```
Define a strategy
to validate
platform.
```
```
Setup a meeting to understand on priority
of backlog.
```
```
Sheth, Hemal
, Nitsche,
Andrea
```
Global Retrospective 09 Jun 2021 - Sprint 6


```
Status Topic Action Assigned
To
```
```
Notes
```
```
Check if parts of pipeline need to
be reviewed
```
```
Setup an infrastructure to
run tests in a consistent
way in OS scenario
Identify tests that need a
full database
Check if issues are related
to parallel running tests
Put activity in backlog
```
```
Trubini,
Piergiorgio
```
```
+ CoP CI/CD
De Pascale,
Mauro
```
```
+ CoP QA Ba
lzer, Peter
```
```
Hegde ,
Ganesh
```
```
No update on this action
due to holidays (Pier,
Heggi)
```
```
Regarding tests with full
database: Sinha , Dheeraj
and team Pratapgad will
take this action to QA CoP
```
```
Check if additional parameter
could be passed to in-memory
connection string to maybe
isolate access to it
```
```
Accorsi,
Carlo
```
```
Accorsi, Carlo has made
some progress and he will
create a US for the same.
```
```
This US is done (ID?)
```
```
There are still some
scenarios where in-
memory db is needed, i.e
for bdd.
```
```
Investigate unit and integration
tests to improve the predictability
```
```
In case of need, if
something goes wrong,
raise hand and discuss
issue with other teams
Open backlog item to do
further investigations
```
```
@all Teams
```
```
Track Issues
```
```
Check issues also locally to
understand if they are
related to Pipeline
Track problems and
discuss them in CoP CI/CD
with TFS item ISSUE
```
```
@all Teams Work item Issue Definition:
TFS Items in Modular
MOM 2
```
Global Retrospective 12 May 2021 - Sprint 5

```
Status Topic Action Assigned
To
```
```
Notes
```
```
During BR, AC should be updated
for US if it requires to be tested on
OS environment.
```
```
All inserted in US "Definition
of Ready":
Modular MOM2's DoR
```
```
Keep it open to monitor it
in next BRs
```
```
Update DoD to run integration test
before merging, if applicable.
```
```
Sheth, Hemal
Nitsche,
Andrea
```
```
Prepare a guideline on merging
code with master.
```
```
Arduini,
Milvia
Pierpaola
```
```
Please see already
existing documentation:
Best practices for Git
Branching and Merging
```
```
All Teams started following it
within sprint 6/7
```

```
If possible, try to frequently merge
changes to master by making
sure that it does not break
anything.
```
```
If possible changes are
directly merged to master
(not feature branches)
```
```
Good practice:
```
```
Continue in this
direction; (merge to
master, even if there
is some commented
code)
put people as
reviewers to let them
understand what
was changed.
```
Global Retrospective 21 Apr 2021 - Sprint 4

```
Status Topic Action Assigned
To
```
```
Notes
```
```
Sprint Goal
```
```
At sprint planning, sprint goal is defined.
Together with PO/Arch teams decide what to
demo.
```
```
All
```
```
Work items State
```
```
Present all closed work (slide or demo)
Active stories are only presented, if feedback
is needed immediately (also valid Spikes)
Be transparent to stakeholders if we show
something active.
```
```
All
```
```
Perspective
```
```
demo from stakeholders perspective (end2end)
technical details, only if there is time (or we
organize internal "Lunch&Learn" sessions)
```
```
All
```
```
Define a test strategy and provide link to PLD of
ModularMOM1, in order to create awareness
```
```
Balzer, Peter ModMO
M1 Test
Strategy
```
```
Contribution of entire team is welcome, joining CoP
or usage of channel.
```
```
All
```
```
We try to create INVEST stories (Independent,
negotiable, valuable, estimate-able, small, testable)
```
```
All
```
```
Organize meeting to discuss particular use cases
where stories are not independent
```
```
Sheth, Hemal
```
Global Retrospective 31 Mar 2021 - Sprint 3


```
Status Topic Action Assigned
To
```
```
Notes
```
```
BRs should be more
focused on feature detailing
and not on capacity.
```
```
Hegde ,
Ganesh
```
```
Bardini,
Matteo
```
```
Split the features logically in
BR2 if required and create
linkage between features
```
```
Team Done, but we keep it open "as
reminder".
```
```
Proposal on how to split on-
hold features
```
```
SMs
```
```
Bardini,
Matteo
```
```
We didn't create any proposal,
but we created awareness.
Action is only open "as
reminder".
```
```
Overview of road map of
current development
activities
```
```
Bardini,
Matteo
```
```
Nagamalli,
Ramesh
```
```
Ongoing activity;
```
```
We have to wait for a complete
and formal roadmap, because
of the transition phase to
Startup organization.
```
```
If roadmap overview is
available, PO/Arch/PM will
share it with the teams.
```
```
Maybe there is an update in
week of 14th of June.
```
```
Hegde , Ganesh will share a list
what needs to be delivered for
MVP.
```
```
Overview of Roadmap was
given in Townhall on
05 Oct 2021
```
Global Retrospective 09 Mar 2021 - Sprint 2

```
Status Topic Action Assigned
To
```
```
Notes
```
```
Proposal how to setup BRs, considering how it is
done in Modular MOM 1.
```
```
Sheth, Hemal
```
```
Nitsche,
Andrea
```
```
Lodha, Vishal
```

Global Retrospective 16 Feb 2021 - Sprint 1

```
Status Topic Action Assigned
To
```
```
Notes
```
```
Prepare a documentation for
dependencies (model repos)
management
```
```
Sheth, Hemal
```
```
Accorsi,
Carlo
```
```
not finished yet;
something is
integrated here: Proce
dure for Pull Request
```
```
Sheth, Hemal : setup
meeting to complete
the procedure
```
```
Documentation is here:
Best practices for Git
Branching and
Merging
```
```
Prepare a documentation for
branching and merging the code.
Document Multi-repo dependencies
also
```
```
Accorsi,
Carlo
12 Mar 2021
```
```
see note above.
```
```
Some documentation
was completed in
sprint 2.
```
```
Documentation is here:
Best practices for Git
Branching and
Merging
```
Global Retrospective 18 Jan 2021

```
Status Topic Action Assigned
```

```
To
```
```
Prepare overall
dashboard in TFS
2019
```
```
Sheth, Hemal
, Nitsche,
Andrea
```
```
Prepare a proposal
for processes to
follow across the
teams
```
```
Sheth, Hemal
, Nitsche,
Andrea
```
```
Agile Scrum Practice Modular MOM2
```
```
Proposal is prepared, reviewed and
shared among all project members.
```
```
Business features
should be part of
backlog
```
```
PO
```
```
Hegde ,
Ganesh , Bar
dini, Matteo
```
```
Action is clear to PM/PO; it becomes an
ongoing activity; no need to keep this
action open anymore.
```
```
Backlog should well
refined and
prioritized
```
```
PO Nagamall
i, Ramesh
```
```
Action is clear to PM/PO/Archs; as we
have also BRs in place now, it is a
normal activity during the sprint; action
is closed.
```
Global Retrospective 09 Dec 2020

```
Status Topic Action Assigned
To
```
```
Notes
```
```
Setting up the meeting on KT session
on Metadata engine
```
```
Sheth, Hemal
```
```
12 Mar 2021
```
```
Hemal is in
discussion with
Ramesh
```

Check with Neil about meeting to
avoid overlapping

```
Dellacha',
Alessio
```
Conduct BRs and prioritize backlog PO


# Internal Collaborative Content

```
This section is used to document specific collaborative content that will help our day by day work.
```

# Nightly Test Run Follow-Up Plan 2.x

A notification email about the outcome of the nightly test run is sent by the system to all members of all teams.

However there is one team and one member (typically Test Expert) who is on duty to follow up on the issues which are reported, defined by a sprint-
wise round-robin schedule.

For this activity, 2h/day are planned in the sprint backlog.

The team member on duty analyzes the results of the nightly test run in the morning, and follows up on any failed test cases.

If (s)he needs help with the analysis, (s)he asks her/his feature teammates before reaching out to other teams.

For issues which cannot be fixed immediately, (s)he creates a bug with the "IntegrationBroken" tag.

The team member on duty assigns Bugs with the "IntegrationBroken" tag immediately to a team and sprint who can fix the bug (put Scrum Master as
assignee).

Who is on duty to follow up on the nightly test run is defined by the following schedule:

## Business Modules & SaaS Readiness

```
Period Team Member
```
```
sprint 19 Pratapgad
```
```
sprint 20 Monviso
```
```
sprint 21 Sinhagad
```
```
sprint 22 Mauna Kea
```
```
sprint 23 Pratapgad
```
```
sprint 24 Rajgad
```
```
sprint 25 Monviso
```
```
sprint 26 Sinhagad
```
```
sprint 27 Sahyadri
```
```
sprint 28 Rajgad
```
## UI Model Designer

```
Period Team Member
```
```
sprint 19 Shivneri
```
```
sprint 20 Shivneri
```
```
sprint 21 Shivneri
```
```
sprint 22 Shivneri
```
```
sprint 23 Shivneri
```
## Operative Instructions

You need to look every morning at three different kinds of tests:

```
BDD API
BDD UI
NFR
```
## Checking Outcomes of BDD API Tests

```
Navigate to the TFS Release Pipeline which runs the tests
In the latest Release, select the "API modmom-10" stage
```

```
Select the "Test" tab
You will see the list of tests which have failed in this test run
If you selected one of the failed test runs, a details panel will open. In "attachments" you will see a log of the Gherkin execution
In the "Work Items" tab you can see if there are already bugs associated to this test case
If there is no bug yet, or there are only old ones in closed state, you must open a new bug, using the "Bug/Create Bug" button above the list of the
failed test cases
If there is already an open bug, nothing needs to be done
If there is a recent bug in Resolved or Closed state, which is about the same issue which is currently present, it should be reopened
When you open a new bug, Copy&Paste the Gherkin log in the bug description
In case of flaky test, add a note on the description so that the team that has to fix the bug is aware that it is not enough to spot a single successful
execution to close the bug.
Fill in the Found In field as desribed in How to open a bug
If you know or have a suspect which team is responsible for the failure, assign the bug to that team, setting the Area Path. Avoid to leave the
Area Path field empty (it will lead to a delay in bugfixing).
If you talked to a person of that team, and that person agreed to take the bug, you can assign the bug directly to that person. Otherwise assign it
to the Scrum Master. Avoid to leave the Assigned To field empty (it will lead to a delay in bugfixing).
Set the Iteration Path to the current sprint. In this way, the bug will show up in the team's backlog. If they don't agree to take it, they can suggest
another team. Avoid to leave the Iteration Path field empty (it will lead to a delay in bugfixing).
Apply the "IntegrationBroken" tag to the bug, plus a tag for the module (MM, OM, TNT, etc.)
```
Checking Outcomes of BDD UI Tests

```
Navigate to the TFS Release Pipeline which runs the tests
In the latest Release, select the "UI modmom-18" stage
Select the "Test" tab
You will see the list of tests which have failed in this test run
Here the Gherkin log cannot be found attached to the single test case, but to the parent node.
You need to select the parent node, then a details panel will open to the right. Download the attached html file and open it.
For the bugs opening process, proceed in the same way as for BDD API Tests.
```
Checking Outcomes of NFR Tests

Outcomes of NFR Tests can be observed in the Sumo dashboards.

NFR Tests are run both on OpenShift and on XCR mom-stage. There are separate dashboards for these two environments.

Each widget must be checked if the time is below the threshold, and the error rate is zero. Otherwise a bug needs to be opened.

Even if the widget hasn't been filled, a bug must be opened.

The bugs must be associated to the test case with relation "Test".


# Building Redis on Windows for Development

Binary distributions of Redis for windows are outdated or are hosted on unreliable or untrustworthy sources.

To run Redis on windows, there are two options, run in a docker container or build and run locally. This article addresses the latter choice.

```
Install Cygwin
Install Node.js and Node Package Manager (NPM)
Download Redis Source Code
Unpack Redis source tarfile
Perform modifications to Redis sources for Cygwin
Build Redis and its dependencies
Start the Redis Server
Install Redis-Commander
Start the redis-commander front end application
Communications and Security
```
## Install Cygwin

Cygwin is a POSIX emulation environment for Windows and comes standard with development laptop builds.

However if it is not installed visit https://www.cygwin.com/ and follow the steps to install.

Once installed, open the cygwin bash terminal and check to see if the apt-cyg packages is installed by issuing the following command:

```
$apt-cyg --version
```
If you get a version back, then the package is present. If not, refer to the cygwin documentation to install this package.

Once installed, you need to verify you have: gcc-core, tcl and make

```
$apt-cyg install gcc-core
```
If the package is already installed, then the message below will result.


```
$apt-cyg install tcl
```
```
$apt-cyg install make
```
The Cygwin environment is now installed.

Install Node.js and Node Package Manager (NPM)

Node.js is a backend javascript runtime environment. The front end package used for redis depends on node.js.

```
At the time of this writing, the current / stable version is
Node.js v14.17.0
NPM 6.14.13
```
Download the install package from https://nodejs.org/en/download/


Run the installer and follow the prompts. A step by step instruction can be found here. Installing Node.js on Cygwin

Verify the node package manager was installed.

```
$npm -v
```
Node.js and Node Package Manager are installed.

Download Redis Source Code

You can download source using any tool however cygwin includes the command line utility wget which downloads files over http/https.

Redis releases are found here : https://download.redis.io/releases/

At the time of this writing, the current version is 6.2.4.

```
$wget https://download.redis.io/releases/redis-6.2.4.tar.gz
```
Verify the file was successfully downloaded. It should have the same date and size as the remote file.


Unpack Redis source tarfile

The source is delivered in tar, gzipped format and must be extracted before use. Issue the command below in the directory where the source file exists.

```
$ tar xzfv redis-6.2.4.tar.gz
```
When a tar, gzipped file is extracted, a directory is created from the file name except the .tar.gz are removed from the end of the file.

In this case, the files are extracted to the directory redis-6.2.4

And will eventually complete.


### 1.

### 2.

The Redis source files are downloaded and extracted.

Perform modifications to Redis sources for Cygwin

Two files require minor modifications to build Redis on cygwin.

```
redis-6.2.4/deps/hiredis/net.c
redis-6.2.4/src/debug.c
```
Follow the steps to make the corrections.

Open the file deps/hiredis/net.c

Add following line before include directives on or around line 34:

```
#define _POSIX_C_SOURCE 200112L
```
Add the following lines just after the include directives, on or around line 52:

```
#ifdef __CYGWIN__
#define TCP_KEEPCNT 8
#define TCP_KEEPINTVL 150
#define TCP_KEEPIDLE 14400
#endif
```
The corrected file will have the modifications shown on the right of the compare window.


Open the file src/debug.c

A struct Dl_info needs to be added along with two other small corrections to the functions dumpX86Calls and dumpCodeAroundEIP.

```
typedef struct Dl_info{
unsigned char *dli_sname;
unsigned char *dli_saddr;
unsigned char *dli_fname;
unsigned char *dli_fbase;
}info;
```
Make the appropriate changes in on the right side of the compare window and update function dumpX86Calls.

Update the function dumpCodeAroundEIP.

Code modifications are now complete.

Build Redis and its dependencies


First build the following dependencies from the deps folder. Some warnings will occur and they can be ignored.

```
lua
hiredis
linenoise
```
```
/usr/src/redis-6.2.4/deps
$make lua hiredis linenoise
```
The make deps process begins

and will complete when the bash prompt returns without error.

Next build the redis server from the main directory. The build starts like this.


and after several minutes depending on your hardware, it ends like this.

Again, there are warnings that can be ignored.

Now make tests and some will fail but for development purposes, the server runs fine.

And finally make install which copies the binaries to the proper locations.


Start the Redis Server

Once the server is installed, simply issue the command. The ampersand causes the process to run in the background.

The server will start listening on all local interfaces at port 6379.

```
$redis-server &
```
Use the redis-cli to verify the server is up.

```
$redis-cli ping
```

Install Redis-Commander

Redis-Commander is a Node.js application that runs as a single page application to view and manage a redis server.

Use the npm command to install redis-commander

```
$npm install -g redis-commander
```
Once the install completes, the commander is ready to use.

Start the redis-commander front end application

The redis-commander application is a node.js app and can be run directly from the command line as shown below.

```
$redis-commander
```
It will start the redis-commander webapp on port 8081. If this port is in use, check documentation to run on a different port.


Once it's running, open a browser to [http://localhost:8081](http://localhost:8081)

And the commander main page opens. In the bottom frame of the page an interactive console may be used.

Alternatively, the user interface can be used to add a test key/value to the server.


The installation is complete!

Communications and Security

Redis has its own client / server communication protocol called RESP which is a text based serialization routine. Being plain text makes it human readable
over a wire and therefore is not secure.

As of Redis 6, TLS is supported which means you can secure the client / server communication by configuring X.509 certificates just as you would for a
webserver.

Alternatively certificates may be installed using Redis' administration user interface.

To leverage the secure communication from the client side, create a connection specifying a keystore and the connection will occur over TLS.

Secure connection example:

```
using StackExchange.Redis;
using System.Security.Cryptography.X509Certificates;
using System.Net.Security;
```
```
var options = new ConfigurationOptions
{
EndPoints = { "10.1.1.1:6379" },
Password = "secret",
Ssl = true
};
options.CertificateSelection += delegate { return new X509Certificate2("/var/cert/keystorepfx", ""); };
```
```
readonly ConnectionMultiplexer muxer = ConnectionMultiplexer.Connect(options);
IDatabase conn = muxer.GetDatabase();
```
Managing certificates and keystores can be error prone, perhaps Kubernetes has facilities for automating / managing certificates and keystores.

References:

https://docs.redislabs.com/latest/rs/security/tls-ssl/

https://www.infoworld.com/article/3541356/redis-6-arrives-with-multithreading-for-faster-io.html

https://redis.io/topics/encryption


# Calculation of Code Coverage in Pipeline

Implementation was done following this article: https://writeabout.net/2019/04/27/net-core-code-coverage-done-right/, to achieve these goals:

```
Runs on Linux and Windows
Displays a nice report in Azure Pipelines
Supports Code Coverage in SonarQube
Tests build and run only one time
```
## Code Coverage for Platform

This is achieved by producing the code coverage output files in opencover format.

Moreover, we need to merge coverage from several test runs in the pipeline, and this can only be done in coverlet's native json format.

Therefore, we need to produce the coverage files in all but the last test tasks in json format, and in the last test task in opencover format.

```
All but last task for execution of Unit Test with Code Coverage
```
- task: DotNetCoreCLI@2
displayName: 'Execute Observability Aspects Tests'
continueOnError: false
inputs:
command: 'test'
projects: '$(aspectsSln)'
arguments: '--no-restore /p:CollectCoverage=true /p:CoverletOutput=$(Build.SourcesDirectory)/Platform
/coverage/coverage.json /p:MergeWith=$(Build.SourcesDirectory)/Platform/coverage/coverage.json -m:1'
testRunTitle: 'Observability Aspects tests'

Note the "MergeWith" parameter, this is needed to merge the code coverage output file from one test task with the coverage from the next one.

```
Last task
```
- task: DotNetCoreCLI@2
displayName: 'Execute Governance Tests'
enabled: 'true'
continueOnError: false
inputs:
command: 'test'
projects: '$(Build.SourcesDirectory)/Platform/Siemens.MOM.Platform.GovernanceTests/Siemens.MOM.Platform.
Governance.Tests.csproj'
arguments: '/p:CollectCoverage=true /p:CoverletOutputFormat="opencover" /p:CoverletOutput=$(Build.
SourcesDirectory)/Platform/coverage/coverage.opencover.xml /p:MergeWith=$(Build.SourcesDirectory)/Platform
/coverage/coverage.json -m:1'
testRunTitle: 'Governance tests'

Note the CoverletOutputFormat="opencover" parameter.

In order for these commands to work, the "coverlet.msbuild" package needs to be installed in the test projects.

Report generator is used to create nice reports and upload them to Azure Pipelines. It picks up the coverage.opencover.xml file produced by the last test
task.

```
Generating report for TFS
```
- script: |
dotnet tool install dotnet-reportgenerator-globaltool --tool-path.
./reportgenerator "-reports:$(Build.SourcesDirectory)/Platform/coverage/coverage.opencover.xml" "-
targetdir:$(Build.SourcesDirectory)/Platform/coverage/Cobertura" "-reporttypes:Cobertura;HTMLInline;HTMLChart"
condition: and(succeeded(), eq(variables['Agent.OS'], 'Linux'))
displayName: Run Reportgenerator on Linux


Another task does the upload of the output from reportgenerator to TFS:

```
Publish coverage to TFS
```
- task: PublishCodeCoverageResults@1
inputs:
summaryFileLocation: $(Build.SourcesDirectory)/Platform/coverage/Cobertura/Cobertura.xml
reportDirectory: $(Build.SourcesDirectory)/Platform/coverage/Cobertura
codecoverageTool: cobertura

And one task to upload the coverage file in opencover format to SonarQube:

```
Publish coverage to SonarQube
```
- task: SonarQubeAnalyze@4
displayName: 'Perform SonarQube analysis'
condition: and(succeeded(), eq(variables['RunSonarAnalysis'], 'true'))
- task: SonarQubePublish@4
continueOnError: true
displayName: 'Publish SonarQube results'
condition: and(succeeded(), eq(variables['RunSonarAnalysis'], 'true'))
inputs:
pollingTimeoutSec: '300'

Code Coverage for Business Modules

This is achieved by producing the code coverage output files through the .net native command.

Moreover, we need to provide a runsettings file where we specify parameters for the code coverage calculation.

```
Task for execution of Unit Test with Code Coverage
```
- task: DotNetCoreCLI@2
displayName: 'Run the tests'
condition: and(succeeded(),ne('${{parameters.isSampleModule}}', 'true'),or(or(eq(variables['Build.
Reason'],'PullRequest'),eq(variables['Build.SourceBranchName'], 'main'),eq(variables['Build.SourceBranchName'],
'master')),eq('${{parameters.enableTests}}','true')))
continueOnError: false
inputs:
command: 'test'
projects: ${{parameters.testsSln}}
arguments: '--no-restore -c ${{parameters.buildConfiguration}} --settings $(Build.SourcesDirectory)/Tests
/CodeCoverage.runsettings'
testRunTitle: 'Module Tests'

Then is needed to merge the code coverage output file with the dotnet-coverage tool

```
Merge coverage on Linux
```
- script: |
dotnet tool install --global dotnet-coverage
dotnet-coverage merge -f xml -r ../../_temp/*.coverage
condition: and(succeeded(), eq(variables['Agent.OS'], 'Linux'), ne('${{parameters.isSampleModule}}', 'true'),
or(or(eq(variables['Build.Reason'],'PullRequest'),eq(variables['Build.SourceBranchName'], 'main'),eq(variables


```
['Build.SourceBranchName'], 'master')),eq('${{parameters.enableTests}}','true')))
displayName: Run dotnet-coverage on Linux
```
```
Merge coverage on Windows
```
- script: |
dotnet tool install --global dotnet-coverage
dotnet-coverage merge -f xml -r ../../_temp/*.coverage
condition: and(succeeded(), eq(variables['Agent.OS'], 'Windows_NT'), ne('${{parameters.isSampleModule}}',
'true'),or(or(eq(variables['Build.Reason'],'PullRequest'),eq(variables['Build.SourceBranchName'], 'main'),eq
(variables['Build.SourceBranchName'], 'master')),eq('${{parameters.enableTests}}','true')))
displayName: Run dotnet-coverage on Windows

Report generator is used to create nice reports and upload them to Azure Pipelines. It picks up the coverage.opencover.xml file produced by the last test
task.

```
Generating report for TFS on Linux
```
- script: |
dotnet tool install dotnet-reportgenerator-globaltool --tool-path.
./reportgenerator "-reports:$(Build.SourcesDirectory)/output.coverage.xml" "-targetdir:$(Build.
SourcesDirectory)/coverage/Cobertura" "-reporttypes:Cobertura;HTMLInline;HTMLChart"
condition: and(succeeded(), eq(variables['Agent.OS'], 'Linux'), ne('${{parameters.isSampleModule}}', 'true'),
or(or(eq(variables['Build.Reason'],'PullRequest'),eq(variables['Build.SourceBranchName'], 'main'),eq(variables
['Build.SourceBranchName'], 'master')),eq('${{parameters.enableTests}}','true')))
displayName: Run Reportgenerator on Linux

```
Generating report for TFS on Windows
```
- script: |
dotnet tool install dotnet-reportgenerator-globaltool --tool-path.
.\reportgenerator.exe "-reports:$(Build.SourcesDirectory)/output.coverage.xml" "-targetdir:$(Build.
SourcesDirectory)/coverage/Cobertura" "-reporttypes:Cobertura;HTMLInline;HTMLChart"
condition: and(succeeded(), eq(variables['Agent.OS'], 'Windows_NT'), ne('${{parameters.isSampleModule}}',
'true'),or(or(eq(variables['Build.Reason'],'PullRequest'),eq(variables['Build.SourceBranchName'], 'main'),eq
(variables['Build.SourceBranchName'], 'master')),eq('${{parameters.enableTests}}','true')))
displayName: Run Reportgenerator on Windows

Another task does the upload of the output from reportgenerator to TFS:

```
Publish coverage to TFS
```
- task: PublishCodeCoverageResults@1
inputs:
summaryFileLocation: $(Build.SourcesDirectory)/coverage/Cobertura/Cobertura.xml
reportDirectory: $(Build.SourcesDirectory)/coverage/Cobertura
codecoverageTool: cobertura
condition: and(succeeded(),ne('${{parameters.isSampleModule}}', 'true'),or(or(eq(variables['Build.
Reason'],'PullRequest'),eq(variables['Build.SourceBranchName'], 'main'),eq(variables['Build.SourceBranchName'],
'master')),eq('${{parameters.enableTests}}','true')))

And two tasks to upload the coverage file to SonarQube:

```
Publish coverage to SonarQube
```

- task: SonarQubeAnalyze@4
displayName: 'Run SonarQube Analysis'
condition: eq('${{parameters.isSonarEnabled}}', true)
- task: SonarQubePublish@4
displayName: 'Publish SonarQube results'
condition: eq('${{parameters.isSonarEnabled}}', true)
inputs:
pollingTimeoutSec: '300'


### 1.

### 2.

### 3.

### 4.

# Code Coverage using Unit Test

```
What is Code Coverage
Which Tests are considered for Coverage
How to check Code Coverage using Unit Test
Code Coverage in Visual Studio
Setup
SonarLint Report for Code Coverage
Useful links for Code Coverage
```
## What is Code Coverage

Code Coverage Percentage = (Number of lines of code executed by a testing algorithm / Total number of lines of code in a system component) * 100.

## Which Tests are considered for Coverage

Unit Tests, Backend Component Tests (aka "Integration Tests") and BDD API Tests when run with Asp.net Core Test Server in the pipeline.

## How to check Code Coverage using Unit Test

There are several places where code coverage is visualized:

```
In the Azure DevOps dashboards, for a quick overview, "official" where evidences for QA are taken from
On SonarQube's pages (very detailed, interactive and powerful)
In the Azure DevOps pipeline (just for convenience, not official)
In Visual Studio - for local work
```
## Code Coverage in Visual Studio

```
Open any solution in Visual Studio and select the Test option from the menu bar,
Select “Analyze Code Coverage For All Tests” option and let all Unit test gets run.
After executing all unit tests, open the Code Coverage Results tab and see to code coverage.
You can see each C# file’s code coverage report (Not Covered and Covered) with the Unit tests.
```
## Setup

Use Moq which is a mocking framework for C#/.NET for writing Unit Tests.


SonarLint Report for Code Coverage

Refer to the screenshot above.

Useful links for Code Coverage

```
https://azuredevopslabs.com/labs/devopsserver/liveunittesting/
http://codereform.com/blog/post/unit-testing-and-code-coverage-for-asp-net-web-api-12/
https://community.sonarsource.com/t/coverage-troubleshooting-guide-for-net-code-coverage-import/37151
```

# Debugging BDD API Framework and Common Steps

The BDD API Framework's NuGet packages (Siemens.ModularMOM.API.Automation, Siemens.Mom.MomAutomationFramework, Siemens.Mom.OAuth2)
are built as as NuGet Package with Symbols and Sources. So it is possible to debug the shared step definitions and the framework code.

Just be sure to disable the Visual Studio option "Debugging/General/Enable Just My Code".

Unfortunately F11 will not step into package code, however, it is possible to get to the package source code and debug by double-clicking on a line in the
VS call stack which is relative to package code.

When VS asks for the location of the .pdb file or the source file, you need to provide the location of the NuGet package on your VDI, e.g. C:
\Users\<username>\.nuget\packages\siemens.mom.oauth2.


# CI/CD Processes and Practices


# Process And Phases


# Modular Manufacturing 2112

List of versions, builds and deployments updated to latest features on the deployment side.

Tracing and logging reporting is enabled, metrics collection is enabled.

## List of components with corresponding version

```
landingpageui: 1.0.3
ordermanagment: 1.0.2
ordermanagmentui: 1.0.2
trackandtrace: 1.0.1
trackandtraceui: 1.0.3
materialmanagement: 1.0. 1
materialmanagementui: 1.0.0
factory: 1.0.0
factoryui: 1.0.1
```
## List of corresponding builds and releases

```
landingpageui: 1.0.3
Build: https://tfs05mom.industrysoftware.automation.siemens.com/MOM/ModularMOM/_apps/hub/ms.vss-releaseManagement-web.cd-
release-progress?_a=release-pipeline-progress&releaseId=3475
Release: Deploy LandingPage UI - Release-42 - Pipelines (siemens.com)
ordermanagment: 1.0.2
Build: https://tfs05mom.industrysoftware.automation.siemens.com/MOM/ModularMOM/_build/results?buildId=21910
Release: Deploy OrderManagement - Release-215 - Pipelines (siemens.com)
ordermanagmentui: 1.0.2
Build: https://tfs05mom.industrysoftware.automation.siemens.com/MOM/ec462e3d-fd2d-412e-8038-a5118d00ae2d/_build/index?
buildId=21974
Release: Deploy OrderManagement UI - Release-191 - Pipelines (siemens.com)
trackandtrace: 1.0.1
Build: https://tfs05mom.industrysoftware.automation.siemens.com/MOM/ModularMOM/_build/results?buildId=21652
Release: Deploy TrackAndTrace - Release-182 - Pipelines (siemens.com)
trackandtraceui: 1.0.3
Build: https://tfs05mom.industrysoftware.automation.siemens.com/MOM/ec462e3d-fd2d-412e-8038-a5118d00ae2d/_build/index?
buildId=21962
Release: Deploy TrackAndTraceUI - Release-283 - Pipelines (siemens.com)
materialmanagement: 1.0. 1
Build: https://tfs05mom.industrysoftware.automation.siemens.com/MOM/ModularMOM/_build/results?buildId=21897
Release: Deploy MaterialManagement - Release-176 - Pipelines (siemens.com)
materialmanagementui: 1.0.0
Build: https://tfs05mom.industrysoftware.automation.siemens.com/MOM/ModularMOM/_build/results?buildId=21193&view=results
Release: Deploy MaterialManagement UI - Release-95 - Pipelines (siemens.com)
factory: 1.0.0
Build: https://tfs05mom.industrysoftware.automation.siemens.com/MOM/ModularMOM/_build/results?buildId=21194
Release: Deploy Factory - Release-60 - Pipelines (siemens.com)
factoryui: 1.0.1
Build: https://tfs05mom.industrysoftware.automation.siemens.com/MOM/ModularMOM/_build/results?buildId=21663
Release: Deploy Factory UI - Release-76 - Pipelines (siemens.com)
```

# ModularMOM 2.0 Private Registry

Development workflow is structured on two loops:

```
inner loop: where the developer works locally
outer loop: where the developer integrates is own work into the shared respository triggering the CI/CD flow.
```
When working with microservices. the outer loop needs a (private) registry where to put the docker images generated by the build process to be used for
deployment as depicted into the following image:

For this reason, we decided to setup a private registry for the ModularMOM 2.0 project and to integrate it into the pipeline.

We choosed to use the plain Docker Distribution 2.0 registry implementation because it is the official one, it is simple but complete and it could be
customized with ease.

The registry is a docker image, so we deployed it into the Openshift cluster into the momosprj10 environment: this is a temporary location and could be
changed based on security, opportunity and monitoring needs.

We choosed not to use the Opensift internal registry to avoid a lockin on the platform: the current registry could be moved elsewhere at any time.

Additionally, we can avoid mixing project's images with system/external ones permitting us some more freedom on images handling and registry's usage
monitoring.

## Registry coordinates

To access the registry, the following coordinates shall be used:

```
Registry URL: modmom-registry.apps.openshift03.swqa.tst
Login and password: modmom20:registry2021
```
All the developers' machines shall be able to connect to it (it's into the SWQA domain), same for all the existing Openshift environment.

## Registry UI

The registry could be browsed with a UI interface, the interface is very simple but functional and can be accessed from here:


```
Registry UI URL: http://web-modmom-registry.apps.openshift03.swqa.tst
```
Access from Openshift instances

Openshift needs to know which credentials to use to access the registry for deploying its images.

The provided and existing environments are usually already configured, but in case of new environments or a new OS instance, the following commands
must be performed:

```
oc create secret docker-registry registry-credentials --docker-server=modmom-registry.apps.openshift03.swqa.tst --docker-
username=modmom20 --docker-password=registry2021
oc secret link default registry-credentials --for=pull
```

# ModularMOM 2.x Build and Images Versioning

## Introduction

ModularMOM 2.x is made of two major components:

```
SDK docker image: to be used to build models and as basis for resulting modules. There is ONE version of SDK for each given version of
platform. All SDK images have name modmom/sdk
Modules docker images: docker images implementing specific domain's functionalities i.e. OrderManagement. There is ONE version of docker
image for each version of given module; However we expect to have a quite large number of different modules, each one with its own versioning
sequence. All Module images share the same repository 'modmom' but differs by image name, usually such name is directly referring the
provided function so example of modules' names are: modmom/ordermanagement, modmom/factory and so on.
```
Both mentioned docker images type are built as result of a pipeline and they can exist in two major flavors: DEV versions and STABLE versions.

DEV versions are identified by a tag respecting this pattern: <Branch-Descriptor>-<BuildID> and are the result of the builds done in the day by day
development activity.

Dependently on the branch from where are built, such images can have different degree of quality and stability, so, for instance, an image built on a dev's
specific branch will likely contains partially implemented features, potentially broken and poor quality code and experimental developments. Such images
are identified by a pattern where the branch descriptor is varying, for instance it could be: modmom/ordermanagement:mdp.tsk-20112-12345;

Once the development is likely completed a developer could queue a Pull Request to perform a validation build asking the system to merge on the master
/main branch the new content if the validation is successful. When building into a Pull Request the images have a different naming pattern resulting in
something like this: merge-mdp.tsk-20112-12350. Please not that as the build is new, the BuildId is different and always increased in value.

Finally, if the merge on main/master succeeds it is finalized with a new build on such branches: master for the MetadataRuntime and main for any given
module.

The corresponding resulting images will get names with patterns modmom/sdk:master-12387 and modmom/ordermanagement:main-12391 respectively
for SDK and for, in this case, the OrderManagement Module.

As you can see, the concept of "versioning" is present but, as anticipated, it's completely driven by the <Branch-Descriptor>-<BuildID>.

So, stated that into MetadataRuntime there is a platform descriptor reporting the main platform version, where it is possible to retrieve it at this stage?

The generated docker image always contains informations about the version of the used components to realize it, for instance, referring a given SDK
image named modmom/sdk:mdp.laterna-15719, it is possible to check the versions this way:

docker run --entrypoint=/bin/sh -ti modmom/sdk:mdp.lanterna-15719
for i in *version; do cat $i; done
dotnet version: 5.0.401
platform binaries version: 1.0.0-mdp.lanterna-15719
platform image tag: mdp.lanterna-15719
platform version: 1.0.0

We can then understand that the given image is a DEV version (from the image name tag pattern and the platform binaries version information) and that it
is part of the 1.0.0 planned platform version.

If we do the same for a given module image, for instance modmom/materialmanagement:mdp.lanterna-15727 the result will be richer as a module is
made of two major components: the base platform used to build it, and the module's own version.

docker run --entrypoint=/bin/sh -ti modmom/materialmanagement:mdp.lanterna-15727
for i in *version ; do cat $i; done
dotnet version: 5.0
module binaries version: 1.0.0-mdp.lanterna-15727
module image tag: mdp.lanterna-15727
module version: 1.0.0
platform binaries version: 1.0.0-1.0.0
platform image tag: 1.0.0
platform version: 1.0.0


Please note that while the module it's still a DEV version, the platform used to build it is not: it's a STABLE platform version, this could be identified by the
platform image tag and platform binaries version that all report the 1.0.0 information. In this case this module implementation as been built using STABLE
Platform Version 1.0.0.

MetadataRuntime versioning

MetadataRuntime is the main platform's repository and the main source for the modmom/sdk docker image.

Its version is controlled by a property text file stored into the repo's filesystem at path: Platform/platform.common.properties. In the following a chunk of the
file's content with the relevant version information in bold:

### ...

<PropertyGroup>
<Product>Modular MOM Metadata Runtime</Product>
<Description>Siemens Platform for Modular MOM</Description>
<Authors>Siemens</Authors>
<Company>Siemens Inc.</Company>
<Version> 1. 0. 2 </Version>
<Copyright>Copyright $([System.DateTime]::UtcNow.ToString(yyyy))</Copyright>
</PropertyGroup>

### ...

What it means, in practical terms? It means that any build of MetadataRuntime will produce prerelease artifacts, that is, temporary artifacts toward the
creation of a STABLE release.

Nuget packages created by such build will take names like: Siemens.MOM.Platform.Tools.IdGenerator 1.0.0-CI-20211018-064143.nuget. The suffix after
the 1.0.0 part qualifies this binary as a prerelease number.


# Authentication POC - tooling

In order to experiment on the authentication topic, multiple tools shall be installed and configured.

Actual architecture requires:

```
keycloak: as originating OIDC identity provider
DEX: as primary OIDC identity provider. DEX is used as interface to be able to get authentication information from a wide variety of sources while
keeping the same semantical approach using OIDC protocol.
api-gateway: the api-gateway is realizing the "physical" protection in between the application and the external world (UI included). It shall be
configured so it could leverage on DEX for validating accesses to the underlying APIs.
Authorization Service: We need an Authorization service to connect Ambassador api gateway to OIDC provider
UI module: UI module shall be extended/updated in order to use OIDC protocol to authorize itself on the api-gateway.
Backend module: backend module are hidden behind the api-gateway, so, theoretically, they don't need to authenticate to the api-gateway. This
need could arise in case a "custom" module needs to access the application from outside.
Integrating application: it is possible to have multiple applications in need to access our backend microservices in order to realize an integrated
feature. Such applications shall be able to connect to the final endpoint going through the api-gateway programmatically.
```
Below is a explample of authentication workflow for backend modules

OIDC provider: DEX with Keycloak connector

## Installation

First component to install is api-gateway, for this goal a helm chart is available into the ModularMOM repo at path : Deployment/Kubernetes/HelmCharts.

The chart provides options for installation both into Kubernetes and Openshift however requirements can differ in between the two environments.

## Installing into Kubernetes

Installation into Kubernetes is performed using the following command line:

helm install api-gateway ./api-gateway -f api-gateway-K8S.values

Note:


```
user running the command must play the cluster-admin role into the target environment.
```
```
installation is performed into the default namespace. To install elsewhere, provide the -n <namespace> parameter to the command above.
```
Installing into Openshift

Installation into Openshift is performed using the following command line:

helm install api-gateway ./api-gateway -f api-gateway-OS.values

Note:

```
user running the command must play the cluster-admin role into the target environment.
```
```
installation is performed into the currently selected project. To install in a specific project, provide the -n <project> parameter to the command
above.
```
Analysis Points

Below are few important points we have collected while doing POC

DEX

```
Grant type supported: Authcode, Authcode with PKCE, Password, device code
```
```
Authcode with PKCE implementation: https://github.com/dexidp/dex/pull/1784/files
```
```
https://github.com/dexidp/dex/issues/2244
```
```
Dex doesn’t support client credential grant flow out of the box (https://github.com/dexidp/dex/issues/764) but there are few Implementations which
we can leverage
```
```
https://github.com/dexidp/dex/pull/1629
```
```
https://github.com/dexidp/dex/commit/2a4729e927fd9f93fcdfc8f270efa4be7a03017a
```
```
CORS: Dex could be configured to allow CORS
```
```
web:
```
```
allowedOrigins: [“*”] : it would allow traffic from all origins
```
https://github.com/dexidp/dex/pull/1570/files: This branch is not yet in master but it has some useful information

```
Multiple connectors: There is possibility to select connector in client request, so when we have multiple connectors configured we can select
from client which connector we want to use. From client we have to pass ‘connector_id’ as query parameter, so the auth url should be: <dexurl>
/auth? connector_id=<connector id>
```
```
https://github.com/dexidp/dex/issues/1084
```
```
https://github.com/dexidp/dex/commit/07a77e0dace9a9d81a22e868a1ec9e12a4d02793
```
```
Cross-client trust and authorized party:
```
```
Dex has the ability to issue ID tokens to clients on behalf of other clients. In OpenID Connect terms, this means the ID token’s aud (audience)
claim being a different client ID than the client that performed the login. It is useful as we need to create trust between UI and backend client.
```
```
config
```
```
staticClients:
```
- id: web-app
redirectURIs:
- 'https://web-app.example.com/callback'
name: 'Web app'
secret: web-app-secret


- id: cli-app
redirectURIs:
- 'https://cli-app.example.com/callback'
name: 'Command line tool'
secret: cli-app-secret
# The command line tool lets the web app issue ID tokens on its behalf.
trustedPeers:
- web-app

```
The web app can then use the following scope to request an ID token that’s issued for the command line tool.
```
```
audience:server:client_id:cli-app
```
Authorization Service

We need an Authorization service to connect Ambassador api gateway to OIDC provider. The opensource version of Ambassador doesn’t support feature
to directly connect with OIDC provider. We have used the oidc-authservice application as our AuthService in POC. Its configured to work as
AuthService in Ambassador and use DEX as OIDC provider.

Doc: https://github.com/arrikto/oidc-authservice/blob/master/README.md

Token Validation

Both oidc-authservice and Dex verify id tokens using go-oidc package.

https://pkg.go.dev/github.com/coreos/go-oidc/v3@v3.0.0/oidc#IDTokenVerifier.Verify

They additionally check if the id token is containing USERID_CLAIM,claim whose value will be used as the userid (default email).

Dex additionally verify ‘name’(default) in the claim.

So, with default configuration name and email are mandatory while creating users.

Refresh Tokens

Refresh tokens are credentials used to obtain access tokens. Refresh tokens are issued to the client by the authorization server and are used to obtain a
new id token when the current id token becomes invalid or expires. Issuing a refresh token is optional and is provided by passing offline_access scope to
Dex server.

NOTE: For every refresh of an id token, Dex issues a new refresh token. This security measure is called refresh token rotation and prevents someone
stealing it.

Expiration and rotation settings

Dex has a section in the config file where we can specify expiration and rotation settings for id tokens and refresh tokens. NOTE: All duration options
should be set in the format: number + time unit (s, m, h), e.g., 10m.

```
expire - section for various expiration settings, including token settings:
idTokens - the lifetime of id_token. It is preferable to use short-lived id tokens.
refreshTokens - section for various refresh token settings:
validForIfNotUsed - invalidate a refresh token if it is not used for a specified amount of time.
absoluteLifetime - a stricter variant of the previous option, absolute lifetime of a refresh token. It forces users to reauthenticate
and obtain a new refresh token.
disableRotation - completely disables every-request rotation. The user will also have to specify one of the previous refresh
token options to keep refresh tokens secure when toggling this.
reuseInterval - allows getting the same refresh token from refresh endpoint within a specified interval, but only if the user's
request contains the previous refresh token.
```
NOTE: disableRotation and reuseInterval options help effectively deal with network lags, concurrent requests, and so on in tradeoff for security.

Dex provides option to revoke refresh token though calling the grpc api.

https://github.com/dexidp/dex/issues/1203


Signout

Dex doesn't expose any "end_session_endpoint" so it couldn’t be used to logout user. There are already multiple issues open and even some
implementation proposal but till now its not supported.

https://github.com/concourse/concourse/issues/2718

https://github.com/dexidp/dex/issues/1697

https://github.com/dexidp/dex/issues/963

https://github.com/dexidp/dex/issues/32

Implementation: https://github.com/scality/metalk8s/commit/e6566809c79b0f9ea3cb11fa81d856096bb04d06

One argument mentioned in one discussion is “Dex doesn't implement any session management or track that information, so users are never really
"logged into" dex. E.g. there's no way for a user to logout.” so it might never be supported. A workaround could be to revoke refresh tokens as signout
process.

Connect with LDAP

Keycloak and Dex can be configured to connect with ldap and import users. In POC we have configured keycloak with swqa AD and able to login with
swqa account details. We can import a subset of users based on the filter.

```
search ldap command
```

ldapsearch -v -x -b "ou=Users,ou=SWQA,dc=SWQA,dc=TST" -s one -H ldap://goaw065.swqa.tst -D "cn=ldapusr_modmom20,
ou=LDAP,ou=Special_Users,ou=SWQA,dc=SWQA,dc=TST" -W "(|(cn=Sengupta Ankita)(cn=Sinha Dheeraj))"


### 1.

### 2.

### 3.

# Modular Manufacturing 2204

## Content for April 2204 release

### ????

## Production Environments for April 2204 release

For 2204 (April) release, below mentioned production env are created via tenant operator.

```
Sl. No. Environments Owner Additional Users Landing page url
```
```
1 Morf3D Diego durgaprasad.pottigar@siemens.com
```
```
matteo.bardini@siemens.com
```
```
julia.haas@siemens.com
```
```
heggi@siemens.com
```
```
franck.mouriaux@M3DADMC.COM
```
```
roxanne.warren@morf3d.com
```
```
andreas.j.saar@gmail.com
```
```
https://morf3d.mod.sws.siemens.com/
```
```
2 Zug Jan christian.jaehnert@siemens.com
```
```
matteo.bardini@siemens.com
```
```
igor.usykov@siemens.com
```
```
bernhard.bucholtz@siemens.com
```
```
matteo.odermatt@siemens.com
```
```
mario.koos@siemens.com
```
```
rene.dietschweiler@siemens.com
```
```
susi.schweingruber@siemens.com
```
```
daria.goryacheva@siemens.com
```
```
ruedi.zumbuehl@siemens.com
```
```
daniel.merz@siemens.com
```
```
marcelo.bovo.ext@siemens.com
```
```
https://zug.mod.sws.siemens.com/
```
```
3 Industry Solution Heggi prashant.dinde.ext@siemens.com (admin)
```
```
abdulrahiman.shaikh.ext@siemens.com
```
```
dipali.kandarkar.ext@siemens.com
```
```
https://industrysolution.mod.sws.siemens.com//
```
```
4 Factory Digitalization Mauro bernhard.hausmann@siemens.com
benjamin.sauer@siemens.com
```
```
vladimir.zahorcak@siemens.com
```
```
https://factorydigitalization.mod.sws.siemens.com/
```
## Process for manual deployment of April (2204) release in production env.

## Pre-requisite

```
Create required namespace (for e.g., ocmod-prod-2204, for April release)
Deploy version of tenant operator with this merge https://gitlab.industrysoftware.automation.siemens.com/mom-devops/tenant-operator/-
/merge_requests/50
```

### 3.

### 4.

```
Note: - If Onboard manager is deployed, turn it down
Access should be granted for AWS SQS.
```
Steps

```
Open Xcelertaor admin console and select required env (for e.g, Morf3d), https://admin.sw.siemens.com/
Once in Admin Console, go to Product Configuration and create a new product.
Click on Add configuration > select any of the template in product name > insert url and app display name.
```
```
Once product is added, a pop up for secret download will appear. Download the file.
```
```
Now add user from the dashboard > Assign user.
Provide appropriate values in the assign user dialog. (First user will always be added as Administrator)
Now login to AWS SQS, and find the recent message from the queue.
Copy the content from the message in local notepad.
```

Download api file https://tfs05mom.industrysoftware.automation.siemens.com/MOM/ModularMOM/_git/TenantOperatorDeployment?path=%
2FRestAPI%2FTenantCreater.json&version=GBmain, in your local.
Open this file in VS code.
Configure local kubeconfig with production env.
Launch cmd, and confirm config current-context is set to production env
Execute command "kubectl -n <namespace name> port-forward service/tenant-operator 8082:8082" to set up port forwarding.
Edit file opened in VS code as:-

id - add tenant id from SQS message
name - add name of product
data prefix - it should be unique is starting with alphabet (tip- modify few character in the existing id to make it unique)
domain prefix - you can keep it same as name
product - should be modular-manufacturing
clientID - copy it from downloaded product secret file (during product creation)
clientSecret - copy it from downloaded product secret file (during product creation)
baseurl - maintain same as shown in image
keycloak Secrets - no change from what shown in image
Once the changes are done > Check port forwarding connection > send the request for tenant creation from code
Tenant creation will take 10 to 15 mins > can be verified in rancher production env.
Refer Manual configuration of User with Permission for next steps.
Update event id (highlighted in below image) from SQS message > and send request.

Add "service-account-modmom" in AM and admin group in keycloak
Login to keycloak configured for respective env.
Copy modmom client secret as highlighted in below image

```
Go to Postman collection > confirm env is set for Prod
```

Generate and copy auth token using modmom as client id and secret, copied from above step (this can be done using any step from the
collection)
Tip:- Do not use the token generated in above step. Make sure access token url corresponds to respective env.
Copy token and decode it using jwt.io
Once decoded, copy the "sub" (as modmom user id) and "preferred_username" (as modmom account user name)
Now perform "Create user into AM" step and then "Add user to UG" (refer from here Manual configuration of User with Permission).


# User Tasks


# Prepare


# Configuration of Microk8s/Ubuntu VDI

```
Introduction
Install Microk8s
Configuration of Microk8s
Configure a local DNS resolver
Install Helm
Install Dapr [Obsolete]
Install SWQA CA Authority certificate
Persistent Volumes on Local host path
Connect from your Windows VDI
```
## Introduction

To login use the SWQA user account without the domain (itrxxxx).

During the installation of some packages a specific local user group may be created to use the software.

The swqa user may not be part of that group and you should add your user to that local group.

To do that:

```
sudo gpasswd -a <itrxxx> <local group>
```
To be sure that the settings is acquired and applied to all the sessions please restart the VDI or logout/login again.

You can verify the applied settings for your user within your session:

```
id | grep -i <local group>
```
## Install Microk8s

In Ubuntu you can install the microk8s and kubectl package with the snap tool.

```
snap install kubectl --classic
```
Add the kubectl bash completion to the system

```
sudo sh -c "kubectl completion bash > /etc/bash_completion.d/kubectl"
```
Microk8s:

```
sudo snap install microk8s --classic --channel=latest/stable
```
Add your domain user to the microk8s local group:

```
sudo gpasswd -a <itrxxx> microk8s
logout/login
```
Enable microk8s addons with local swqa forwarder.

```
microk8s enable dns:172.27.0.65,172.27.0.3
microk8s enable storage
```

Verify with kubectl that everything is configured correctly:

```
kubectl config current-context
```
It should be: microk8s.

Use kubectl command without microk8s prefix

```
cd .kube
microk8s config > config
```
Check the overall Microk8s status

```
microk8s status
```
```
microk8s is running
high-availability: no
datastore master nodes: 127.0.0.1:19001
datastore standby nodes: none
addons:
enabled:
dashboard # The Kubernetes dashboard
dns # CoreDNS
ha-cluster # Configure high availability on the current node
metrics-server # K8s Metrics Server for API access to service metrics
storage # Storage class; allocates storage from host directory
disabled:
ambassador # Ambassador API Gateway and Ingress
cilium # SDN, fast with full network policy
fluentd # Elasticsearch-Fluentd-Kibana logging and monitoring
gpu # Automatic enablement of Nvidia CUDA
helm # Helm 2 - the package manager for Kubernetes
helm3 # Helm 3 - Kubernetes package manager
host-access # Allow Pods connecting to Host services smoothly
ingress # Ingress controller for external access
istio # Core Istio service mesh services
jaeger # Kubernetes Jaeger operator with its simple config
keda # Kubernetes-based Event Driven Autoscaling
knative # The Knative framework on Kubernetes.
kubeflow # Kubeflow for easy ML deployments
linkerd # Linkerd is a service mesh for Kubernetes and other frameworks
metallb # Loadbalancer for your Kubernetes cluster
multus # Multus CNI enables attaching multiple network interfaces to pods
openebs # OpenEBS is the open-source storage solution for Kubernetes
openfaas # openfaas serverless framework
portainer # Portainer UI for your Kubernetes cluster
prometheus # Prometheus operator for monitoring and logging
rbac # Role-Based Access Control for authorisation
registry # Private image registry exposed on localhost:32000
traefik # traefik Ingress controller for external access
```
Start the Kubernetes dashboard in a dedicated terminal

```
microk8s dashboard-proxy
```
```
Checking if Dashboard is running.
Dashboard will be available at https://127.0.0.1:10443
Use the following token to login:
ey......
```

To login choose Kube config file under ~/.kube/config

Configuration of Microk8s

```
Private Registry credentials
Private Registry Cache
Add private registry as insecure registries
Add registry credentials with the default service account
Docker hub credentials
```
Use the private registry credentials to pull our images:

```
kubectl create secret docker-registry registry-credentials --docker-server=modmom-registry.apps.openshift03.
swqa.tst --docker-username=modmom20 --docker-password=<redacted>
kubectl patch serviceaccount default -p '{"imagePullSecrets": [{"name": "registry-credentials"}]}'
```
Add the Private registry Cache:

to mitigate the Rate Limit issue on dockerHub image pulling, edit the docker backend configuration file, its location could vary based on the kind of
installation you made:

```
Docker installed using apt
```
```
file is: /etc/docker/daemon.json
```
```
Docker installed using snap
```
```
file is: /var/snap/docker/current/config/daemon.json
```
Add the row "registry-mirrors": ["https://cache-modmom-registry.apps.openshift03.swqa.tst/"] then restart docker backend, again with different commands
based on installation type:

### {

```
"registry-mirrors": ["https://cache-modmom-registry.apps.openshift03.swqa.tst/"]
}
```
Restart the docker daemon for settings to take effect:

```
Docker installed using apt
```
```
sudo systemctl restart docker
```
```
Docker installed using snap
```
```
sudo snap restart docker
```
Setting Docker Hub credentials in Kubernetes:

In case working with the registry cache is not enough, use your Docker Hub account with this two commands:

```
kubectl create secret docker-registry dockerhub-registry-credentials --namespace=<namespace_to_use> --docker-
server=docker.io --docker-username=<username> --docker-password=<password>
kubectl patch serviceaccount dapr-system -p '{"imagePullSecrets": [{"name": "dockerhub-registry-credentials"}]}'
```
Add private registry as insecure registries:
Add the private registries as insecure-registries to avoid Certification Authority errors:

```
Edit the daemon.json file and add the below content. (refer previous section for the file path)
```

```
"insecure-registries" : ["modmom-registry.apps.openshift03.swqa.tst", "cache-modmom-registry.apps.
openshift03.swqa.tst"]
```
```
Restart docker (refer previous section for command)
edit /var/snap/microk8s/current/args/containerd-template.toml and add the following under [plugins] -> [plugins."io.
containerd.grpc.v1.cri".registry] -> [plugins."io.containerd.grpc.v1.cri".registry.mirrors] (at the end of
the file)
```
```
[plugins."io.containerd.grpc.v1.cri".registry.mirrors."modmom-registry.apps.openshift03.swqa.tst"]
endpoint = ["http://modmom-registry.apps.openshift03.swqa.tst"]
[plugins."io.containerd.grpc.v1.cri".registry.mirrors."cache-modmom-registry.apps.openshift03.swqa.tst"]
endpoint = ["http://cache-modmom-registry.apps.openshift03.swqa.tst"]
```
```
Add Lanterna:
```
```
[plugins."io.containerd.grpc.v1.cri".registry.configs."lanterna.industrysoftware.automation.siemens.com".
tls]
insecure_skip_verify = true
[plugins."io.containerd.grpc.v1.cri".registry.configs."lanterna.industrysoftware.automation.siemens.com".
auth]
username = "<your swqa user without swqa>"
password = '<yourpassword>'
```
```
Restart MicroK8s.
```
```
microk8s stop
microk8s start
```
Add Registry Credentials with the Default Service Account :

```
kubectl create secret docker-registry registry-credentials --docker-username=modmom20 --docker-
password=registry2021 --docker-server=https://modmom-registry.apps.openshift03.swqa.tst -n <namespace>
kubectl patch serviceaccount default -p '{"imagePullSecrets": [{"name": "registry-credentials"}]}' -n
<namespace>
```
Configure a local DNS resolver

Inside a Kubernetes runtime, pods can be accessed thorugh a service registered by the name. When a service is created the Kubernetes name service
will return the internal IP (typically 10.x.y.z on microk8s).

On Microk8s you should enable the dns add-on, this service is commonly required by other add-ons:

```
microk8s enable dns (default dns on google public servers)
# or
microk8s enable dns:172.27.0.65,172.27.0.3 # to enable dns resolution inside a pod using a forwarder to the
swqa.tst dns servers (for example to resolve internal server, such as tfs05)
# Verify with:
kubectl -n kube-system describe configmap coredns
# Temporary change/edit it with:
kubectl -n kube-system edit configmap coredns
```
But the Linux host machine is not configured by default to use it.

In order to use it, retrieve the IP address of the name server deployed and change the resolved.conf file like so:

```
kubectl get -n kube-system service/kube-dns | awk '{print $3}'
```
From root put the ClusterIP address read from the command above in the file under /etc/systemd/resolved.conf like so:


### DNS=10.152.183.10

```
Domains=svc.cluster.local
```
Restart systemd resolved service:

```
service systemd-resolved restart
```
After this you can resolve addresses on your local machine in the form:

<service>.<namespace>.svc.cluster.local

Example: ui-module.default.svc.cluster.local

This is an advantage because it can make the deployment yaml definitions (example: helm charts for microk8s) a bit more abstract, without relying on
parameters like the local IP address of a dev machine, furthermore, each time you scale down a deployment, you do not have to change the configuration
because the IP has changed as well.

Install Helm

First download and install the Helm command line tool from here, using the latest Linux amd64 tar ball.

```
sudo tar xzvf helm-v3.5.4-linux-amd64.tar.gz -C /usr/local/bin
```
And add the helm bash completion to the system:

```
sudo sh -c "helm completion bash > /etc/bash_completion.d/helm"
```
Install Dapr [Obsolete]

Firstly, install Helm.

Then add the Dapr repo and update

```
helm repo add dapr https://dapr.github.io/helm-charts/
helm repo update
helm search repo dapr
```
Install a version of Dapr

```
helm install dapr dapr/dapr --namespace dapr-system --version 1.1.2
```
Install the dapr command line tool from here

```
sudo sh - c "wget -q https://raw.githubusercontent.com/dapr/cli/master/install/install.sh -O - | /bin/bash -s
1.0.0-rc.2"
```
Add the dapr bash completion to the system:

```
sudo sh -c "dapr completion bash > /etc/bash_completion.d/dapr"
```
Verify Dapr installation status with

```
dapr status -k
```
### NAME NAMESPACE HEALTHY STATUS REPLICAS VERSION AGE CREATED

```
dapr-dashboard dapr-system True Running 1 0.6.0 1d 2021-05-11 19:46.11
```

```
dapr-sentry dapr-system True Running 1 1.1.2 1d 2021-05-11 19:46.11
dapr-placement-server dapr-system True Running 1 1.1.2 1d 2021-05-11 20:00.06
dapr-sidecar-injector dapr-system True Running 1 1.1.2 1d 2021-05-11 19:46.11
dapr-operator dapr-system True Running 1 1.1.2 1d 2021-05-11 19:54.30
```
Note: Because of the global limit on Docker Hub registry, you may fail to get the dapr images, in this case you should use your own Docker Hub
account and set microk8s to use it in the dapr-system name space.

To do that:

```
kubectl create secret docker-registry dockerhub-registry-credentials --namespace=dapr-system --docker-
server=docker.io --docker-username=<your_dockerhub_username> --docker-password=<your_password>
kubectl patch serviceaccount default -p '{"imagePullSecrets": [{"name": "dockerhub-registry-credentials"}]}'
```
Install redis state.store and pubsub component:

Install SWQA CA Authority certificate

To use openshift endpoints on https without trusting the subdomain every time:

Download the .crt file from here:

https://goaw065.swqa.tst/CertEnroll/GOAW065.SWQA.TST_SWQA-GOAW065-CA(1).crt

Install with:

```
sudo cp GOAW065.SWQA.TST_SWQA-GOAW065-CA\(1\).crt /usr/local/share/ca-certificates/
sudo update-ca-certificates
```
Do the same for Lanterna crt file :

https://lanterna.industrysoftware.automation.siemens.com/api/v2.0/systeminfo/getcert

Persistent Volumes on Local host path

In case microk8s is not able to bring up persistent volume claim (pvc) using the storage class hostpath, do the following:

```
microk8s disable hostpath-storage
microk8s enable hostpath-storage
```
This will upgrade the hostpath storage to the latest version.

Re-install the helm/deployment to activate the pvc into the deployment.

Diagnostic:

```
kubectl logs -n kube-system hostpath-provisioner-<xxxxxxxxxx>
```
The persistent volumes are store under:

```
/var/snap/microk8s/common/default-storage/
```

Connect from your Windows VDI

```
ssh keys
Visual Studio integration
```
Connect from SQL Management Studio to your Ubuntu VDI sql server pod with:

```
kubectl port-forward sqlserver-0 1433:1433 --address=0.0.0.0
Connect with user and password to your Ubuntu vdi hostname from SQL Managegement Studio
```

# Configure SQL Server on Ubuntu 20.04 VDI

```
Install SQL Server
```
## Install SQL Server

To configure SQL Server on Ubuntu, run the following commands in a terminal to install the mssql-server package. Open the terminal and follow below
steps.

```
Import the public repository GPG keys:
```
```
wget -qO- https://packages.microsoft.com/keys/microsoft.asc | sudo apt-key add -
```
```
Register the Microsoft SQL Server Ubuntu repository for SQL Server 2019:
```
```
sudo add-apt-repository "$(wget -qO- https://packages.microsoft.com/config/ubuntu/20.04/mssql-server-2019.list)"
```
```
Run the following commands to install SQL Server:
```
```
sudo apt-get update
sudo apt-get install -y mssql-server
```
```
After the package installation finishes, run mssql-conf setup and follow the prompts to set the SA password and choose your edition.
```
```
sudo /opt/mssql/bin/mssql-conf setup
```
```
Once the configuration is done, verify that the service is running:
```
```
systemctl status mssql-server --no-pager
```
At this point, SQL Server 2019 is running on your Ubuntu machine and is ready to use!

## Install the SQL Server command-line tools

To create a database, you need to connect with a tool that can run Transact-SQL statements on the SQL Server.

```
Install curl
```
```
sudo apt-get update
sudo apt install curl
```
```
Import the public repository GPG keys.
```
```
curl https://packages.microsoft.com/keys/microsoft.asc | sudo apt-key add -
```

```
Register the Microsoft Ubuntu repository.
```
```
curl https://packages.microsoft.com/config/ubuntu/20.04/prod.list | sudo tee /etc/apt/sources.list.d/msprod.list
```
```
Update the sources list and run the installation command with the unixODBC developer package.
```
```
sudo apt-get update
sudo apt-get install mssql-tools unixodbc-dev
```
```
Update PATH environment variable
```
```
echo 'export PATH="$PATH:/opt/mssql-tools/bin"' >> ~/.bash_profile
```
```
echo 'export PATH="$PATH:/opt/mssql-tools/bin"' >> ~/.bashrc
source ~/.bashrc
```
Connect locally

```
Run sqlcmd with parameters
```
```
sqlcmd -S localhost -U SA -P '<YourPassword>'
```
```
If successful, you should get to a sqlcmd command prompt: 1>.
```
Exit the sqlcmd command prompt

```
To end your sqlcmd session, type QUIT:
```
### QUIT


# Local Development Environment Setup in Linux VDI

List of activities.

```
Install docker
Install microk8s
Run app in microk8s and access outside cluster
Install git and connect with tfs and pull latest code
Install VS Code and add git,docker,C# extension in VS
Add private registry as insecure registries
connect to registry and pull sdk image
deploy sqlserver in microk8s
Install helm
Run registry model image in microk8s
Configure registry mirror for docker
Build platform image in docker
build model image in docker
run local model image in microk8s
use moicrok8s local registry to run local docker images
Install PostMan
Install oc cmdline to connect with OpenShift
Debug in VS Code
```
Install git and pull codes from repository

```
Install git
```
```
sudo apt-get install git
```
Cache git credential if you want to avoid providing credential multiple time.

```
git config --global credential.helper 'cache --timeout 3600'
```
```
MetadataRuntime
```
```
git clone https://tfs05mom.industrysoftware.automation.siemens.com/MOM/ModularMOM/_git/MetadataRuntime
```
```
Dispatch
```
```
git clone https://tfs05mom.industrysoftware.automation.siemens.com/MOM/ModularMOM/_git/WS_M1_Dispatch
cd WS_M1_Dispatch
git submodule update --init --depth=1
```
```
TrackAndTrace
```
```
git clone https://tfs05mom.industrysoftware.automation.siemens.com/MOM/ModularMOM/_git/WS_M1_TrackAndTrace
cd WS_M1_TrackAndTrace
git submodule update --init --depth=1
```
```
ModularMOM
```
```
git clone https://tfs05mom.industrysoftware.automation.siemens.com/MOM/ModularMOM/_git/ModularMOM
```
Build Platform Image in Docker

Open the MatedataRuntime repository and Run the /docker/BuildPlatformImage.sh file. Follow the instructions mentioned in the script.

Follow this link to generate PAT: https://docs.microsoft.com/en-us/azure/devops/organizations/accounts/use-personal-access-tokens-to-authenticate?
view=azure-devops&viewFallbackFrom=vsts&tabs=preview-page#create-personal-access-tokens-to-authenticate-access


Build Module Image in Docker

Open the MatedataRuntime repository and Run the /docker/BuildModelImage.sh file. Follow the instructions mentioned in the script.

Use local images in microk8s

There are two way to use local images in microk8. One is to push the image in a local private registry and other is to create package of the image and
import it in microk8s.

```
Use microk8s registry
```
```
microk8s enable registry
```
The registry run in [http://localhost:32000.](http://localhost:32000.) Tag the local image with localhost:32000/<imagename> and push into the registry.

Or

```
Import image in microk8s
```
```
docker save dispatch > dispatch.tar
microk8s ctr image import dispatch.tar
microk8s ctr images ls
```
Deploy sqlserver in microk8s

Open the ModularMOM repository, open Deployment/Kubernetes/HelmCharts folder and install the sqlserver helm chart based on the notes.

Deploy Modules in microk8s using helm chart

Open the ModularMOM repository, open Deployment/Kubernetes/HelmCharts folder and install the module helm chart based on the notes.

```
Deploy Modules
```
```
helm install <<ReleaseName>> module --set dbNAME=<<database name>>,imageName=<<module image name>>,
project=<<namespace>>,registry=<<registry name>> -n <<namespace>>
Example:
helm install dispatch module --set dbNAME=DispatchDb,imageName=dispatch:latest,project=dafault,registry=modmom-
registry.apps.openshift03.swqa.tst
```
Expose service as Nodeport to connect module application from browser

```
kubectl expose deployment <<deployment name/module name> --type=NodePort --name=<<name of service>> -n
<<namespace>>
```

### 1.

### 2.

### 3.

Debug in VS Code

```
Add C# for Visual Studio Code extension in vs code
Copy below configurations in the launch.json file
Select the Run and Debug option from the left menu(Ctrl + Shift +D) and start debugging(f5)
```
For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387

### {

```
// Use IntelliSense to learn about possible attributes.
// Hover to view descriptions of existing attributes.
// For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387
"version": "0.2.0",
"configurations": [
{
"name": ".NET Core Launch (console)",
"type": "coreclr",
"request": "launch",
"preLaunchTask": "build",
"program": "${workspaceFolder}/bin/Debug/net5.0/Siemens.MOM.Platform.Api.dll",
"args": [],
"cwd": "${workspaceFolder}/Siemens.MOM.Platform.Api",
"stopAtEntry": false,
"console": "internalConsole"
},
{
"name": ".NET Core Attach",
"type": "coreclr",
"request": "attach"
}
]
}
```

# Configure CI/CD Pipeline for New Module Repository

Follow the below steps to Configure build and release pipelines for new Repository

Build Pipeline

```
Click on ‘New Pipeline’ from the Pipelines menu of the Azure DevOps.
Select ‘Azure Repos Git’ and then select the repository you want to create the pipeline for.
Select ‘Select an existing YAML file’ option and select ‘/buildModulePipeline.yml’ in the path
```
Release Pipeline

```
Open ‘Release’ from the menu and clone any existing release like ‘Deploy TrackAndTrace to Openshift’
Delete the primary artifact (WS_M1_TrackAndTrace) and add a new artifact pointing to the build of the module and make it the primary artifact.
```
```
Modify the name of the Release and save.
```

# Develop


### 1.

### 2.

### 3.

# Integrate

## Overview

The "integrate" step allows you to merge your changes into the application repository main branch and make them available in the integration
environments. You may need to perform the following activities:

```
Perform a pull request
Verify continuous integration results
Publish a stable version
```
## Perform a pull request

## Verify continuous integration

## Publish a stable version

Applies to: modules, platform

Publishing a stable version allows you to produce a new image that will be released as stable. The image will be deployed automatically in the integration
environments of OpenShift and it can be deployed to XCR environment upon manual approval.

## Required actions

1. Make sure the your commits have been merged to the master branch.

```
You are going to run automation for creating a git tag referring to the last commit of the main branch.
```
2. Make sure that modelconfig of your repository has the version you want to promote as stable.

```
Otherwise, update the version manually and commit the new change to the main branch.
```
3. Run a new build of the application and select "Publish as stable version"

```
In the "Next version" option you can choose how to update the modelconfig version AFTER having published the current version.
```
```
Use Major to increment the first digit version, minor to increment second digit or patch to increment the last digit.
```
```
In this case, binaries will always be implicitly digitally signed.
```
4. Verify that the build completes successfully.

## Expected results

You should be able to verify the following results produced by this automation process:

```
The produced binaries have been digitally signed
A new image is available in the stable project of the Harbor Lanterna repository
```

```
A new release has been created with name and the application has been deployed to the integration environments.
```
Notice that:

```
The main branch last commit is now the one related to the update of the manifest file
The main branch contains a manifest that is set to the version you selected as "next version"
A new git annotated tag is produced for the version provided in the repo manifest
The tag branch last commit is actually the second-last commit of the current main branch
The tag branch contains a manifest that is set to the version you wanted to publish as stable
```

# Deploy


# Deploy in Openshift from Feature Branch[Obsolete]

Follow the below steps to deploy your local code of MetadataRuntime in Openshift.

```
Create a new branch from master in MetadataRuntime and commit your local changes.
Run MetadataRuntime pipeline selecting your branch
```
```
Once the build is successful, go to the summary of the build and open the job.
Click on the ‘Building Platform Docker image for Model Build’ step. Scroll down and copy the image name (only the tag)
```
```
Create a new branch of the module (even if there is no change in module) you want to deploy.
Run module pipeline, select your branch, click on variables. Put the platform sdk image tag you have copied from the previous step in platformVe
rsion variable and click on update.
```
```
Once the build is successful, go to the summary of the build and open the job. Click on the ‘Building Module Docker image’ step. Scroll down
and copy the image name (only the tag)
```
```
Open Release section from the pipeline, select the release for the module you want to deploy. Click on ‘Create Release’. Select the correct versi
on of the artifacts from the dropdown and put the ImageTag copied from the previous step. Click on ‘Create’.
```

Open the new release and deploy in the environment specified for your team(Openshift Internal Project Links).


# Devops Implementation


# Build


# Build parameters

## Overview

With parameters users can set a build instance to run with specific runtime values and change the build behavior and workflow accordingly.

## Parameters

```
Parameter Default Description Build
```
```
Branch/tag main
/master
```
```
Builds the solution getting the sources from a specific branch or tag All pipelines
```
```
Publish as stable
version
```
```
false Promotes the build to produce a stable image. Any module, UI,
Platform
```
```
Next version patch Works only when "Publish as stable version" is selected.
```
```
Sets how to update the version manifest of the repository.
```
```
Any module, UI,
Platform
```
```
Digitally sign binaries false Sets whether the produced binaries must be digitally signed.
```
```
When using "Publish as stable version" this parameter has not any effect and the binaries are digitally
signed.
```
```
Any module, Platform
```
```
Enable tests true Sets whether the build must execute the test steps.
```
```
When building on main branch this parameter has not any effect and test steps are executed.
```
```
Any module, Platform
```
```
Run quality checks true Sets whether the build must execute the test steps.
```
```
When building on main branch this parameter has not any effect and the quality checks are executed.
```
```
n/a
```

# Variable groups


# NET Version X

## Overview

NET version pipeline variables used to build the platform and the backend modules are stored in the TFS library as variable group.

The variable group is used to store information about a major version of the .NET framework.

Any change in the minor or patch digit of the current framework version can be handled by editing the variable group directly. While a change to a new
major version should be handled by creating a new variable group as described in the paragraphs below.

## NET Version 6

Current variable group used is NET Version 6 and is available here https://tfs05mom.industrysoftware.automation.siemens.com/MOM/ModularMOM
/_library?itemType=VariableGroups

It provides the following variables to the pipelines:

```
Variable name Current
value
```
```
Description
```
```
netSdkVersion 6.0.201 SDK version used to install and use for building the solution (platform/model).
```
```
https://dotnet.microsoft.com/en-us/download/dotnet/6.0
```
```
netSdkImage 6.0.201-focal Official Microsoft docker image that is used to build the platform.
```
```
https://hub.docker.com/_/microsoft-dotnet-sdk/
```
```
netTfVersion 6.0 target framework used for publishing the application and its dependencies to the folder for deployment to the
hosting system.
```
```
https://docs.microsoft.com/en-us/dotnet/standard/frameworks
```
```
netCoreRuntimeIma
ge
```
```
6.0.3-focal Official Microsoft for ASP.NET Core runtime docker image that is used to build the model.
```
```
https://hub.docker.com/_/microsoft-dotnet-aspnet
```
## How to configure a new .NET version in pipelines

```
Step Logical location What to do
```
1. TFS library

```
https://tfs05mom.industrysoftware.automation.siemens.com
/MOM/ModularMOM/_library?itemType=VariableGroups
```
```
Make sure that the variable group is available in the Library or create a new
one as described below.
```
2. Module repository Edit the <Module>/buildVariables.yml variable with the new variable group
    reference:
    - group: "NET Version 6"
3. MetadataRuntime repository Edit the MetadataRuntime/azure-pipelines.yml variable and the
    MetadataRuntime/automation-pipeline.yml with the new variable group:
    - group: "NET Version 6"

## How to create a new .NET version variable

In the library https://tfs05mom.industrysoftware.automation.siemens.com/MOM/ModularMOM/_library?itemType=VariableGroups create a new variable
group with the following variables:

```
Variable
name
```
```
Description Example
value
```
```
How is used
```
```
netSdkVersion .NET SDK Version:
```
```
https://dotnet.microsoft.com/en-us/download
/dotnet/6.0
```
```
6.0.201 Specify he SDK version to install and use for building the solution (platform/model).
```
```
ModularMOM/modelSteps.yml
```

netSdkImage Official image for the .NET SDK:

```
https://hub.docker.com/_/microsoft-dotnet-sdk/
```
```
6.0.201-
focal
```
```
Specify the official Microsoft docker image that will be used to build the platform.
```
```
MetadataRuntime/azurepipelines.yml
MetadataRuntime/docker/dockerfile.platform
```
netTfVersion The target framework version.

```
https://docs.microsoft.com/en-us/dotnet
/standard/frameworks
```
```
6.0 Specify the target framework used for publishing the application and its dependencies
to the folder for deployment to the hosting system.
```
```
ModularMOM/modelDockerSteps.yml
MetadataRuntime/docker/dockerfile.module
```
netCoreRunti
meImage

```
Official image for ASP.NET Core runtime.
Select an image from:
```
```
https://hub.docker.com/_/microsoft-dotnet-
aspnet
```
```
The image version must be coherent with the
above specified net versions:
```
```
https://dotnet.microsoft.com/en-us/download
/dotnet/6.0
```
```
6.0.3-focal Specify the official Microsoft docker image that will be used to build the model.
```
```
ModularMOM/modelDockerSteps.yml
MetadataRuntime/docker/dockerfile.module
```

# Build features

Matrix of the features how they apply to the existing builds.

(r) need refactoring, (s) applied with special configuration

```
Build/Feature Build number Digital signature
```
```
of binaries
```
```
Code analysis Publish stable version TFS tags Enable test steps
```
```
M1_MaterialManagement x x x-r x x x
```

# Feature: Code analysis


# Feature: Digital signature of binaries

## Overview

Feature "Digital signature of binaries" can be integrated in the pipeline by inserting the related template right after the step that produces the binaries for
instance.

The pipeline agent pool section must be modified because digitally signature can be performed only from a specific agent.

The tool used for digital sign of the binaries is the Microsoft SignTool, see the related template for more information.

## Configuration

```
Parameters - name: digitalSignature
displayName: Digitally sign binaries
default: false
type: boolean
```
```
Variables variables:
```
- name: digitalSignature
value: '$[ or( ${{parameters.publishStable}}, ${{parameters.digitalSignature}} )]'

```
Resources ModularMOM
```
```
Pools - job: 'job'
pool:
name: 'ModularMOMPool'
${{ if eq(parameters.digitalSignature, true) }}:
demands: Agent.Name -equals ModMOM_AgtBld003
${{ if eq(parameters.digitalSignature, false) }}:
demands:
```
- agent.os -equals Windows_NT
- DOCKER_HOST -equals GOAX510

```
Templates - template: ../Steps/signBinaries.yml
parameters:
searchRoot: $(Build.ArtifactStagingDirectory)/models
```
```
The template directly checks the digitalSignature variable in its condition property.
```

# Feature: Publish a stable version

## Overview

This feature supports the following functional capabilities and options:

```
can be enabled/disabled from a build runtime parameter
create an annotated git tag based on the current head of the master repository
update the specified manifest file to the next version according to the selected options major/minor/patch
```
If the build implements "digital signature of binaries" , a "stable" feature. See related topic.

## Configuration

Make sure the application repository has its manifest file. For example:

```
Path Code/Config/${{parameters.moduleName}}.modelconfig.json
```
```
Content {
"name": ${{parameters.moduleName}},
"description": "TODO: Provide a description of the model.",
"version": "0.0.1",
}
```
Add the following configuration to the pipeline:

```
Parameters publishStable
nextVersion
```
```
Variables publishStable, moduleVersion (deprecated), moduleName (deprecated), appVersion, appName
```
```
Resources ModularMOM
```
```
Templates/Tasks - checkout: self
persistCredentials: true
submodules: true
fetchDepth: 0
clean: true
```
```
Templates/Build/updateBuildNumber.yml
Templates/Build/updateTfsTags.yml
Templates/Build/addGitTag.yml
Templates/Build/updateModuleVersion.yml (Deprecated)
```
```
Templates/Build/updateAppManifest.yml
```
```
Example - job: 'publishstable'
dependsOn: 'build'
displayName: 'Publish as stable version'
condition: and(succeeded('build'),eq( ${{parameters.publishStable}}, 'true'),eq(variables['Build.SourceBranchName'], 'main'))
steps:
```
- checkout: self
persistCredentials: true
submodules: true
fetchDepth: 0
clean: true
- powershell: |
$a = Get-Content "$(Build.Repository.LocalPath)/Code/Config/$(moduleName).modelconfig.json" -raw | ConvertFrom-Json
Write-Host "##vso[task.setvariable variable=moduleVersion;]$($a.version)"
Write-Host "##vso[task.setvariable variable=moduleName;]$($a.name)"
displayName: "Read manifest"
- template: Templates/Build/updateBuildNumber.yml@templates
parameters:
moduleName: '$(moduleName)'
moduleVersion: '$(moduleVersion)'
publishStable: ${{parameters.publishStable}}
- template: Templates/Build/updateTfsTags.yml@templates
parameters:
moduleName: '$(moduleName)'


```
moduleVersion: '$(moduleVersion)'
publishStable: ${{parameters.publishStable}}
uiModule: false
```
- template: Templates/Build/addGitTag.yml@templates
parameters:
tagValue: "v$(moduleVersion)"
tagComment: "Publishing stable version"
publishStable: ${{parameters.publishStable}}
- template: Templates/Build/updateModuleVersion.yml@templates
parameters:
moduleName: '$(moduleName)'
moduleVersion: '$(moduleVersion)'
publishStable: ${{parameters.publishStable}}
nextVersion: ${{parameters.nextVersion}}
uiModule: false

Improvements/Changes


# Release


# Deployment


# Analysis and tuning of K8s resource usage

## Overview

This example of analysis and tuning follows up the feature https://tfs05mom.industrysoftware.automation.siemens.com/MOM/ModularMOM/_workitems/edit
/43680.

Continuous deployment of ModularMOM components into OpenShift cluster environments is running into stability issues as teams and components are
growing. As immediate actions Devops need to analyze resource usage and tune it so that resource limits and request are optimized according to use
cases and scenarios. We need to reach a compromise between application resource requirements and actual usage.

To analyze K8s resource usage it is important to look at how pipelines are currently deploying Pods, how resource limits and requests are configured for
each container versus how much resources we have in the OpenShift cluster.

For more information on resources limits and requests: https://cloud.google.com/blog/products/containers-kubernetes/kubernetes-best-practices-resource-
requests-and-limits

## Analysis of module-pod resource usage (OpenShift cluster May 2022)

A typical module-pod (backend component like Order Management API) is composed by 2 containers: module-container + opentelemetry-container. The
resources specified by each container are additive on the node.

The following slide shows the module-pod deployment configuration for the resources:

Starting from the deployment configuration we need to compare the requested resources at the deployment stage with the actual usage of them in the
operation stage. The resource usage must be monitored across the various projects and along a meaningful time sample.

We have selected to monitor a stressed environment like NFR modmom-18 namespace in OpenShift cluster. The peak activity in a 24-hour time sample
clearly shows that CPU/memory usage are below the container resource specifications:


The resource usage in other environments is actually far below the modmom-18.

The current analysis clearly shows that the actual configuration for module pods is overestimated and also may quickly saturate the capacity of the overall
cluster as shown in the following queries:


Tuning of the module resource specifications

To mitigate and improve the current situation the module deployment was tuned as follows:

This table shows the sum of the values at the pod-level respect to the previous configuration

```
Previous config New config
```
```
CPU limits: 450m limits: 250m
```

```
requests: 160m requests: 60m
```
```
Memory limits: 2.5Gi
```
```
requests: 1.1Gi
```
```
limits: 1.5Gi
```
```
requests: 356Mi
```
The following graph shows how the CPU limits have been tuned and improved in a time-span of 2 weeks


### 1.

### 2.

### 3.

### 1.

# Azure deployment model for modules (yaml)

## Overview

Scope of this topic is to provide a description of the new deployment model for modules. This new model allows Devops to configure deployment of a
single module to a K8 namespace using yaml azure and kustomize.

## Module build pipeline

The module repository requires the following configuration:

```
Create pipeline azureBuild.yml this will be used to host the new pipeline workflow without affecting current one
Create pipeline azureBuildVariables.yml
Create folder for Deployment
```
## ModularMOM repository

In order to adopt the new deployment model the ModularMOM repository must be upgraded to a new version.

The following changes need to be merged from branch asg/kustomize:

This merge shouldn't affect old pipeline model.

## Upgrade strategy

Start from the repository of a module (say M1_MaterialManagement).

Trigger of azure_build.yml must be disabled.

Merge changes into main branch.

Now, main branch has two main pipelines:

```
the buildModule.yml pipeline (provides connection to the old deployment model and old version of ModularMOM)
the azure_build.yml pipeline (provides connection to the new model through the zure_deployment.yml).
```
To test the pipeline:

```
Make the current build point to azure_pipeline.yml
```

2. Disable the trigger of buildPipeline.yml and enable the trigger of the azure_build.yml


# Azure deployment model for OBM

Deployment stages

```
set image
```
```
create config file
```
```
create secret
```
```
kubernetes manifest
```
```
set image
```
```
helm?
```
```
kubectl?
```
```
create config file
```
```
create secret
```
```
create secret for db user
```
```
deploy sql server
```
```
Check if module-db-credentials secret exists
```
```
"Check if sapassword secret exists"
```
```
k8 manifest
```
```
set image
```
```
create config file
```
```
create secret
```
```
create harbor pull secret
```
```
create secret for dbuser
```
```
Check if database-admin secret exists
```
```
k8 manifest
```

# Control Plane Deployment

## Overview

Tenant Operator is an K8 application extension specifically designed to manage ModularMOM application and components in the tenant namespaces of
Rancher Kubernetes Clusters.

Tenant Operator is deployed to the tenant namespace as a container in a K8 pod, along with On Board Manager and the ModularMOM Helm Catalog.

Tenant Operator configuration files are stored in the tfs05 ModularMOM team project, under the TenantOperatorDeployment repository.

## How to deal with updates from git-lab remote

Prerequisites: local clones available (git-lab) tenant-operator, and (tfs) TenantOperatorDeployment

# bin bash

cd tenant-operator

git pull origin master

cd ../TenantOperatorDeployment

git checkout -b update

cp -R ../tenant-operator/deploy/*.

git status

git add.

git commt -m "update"

# two-dots diff between the current branch and the main head

git diff main..update

# three-dots diff between the current branch and the common ancestor

git diff main...update


Structure of TenantOperatorDeployment repository

```
Folder Description
```
```
environme
nts
```
```
Contains configuration data of the Rancher environments.
```
```
Each folder is related to a specific k8 cluster namespace.
```
```
Additional namespaces can be created and customized starting from these base folders
```
```
by following the hierarchy. For example this configuration allows to create a new tenant namespace with name ocmod-dev-devops-test
based on ocmod-dev:
```
```
The additional devops-test folder is required to apply specific customization for the ocmod-dev-devops-test
```

Control plane deployment steps

1) Deploy Operator with Kustomization (Repo : TenantOperatorDeployment)

Create Namespace: - kubectl apply -k environments/ocmod-dev/envs

Deploy Operator - kubectl apply -k environments/ocmod-dev/devops-test

2) Create serviceAccounts, role and rolebinding (files provided by Swapnil)

- kubectl apply -f azure-devops-integration-sa.yaml -n ocmod-integration-2204
- kubectl apply -f role-admin.yaml -n ocmod-integration-2204 - kubectl apply -f rolebinding-azure-devops-integration.yaml -n ocmod-integration-2204

3) Create azure devops environment and service connection in TFS

4) Update Obm pipeline for new environment (Repo : OcModOnboardManager)

5) Deploy Obm

6) Import the collection and environment json in the postman (Repo : RestAPISamples, SaaS folder) Select the environment(preprod/prod) in postman

7) Before sending provision product request, make sure that obm is running only on the environment we want to create tenant for.

8) Send Postman requests (We can refer recording of Hemal's call for detail info) - send request for setup HMAC - In provision product, update the
user info and Tenant Name. - send request for provision product

9) check onboardmanager logs if it receives to messages from msg queue or not.

10) let's wait for ~10min for Tenant creation Check all applications are up and running in Tenant.

11) We will receive an email having link of LandingPage, Login to the application and assign User group and roles permissions.


# Helm Chart for Module Deployments

Helm is a package manager for Kubernetes. Helm makes managing the deployment of applications easier inside Kubernetes through a templated
approach. All Helm charts follow the same structure while still having a structure flexible enough to represent any type of application we could run on
Kubernetes.

Repository: https://tfs05mom.industrysoftware.automation.siemens.com/MOM/ModularMOM/_git/ModularMOM?path=%2FDeployment%2FKubernetes%
2FHelmCharts

```
Module Chart Structure
```
```
module/
Charts.yaml
values.yaml
Templates/
deployment.yaml
service.yaml
ingress.yaml
_helpers.tpl
```
module: Folder name of our module chart

chart.yaml: Metadata information of charts

values.yaml: Values for the template file. We have only kept placeholder in the values, we are setting the values during release with --set argument.

templates: folder to keep the actual template files for various Kubernetes resources. Currently we have template for creating deployment, service and
ingress. _helpers.tpl is default location for template partials, we can define named templates(helpers) here and they will be available everywhere within
other chart templates for use.

## Install Helm

Helm version: 3 (currently using in the project)

Please follow the link to intsall helm in your local machine. Install Helm

You can go through the link, it contains the most useful and common commands. Helm Commands

```
Helm Install Command
```
```
helm upgrade --install <<ReleaseName>> <<chart location>> --set <<values>> --atomic -n <<openshiftProject>>
```
upgrade: This command upgrades a release to a new version of a chart.

--install: if a release by this name doesn't already exist, run an install

--automatic: if set, upgrade process rolls back changes made in case of failed upgrade. It will wait until all Pods, PVCs, Services, and minimum number of
Pods of a Deployment, StatefulSet, or ReplicaSet are in a ready state before marking the release as successful. It will wait for as long as --timeout


# ModularMoM Helm Catalog

## Open points

ModularMOM helm catalog is currently available in the tfs05 ModularMOM repository mdp/helmcatalog branch. To merge current changes in the master
branch we should upgrade all ADO releases to yaml deployment since current changes are not compatible with ADO releases.

## Overview

ModularMOM helm catalog provides all the helm charts required to deploy and run ModularMOM application and components in a K8 cluster environment.

## How to build the docker image

To build the docker image for the ModMOM helm catalog:

```
Command
```
```
cd Deployment
```
```
docker build -f Containerization/dockerfile
```
```
--build-arg=HELMS_ROOT_PATH=..\Kuberetes\HelmCharts
```
```
--build-arg=TOOLS_ROOT_PATH=.\Containerization --build-arg=CHARTS_VERSION=1.0.0-beta.7
```
```
-t helmcharts:1.0.0-beta-7
```
```
docker run -d -p 8080:8080 helmcharts:<version>
```
```
docker container list
```
```
helm repo list
```
```
helm repo add modmom http://127.0.0.1:8080
```
```
helm search repo modmom
```
```
helm template modmom/<component>
```

How to release the docker image

```
Command Description
```
```
docker tag helmcharts:<tag> harbor.devops.sws.siemens.com/modular_mom/helmcharts:<version> tag for harbor
```
```
docker push harbor.devops.sws.siemens.com/modular_mom/helmcharts:<version> push to harbor. You may need to docker login
```
Local dry-run test for a specific helmchart

Prerequisites: docker/helm/kubectl

Add user to docker group

sudo usermod -G docker ita46821

Login/pull/run helmcharts catalog image from registry (The customer may have the helmcharts into another registry and need to pull with different url)

docker login harbor.devops.sws.siemens.com

docker pull harbor.devops.sws.siemens.com/modular_mom/helmcharts:1.0.0-beta.4

docker run -p 8080:8080 harbor.devops.sws.siemens.com/modular_mom/helmcharts:1.0.0-beta.4

Add the helm repo

helm repo add modmom [http://127.0.0.1:8080/](http://127.0.0.1:8080/)

Browse repositories

helm search repo modmom

Configure and use k8s cluster and context. Example commands to work with OpenShift swqa modmom cluster:

kubectl config set-cluster os3 --server=https://api.openshift03.swqa.tst:6443

kubectl config set-context os3ctx --cluster=os3 --user=ita46821 --namespace=modmom-06

kubectl config set-credentials --user ita46821 --token=...


kubectl config use-context os3ctx

Test install from helm repository

helm install tnt modmom/trackandtrace --dry-run --debug


# Resource requests and limits specifications

## Overview

To avoid resource capacity issues as teams and components grow, continuous deployment of ModularMOM components into OpenShift cluster
environments must be carefully tuned so that resource limits and request are optimized according to use cases and scenarios. We need to reach a
compromise between application resource requirements and actual usage.

To analyze K8s resource usage it is important to document how pipelines are currently deploying Pods, how resource limits and requests are configured
for each container versus how much resources we have in a specific cluster like OpenShift.

For more information on resources limits and requests: https://cloud.google.com/blog/products/containers-kubernetes/kubernetes-best-practices-resource-
requests-and-limits

## Resource allocation by environment

These are the deployment specifications for a ModularMOM K8 environment.

These specifications are configured in the K8 deployment HelmCharts and currently stored in the ModularMOM devops repository.

```
Deployment Pods Containers CPU Limits (m) CPU Requests (m)
```
```
redis
```
```
otel-collector 1 1 1
```
```
jaeger 1 1 n/a
```
```
grafana 1 1 200
```
```
prometheus 1 2 200
```
```
eks 1 n/a n/a
```
```
keycloak 150 250
```
```
sql-server 4 400 15
```
```
api-gateway 1 1 150 250
```
```
module 1 2 250 60
```
```
ui-module 1 2 60 35
```
```
TOTAL 1010 610
```
The specification may need to be customized according to the use case scenario (environment goal).

## Available resources in OpenShift cluster

To get the worker nodes available "oc get nodes" will list the available nodes.

The CPU capacity for each node is about: 8 CPU (7.5 allocatable). Because the OS cluster is currently equipped with 8 work nodes, this sums up to a total
of 60 CPUs.

To get the usage data "oc describe node <node>" will provide information on resource usage vs resource requests and limits.


# Delivery


# Monitoring


# Observability Tools

The current architecture of Observability tools deployed in OpenShift environment.

Logging: The logs are collected using Serilog Elasticsearch sink. The Serilog Elasticsearch sink project is a sink for the Serilog logging framework.
Structured log events are written to sinks and each sink is responsible for writing it to its own backend, database, store etc. This sink delivers the data to
Elasticsearch, a NoSQL search engine. It does this in a similar structure as Logstash and makes it easy to use Kibana for visualizing logs.

ElasticsearchJsonFormatter, custom json formatter has been used to covert the logs in Elasticsearch accepted format.

Serilog sinks Elasticsearch

Elasticsearch in kubernetes

Traces: Traces are collected using OpenTelemetry Collector. OpenTelemetry Collector is an executable that allows to receive telemetry data, optionally
transform it and send the data further. Each application is deployed with a Otel agent as sidecar which is collecting telemetry data and forwarding it to the
OpenTelemetry collector. The collector is forwarding the trace data to Jaeger and we can view the traces in Jaeger UI.

Elasticsearch has been configured as backed of Jaeger so traces are stored there and we can view the traces and query them using Kibana UI.

Opentelemetry Collector Architecture

Opentelemetry Collector Configuration

Jaeger Deployment

Recommended storage backend of Jaeger

For troubleshooting the following links are useful:

https://github.com/open-telemetry/opentelemetry-collector/blob/main/docs/troubleshooting.md

https://github.com/open-telemetry/opentelemetry-collector/blob/main/docs/performance.md

https://www.jaegertracing.io/docs/1.21/troubleshooting/

https://www.jaegertracing.io/docs/1.21/perf


https://www.jaegertracing.io/docs/1.21/monitoring/ormance-tuning/

Metrics: Metrics are enabled using prometheus-net library. Based on the values in environment variable EnableMetrics(true/false) system and http
metrics are enabled and exposed in /metrics api. Prometheus is configured to scrape applications in regular interval. The metrics and the status of each
target application can be viewed in Prometheus UI.

Grafana has been configured with Prometheus so we can monitor the applications with the visualization tools provided by the Grafana.

prometheus configuration

grafana-fundamentals


# Azure Build Machines and Agents Administration

## Overview

## Azure maintenance system-jobs

Windows build machines space is managed as follows:

## Azure maintenance custom-jobs

## Clean up agent dir task group

There are cases where a build/release pipeline takes lot of space and at the same time are over-used. In that case it may happen that the agent build
folder cannot be cleaned up by the scheduled jobs.

For this kind of build/releases it is advisable to add at the end of the pipeline the "Clean up agent working dir" task available as a task group in tfs05:

## Analyzing agents resource usage

Resource usage in Windows and Linux build machines can be monitored through the SUMO logic dashboard: https://siemens-disw.sumologic.com/ui/#
/dashboard/p7jBJJsocy7qsvHVcJp3H0lxAHY1GSzsIdF64gbxFdE1vG8HU39Vt7C4tb2i

The dashboards are connected to threshold monitors. Thresholds are tuned to send alerts on devops team channel whenever the used space is below an
average value defined according to capacity observations and issues.


If the alerts notifications start to be received too often (more than once a day) it might be a symptom that capacity is decreasing and investigation is
needed to restore normal conditions.

After capacity analysis, in case normal conditions could not be restored there are few options:

```
Decrease number of used agents: this way we run the risk to increase the queues on the agents
Lower the thresholds: this way we are defining a new lower limit for capacity which is not the best option
```
If these options cannot overcome the issues we need to raise the capacity limit by adding new resources to the build machines (memory/space)


# Enabling LongPath Support in Windows

Git repositories allow paths longer than 260 characters. Unfortunately, Windows does not support that by default.

The common system is that at build time you receive an error like this:

```
exceeds the OS max path limit. The fully qualified file name must be less than 260 characters
```
Follow this official guide to enable LongPath support in Windows:

https://docs.microsoft.com/en-us/windows/win32/fileio/maximum-file-path-limitation#enable-long-paths-in-windows-10-version-1607-and-later

In brief, you can achieve it by setting the value of the following key to 1.

```
Computer\HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Control\FileSystem\LongPathsEnabled
```
```
Some VDI templates have this value already enabled, so, you may find it already enabled.
```

# Git How To

## Tip #1: rename local and remote branch

```
branch renaming
```
```
# Rename the local branch to the new name
git branch -m <old_name> <new_name>
```
```
# Delete the old branch on remote - where <remote> is, for example, origin
git push <remote> --delete <old_name>
```
```
# OPTIONAL Or shorter way to delete remote branch [:]
git push <remote> :<old_name>
```
```
# Prevent git from using the old name when pushing in the next step.
# Otherwise, git will use the old upstream name instead of <new_name>.
git branch --unset-upstream <new_name>
```
```
# Push the new branch to remote
git push <remote> <new_name>
```
```
# Reset the upstream branch for the new_name local branch
git push <remote> -u <new_name>
```
## Tip #2: Checking out submodule branches

Sometimes when you try and checkout a submodule branch known to exist on the remote, the following error occurs.

```
ib9pfg@PL2USWAA0006NB MINGW64 /c/Work/ModMOM/M1_POC/Code/M1_Common ((df2778a...))
$ git checkout na/HistoryDetails
error: pathspec 'na/HistoryDetails' did not match any file(s) known to git
```
Even when you first fetch, the error still persists.

```
$ git fetch
```
```
ib9pfg@PL2USWAA0006NB MINGW64 /c/Work/ModMOM/M1_POC/Code/M1_Common ((df2778a...))
$ git show-ref
62612e42dee72de1f3519df2b2c2f92851ea66d9 refs/heads/list
62612e42dee72de1f3519df2b2c2f92851ea66d9 refs/heads/main
0bcb133ad9bee47a0d656fb58717169f2fc4faf1 refs/remotes/origin/HEAD
0bcb133ad9bee47a0d656fb58717169f2fc4faf1 refs/remotes/origin/main
3660904e93801be142107c88f6e0956eb611758a refs/tags/v1.0.7
```
Interestingly, you can checkout this branch by it's commit hash even though it's not present in the refs.

When this happens, the issue is the fetch pattern is incorrect. To set it issue the following command:

git config remote.origin.fetch "+refs/heads/*:refs/remotes/origin/*"

```
Before write some tips....first please test on your machine
```

Then fetch

```
ib9pfg@PL2USWAA0006NB MINGW64 /c/Work/ModMOM/M1_POC/Code/M1_Common ((df2778a...))
$ git config remote.origin.fetch "+refs/heads/*:refs/remotes/origin/*"
```
```
ib9pfg@PL2USWAA0006NB MINGW64 /c/Work/ModMOM/M1_POC/Code/M1_Common ((df2778a...))
$ git fetch
remote: Azure Repos
remote: Found 695 objects to send. (14 ms)
Receiving objects: 100% (695/695), 168.58 KiB | 396.00 KiB/s, done.
Resolving deltas: 100% (396/396), completed with 22 local objects.
From https://tfs05mom.industrysoftware.automation.siemens.com/MOM/ModularMOM/_git/M1_Common
* [new branch] ... several results removed for brevity
* [new branch] na/HistoryDetails -> origin/na/HistoryDetails
remote: Azure Repos
remote: Found 5 objects to send. (0 ms)
Unpacking objects: 100% (5/5), 821 bytes | 13.00 KiB/s, done.
* [new tag] v1.0.3 -> v1.0.3
* [new tag] v1.0.4 -> v1.0.4
* [new tag] v1.0.4.1 -> v1.0.4.1
Fetching submodule Tests/M1_Common.Tests
From https://tfs05mom.industrysoftware.automation.siemens.com/MOM/ModularMOM/_git/M1_Common.Tests
* branch c8fd881891249188b3dbe2b7daf616bdfac91ba0 -> FETCH_HEAD
* branch da58444bd7612ae9dc3b54768edb04f43684d201 -> FETCH_HEAD
```
```
ib9pfg@PL2USWAA0006NB MINGW64 /c/Work/ModMOM/M1_POC/Code/M1_Common ((df2778a...))
$ git checkout na/HistoryDetails
Switched to a new branch 'na/HistoryDetails'
Branch 'na/HistoryDetails' set up to track remote branch 'na/HistoryDetails' from 'origin'.
```
At this point, you can checkout the submodule branch successfully.

```
ib9pfg@PL2USWAA0006NB MINGW64 /c/Work/ModMOM/M1_POC/Code/M1_Common ((df2778a...))
$ git checkout na/HistoryDetails
Switched to a new branch 'na/HistoryDetails'
Branch 'na/HistoryDetails' set up to track remote branch 'na/HistoryDetails' from 'origin'.
```
Tip #3: Working with Git submodules.

```
#Clone a module & submodules to a new directory (M1_POC is used in this example)
$git clone https://tfs05mom.industrysoftware.automation.siemens.com/MOM/ModularMOM/_git/M1_POC
```
```
#Change to the repository directory
$cd M1_POC
```
```
#Update the submodules
M1_POC$ git submodule update --init --depth=1
```
```
#Your (main) branch is in a clean state.
```
```
#After some development, you want abandon your changes.
#If you've only made changes to the main module (not the submodules) then here are two approaches.
```
```
#First simply checkout again using dot (.)
M1_POC$ git checkout.
Updated 0 paths from the index
```

#The above does not always work, you can use reset with the --hard option - Note: This will delete all
uncommitted files!
#Using reset with the --soft option, reverts the branch back to the last commit and leaves your uncommitted
changes.
$ git reset --hard
HEAD is now at c295bd2 ...

#Checking that everything is clean
$ git status
On branch main
Your branch is up to date with 'origin/main'.

nothing to commit, working tree clean

#Sometimes your main module code is clean, but the submodules have changes you want to abandon.
$ git status
On branch main
Your branch is up to date with 'origin/main'.

Changes not staged for commit:
(use "git add <file>..." to update what will be committed)
(use "git restore <file>..." to discard changes in working directory)
(commit or discard the untracked or modified content in submodules)
modified: Code/M1_Common (modified content)
modified: Tests/M1_POC.Tests (modified content)

no changes added to commit (use "git add" and/or "git commit -a")

#In this case, to get back to a clean module and submodule, perform the following two commands in succession.
M1_POC$ git submodule deinit -f.
M1_POC$ git submodule update --init --depth=1

#Below is the result
M1_POC$ git submodule deinit -f.
Cleared directory 'Code/M1_Common'
Submodule 'Code/M1_Common' (https://tfs05mom.industrysoftware.automation.siemens.com/MOM/ModularMOM/_git
/M1_Common) unregistered for path 'Code/M1_Common'
Cleared directory 'Tests/M1_POC.Tests'
Submodule 'Tests/M1_POC.Tests' (https://tfs05mom.industrysoftware.automation.siemens.com/MOM/ModularMOM/_git
/M1_POC.Tests) unregistered for path 'Tests/M1_POC.Tests'

M1_POC$ git submodule update --init --depth=1
Submodule 'Code/M1_Common' (https://tfs05mom.industrysoftware.automation.siemens.com/MOM/ModularMOM/_git
/M1_Common) registered for path 'Code/M1_Common'
Submodule 'Tests/M1_POC.Tests' (https://tfs05mom.industrysoftware.automation.siemens.com/MOM/ModularMOM/_git
/M1_POC.Tests) registered for path 'Tests/M1_POC.Tests'
Submodule path 'Code/M1_Common': checked out '62612e42dee72de1f3519df2b2c2f92851ea66d9'
Submodule path 'Tests/M1_POC.Tests': checked out '6d44fd21df8dc63cb1d399bc49770cb9693f7791'

#Checking the status after the previous two commands shows the branch is in a clean state.
M1_POC$ git status
On branch main
Your branch is up to date with 'origin/main'.

nothing to commit, working tree clean

#Your module and submodule branches are now clean again.


### 1.

### 2.

# Monitoring Traces and Logs in Development Environment

```
Introduction
Monitoring Traces with Jaeger
Step 1: Install Jaeger
Step 2: Configure local environment variable
Step 3: Monitor traces with Jaeger UI
Monitoring Logs with Seq
Step 1: Install Seq
Step 2: Enable Seq backend
Step 3: Monitor logs with Seq UI
Step 4: Query produced logs
Step 5: Free disk space by configuring retention policies
```
## Introduction

This page contains installation and usage guides of lightweight backends to be used on local development environment.

Using these backends, developers will experience directly if the traces/logs inserted during development are helpful for troubleshooting and manual
exploratory tests of new functionalities.
This activity is mandatory to build up an effective set of traces and logs for our application.

## Monitoring Traces with Jaeger

## Step 1: Install Jaeger

```
To install and run Jaeger on your local development environment, do either of the following:
Follow the instructions for All-in-one executable: https://www.jaegertracing.io/docs/1.24/getting-started/#all-in-one
Download Jaeger binaries : https://www.jaegertracing.io/download/
Since platform is designed to work with default Jaeger ports, you can start locally the executable with this command line:
```
```
Start Jaeger all-in-one executable
```
```
jaeger-all-in-one --collector.zipkin.host-port=:9411
```
## Step 2: Configure local environment variable

Platform configuration for trace exporter relies on environment variables.

To properly configure Jaeger on local development environment, the following environment variables must be taken into account:

```
Environment variable Description Default value Required value to run jaeger locally
```
```
MODMOM_EXPORTER Determines which trace exporter will be used console jaeger
```
```
MODMOM_JAEGER_HOST Jaeger agent host localhost localhost
```
```
MODMOM_JAEGER_PORT Jaeger agent UDP port (using jaeger compact thrift protocol) 6831 6831
```
To leverage default values, configure only the MODMOM_EXPORTER environment variable.

## Step 3: Monitor traces with Jaeger UI

Once Jaeger back-end is started , access to Jaeger UI with this local URL: [http://localhost:16686](http://localhost:16686)

You can select traces:


```
from the Jaeger-query service when Jaeger back-end starts
from one or more microservices after running them.
```
By selecting a trace you can:

```
See all spans belonging to that trace in a tree view (see Figure 1: Trace spans visualization).
Compare the execution time occupied by each trace
Expand tree view elements
View tags associated with each span.
View SpanID information to correlate traces with logs ( see Figure 2: Correlation of traces and logs)
```
Figure 1: Trace spans visualization

Figure 2: Correlation of traces and logs:


### 1.

### 2.

To retrieve TraceID information, in the upper right corner select the Trace Timeline > Trace JSON command: the TraceID information is displayed in the
first row (see Figure 3: TraceID information)

Figure 3: TraceID information

Monitoring Logs with Seq

Step 1: Install Seq

Seq is a simple log collector for structured logs (free for individual installation). For more information, see https://datalust.co/seq

Procedure:

```
Download Seq https://datalust.co/download
Start automatically the Seq service on your machine and leave default values for required configurations.
```
Step 2: Enable Seq backend

Run Platform solution with Debug configuration in order to enable Seq backend.
You can run multiple Platform solutions in Debug configuration to simulate different microservices and see the produced logs in Seq UI.

Step 3: Monitor logs with Seq UI


Access Seq UI with this local URL: [http://localhost:5341](http://localhost:5341)

To see all contextual properties of a log message, expand it as displayed in the image below:

Step 4: Query produced logs

```
To Consult
```
```
Search for specific logs leveraging Seq query sintax https://docs.datalust.co/docs/the-seq-query-language
```
```
Store useful queries as signals in order to reuse them https://docs.datalust.co/docs/signals
```
Example: Store queries to filter only logs with high level, such as Errors or Warnings.

Step 5: Free disk space by configuring retention policies

Seq is a centralized log file: over time the file may grow and consume system resources. To get through this problem, use Retention setting to clear out
old logs.
From Retention setting, do either of the following:

```
Configure retention policies: https://docs.datalust.co/docs/retention-policies
Manually delete the displayed events
```
Figure 4: Retention Setting



# Production Environment Allocation

```
Environment Assignment Owner Usage
```
```
modularmom.mod.sws.siemens.com Demoing Bardini, Matteo Hegde , Ganesh Used mostly for demoing and showcasing
```
```
ocmod01.mod.sws.siemens.com Khairnar, Prashant Mendix (not to touch 'til June, 29th)
```
```
ocmod02.mod.sws.siemens.com SI BP Zug (FD) Schmidt, Jan Jaehnert, Christian Ralf Customer Scenario
```
```
ocmod03.mod.sws.siemens.com Validation Hegde , Ganesh Used by IndustrySolution for validating/testing the application.
```
```
ocmod04.mod.sws.siemens.com
```
```
ocmod05.mod.sws.siemens.com Morf3D Pottigar, Durgaprasad Anand Used for Morf3D development and showcasing.
```

# Openshift Internal Project Links

## Storage Systems

SQL Server on deployment

How to DROP Module's DB

Here follow the list of the current Openshift Internal Projects, their purpose and links

```
Purpose Management Name UI Database server
references
```
```
OpenID public endpoints
```
```
Integration https://console-openshift-console.apps.
openshift03.swqa.tst/topology/ns
/modmom-10/graph
```
```
modmom-10 http://modularmom.integration.
swqa.tst/#/LandingPage/home
```
```
http://modularmom.integration.
swqa.tst/auth/realms/modmom
```
```
Integration
AM
```
```
https://console-openshift-console.apps.
openshift03.swqa.tst/topology/ns
/modmom-07/graph
```
```
modmom-07 http://api-gateway-modmom-
07.apps.openshift03.swqa.tst/#
/LandingPage/home
```
```
172.21.8.148,32150 http://api-gateway-modmom-
07.apps.openshift03.swqa.tst
/auth/realms/modmom
```
```
Sinhagad https://console-openshift-console.apps.
openshift03.swqa.tst/topology/ns
/modmom-01/graph
```
```
modmom-01
```
```
Rajgad https://console-openshift-console.apps.
openshift03.swqa.tst/topology/ns
/modmom-02/graph
```
```
modmom-02
```
```
Shivneri https://console-openshift-console.apps.
openshift03.swqa.tst/topology/ns
/modmom-03/graph
```
```
modmom-03 172.21.8.79,31594
```
```
Monviso https://console-openshift-console.apps.
openshift03.swqa.tst/topology/ns
/modmom-04/graph
```
```
modmom-04 http://api-gateway-modmom-
04.apps.openshift03.swqa.tst
/auth/
```
```
QA
Infrastructu
re Team
```
```
https://console-openshift-console.apps.
openshift03.swqa.tst/topology/ns
/modmom-05/graph
```
```
modmom-05
```
```
DevOps https://console-openshift-console.apps.
openshift03.swqa.tst/topology/ns
/modmom-06/graph
```
```
modmom-06
```
```
Morf3D/Zug https://console-openshift-console.apps.
openshift03.swqa.tst/topology/ns
/modmom-07/graph
```
```
modmom-07
```
```
Sayhadri https://console-openshift-console.apps.
openshift03.swqa.tst/topology/ns
/modmom-08/graph
```
```
modmom-08
```
```
Mauna Kea https://console-openshift-console.apps.
openshift03.swqa.tst/topology/ns
/modmom-13/graph
```
```
modmom-13
```
```
Pratapgad https://console-openshift-console.apps.
openshift03.swqa.tst/topology/ns
/modmom-14/graph
```
```
modmom-14
```
```
Demo https://console-openshift-console.apps.
openshift03.swqa.tst/topology/ns
/modmom-15/graph
```
```
modmom-15
```
```
NFR https://console-openshift-console.apps.
openshift03.swqa.tst/topology/ns
/modmom-18/graph
```
```
modmom-18 ModularMOM/Deployment
/Openshift/Environments/mod
mom-18.MSSQL.vars
```
```
http://api-gateway-modmom-
18.apps.openshift03.swqa.tst
/auth/
```
Chimera / XCR

```
Purpose Management Name Example UI Test
User
```
```
Test
User
```
```
ClientId Client
Secret
```
```
OpenID public
endpoints
```
```
MVP
Test
```
```
mom-stage deprovisioned to preserve resources for 2204.1 release (see: De Pascale, Mauro (DI SW DM MOM R&D MM): mom-stage
deprovisioning
posted in Modular MOM 2.x / Development at giovedì 26 maggio 2022 13:59:36)
```

```
Name Password
```
Integration
(pre-prod
environment)

```
https://k8s.prod.us-east-1.kaas.sws.
siemens.com/dashboard/c/c-dvtct
/explorer/namespace/mom-
dev#conditions
```
```
mom-dev https://modularmom.mom-dev.
preprod01.prod.us-east-1.kaas.
sws.siemens.com
/materialmanagementui/index.html
```
```
dev1 modmom https://modularmom.
mom-dev.preprod01.
prod.us-east-1.kaas.
sws.siemens.com
/auth/realms
/modmom/.well-
known/openid-
configuration
```
```
MVP
Explorat
ory Test
```
Staging
(pre-prod
environment)

```
https://k8s.prod.us-east-1.kaas.sws.
siemens.com/dashboard/c/c-dvtct
/explorer/namespace/mom-
stage#conditions
```
```
mom-
stage
```
```
https://modularmom.mom-stage.
preprod01.prod.us-east-1.kaas.
sws.siemens.com
/materialmanagementui/index.html
```
```
dev1 modmom https://modularmom.
mom-stage.
preprod01.prod.us-
east-1.kaas.sws.
siemens.com/auth
/realms/modmom/.
well-known/openid-
configuration
```
```
NFR
and
automat
ed tests
```
Production
(production
environment)


# Postman Examples

Repo

Postman examples are stored in the following repo:

```
https://tfs05mom.industrysoftware.automation.siemens.com/MOM/ModularMOM/_git/RestAPISamples
```
```
please check the endpoint before running them, some endpoints may be local or pointing to the OpenShift cluster
```

### 1.

### 2.

### 3.

### 4.

### 5.

```
a.
b.
c.
6.
```
```
7.
```
# SF experts exchange topics

```
# Topic Comments Recording Proposer Date
```
```
Observability
```
```
Audit n trail Still being developed
```
```
Exception mapping with Rest Api
RestAPI ErrorMessages
```
```
19 Oct 2021
```
```
Exception Framework Deep Dive 21-10-2021
```
```
Grpc communication
```
```
Localization
```
```
Authentication:
```
```
UI Common module for authentication
API Gateway URI Module Mapping (for validation)
Deployment diagram of Keycloak and TLS considerations
MOMUser mappings
```
```
26 Oct 2021
```
```
28-10-2021
```
```
Platform and Common submodules version handling
```
```
Audit&Trail
```
```
for Modelling services
for Shopfloor services (coming)
```
```
Query Selection Values
```
```
18 Nov 2021
```
## General questions:

```
SonarQube: SonarQube code smell: "Make this field 'private' and encapsulate it in a 'public' property." for mom entities’ fields\properties.
Currently the properties must be declared as public (if we understood correctly). Instead of suppressing this rule in each file we could think to
suppress it for all the Ifield properties declaration, setting up a general rule. What do you think? To permissive or useless since in the future the
properties can be defined as private?
Troubleshooting: module(platform) versioning information at runtime. Wouldn’t be useful to have an endpoint which shows these information (or
to add them to the swagger ui).
Platform: (From Heggi)Is there a generic component to handle Logical Transaction? I would say no, since for the TNT poc a new entity has been
defined for this scope. (maybe already answered in the last BR0 I was not there).
Platform: Could be useful to define a generic component which hold the configuration information of the module (I mean OM,MM,TNT) and
provide it via M1_Common repo. A sort of ModuleRegistry every component can use to setup and handle its configuration data. With this Feature
24386 : “Perform Collect TP From Operator – API”
BDD shared steps: in order to setup generic test steps to produce\consume messages what do we need?I think we need these steps for both:
pipeline and integration tests since messages are part of the business feature (WIP Feature 24386: “Perform Collect TP From Operator – API”).
Just reference the platform nuget(s).
Create a minimal nuget package(s) with the necessary binaries
Clone the necessary code in the API automation library :(
BDD shared steps: could be possible to use the index page as a source of dto information to setup generic steps to create\delete\update
entities?
Platform: Q:Would be useful to clarify who is the owner\driver of the platform version information for a given modmom module (see also bug: Bug
25616 : ModMoM Module template is out of date (repos: M1_Template\.Tests))
A: see wiki page Platform & Model Versioning
```

# Solution Structure

The following image generated with Visual Studio explains the structure of the solutions:


### 1.

### 2.

### 3.

# SQL Server on deployment

In the deployment phase our modules need a persistency provider to be configured and accessible: currently, this feature is provided by a SQL Server
instance.

Up to now, the SQL server instance is configured using environment variables passed to the running module.

The information provided is:

```
MODMOM_DBHOSTNAME : the name of the host and instance to access.
MODMOM_DBNAME: the name of the module's database.
MODMOM_DBUSERNAME: the user to be used to access the database.
MODMOM_DBPASSWORD: the user password associated to the user above.
```
Additionally, the system uses a SA_PASSWORD, the password for the SQL Server sa user, required to perform administrative task on the server like
creting the databases, the logins and so on.

The SA_PASSWORD is not intended to be used by developers and users.

The SQL Server instances usually reside out of the Openshift environment into a vcloud space, this leads to some issues in provisioning and monitoring
resources 'cause the access and permissions on the vcloud are limited.

We decided to evolve the deployment design adding the SQL Server instance to the target environment so now SQL Server is deployed together with the
application in the same namespace: there will be one SQL Server instance for each environment.

The database, login and user are created using an initContainer image with the details as in the following:

```
MODMOM_DBHOSTNAME: sqlserver
MODMOM_DBNAME: ModMOMmodule_nameDb , but it really depends on the value provided to the deployer.
MODMOM_DBUSERNAME: VMUser
MODMOM_DBPASSWORD: Siemens$01
```
All the values above could be overridden providing specific values, but keeping the default will permit user to access the resources with ease.

## How to access the Openshift SQL Server instance

The SQL Server instance can be accessed locally from the SQL Server container terminal using the commandline tool.

```
i.e. oc exec -ti sqlserver-7866b95b7f-d5sxr -- sh
```
The command will open a shell on the sqlserver container, to use the commandline tool the command is:

```
/opt/mssql-tools/bin/sqlcmd -S localhost -U vmuser -P Siemens$01
```
It could also be accessed remotely using Microsoft SQL Server Management console, however to successfully connect the NodePort must be used.

The sequence is:

```
Identify the Openshift worker the SQL Server instance is running on.
Identify the NodePort where the SQL Server port is exposed.
Connect to the NodePort using the Microsoft SQL Server Management console.
```
Identify the Openshift worker the SQL Server instance is running on.

The worker a pod is running on can be identified through the Openshift UI going at the pod's Details tab and looking under the Node title.

It could also be retrieved using the commandline with the command:

```
oc describe <sqlserver-pod-instance>
```
for instance, the command could be: oc describe pod/sqlserver-7866b95b7f-d5sxr

Looking through the output, we should find a row in the form:

```
Node: worker4.openshift01.swqa.tst/172.21.8.29
```
The worker node could be identified both by its name (worker4.openshift01.swqa.tst) or its IP address (172.21.8.29).


Identify the NodePort where the SQL Server port is exposed.

The NodePort, again, can be identified through the Openshift UI going at the sqlserver service's Details tab and looking at the Service Port Mapping table
for "NodePort", usually its value is bigger than 30000.

It could also be retrieved using the commandline with the command:

```
oc describe service/sqlserver
```
Looking through the output, we should find the property NodePort, which provides the port number to connect to.

Connect to the NodePort using the Microsoft SQL Server Management console.

To Connect to the SQL Server instance deployed into the Openshift environment use the informations collected through the steps depicted above.

an example is available in the following image, password is Siemens$01, if not explicitly declared differently:

```
While connecting from SSMS use <nodename>.openshift03.swqa.tst
```

# SWF-6 UI Migration


# TimeZone issues - find a meeting slot

When feature teams are dispersed across geographies, the World Clock Meeting Planner is a helpful tool to use when deciding meeting dates and times:

https://www.timeanddate.com/worldclock/meetingtime.html?day=1&month=9&year=2021&p1=137&p2=179&p3=48&p4=285&p5=1038&iv=0


# Introducing Kustomize

```
Background
What is Kustomize and How to install it
kustomization.yaml file for base resources and overlays
kustomization
base
overlay
Example
```
## Background

This page will highlight the use cases of Kustomize to shape up the deployment of K8s components into different environment. This page will serve as a
board to write down the efforts being made to implement Kustomize.

## What is Kustomize and How to install it

Kustomize is a tool for customizing Kubernetes configurations. It has the following features to manage application configuration files:

```
generating resources from other sources
setting cross-cutting fields for resources
composing and customizing collections of resources
```
Kustomize can be installed as a standard binary and can be downloaded from here - https://github.com/kubernetes-sigs/kustomize/releases

## kustomization.yaml file for base resources and overlays

## kustomization

The term kustomization refers to a kustomization.yaml file, or more generally to a directory (the root) containing the kustomization.yaml file and
all the relative file paths that it immediately references (all the local data that doesn’t require a URL specification).

I.e. if someone gives you a kustomization for use with kustomize, it could be in the form of

```
one file called kustomization.yaml,
a tarball (containing that YAML file plus what it references),
a git archive (ditto),
a URL to a git repo (ditto), etc.
```
A kustomization file contains fields falling into four categories:

```
resources - what existing resources are to be customized. Example fields: resources, crds.
generators - what new resources should be created. Example fields: configMapGenerator (legacy), secretGenerator (legacy), generators (v2.1).
transformers - what to do to the aforementioned resources. Example fields: namePrefix, nameSuffix, images, commonLabels, patchesJson6902,
etc. and the more general transformers (v2.1) field.
meta - fields which may influence all or some of the above. Example fields: vars, namespace, apiVersion, kind, etc
```
## base

A base is a kustomization referred to by some other kustomization.

Any kustomization, including an overlay, can be a base to another kustomization.

A base has no knowledge of the overlays that refer to it.

## overlay

An overlay is a kustomization that depends on another kustomization.

The kustomizations an overlay refers to (via file path, URI or other method) are called bases.

An overlay is unusable without its bases.

An overlay may act as a base to another overlay.

Overlays make the most sense when there is more than one, because they create different variants of a common base - e.g. development, QA, staging
and production environment variants.

## Example


Below is the hello-world app deployment example to demonstrate basic kustomize usage

```
Directory Structure
```
```
kustomize-poc
base
configMap.yaml
deployment.yaml
kustomization.yaml
service.yaml
overlays
dev
map.yaml
kustomization.yaml
prod
kustomization.yaml
deployment.yaml
```
yaml files to demonstrate the POC -

```
kustomize-poc/base/kustomization.yaml
```
```
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
metadata:
name: arbitrary
```
```
commonLabels:
app: my-hello
```
```
resources:
```
- deployment.yaml
- service.yaml
- configMap.yaml

```
kustomize-poc/base/configMap.yaml
```
```
apiVersion: v1
kind: ConfigMap
metadata:
name: the-map
data:
altGreeting: "Good Morning!"
enableRisky: "false"
```
```
kustomize-poc/base/deployment.yaml
```
```
apiVersion: apps/v1
kind: Deployment
metadata:
name: the-deployment
spec:
replicas: 3
selector:
matchLabels:
deployment: hello
template:
metadata:
labels:
deployment: hello
spec:
containers:
```
- name: the-container
image: monopole/hello:1


command: ["/hello",
"--port=8080",
"--enableRiskyFeature=$(ENABLE_RISKY)"]
ports:

- containerPort: 8080
env:
- name: ALT_GREETING
valueFrom:
configMapKeyRef:
name: the-map
key: altGreeting
- name: ENABLE_RISKY
valueFrom:
configMapKeyRef:
name: the-map
key: enableRisky

kustomize-poc/base/service.yaml

apiVersion: v1
kind: Service
metadata:
name: the-service
spec:
selector:
deployment: hello
type: LoadBalancer
ports:

- protocol: TCP
port: 8666
targetPort: 8080

kustomize-poc/overlays/dev/kustomization.yaml

namePrefix: dev-
commonLabels:
variant: dev
org: ModularMOM
commonAnnotations:
note: Hello, We are in Dev Environment!
resources:

- ../../base
patchesStrategicMerge:
- map.yaml

kustomize-poc/overlays/dev/map.yaml

apiVersion: v1
kind: ConfigMap
metadata:
name: the-map
data:
altGreeting: "Have a pineapple!"
enableRisky: "true"

kustomize-poc/overlays/prod/kustomization.yaml

namePrefix: production-
commonLabels:
variant: production
org: ModularMOM
commonAnnotations:


```
note: Hello, We are in Production Environment!
resources:
```
- ../../base
patchesStrategicMerge:
- deployment.yaml

```
kustomize-poc/overlays/prod/deployment.yaml
```
```
apiVersion: apps/v1
kind: Deployment
metadata:
name: the-deployment
spec:
replicas: 10
```
Instruction to run kustomazation -

```
How To Generate Manifests using kustomize
```
```
swapnil.joshi@vdip053m045:~/my-workspace/kustomize-poc$ kustomize build overlays/dev/
```
```
swapnil.joshi@vdip053m045:~/my-workspace/kustomize-poc$ kustomize build overlays/prod/
```

# ModMOM - SqlServer

```
Directory Structure
Implementation
Overlay Changes
secret.yaml
statefulset.yaml
Future Scope
```
## Directory Structure

This below example directory structure and YAML files are created to demonstrate use of kustomize to render ModularMOMs SQLServer manifest files.

Kustomization templates are added place-to-place in order to have different set of values for different environments.

## Implementation

I will push the code to approved repository and add the link to this page.

## Overlay Changes

## secret.yaml

Password for different environment is added as an overlay change

## statefulset.yaml

updateStrategy and replicas are changed as per different environment needs.

## Future Scope


Considering the fact that ModularMOM applications may get deployed on different platforms, there is a scope to create Kustomization component called
"Component" in order to Kustomize SQLServer deployment into different platforms and its different environment.


# Modular MOM on Cloud

```
Background
Cloud Provider?
AWS Account Management
AWS Infrastructure Design
AWS Environment Selection / Management
AWS Cloud DevOPS / SRE Implementation
Infrastructure DevOPS
Application DevOPS
SRE Implementation
Technology Stack
```
## Background

This page will probably serve as a starting view point to look at OR to set up ultimate goal for Modular MOM product on Cloud. In this page we will
basically try to touch base every aspect of product development to deployment by following standard Cloud DevOPS practices.

Being a new joiner, my perspective is as generic as any other Cloud DevOPS Guy looking from external and of course there are Siemens specific
unknowns for me. I request all Veterans and every member of project to chip in to try and fill in gaps for every outlined unknowns below.

Please feel free to edit the document with every possible relevant information.

[Rocklin, Joe]: I am also a new person on the team, and not familiar with using a wiki as a conversation tool. I'm adding some of my thoughts as I have
time, and prefixed with my name tag as on this line.

## Cloud Provider?

```
Based on the conversations, it seems like we have decided to go ahead with AWS as a first-go cloud provider.
Considering long term approach, we would also need to consider exit strategy for our product. As part of exit strategy, we would need to consider
other than AWS cloud provider and start thinking on setting it up. Of course later on OR after making it live on AWS.
[Rocklin, Joe]: I am a firm believer in keeping the abstractions at the service level, and then choosing services that allow us to be
portable. For instance, if we need a database, let's pick a database that isn't provided by a single cloud vendor. Or, if we need something
like a queue or blob storage, we select services that have portable interfaces. That portability may be at the service provider level, as is
the case with S3/S3-compatible interfaces. But it may also happen at the application level, as might be the case with an abstracted
pubsub model that can use Kafka, MQTT, or SQS with a configuration change (and maybe a recompile). The point is: we should select
our cloud/managed services carefully.
```
## AWS Account Management

```
Has Siemens yet adopted AWS Organization Service as a whole? https://docs.aws.amazon.com/organizations/latest/userguide/orgs_introduction.
html
[Rocklin, Joe]: yes - there are a couple of different organizations we can fit under. I am most familiar with the organization managed by
Jason Powers. My understand is that he was handling all DISW AWS accounts.
If Yes, Then
We need to request an AWS Account from the team who is managing AWS Organization within Siemens.
To that account, we need to get DevOPS role based access ( generally an IAM role, which is used to login to an account using SAML
based tokens ) to setup Infrastructure.
We would also need to allow Service Control Policies ( SCPs ) for the account to use desired AWS Services as per the application
infrastructure design.
If No, Then
We need to check if there are any discussions/engagements happening at Siemens as an Organization level to adopt AWS Cloud and
Adopt AWS Organization Service.
While googling out, I could find out this page on AWS engagement - https://new.siemens.com/global/en/products/services/iot-siemens
/aws-partnership.html not sure though, what level of engagement we have established yet!
If not, we can ask AWS Standard Support for engagement with Siemens.
```
## AWS Infrastructure Design

```
As an architectural effort, to setup an application on AWS ( effectively on any cloud), we should first have ( OR prepare) complete Infrastructure
Design Document which serves as a starting point to start implementing / deploying on to cloud.
Infrastructure Design should consider below parameters while opting out services -
Application Native Platform (Java Based Application, .NET Application, NodeJS, Python)
If Strategic decision is to go for Containerization - We can leverage Kubernetes Service ( AWS EKS ) OR Elastic Container Service (
AWS ECS )
[Rocklin, Joe]: I believe the decision was made to leverage the Chimera services as a management kubernetes layer. This is a
service provided internally by the CTO organization. The idea here is that we have that team manage the kubernetes layers and
allow us to focus more on the application side of things.
Connectivity Patterns for Application like -
Ingress to Applications - From Internet OR From Siemens on-premise n/w OR From Other AWS account ( in case of interfacing
applications )
```

```
Egress from Application - To Internet OR To Siemens on-premise n/w OR To Other AWS account ( in case of interfacing
applications )
Application Supported Components like - Databases (LDAPs, DBs? ), Key Management ( HSMs? KMS? )?
[Rocklin, Joe]: I believe the decision was made for the December target to leverage RDS for the database. I hope we can stay
far away from LDAP because the Siemens/DISW LDAP setup is a mess and running our own is probably not going to be fun.
The Chimera team has a keycloak server that will be used to access the k8s clusters and it is tied to the Siemens MyID service
as a backend. We can have some group admin capabilities if we need to with this service. It may not be a permanent solution,
but it also might work for what we need (though, I'm not entirely sure just what we need at this point).
Key management is another interesting one. I think we will probably end up with a combination of services to handle the
encryption of databases and other cloud-specific services. Also available for us to use is a managed instance of Hashicorp's
Vault (provided by the Chimera team). 'Seamless' integration of Vault into K8s is possible though currently blocked by an issue
awaiting resolution from Rancher.
```
AWS Environment Selection / Management

```
We need to determine Path-To-Live for going live and run production grade workloads on Cloud Network.
Generally as a standard practice, we can have different AWS Accounts which serves as a different environments for our product applications and
are categorised mainly as -
Development Account (Playing Ground for Application Devlopment and DevOPS Teams to test out different PoCs)
PrePROD Account (Live Proving Account for deploying Fully automated application as well as Infrastructure Code using standard
DevOPS Pipeline )
Production Account ( Actual Live Account Service Production workloads )
```
AWS Cloud DevOPS / SRE Implementation

```
As a fundamental principal, while working on cloud, we should consider zero-manual approach.
Looking from the Top, Cloud DevOPS Implementation mainly falls into two main types -
Infrastructure Deployment Automation using IaaC ( Terraforms ) and DevOPS Pipeline.
Application Build, Containerization and Deployment CI/CD DevOPS Pipeline
```
Infrastructure DevOPS

```
Infrastructure Automation can be achieved using standard Infrastructure-as-a-Code tool, mainly Terraform.
Terraforms can be scripted in modularized fashion in order to support code simplicity and consistency throughout environments
```
Application DevOPS

```
Standard Application Build and Release Deployments CI/CD workflow Pipeline
```
SRE Implementation

```
SRE implementation would take care of observability ( monitoring and alerting and logging ) mechanisms of both Infrastructure and Application.
Kubernetes platform components can be monitored effectively using Datadog. Have we already adopted it? OR is there any other tool adopted
instead?
What is Siemens' standard alerting mechanism for Production workloads?
What is standard logging tool adopted in Siemens?
```
Technology Stack

```
Cloud Platform - AWS
Containerization - Docker and Kubernetes
Infrastructure-as-a-Code Tool - Terraforms, Ansible
Source Control - Github
Integration - Jenkins
Programming -
HCL - Terraforms
YAML - Helm Charts for K8s Components Automation and Ansible Playbooks
[Rocklin, Joe]: I would also highly encourage looking at kustomize for K8s things. It can be a lot simpler than helm charts in
some cases and can also perform 'last-mile' helm chart customizations.
Groovy - Jenkins Pipeline
Monitoring Tools - Datadog
```

### 1.

### 2.

### 3.

### 4.

# How to run BDD UI against deployed environment

Below are the steps required the run BDD UI test cases against deployed environment (openshift):

```
Make changes in configuration.json (located under the folder test\cucumber\src\Configurations) like:
{
"serviceHost": "http://api-gateway-modmom-14.apps.openshift03.swqa.tst/ordermanagement/",
"grant_type": "client_credentials",
"client_id": "modmom",
"client_secret": "2cac2947-5647-4c17-8b70-3110294503bd",
"scope": "client_cred",
"token_url": "http://keycloak-modmom-14.apps.openshift03.swqa.tst/auth/realms/modmom/protocol/openid-connect/token",
"launchUrl": "http://api-gateway-modmom-14.apps.openshift03.swqa.tst/",
"username": "test",
"password": "1234",
```
```
"samAuthBasedLogin" : "false"
```
### }

```
Make changes in launch.json (located under the folder .vscode) like:
```
```
"args": [
"--browser",
"Chrome",
"--url",
"http://api-gateway-modmom-14.apps.openshift03.swqa.tst/"
],
Add any tag above the BDD UI scenario in feature file like:
```
```
@test
@ClearProductionOrderAfterCSVImport
Scenario: Validate successful CSV Import
Given Order Management UI is open
Run the command: npm run cucumber --tags @test from cmd
```
## Note

serviceHost, launchUrl, client_secret, token_url are subject to change if you try to run BDD UI on different pod. So make sure you have correct details
there.

For all the test cases user will redirect to the landing page after login.

BDD UI Test scenarios must be marked with the tag "@keycloakstandalone" into the feature file, in order to be executed by the nightly pipeline
'AllBizModules_BDD API & UI Test Execution on OpenShift' on Deployed scenarios.
BDD UI Test scenarios must be marked with the tag "@samauth" into the feature file, in order to be executed by the nightly pipeline 'AllBizModules_BDD
API & UI Test Execution on XCR' environment where sam-auth assigned.


### 1.

# Executing BDD API Tests with AspNetCore Test Server

For insuring your new code changes are bug free or not adding any new regression in existing code, system provide new policy to 'Execute BDD API Test
on Isolated Module'.

This policy trigger new release pipeline to 'Execute BDD API tests with AspNetCore Test server', which runs all BDD API tests and provide results to verify
that you code is not introduce new regression.

You can merge, your 'Pull Request' on master branch, only when this policy mark as passed, as after verifying that all BDD API tests are passing.

There are two workflows to execute BDD API Tests with AspNetCore Test Server:

```
Executing BDD API Tests with AspNetCore Test Server in the Release Pipeline
```
This is new implementation, which provide system default feature to 'Execute BDD API tests with AspNetCore Test server' in release pipeline, which
involves to set build machine to execute these BDD API tests and provide execution results for BDD API tests.

2.Executing BDD API Tests with AspNetCore Test Server on VDI

This is new implementation, which provide system default feature to 'Execute BDD API tests with AspNetCore Test server' on your VDI, which
involves to setup for VDI to execute these BDD API tests and provide execution results for BDD API tests.

Test scenarios must be marked with the tag "@AspNetCoreTestServer" into the feature file, in order to be executed by the release pipeline 'Execute BDD
API tests with AspNetCore Test server'.

Note: only tests that can be executed in "isolated" scenario can be tagged with "@AspNetCoreTestServer" that is, tests that do not require interaction with
other modules.
Test scenarios must be marked with the tag "@DeployedSystem" into the feature file, in order to be executed by the nightly pipeline 'AllBizModules_BDD
API & UI Test Execution on OpenShift' on Deployed scenarios.
Both tags can be specified for the same scenario.
Test scenarios that verify if a Grpc message is sent out must be marked with the tags "@AspNetCoreTestServer" and "@GrpcServer".
Only listed above tags are allowed. Other tags (suche as @Isolated, @excludeAspNetCoreTestServer) are to be considered obsolete and cannot be used.

Executing BDD API Tests on VDI, user can choose which kind of tests should be executed, through the "CurrentLicensingScenario" parameter defined in
the <ModuleName>.API.Tests.json file.
If a scenario doesn't have any tag associated in the feature file, it is always executed.
Otherwise, if it has at least one tag, it is executed only if one of his tag match with the one configured into the <ModuleName>.API.Tests.json file.

Note: Test executed with AspNetCoreTestServer will raise an exception if an http return code different from 200 is returned. Every Step Definition must not
have try-catch block. Only Step Definition expecting error which need to be verified, should have try-catch block.
In general case, the framework will raise exception and will fail the TC.


### 1.

### 2.

### 3.

### 4.

### 5.

# Executing BDD API Tests with AspNetCore Test Server in

# the Release Pipeline

## Procedure

```
Creating 'Pull Request' for parent module, you will find new default policy for 'Execute BDD API Test on Isolated Module' will added by system to
run BDD API tests with ASPNetCore Test server;
Triggering of 'Release pipeline' for that Pull Request executed automatically, once build pipeline process completed by system;
During execution of this release pipeline, all respective tasks should succeeded/passed, then only 'Pull Request' policies marked as Passed and
you can mark your 'Pull Request' as 'complete';
Do either of the following:
If any of release pipeline tasks failed to execute: Check logs and if issue found related to build machine, re-trigger deployment of release
pipeline;
If release pipeline failed for task 'Execute BDD API Tests': Check logs for details where you can find which BDD API test failed, due to
your new changes; Details related to tests failed can be found on the Tests tab: clicking on the failed test, the Error message and the
Stack trace will be shown.
If BDD API test case fails: Debug this failure on your VDI as described in Executing BDD API Tests with AspNetCore Test Server on VDI
For submitting updated/modified changes in 'Pull Request', follow steps 2 and 3;
```
## Result

```
All BDD API tests are passing in release pipeline, before merging 'Pull Request' in main branch.
No new regression or bug will introduce through new 'Pull Request' submission.
```
```
Executing BDD API Tests on Isolated Module, while merge 'Pull Request' on main branch.
```

### 1.

### 2.

```
a.
b.
```
# Executing BDD API Tests with AspNetCore Test Server on

# VDI

## Prerequisites

```
Clone 'Material Management' module repos and modify below mentioned (in 'Procedure' section) the Test submodule files, which are available at
'M1_MaterialManagement\Tests\M1_MaterialManagement.Tests\ APIAutomationTests\M1_MaterialManagement.API.Tests';
To generate model dlls below are two methods, you can use any one method from below methods:
```
1. Generating model dlls on VDI (manually), see Generating New Models
2. Copying model dlls from the business services' docker images on the Docker Harbour Registry

```
Check docker already installed and running on your VDI and create 'C:\Generatedmodels' folder;
Open Windows Powershell as administrator pointing to path "C:\Generatedmodels\" and run below commands one by one
(Note: example is for materialmanagement model dlls):
```
```
docker login lanterna.industrysoftware.automation.siemens.com
docker pull lanterna.industrysoftware.automation.siemens.com/modularmom20-dev/modmom/materialmanagement:
latest
$myVar = docker create lanterna.industrysoftware.automation.siemens.com/modularmom20-dev/modmom
/materialmanagement:latest
$dllPath = $myVar + ":/app"
mkdir modelDllDir
docker cp $dllPath modelDllDir
```
```
c. Once all above commands run successfully, your current directory updated with new directory as 'modelDllDir', where in 'app'
subfolder, you can found respective model dlls files.
```
## Procedure

1. Add to your VDI the System Environment Variables specified in the appsettings.json file in order to configure your local DB parameters connection:

"Database": {
"DbName": "$$MODMOM_DBNAME$$",
"DbHostName": "$$MODMOM_DBHOSTNAME$$",
"DbUserName": "$$MODMOM_DBUSERNAME$$",
"DbPassword": "$$MODMOM_DBPASSWORD$$"}

If you specify "Trusted_Connection" for the "MODMOM_DBUSERNAME" variable you don't need to specify the password.

2. In the modelsettings.json file add your local model dlls paths where you have generated or copied model dlls from docker (see section below ):

{

"ModularMOM": {
"Metadata": {
"domainmodellocation": "C:\\Generatedmodels\\modelDllDir\\app\\_models\\M1_MaterialManagement.dll",
"inputmodellocation": "C:\\Generatedmodels\\modelDllDir\\app\\_models\\Siemens.MOM.MaterialManagement.Dto.dll",
"inputmodelnamespace": "Siemens.MOM.MaterialManagement.Dto",
"requestmodelnamespace": "Siemens.MOM.MaterialManagement.Request",
"responsemodelnamespace": "Siemens.MOM.MaterialManagement.Response",
"writemodellocation": "C:\\Generatedmodels\\modelDllDir\\app\\_models\\Siemens.MOM.MaterialManagement.Write.dll",
"writemodelnamespace": "Siemens.MOM.MaterialManagement.Write",
"messagemodellocation": "C:\\Generatedmodels\\modelDllDir\\app\\_models\\Siemens.MOM.MaterialManagement.Proto.dll"
},
"ModelInstrumentation": {

```
Executing BDD API Tests on Isolated Module on VDI.
```

"Providers": [
"M1_MaterialManagement"
]
}
}
}

If you have generated model dlls manually, update modelsettings.json file with respective paths, for more details available on: Configuring Platform API

{
"ModularMOM": {
"Metadata": {
"domainmodellocation": C:\\M1_OrderManagement_GeneratedModels\\bin\\M1_OrderManagement.dll",
"inputmodellocation": "C:\\M1_OrderManagement_GeneratedModels\\bin\\M1_OrderManagement.Dto.dll",
"inputmodelnamespace": "M1_OrderManagement.Dto",
"requestmodelnamespace": "M1_OrderManagement.Request",
"responsemodelnamespace": "M1_OrderManagement.Response",
"writemodellocation": "C:\\M1_OrderManagement_GeneratedModels\\bin\\M1_OrderManagement.Write.dll",
"writemodelnamespace": "M1_OrderManagement.Write",
"messagemodellocation": "C:\\M1_OrderManagement_GeneratedModels\\bin\\M1_OrderManagement.Proto.dll"
},
"ModelInstrumentation": {
"Providers": [
"M1_OrderManagement"
]
}
}
}

3. Change 'AspNetCoreTestServer' details in the file M1_MaterialManagement.API.Tests.json:

{
"CurrentLicensingScenario": "AspNetCoreTestServer",
"LicensingScenarios": [
{
"Name": "AspNetCoreTestServer",
"Environment": "OnPremise-QA"}

4. Run BDD API Test by going to Test explorer, running all BDD API tests and checking all should passed.
5. In case of BDD API test failure, debug and resolve issue and then re-run tests.

Result

```
All BDD API tests are passing on VDI and when you submit these new changes in 'Pull Request', expected to pass 'Release Pipeline' policy to
mark 'Pull Request' complete;
Avoid regression/bug to introduce through your new changes, while merging 'Pull Request' on master branch.
```
Advanced configuration

Here you can find some hints for Advanced configuration for Executing BDD API Tests with AspNetCore Test Server on VDI.

```
Note
```
```
Editing the modelsettings.json file, be particularly careful to:
```
```
the value provided for parameters named <xyz>location:
the path must specify the folder where the model dlls are present;
it could be an absolute path, or a relative path (starting from the folder where the modesettings.json itself resides).
the value provided for parameters named <xyz>namespace
the namespace must be specified accordingly to the names of the model dlls;
the value should be different if the model dlls are generated manually on your VDI or if they are copied from the business
services' docker images.
```

### 1.

### 2.

# Advanced configuration for Executing BDD API Tests with

# AspNetCore Test Server on VDI

## Settings for Debug session

If you need to debug Automation Framework or Platform code, from your BDD API Test project, then you need to:

```
generate related NuGet packages locally
configure the location of local NuGet Packages from the NuGet Manager settings.
```
An example of the command to generate NuGet package for MomAutomationFramework is

dotnet pack -p:PackageVersion=0.0.3-master-9999-001 Siemens.Mom.MomAutomationFramework.csproj

This sample command must be executed from a Command Prompt window, starting from folder where 'Siemens.Mom.MomAutomationFramework.csproj'
is located.

The package will be generated under the Bin\Debug o Bin\Relase folder of the project.

Note: each time you modify the source code, you need to generate a new NuGet package increasing his version number (i.e. from 0.0.3-master-9999-001 t
o 0.0.3-master-9999-002).

Then you have to had this folder to the "Package sources" of Visual Studio, from the Menu Tools => Nuget Package Manager => Package Manager
Settings

After that, you need to add the reference to your local NuGet Package, installing it through the Menu Tools => Nuget Package Manager => Manage NuGet
Packages for Solution, selecting the Package source folder configured at previous step.



# BDD API Tests: projects structure

## API.Tests project

M1_<ModuleName>.API.Tests is the main project where developers can write test cases through the SpecFlow feature file.

## API.StepDefs

M1_<ModuleName>.API.StepDefs is the project where developers can implement the step definition.

For both project, when a PR for the Repo complete, a Nuget Package will be created and stored on ModMom2 feed.

In this way, test project of business modules can import steps defined into other test projects, by importing the StepDefs Nuget Package.

```
Note
```
```
API.Tests project is referencing the dependencies.props file of the main business module project, in order to include the same package
refences of the business module project itself.
```

# Executing BDD API Tests from VDI on deployed system

## Prerequisites

The prerequisites are the same as described for Executing BDD API Tests with AspNetCore Test Server on VDI.

## Procedure

1. Configuration of VDI System Environment Variables and appsettings.json file: see Procedure described for Executing BDD API Tests with
AspNetCore Test Server on VDI (point 1.)
2. Configuration of modelsettings.json file: see Procedure described for Executing BDD API Tests with AspNetCore Test Server on VDI (point 2.)
3. Configuration of M1_<ModuleName>.API.Tests.json file (i.e. M1_MaterialManagement.API.Tests.json):
- Set the value "DeployedSystem" for the parameter "CurrentLicensingScenario"
- Set the value "DeployedSystem" for each parameter "LicensingScenarios"
- Configure the url for each parameter "HostName" listesd in the "Servers" section

{
"CurrentLicensingScenario": "DeployedSystem",
"LicensingScenarios": [
{
"Name": "DeployedSystem",
"Environment": "OnPremise-QA"
}
],
"Environments": [
{
"Name": "OnPremise-QA",
"Default_API_Timeout_Seconds": 30,
"Servers": [
{
"Name": "MaterialServer",
"Protocol": "http",
"HostName": "api-gateway-modmom-05.apps.openshift03.swqa.tst/materialmodelingapi"
},
{
"Name": "FactoryServer",
"Protocol": "http",
"HostName": "api-gateway-modmom-05.apps.openshift03.swqa.tst/factorymodelingapi"
},
{
"Name": "OrderServer",
"Protocol": "http",
"HostName": "api-gateway-modmom-05.apps.openshift03.swqa.tst/ordermanagementapi"
},
{
"Name": "TokenServer",
"Protocol": "http",
"HostName": "keycloak-modmom-05.apps.openshift03.swqa.tst"
},
{
"Name": "TrackAndTraceServer",
"Protocol": "http",
"HostName": "api-gateway-modmom-05.apps.openshift03.swqa.tst/trackandtraceapi"
},
{
"Name": "UserServer",
"Protocol": "http",
"HostName": "api-gateway-modmom-05.apps.openshift03.swqa.tst/accessmanagementapi"
},
{
"Name": "UserManagement",
"Url": "api",
"Server": "UserServer",
"APITimeoutSeconds": 30
}
],
"MicroServices": [
{
"Name": "MaterialManagement",
"Url": "api",
"Server": "MaterialServer",


"APITimeoutSeconds": 60
},
{
"Name": "Factory",
"Url": "api",
"Server": "FactoryServer",
"APITimeoutSeconds": 30
},
{
"Name": "OrderManagement",
"Url": "api",
"Server": "OrderServer",
"APITimeoutSeconds": 60
},
{
"Name": "TokenService",
"Url": "auth/realms/modmom/protocol/openid-connect/token",
"Server": "TokenServer",
"APITimeoutSeconds": 60
},
{
"Name": "TrackAndTrace",
"Url": "api",
"Server": "TrackAndTraceServer",
"APITimeoutSeconds": 60
}
],
"Users": [
{
"Name": "thirdpartyclient"
},
{
"Name": "modmom"
}
],
"ServiceAccount": {
"Name": "modmom"
},
"Roles": [
{
"Name": "Administrator",
"Users": [ "modmom" ]
},
{
"Name": "AutomationRole",
"Users": [ "thirdpartyclient" ]
}
]
}
],
"Users": {
"Cognito": [
{
"AccessKey": "",
"SecretKey": "",
"RegionName": "",
"PoolId": "",
"AppClientId": "",
"CognitoUser": "",
"CognitoUserPwd": "",
"LoginTimeout": "20000"
}
],
"Custom": [
{
"UserName": ""
}
],
"OAuth": [
{
"ClientId": "modmom",
"ClientSecret": "24454cec-d80d-4037-8cf0-8e4fb5bfe333",
"Scope": "client_cred"
},
{
"ClientId": "thirdpartyclient",
"ClientSecret": "clientsecret",
"Scope": "ModularMOM.ServiceClient"
}


### ]

### }

### }

4. Run BDD API Test by going to Test explorer, running all BDD API tests and checking all should passed.
5. In case of BDD API test failure, debug and resolve issue and then re-run tests.


### 1.

### 2.

### 3.

### 4.

```
a.
b.
```
### 1.

### 2.

# Backend Localization

This procedure describes how to support localization of a Modular MOM module. The examples described below regard the OrderManagement module (M
1_OrderManagement repository).

## Prerequisite

Minimum requirements of module dependency:

```
Platform version at least 2.1.10 or higher
Common version at least 2.1.15 or higher
```
## Preliminary operations

```
Clone the MetadataRuntime repository (version at least 2.1.10)
Run the BuildAll.ps1 script from the MetadataRuntime repository
Clone the module repository (e.g M1_OrderManagement)
For the extended modules, check whether ModelSharedResources.cs is correctly filled (and open a bug if necessary).
```
```
For the current module:
```
```
Open the ModelSharedResource.cs file at path: ...\CurrentModule\Code (e.g c:\git\M1_OrderManagement\Code)
The namespace name must be the same of the field name present in the file: modulename.modelconfig.json (e.g.
M1_OrderManagement_modelconfig.json)
```
## Further operations for Extended Modules

```
Add the folder Resources in the module path: ModulePath\Code\Resources
If not present, add the code below in the module csproj
```
```
module project csproj
```
```
<ItemGroup>
<EmbeddedResource Remove="Resources\**" />
</ItemGroup>
<ItemGroup>
<None Include="Resources\ModelSharedResources.*.resx">
<CopyToOutputDirectory>PreserveNewest</CopyToOutputDirectory>
</None>
</ItemGroup>
```
```
In case of extended module, repeat the code line below for each submodule using a single repo where submodule_1 is the name of the first
submodule and submodule_2 is the name of the second submodule.
```
```
<ItemGroup>
<!--
The following line must be repeated for each submodule using a single repo.
This is done automatically by the Add-Submodule.ps1 script
-->
<!-- <EmbeddedResource Remove="SUBMODULE_1\Code\Resources\**" /> -->
<EmbeddedResource Remove="SUBMODULE_2\Code\Resources\**" />
<EmbeddedResource Remove="M1_Common\Code\Resources\**" />
</ItemGroup>
```
## Supporting new target languages

```
Build the module solution (e.g. M1_OrderManagement.sln)
Run the script GenerateModels.cmd located in the Code\M1_Common\Script folder (e.g C:
\git_source\M1_OrderManagement\Code\M1_Common\Scripts) with the following parameters:
```

### 2.

### 3.

### 4.

### 5.

### 1.

### 2.

### 3.

```
GenerateModels.cmd SourceModelRootpath Config TargetGenerationPath TargetModelName
MetaDataRuntimeRoot SkipGeneration TargetModelNamespace
```
```
Parameter Description
```
```
Source Model Root path Root folder of the repository
```
```
Config set to Debug or to Release
```
```
TargetGenerationPath The path where we want to the generated the models
```
```
TargetModelName Name of the Model to be generated
```
```
MetaDataRuntimeRoot Root folder of the MetadataRuntime repository
```
```
SkipGeneration set to Gen
```
```
TargetModelNamespace Namespace of the model
```
```
Example for M1_OrderManagement:
```
```
CreateModels.cmd C:\git_source\M1_OrderManagement Debug C:
\git_source\MetadataRuntime\Platform\GeneratedModels_M1_OrderManagement M1_OrderManagement C:\git_source\MetadataRuntime Gen
Siemens.MOM.OrderManagement
```
```
Collect the file ModelSharedResources.resx for the standard language (English) produced by the model generator in the previous step and get
the resx file translated in the supported culture: the resx file for the standard language can be found in: TargetGenerationPath\GeneratedModels_
RepoName>\TargetModelNamespace.Common.
```
```
Example for M1_OrderManagement:
C:\git_source\Metadataruntime\Platform\GeneratedModels_M1_OrderManagement\Siemens.MOM.OrderManagement.Common
```
```
Save the resx file for the new supported culture with name: ModelSharedResources.culture.resx (e.g. ModelSharedResources.de-DE.resx) in a
dedicated folder of the module: ModulePath\Code\Resources
```
```
Example for M1_OrderManagement:
```
```
C:\git_source\Metadataruntime\Platform\GeneratedModels_M1_OrderManagement\Siemens.MOM.OrderManagement.Common
```
```
Commit and push the changes for the resx file.
```
Testing the culture on Windows development VDI

```
Publish the module:
```
```
dotnet publish "%ModelRootDir%\code\%ModelName%.csproj" -f %frameworkVersion% -c %Configuration% -o "%
TargetGenerateDir%\bin" -v q
```
```
Run the model generator for the current module.
Run the platform on the target generation path.
```
```
Culture suffix and Linux policy
```
```
The "culture" suffix of ModelSharedResource is a combination of an ISO 639 two-letter lowercase culture code associated with a
language and an ISO 3166 two-letter uppercase subculture code associated with a country or region (see
```
```
Copy-Item (Microsoft.PowerShell.Management) - PowerShell | Microsoft Docs ), therefore, the culture is case sensitive, for example,
for German (Germany) the culture is de-DE.
```
```
Be aware that Linux operating system is case sensitive
```

### 1.

### 2.

### 3.

### 1.

### 2.

### 3.

# Localization(L10N) support provided by Siemens Web

# Framework (SWF)

## What strings needs to be localized?

We are localizing all the visible strings in UI. It could be table header, Page Title, Tile names, Tab names etc.

## How Localization works?

Instead of using static string directly, one has to declare the string in respective module's localization file as a key value pair as following. This localization
file present under "i18n" folder.

```
trackAndTraceUIMessages.json
```
### {

```
"tabMaterials": "Materials",
"tabHistory":"History",
}
```
In above Example, "tabMaterials" is the locale key, and "Materials" is the English value for the given key. going forward one has to use locale key instead
of static value.

Above file is for English translation, similarly we do have to provide translation files for other supported languages also. Translation Team provides us this
translation files.

Filename for these translation file should have their respective country code appended to the file name. i.e. for German language's translation file name
would be trackAndTraceUIMessages_de.json. for all the supported list of country and their country code please refer SWF wiki.

## Steps to localize any component's label (i.e. button's label, textbox's label)

```
Open localization file we were discussing in previous section, declare the locale key - value pair for your string. i.e for 'Open' button you would
declare locale key as 'openButton' and 'Open' as value.
Open model file of respective component, go to data section. And the replace the label string ('Open') with "{{i18n.openButton}}.
In the same model file, declare the locale key in 'i18n' section with it's localization file name.
```
Note: Localization is not limited to component's label only, one can use them directly in html or can be passed to JS function as argument.

## How to test localization?

Let's say you wants to test your UI in German Language.

```
For that make sure that UI has localization file for German Language.
Change your browser display language to "German", and relaunch the Browser
browser URL for your UI, now your UI should in German language. ( if this step doesn't work due to some catching issue, try in incognito mode)
```
## References

https://gitlab.industrysoftware.automation.siemens.com/Apollo/swf/-/blob/master/documentation/tutorials/localization.md#step-1-localizing-command-text


# Viewing Logs from OpenShift with Kibana for Dummies

```
Open Kibana on https://kibana-openshift-logging.apps.openshift03.swqa.tst/app/kibana#/home?_g=()
if it asks you for index pattern, insert "log*" in the first step and "timestamp" in the second step
go to the "Discover" tab on the left hand side
now you need to add filters. You'll want to select a specific OpenShift namespace, you do this by selecting kubernetes.namespace_name:
"modmom-10" (example)
you'll want to add more filters for timestamp, business module etc.
```

# How to initialize Access Management

```
Introduction
Pre-requisites
Procedure
Preliminary steps
```
1. Admin user data seed script
2. BDD data seed script
3. Use the Import-MOMUser.ps1 script
4. Configure local test users
Notes

## Introduction

When the AM database is empty, there are some actions to do to properly initialize the scenario before use it, either for interactive use or for BDD UI and
API Test runs.

## Pre-requisites

Empty AM Database.

Pre-requisite on Keycloak administration:

```
Modular MOM "Admin" user is created in Keycloak and the User id is available for each scenario as well as the password.
Modular MOM "Admin" in Keycloak is assigned to the "Role Mappings SettingsClient Roles realm-management view-users" permission (see
picture below).
"Guest" user is created in Keycloak and the User id is available for each scenario as well as the password (when you set the password remember
to turn off the 'Temporary' flag).
Keycloak "modmom" client definition, inside the realm "modmom", has the "Direct Access Grants" option Enabled.
Keycloak local users to import have the properties: "Email", "First Name" and "Last Name" all set. Example: Email="test@nomail.invalid"; First
Name= "Test"; Last Name = "User"
```
```
Scope:
```
```
This page is a collaborative effort to gather all the correct actions to apply on each internal scenario after deleting the AM (usermanagement)
DB.
```
```
This page applies ONLY if the authorization feature IS enabled (platform version >= 2.1.1)
```
```
This page is for the configuration of the internal R&D scenarios running on OpenShift. This does not apply for SaaS or other XCR environments.
```

### 1.

### 2.

### 3.

### 4.

Procedure

High level steps:

```
'Admin' user has to be imported the first time and be assigned to the proper AM permission groups through SQL data seeding.
BDD API User Groups, Roles, Permission Groups have to be imported the first time through data seeding.
Each module, ported to the platform version supporting authorization, has been deployed on a given scenario.
Import-MOMUsers.ps1 script must run as the 'Admin' user with -Setup -Type User and -ImportLocalUsers, in order to import any local Keycloak
users into the AM model.
The interactive Admin user logs into the Access Management UI and configures all the available Permission, assigning them to each Role, User
Group and User.
```
Inputs required before running the procedure:

```
Keycloak endpoint base URI of the scenario to configure.
User: "admin" and its Password
Client Id: "modmom" and its Client Secret
The name of the interactive user for BDD UI test
The name of the client for BDD API test
```
Preliminary steps

Connect the the scenario SQL database instance, following the procedure described SQL Server on deployment.

```
connect to OS cluster 03 (api.openshift03.swqa.tst): .\oc.exe login --token=<INSERT TOKEN HERE> --server=https://api.openshift03.swqa.tst:
6443
switch to modmom-<number> project: .\oc.exe project modmom-<number>
check sql pod info to get the node name\ip: .\oc.exe get pod sqlserver-0 -o jsonpath='{.status.hostIP}'
check sql service to get the port number: .\oc.exe get service sqlserver -o jsonpath='{.spec.ports[].nodePort}'
```
Connect via SQL Management Studio with

<node name,ip>,<port number>

Insert User: VMUser and its password (oc get configmaps mssql-vars -o jsonpath={.data.MODMOM_DBPASSWORD})

1. Admin user data seed script

This step is always required to create the first user, which is assigned to the AM permissions groups, in order to do other configurations regarding users,
groups, roles, permission groups assignments.

Prerequisite is to have a deployed instance of the Access Management service running.

Please, keep this section up to date:

```
Scenario User UserID
```
```
modmom-1 Admin a72eb8a5-aa4b-4f65-8e05-97191d056a82
```
```
modmom-2 Admin <guid>
```
```
modmom-3 Admin 734b5e17-ca03-42c8-97d6-38f336824feb
```
```
modmom-4 Admin 5b876f69-737f-4635-addf-51aa0c1b106d
```
```
modmom-5 Admin 6367b8c4-b5c5-4e2d-ba48-56f9ca06cdfe
```
```
modmom-6 Admin 41f7b5fa-5ac6-4cfc-ade3-10ba134b5528
```
```
25bbabfa-fea0-4b0e-821a-18810db29bb7
```
```
modmom-13 Admin 9d3d888e-f805-44ba-9b66-6ea50421e6a0
```
```
modmom-14 Admin <guid>
```
```
modmom-15 Admin <guid>
```
```
modmom-10 Admin 957b630a-9b07-438b-99a3-88e3193a5cdb
```
```
modmom-18 Admin df3e2821-0a72-4d8b-99db-7b5e5c800df6
```

```
Deploy the Release M1_AccessManagementApi and please wait until it is ready. In case it was already deployed, please verify whether the pod
is ready, and start it in case it was previously stopped, before running the following step.
```
```
From the SQL Management Studio connected to the correct instance, run the following script, changing the KeyCloakId to the UserId
documented for each scenario: https://tfs05mom.industrysoftware.automation.siemens.com/MOM/ModularMOM/_git/M1_UserManagement?
path=%2FScripts%2FAdminPermission.sql
```
2. BDD data seed script

This step is required to create the initial configuration for BDD API test runs.

From the SQL Management Studio connected to the correct instance, run this script:

https://tfs05mom.industrysoftware.automation.siemens.com/MOM/ModularMOM/_git/M1_UserManagement?path=%2FScripts%2FDBB_UserPermission.
sql

3. Use the Import-MOMUser.ps1 script

This step is required to import any keycloak local users, such as "test", "dev01", "dev02", etc., to the AM Users list.

In order to do so this script must use the Admin user (previously imported on step 1).

From a Powershell console, please run the following script:

https://tfs05mom.industrysoftware.automation.siemens.com/MOM/ModularMOM/_git/M1_UserManagement?path=%2FScripts%2FImport-MomUsers.ps1

```
./Import-MOMUsers -ImportLocalUsers -Setup -Type User -KeycloakBaseUrl 'http://<OS apigateway route>/auth' -
KeycloakRealm 'modmom' -ImportUsersUrl 'http://<OS apigateway route>/accessmanagementapi/api/ImportUsers
/Execute' -User 'admin' -ClientId 'modmom'
```
```
Example:
./Import-MOMUsers -ImportLocalUsers -Setup -Type User -KeycloakBaseUrl 'http://api-gateway-modmom-13.apps.
openshift03.swqa.tst/auth/' -KeycloakRealm 'modmom' -ImportUsersUrl 'http://api-gateway-modmom-13.apps.
openshift03.swqa.tst/accessmanagementapi/api/ImportUsers/Execute' -User 'admin' -ClientId 'modmom'
```
The script will ask the password for the 'admin' user and the modmom client secret interactively.

Note: client secret for 'modmom' can be found with this command: oc get secrets keycloak-realm-secret -o jsonpath='{.data.client-secret}' | base64 -d

You can find additional help looking at the official documentation here: https://momwiki01.industrysoftware.automation.siemens.com/display/ModMOM2112
/Importing+Users

4. Configure local test users

After deploying all the module back-ends (or restarting them if you have already deployed the newer version):

Log into the Access Management UI as the interactive user "Admin" and do the following steps:

```
Assign all the permission groups available to the Role "RL_BDDAPI_TEST"
Assign all the relevant imported users (from step 3), such as "test" and "service-account-modmom", to the User Group "UG_BDDAPI_TEST".
```
Notes

If you have completed the configuration on a scenario, every time a new Permission Group is defined for a module and that module is deployed, the
permission group must be added to the role explicitly with just step 4. Conversely, if only the permissions definition is changed, the new configuration is
applied automatically and no further steps are required after the deployment.

If a new user is defined, start this procedure from step 3.

In case you notice missing permissions for a specific module, please restart that module scaling its deployment down and up again. This may happens
when the Access Management DB is deleted while the other modules were already running. The restart of the external module is sufficient to populate the
Access Management DB with the specific permissions.


# How to add another NFR to the nightly test run & Sumo

# Dashboard

The NFR dashboard itself will be edited by Michael Adams.

The NFR Release Pipeline needs to be extended for the execution of the new test.

```
Edit the NFR Release Pipeline: https://tfs05mom.industrysoftware.automation.siemens.com/MOM/ModularMOM/_releaseDefinition?
definitionId=55&_a=environments-editor-preview
Select the stage according to the business module
Select the "Run NFR Tests" task group, then right click & "Manage Task Group"
You will find the task group which lists all the Test Cases to be executed
Clone one of these tasks group invocations, rename it according to the new Test Case, set "testid" and the other parameters accordingly
```

### 1.

### 2.

### 3.

### 4.

### 5.

### 1.

### 2.

### 3.

# Custom component using SWF

## Requirement

Team should be able to use view or component developed for other modules or third party SWF application. It should have following capabilities

```
Data feeding from different module
Handling user input
NPM package
Simple\Minimal configuration
Data accessibility from outside the widget
```
## Solution

You can use any SWF view/page as custom component/widget if that view satisfy certain guidelines (mentioned below). Please note in order to use view
/page as component, it should not have any dependency on other page and avoid using global data(ctx).

## Characteristics Component Architecture

```
Single Responsibility
component has one reason to change when it implements one responsibility, or simpler when it does one thing.
Separation of concerns
Understanding SOLID Principles: Single Responsibility
Encapsulation of a Component
Defines a fence or wall around a chunk (single responsibility) of re-usable functionality
Props are like gates on how data flows into a component. A well encapsulated component hides its internal structure and provides a set
of props to control its behavior
Enables “loose coupling” of dependencies making it easier to test and re-use (Don’t repeat yourself - DRY)
Props and State
props get passed to the component (similar to function parameters) whereas state is private data managed within the component (similar
to variables declared within a function). React Fundamentals: Props vs State
Render a component
Returns the complete virtual DOM representation based on the current value of the passed in props and current state for this component
Introducing JSX
Rendering elements
Composing Components
Composition is a way to combine components to create a bigger (composed) component
Piece-wise refinement of re-usable chunks
React is all about composition
Lifecycle of Component
onMount
onUpdate
render
onUnmount
```
## How to create component

```
Make sure view has no or minimum dependency.
Rename view and view model file using PascalCase.
```
```
When creating a new Component, keep in mind the following naming rules:
```
```
The Component name must be in PascalCase and be composed by at least two words
The View file must start with the Component name in PascalCase and end with View.html.
The ViewModel file must start with the Component name in PascalCase and end with ViewModel.json.
There are no enforced naming conventions for Component Services, as long as they are referenced correctly in ViewModel actions. If
one Service in particular is used exclusively for a specific Component, its name should start with the Component name in PascalCase
and end it with Service.js.
The resulting UI Element tag will be set to the name the Component in kebab-case, so for example if your component is called MyComp
onent the corresponding tag will be <my-component>.
To accept data from calling/parent view, use props.
Go to <abc>viewModel.json file and add props object as shown below
```
```
"props": {
"action": {
"type": "action"
},
"prop": {
```

### 3.

### 4.

### 5.

### 6.

### 7.

```
"type": "field"
},
"is-negated": {
"type": "object"
}
}
```
```
You may use LifecycleHooks in order to track values and change behavior of component accordingly.
Example-
```
```
"lifecycleHooks": {
"onUpdate": {
"action": "reEvaluate",
"observers": [ "props.selection" ]
}
}
```
```
In case if you want to expose any command action outside of component , then use events to trigger certain action.
```
```
Add following content to .npmignore file
```
```
src/solution/*
!src/solution/kit.json
src/*/commandsViewModel.json
src/*/states.json
src/*/test
src/setupTests.js
out
.vscode
public
test
```
```
optional: to create npm package locally , run command "npm pack".
```
How to consume custom component


# How to use observability data in Sumologic

How to use observability data? provides guidelines about producing observability data inside applications as well as their analysis using opensource
visualization tools in Openshift (Kibana, Grafana, Jaeger UI). Here, we explain how Sumologic which will be the ultimate target for observability data can
be used for analysis of this data. There exist currently dashboards in Sumologic for visualizing and summarizing the results of NFR test runs, namely
response time.

## Link to tracing dashboard

However, in addition to response time, inspecting of observability data including traces, metrics and logs is necessary especially for troubleshooting and
debugging purposes. Therefore, there is a link from the dashboard of NFR tests results to, for example, dashboard of traces. To access this dashboard,
you can click on a panel in the dashboard of NFR tests results like "AVG" to open a window with the links on the righthand side:


In the "Linked Dashboards" part, we can, for example, find the link to the traces dashboard. Since for every module like "Track and Trace", "Material
Management" and "Order Management" we have separate dashboards for traces of each module (work in progress). The time interval of the linked
dashboard is the same as the one in the NFR tests results dashboard:


Tracing dashboard

The tracing dashboard extracts all traces of a specific module such as "Track and Trace" for the chosen time interval and displays them in the top panel
"Traces for Operation:*":

To extract traces related to some specific REST call, you can give the name of the corresponding endpoint such as "api/Collect/Execute" to the parameter
named "operation" of the dashboard.


Traces show the end-to-end journey of a specific REST call including asynchronous calls between the module. Therefore, the duration of a trace is not the
same as response time measured by JMeter for each REST call. Each trace consists of a number of spans which are the function calls (only instrumented
ones) during the processing of a REST call. For each trace, the number of spans is shown in a column of the table which lists traces. Moreover, the
duration breakdown column gives an overview of the percentage of time spent in different modules.



Individual traces can be inspected further by clicking on them and opening the trace view:

Here, function calls (instrumented ones inside an app) from different layers of an app or from other apps which has been called asynchronously via Grpc
messaging can be tracked. Especially, their runtime can be investigated to identify bottlenecks inside an app.


Further inspection of a trace is also possible by giving the id of a specific trace to the parameter named "traceid" in the dashboard:

The first two panels aggregate count of function calls as well as average duration of them. The panel at the bottom lists all the spans of a trace. Each panel
can be opened in a separate window by clicking on the vertical three dots at the righthand side:


Logs related to an individual trace

Logs related to one specific trace can be accessed via its trace view. As the following figure shows, by clicking on any span, the related details window will
be opened on the righthand side. From there, links to corresponding logs are available: "logs related to one specific span" or "logs related to the entire
trace".


Note that currently only logs produced from the apps running in XCR are collected in Sumologic. Logs from the apps running in Openshift can
be accessed from the Kibana running in Openshift.

Currently, only traces dashboard for "Track and Trace" which is running in OS 18 is available. For other modules as well as modules running in
XCR individual dashboards will be created. Until then the queries from the "Track and Trace" dashboard can be adapted for other modules:


### 1.

### 2.

```
a.
b.
c.
3.
4.
a.
b.
5.
6.
a.
b.
```
# How to provide and use a new messaging model version

```
Introduction
Semantic Versioning
Procedure
Messages repos examples
Modules repos examples
Notes
```
## Introduction

Message repositories are updated to work with the pipeline. Whenever a new version is merged to the master branch, it should be considered a stable
version.

This have two consequences:

```
A git tag is produced with the declared version numbering
A stable nuget package is published to the shared feed.
```
Whenever users needs to change the messaging models, they should declare the new version upfront.

Files to modify to declare a new version in the message repository:

```
<repo>.modelconfig.json (used to generate the metamodel definition)
dependencies.props (used to generate the nuget packages)
CHANGELOG.MD (used to release the new version with a valid description of the changes)
```
The modules will integrate a specific stable version of the message repo artifacts changing:

```
modelconfig.json, binaryDependencies section
dependencies.props, to reference the nuget packages.
```
The build pipeline of the message repo checks for any validation errors, preventing two competing Pull Requests to generate another stable version when
the versioning is not consistent with the master branch.

## Semantic Versioning

For Messaging Models the suggested semantic version schema to adopt is the following:

```
Version
Change
```
```
Description Notes
```
```
major N.A. Major compatibility break TBD
```
```
minor Some fields are added to an existing message
type definition.
```
```
The dependent models are impacted by the change: they need to implement the
new message interface.
```
```
patch Some message types are added to an existing
message model definition.
```
```
A newer version is available but dependent models do not require to implement the
version until the new messages are used.
```
```
pre-releases Used only in dev branches. This is reserved only for development and testing purposes, before releasing the
official version.
```
## Procedure

When the message model changes, choose the new version according to the Semantic Version described above, and apply the following steps:

```
Prepare a new dev branch of the messages repository.
Declare the new version in
<repo>.modelconfig.json, setting the "version" property.
dependencies.props, setting the <ModelVersion> property.
Add a description entry in the CHANGELOG.MD with the title (#) matching the same version number.
Apply the changes to the metamodel of the messages.
Manually trigger a Message Repo pipeline build.
This build will generate a pre-release version of the nuget package with the following pattern: <version>-<branch id>-<build id>
The package is pushed to the shared Nuget Feed.
Modules: Prepare a new dev branch of the modules repo.
Update the references:
Set the final stable version in the modelconfig.json within "binary_depedencies".
```

### 6.

```
b.
7.
8.
a.
```
```
b.
```
```
i.
ii.
9.
a.
10.
```
```
Reference the pre-release version in the dependencies.props (<version>-<branch id>-<build id>)
Implement the required changes and test the modules.
Release: Do a Pull Request on Messages repo:
Pull Requests will fail when the newer version is no longer valid, that is, someone else has already incremented the version number so
that a pull + choosing a new version is required.
After the completion of a successful PR, a new stable version is published in the master branch with a specific git tag and the final nuget
package.
git tag format: v<version> (example: v2.0.3)
nuget package version format: <version> (example: 2.0.3)
Change the dependencies.props on each module in order to reference the final stable version of the messages:
Remove the pre-release part of the version inside the dependencies.props
Do a Pull Request on each module
```
Please note that points number 8 to 10 MUST be completed together, as soon as possible, in order not to disrupt the work of other feature teams: when an
updated message model is published on the main stable branch, without the corresponding messages implementation, it will make the modules repo build
inconsistent. The development of a feature must happen in dev branches for both the model of messages and the modules themself (single unit of work).
When a self-consistent work is finished the release should follow. As a consequence, it is deprecated to organize a feature in user stories splitting the work
as separate unrelated steps (e.g. 1 US to modify the messages, 1 US to update a module, 1 US to update another one) closing those items with separate
releases, it is better to organize the user stories in partial, but self-consistent, logical implementation, such as: 1 US to add a new field and handle it across
the impacted modules, 1 US to add a new message and handle it on each impacted modules, etc.

Messages repos examples

```
Message repo modelconfig.json file
```
### {

```
"name": "MaterialManagementMessages",
"description": "Material Management Messages",
"version": "2.0.0",
"platformVersion": "2.1.7",
"gitInfo": {
"repoName": "M1_MaterialManagementMessages",
"url": ""
}
}
```
"version" property is the declared version here (2.0.0).

```
Message repo dependencies.props file
```
```
<Project xmlns="http://schemas.microsoft.com/developer/msbuild/2003">
<PropertyGroup>
<ModelVersion>2.0.0</ModelVersion>
<PlatformVersion>2.1.7</PlatformVersion>
<ObservabilityVersion>2.0.0</ObservabilityVersion>
</PropertyGroup>
<ItemGroup>
<PackageReference Include="Camstar.Metadata.Aspects.Fody" Version="$(ObservabilityVersion)-*"/>
<PackageReference Include="Siemens.MOM.MetaModel.Framework" Version="$(PlatformVersion)"/>
<PackageReference Include="Siemens.MOM.MetaModel.Interfaces" Version="$(PlatformVersion)"/>
</ItemGroup>
</Project>
```
<ModelVersion> tag is the declared version to create the nuget package with 2.0.0 and the assembly and file version on the dlls 2.0.0.0. This tag is
referenced by the csproj project file.

```
Extract from project file (csproj)
```
```
<!-- Project file must import dependencies.props at the beginning -->
<Import Project="./dependencies.props" />
```
```
<PropertyGroup>
....
<!-- Project file must set its Version property to ModelVersion -->
<Version>$(ModelVersion)</Version>
</PropertyGroup>
```

```
Message repo CHANGELOG.md file
```
```
# Changelog
A list of changes made and features added in each version of the model.
```
```
# 2.0.0 - Change title
```
- Change description

The changelog entry, matching the version number, is selected to create a git tag with the description in the git history.

Modules repos examples

```
Module repo modelconfig.json file
```
### {

```
"name": "MaterialManagement",
"description": "TODO: Provide a description of the model.",
"version": "2.0.5",
"platformVersion": "2.1.7",
"gitInfo": {
"repoName": "M1_MaterialManagement",
"url": ""
},
"modelDependencies": [
{
"name": "Common",
"version": "2.1.7"
}
],
"binaryDependencies": [
{
"name": "M1_MaterialManagementMessages",
"version": "2.0.0"
},
{
"name": "M1_OrderManagementMessages",
"version": "2.0.0"
}
]
}
```
```
Module repo dependencies.props file
```
```
<Project xmlns="http://schemas.microsoft.com/developer/msbuild/2003">
<PropertyGroup>
<ModelVersion>2.0.5</ModelVersion>
<Version>$(ModelVersion)</Version>
<PlatformVersion>2.1.7</PlatformVersion>
<ObservabilityVersion>2.0.0</ObservabilityVersion>
</PropertyGroup>
```
```
<ItemGroup>
<PackageReference Include="M1_MaterialManagementMessages" Version="2.0.0" />
<PackageReference Include="M1_OrderManagementMessages" Version="2.0.0" />
</ItemGroup>
</Project>
```
Modules must handle all references to messages only by means of dependencies.props file.


Notes

```
Runtime compatibility between modules referencing different message models versions are still not supported: Publishers and Consumers
implementations must be updated to use the same messages version.
If you manually create a git tag on messages repos, avoid to use the same tag name used by the pipeline, i.e. 'v<version>'; in case of tag names
inconsistencies, pipeline failures may occur.
```

### 1.

### 2.

### 3.

### 1.

```
a.
2.
```
```
a.
3.
```
# Signal R and SWF integration

## Install Signal R

Server:

```
Install below nugets.
```
Client (SWF app):

```
Run below command
```
```
npm i @microsoft/signalr
```
## Server code

Using Signal R we can send message to client with 3 options.

```
Broadcast message to all clients
One to one communication
Sending message to a group of clients
```
Please check the attached file (SignalRConsole.zip) for server code.

## Client (SWF app)

```
Create JS file to establish a connection to server and receive message from sever.
signalRHelper.js
In page ViewModel.json, create an action to call method of JS file
```
```
On page onInit hook, call the action as below.
```


# Stable Builds Definition

```
Module Name Core Date AM Date Team
```
```
Order Management Backend Yes 4 July Yes 4 July Sahyadri
```
```
Order Management UI Yes 4 July Yes 4 July Sahyadri
```
```
TNT Backend Yes 01 Jul 2022 Yes Monviso
```
```
TNT UI Yes 01 Jul 2022 Yes Monviso
```
```
Material Modeling Backend Yes 4 July NA Sinhagad
```
```
Material Modeling UI Yes 4 July NA Sinhagad
```
```
Factory Backend Yes 01 Jul 2022 NA Rajgad
```
```
Factory UI Yes 01 Jul 2022 NA Rajgad
```
```
Access Management
Backend
```
```
Yes 1 July NA Sinhagad
```
```
Access Management UI Yes 1 July NA Sinhagad
```
```
Landing Page UI Yes 05 July NA
```
```
To be filled by SMs
```

# Platform Upgrade 2.2.0

```
Sayahdri Order Management CORE + Messages
```
```
Monviso TNT CORE + Messages
```
```
Sinhagad Material Management CORE + Messages
```
```
Sinhagad FM CORE + Messages
```
```
Matteo Access Management CORE
```
```
Sayahdri OM AM + Messages
```
```
Monviso TNT AM + Messages
```
## Order

0) AcM is independent

1) OM

2) MM

3) TNT

4) FM indirect dependency

## Practice

After the update is done we communicate to the next SM that the team can proceed.

## Planned Date

Monday


# Platform Internal Architecture

```
This wiki section includes all the ModMOM 2.x Internal Architecture topics and pages.
```

# Platform & Model Versioning

## Platform Version:

All platform components will have the same version tagged. The version to be applied to the Platform components can be defined in the following file:
$\MetadataRuntime\Platform\platform.common.properties

<PropertyGroup>

<PlatformVersion>1.0.2</PlatformVersion>

# Model Version & Platform Version Dependency:

Every Model can have its own version specified. It's more of a semantic concept, right now. A Model's version can be specified in the following file:
$\M1_MaterialManagement\Code\Config\M1_MaterialManagement.modelconfig.json

"name": "MaterialManagement",

"version": "1.0.0",

"platformVersion": "1.0.2",

## Platform Dependency Version:

Every model can specify a particular Platform version that the model is dependent on. This version ensures that certain features of the platform are
available to the Model both at development time & at runtime.
The version of the platform that a model depends on can be specified in the above mentioned modelconfig.json file.

"name": "Common",

"version": "1.0.2",

"platformVersion": "1.0.2",

## Model Dependencies:

Every model could have dependency on other models. Model dependencies are realized using the git submodule concepts. Similar to the Platform
Version, each model can specify an exact version of the dependent model.
The versions and the dependent models can be specified in the above mentioned modelconfig.json file.

```
"modelDependencies": [
{
"name": "Common",
"version": "1.0.2"
}
]
```
## Binary Dependencies:

Some models could have dependency on message models. Message Model dependencies are realized using the standard binary package reference
mechanism of NuGet. Similar to the Platform Version, each model can specify an exact version of the dependent model.
The binary dependent models can be specified in the above mentioned modelconfig.json file in the 'binaryDependencies' section.

```
"binaryDependencies": [
{
"name": "M1_MaterialManagementMessages",
"version": "1.0.0"
}
]
```

Model to Platform Version Dependency

Multiple Platform Versions:

Since each model can specify the version of the platform it depends and each model can also have some dependent models specified there could be a
situation where multiple versions of the Platform that come into play - one directly mentioned by the model, others coming from the dependencies to other
models.

Platform Version Resolution:

```
Similar to any package management software, when we have to pick one Platform Version for a particular model when there many different
versions coming from different dependent models, we will need to pick the highest (most recent) version. This step is currently a manual step but
eventually a Model Designer could automate it.
```
Manual Step:

Every Model should have the correct version specified for the Platform version after considering all the dependencies (submodules) and their platform
version dependency.

Currently, there are 2 places the Platform version need to be set and should be matching

```
Model's *.modelconfig.json file
```
```
$\M1_MaterialManagement\Code\Config\M1_MaterialManagement.modelconfig.json
```
```
"name": "MaterialManagement",
```
```
"version": "1.0.0",
```
```
"platformVersion": "1.0.2",
```
```
Model's dependencies.props file
```
```
$\M1_MaterialManagement\Code\dependencies.props
```
```
<PropertyGroup>
<PlatformVersion>1.0.2</PlatformVersion>
<ObservabilityVersion>1.0.0</ObservabilityVersion>
</PropertyGroup>
```
Model to Model Dependencies (Submodules)

As mentioned above, model dependencies are specified in the modelconfig file and are realized by the git submodules concept.
Since Model Designer is not available today, so to setup model to model dependency following manual steps need to be executed:
General Notes:

```
Use tags to identify the exact commit points in a Model's repository to setup dependencies
For Model repos, all changes are done into a separate branch and then a PR is created to merge the changes into the main branch
Tags are created on the main branch after the PR is merged
```
Manual Steps:

```
After every major check-in that constitutes an (inner) release of the model
Update the Model Version using the Semantic Version rules
After the PR completes merging into the main branch of the model's repo, Tag the model repo at this point with the version number as defined in
the modelconfig
git commands (to be executed on the model's repo)
```
```
git tag <tagname=version>
git push --tags
```
```
Or, this can be achieved using the TFS Web UI as well
(assuming the submodule was already configured to setup the dependent model)
To update a dependent model (submodule) inside another model, aka top model
Know the commit point (tagname) of the dependent model
In the top model, navigate to the folder containing the submodule and execute the following git commands:
```

### 1.

### 2.

```
git pull --tags
git checkout <tagname>
```
```
This will update the submodule definition in the local workspace and that needs to be committed and pushed on the top model's repo
Now, navigate out of the submodule and go into the top model's folder and execute the following git commands:
```
```
git commit -m <msg>
git push
```
All these manual steps are easily performed using Visual Studio Code (from the combination of both Terminal & GUI)

Additional git Commands for potential ad-hoc tasks:

```
git command to delete tags from the local workspace
```
```
git tag -d <tagname>
```
```
git command to delete tags from the remote
```
```
git push --delete origin <tagname>
```
Model to Message Model - Binary Dependencies

As mentioned above, model dependencies are specified in the modelconfig file and the binary dependencies are realized by the MS Build project standard
NuGet package references.
Since Model Designer is not available today, so to setup model to model binary dependency following manual steps need to be executed:
General Notes:

```
Use notes from above section to follow the development of the Message Model
Create version tags as mentioned above
```
Manual Steps:

```
In the Model which need to Message Model, add the following line to the 'dependencies.props' file
<PackageReference Include="M1_MaterialManagementMessages" Version="1.0.0" />
The version mentioned here should match the version specified in the binary dependencies section of the modelconfig file
```
Build Process (through Azure Pipelines)

Platform:

```
The components generated by the platform build would be tagged with the version specified in the platform.common.properties
This acts as a pre-release or current development version
We can promote any particular pre-release to a stable version
Stable Version makes the platform components available for external consumers like other startups using the NuGet or through the Container
image repository
Process to promote a pre-release version to a stable version - see section below...
When a pre-release version is promoted to be a stable version then a new pre-release version of the Platform is started
```
Apps:

```
Assuming that all the App's top models are configured correctly in the modelconfig file specifying its version, the platform version & other
dependent model versions
When Apps are built, the Platform Version specified in the modelconfig file of the app's model repo will determine which version of the platform
components (NuGet packages or Container images) get used to build the app artifacts
```
Promotion of a Pre-release to a Stable release:

The promotion of a pre-release to a stable release would be needed to make public the features added to the platform during pre-release. This process is
implemented a Azure DevOps automation leveraging the Pipelines. To promote a particular pre-release of the platform (MetadataRuntime repo), the
pipeline needs to be run/executed manually and passing in a 'true' value for the 'publishStable' variable. This can be achieved using the Azure DevOps
WebUI.

For now, architecture community will periodically publish a stable version as new platform features are being added but eventually this will need to be
somehow mapped to the product's dev/release cycle.



# Metadata/Metamodel


# Model configuration

Each Model will have a configuration file with the same name as the Module/App (e.g. WS_M1_Dispatch) stored in the same directory as the solution.

This configuration will specify the dependencies of the models (composed models). When there are multiple composed models or nested models, the
order in which the overrides must be processed is defined by the sequence specified in this list. It also specifies the version of the dependency that is
supported. The "name" attribute is what is used in the model file name for overriding, e.g. Factory@ESig.cs.

```
Configuration Format
```
### {

```
"configuration": {
"name": "Model Name",
"description": "A brief description of the model",
"version": "Semantic version of the model",
"platformVersion": "Semantic version of the platform",
"gitInfo": {
"repoName": "The name of the git repo",
"url": "The URL of the git repo"
},
"modelDependencies": [
{
"name": "Dependency Name",
"version": "Dependency Version",
"notes": "Any info on how/why this dependency exists"
}
],
"binaryDependencies": [
{
"name": "name of dependency object",
"version": "Dependency Version",
"notes": "Any info on how/why this dependency exists"
}
]
}
}
```
Here is a sample configuration

```
Sample configuration
```
### {

```
"configuration": {
"name": "Semi",
"description": "MES for the Semiconductor industry",
"version": "0.3.5",
"platformVersion": "0.2.11",
"gitInfo": {
"repoName": "WS_M1_Semi",
"url": "https://tfs05mom.industrysoftware.automation.siemens.com/MOM/ModularMOM/_git
/WS_M1_Semi"
},
"modelDependencies": [
{
"name": "Dispatch",
"version": "0.2.8",
},
{
"name": "TrainingReqs",
"version": "0.1.2",
},
{
"name": "ESigs",
"version": "0.1.6",
},
{
```

```
"name": "Common",
"version": "0.1.3",
}
],
"binaryDependencies": [
{
"name": "WS_M1_Messages",
"version": "0.1.8"
}
]
}
}
```
In the example configuration above:

Model Dependencies:

```
ESigs model extends or built upon the Common model
TrainingReqs model extends or built upon the Common model
Dispatch model embeds (composes) the ESig & TrainingReqs models (composed) in order to use/customize/extend the underlying model element
```
Note: The order of the model dependencies listed should be in the order of dependencies starting from the top level model to the leaf level. In the above
example, the model 'WS_M1_Common' is a leaf level model and it does not have any other composed models so it is listed last after all the models that
are dependent on it.

Binary Dependencies:

```
These are the message model assemblies that have a binary dependency (referenced assemblies) with the current model
```

### 1.

### 2.

# Revisioned & Revision Base Types

## Revisioned Objects and the Framework's built in Revision Base:

Object Revision Control is the ability of an object to have multiple revisions and still be able to be managed as one object. Objects that support revision
control are generally referred to as ‘Revisioned Objects’. Within this Metamodel, Revisioned Objects implement the following pattern for revision control
mechanism.

```
Revisioned Objects are standard objects like any other Business object
Revisioned Objects can have one or more ‘Revisions’
Revisioned Objects will have ONE revision marked as 'Revision of Record', aka ‘RoR’
Revisioned Objects have the following 2 alternate logical keys of identification:
Name only, which always refers to the current ‘RoR’ object
Name and Revision, which uniquely identifies a specific single revision
All the revisions of a particular Revisioned Object will have an association to an object system defined type called 'Revision Base'
The Revision Base holds all the revisions of the Revisioned Object
The Revision Base also holds what is the currently active Revision (Revision Of Record)
```
## Steps for creating user-defined Revisioned Objects with Framework's pre-defined Revision

## Base:

```
Subclass from 'RevisionedObject' type defined in the M1_Common
Define persistence attributes for this type on the associated sidecar configuration file
Define additional sidecar configuration for the persistence configuration of the corresponding Revision Base type
Name of the file should be: <your_class_name>`RevisionBase`.json, ex: Product`RevisionBase`.json
Define persistence configuration for the following fields: Name, RevOfRcd & Revisions
Add new fields & configuration as needed
```
## User-defined (Custom) Revision Base:

The need for custom Revision Base type arises when some common properties need to be shared among all the revisions of a Revisioned Object such
that these properties are not required to be set on every revision.

Available from Platform Version: 2.1.0

## Steps for creating user-defined Revisioned Objects with user-defined (Custom) Revision Base:

```
Create a new type by sub-classing from frameworks pre-defined Revision Base type
Ex: public class CustomRevisionBase : RevisionBase<CustomRevisioned>
Define additional sidecar configuration like any other Configurable Object type
Name of the file should be: CustomRevisionBase.json
Define persistence configuration for the following fields: Name, RevOfRcd & Revisions
Add any new fields & configuration as needed
Create a new type by sub-classing from 'RevisionedObject' type defined in the M1_Common
Ex: public class CustomRevisioned : RevisionedObject<CustomRevisioned>
Define persistence attributes for this type on the associated sidecar configuration file
Register the new custom RevisionBase type with the new Revisioned type
Use the standard Field override technics to define the override for the 'Base' field
Ex: RegisterFieldOverride(FieldInitializer.RevisionBaseRefField<CustomRevisioned, CustomRevisionBase>.Get("Base"))
And, also add the following property override
public new CustomRevisionBase Base { get => BaseField.GetObject() as CustomRevisionBase; }
Add new fields & configuration as needed
```
Note: See Sample Model for more concrete code example.


# Enumerations(Enums)

Enumerations can be defined and referenced as fields within Configurable Objects. The field definition follows the same pattern: The backing field is an
EnumField and the property is the native type. There is currently no support for a list of enums.

Note that the Enums must be defined in a namespace and not be nested inside configurable object class types.

## Default Config

Here is a basic enum configuration.

public enum FactoryStateEnum : byte
{
Created = 0,
Active,
Closed
}

public EnumField<FactoryStateEnum> StateField = null;
public FactoryStateEnum State { get => StateField.Value; set => StateField.Value = value; }

By default, the value will be persisted to the table using the value.

"State": {
"Persistence": {
"ColumnName": "State"
},
}

## Platform extensions

There are several attributes and configuration options available to extend the default behaviors.

## Enum Definition Extensions

For each enumeration item, A displayName and description can be configured. Both of these can be localized.

public enum FactoryStateEnum : byte
{
[EnumDisplayName("DN-Created")]
[EnumDescription("Desc for Created")]
Created = 0,

[EnumDisplayName("DN-Active")]
[EnumDescription("Desc for Active")]
Active,

[EnumDisplayName("DN-Closed")]
[EnumDescription("Desc for Closed")]
Closed
}

## Enum Persistence Extensions

As mentioned above, the default behavior is to save the value of the enumeration to the database. To extend this behavior, an attribute can be specified on
the backing field.

[PersistAs(PersistAsAttribute.EnumAs.Name)]
public EnumField<FactoryStateEnum> StateField = null;

Adding this attribute will add an additional column to the table by appending "__Name" to the name of the value field. In this example, the name would be
"State__Name". The purpose here is to make writing queries easier because there won't be a need to translate the enum value to a human friendly string.

## SelectionValues for enums

For fields of type Enum, Static selection values can be configured to assist with presenting a list of options on a UI.

"State": {
"SelectionValues": {
"Mode": "Static"
}
}


Other configurations for enum fields

A default value for the field can be configured, as can a description and label for localization.

"State": {
"DefaultValueExpression": "(byte)1",
"Description": "Factory State",
"Label": "Factory_State",
}

Full config example

"State": {
"Description": "Factory State",
"Label": "Factory_State",
"DefaultValueExpression": "(byte)1",
"Persistence": {
"ColumnName": "State"
},
"SelectionValues": {
"Mode": "Static"
}

}


# Configuring Nullable Native, Enum, and Options Type Fields

## Introduction

Prior to Platform version 2.2.1, native type fields including the Enum and Options types and excluding String type, always write the type's default value into
the database when no value is assigned to the field either through input or through logic. There is a gap that exists when there is a need to differentiate
between when the default value was explicitly set and when the field was not set resulting in the default value being persisted.

## Metamodel Enhancement

To fill this gap, as of Platform version 2.2.1, the metamodel is enhanced to enable the Business Logic developer to configure their intent to write a null
value when the field has not been set. When native, Enum or Options type fields are configured with this intent, the runtime will NOT write the type's
default value into the database when no value is assigned to this field. Similarly, when such fields are requested through the API, null is returned instead of
the type's default value.

## Example Model Definition

Model Class Definition: (No change here)

```
Example Model Configuration (No change here)
```
```
// Example of Nullable Numeric Types
public IntegerField NullableQtyField = null;
public int NullableQty { get { return NullableQtyField.Value; } set { NullableQtyField.Value = value; } }
```
```
// Example of Nullable Options Type
public OptionsField<FactoryStateOptions, byte> NullableStateField = null;
public FactoryStateOptions NullableState { get => NullableStateField.GetOptionsValue(); set =>
NullableStateField.SetOptionsValue(value); }
```
Model Sidecar Json properties: (see the new property usage)

```
Model Sidecar Field Properties
```
```
"NullableQty": {
"Persistence": {
"ColumnName": "NullableQty",
"WriteNullWhenEmpty": true
}
},
"NullableState": {
"Persistence": {
"ColumnName": "NullableState",
"WriteNullWhenEmpty": true
}
}
```
Open Questions and How-To's

Q: Can a field be reset to null?

A: This should work using the __fieldsToClear section of the __inputData.

Q:Does the mapping implementation forward the null value? e.g. service detail with a nullable field is mapped to an existing object.

A: It should. But this needs to be verified.

Q: What happens if you map a null field to a non-nullable field?


A: The default value according to the field type will be written.

Q: Is nullable only valid for persistent objects?

A: Yes. There is an API available in the platform for business logic to determining whether a field is null (or not set)


# List Persistence - Configuration & Characteristics

## List Types

## Characteristics

## /Type

## Dynamic List Persistent List

```
Applicable Field Types Object Ref & Subentity Object Ref & Subentity & Primitive
```
```
Storage Target Object Table Intermediate Table
```
```
Order Order of retrieval is dynamic and can be configured
(configurable sort fields)
```
```
Insertion Order is preserved. Order of retrieval is implicit and is
based on the insertion order
```
```
Navigation Reverse (Id of current object is stored in listItem object) Forward (Id of listItem object is stored in the current object)
```
```
Load Lazy Load on-demand (entire list) Lazy Load on-demand (entire list)
```
```
Access from Write
Model
```
```
Read-Only Read-Write
```
```
Update to Write Model Not Applicable Whole List Replacement
```
```
Write Model List
Property
```
```
IEnumerable<T> IList<T>
```
```
Metadata Configuration Target Object Table + Column names,
```
```
Link Field/Column + Sort Fields
```
```
Intermediate Table + Column names
```
## Configuration Examples

## List of ObjectRefs using reverse navigation - Dynamic List

The system will populate the list by querying a table that has a field containing a reference to the current object. In the Sample model, the Employee has a
reference to a factory, so a list field of Employees can be configured on the Factory with the list entries dynamically retrieved by the system. The list can be
sorted by values in the table containing the list items. In this example from the Sample Model, the retrieval order of the list items can be sorted based on
one or more fields defined in the Employee type, such as Name, Department, etc.

Sample Model reference field: Factory.DynListEmployees

```
Dynamic List
```
```
"DynListEmployees": {
"Retrieval": {
"AssociationField": "Factory",
"Sort": {
"Department": "DESC",
"Name": "Asc"
}
}
}
```
## List of Subentities where the insertion order is not needed to be preserved - Dynamic List

Subentities by definition have a field referencing the parent, and so can be dynamically retrieved when they are in a list. If the Subentity has a field which
can be used for sorting (e.g. Name), it can be specified, too.

Sample Model reference field: Factory.Locations


```
Subentities, insertion order not relevant
```
```
"Locations": {
"Retrieval": {
"AssociationField": "Parent",
"Sort": {
"Name": "Asc"
}
}
}
```
List of ObjectRefs using forward navigation, where polymorphism is involved, or where order of

insertion needs to be preserved - Persistent List

The entries on this list type don't have a field referencing the current object, so the list has to be persisted to an intermediate table. The retrieval order of
the list is implicit and is based on the insertion order maintained in the intermediate table.

Sample Model reference field: Factory.PersistentListEmployees

```
Object refs requiring order or where the target doesn't have a back reference.
```
```
// In this example,
// The Id of the object owning this list is stored in the system generated column called Id
// The Column Name where the list item is stored is configured here
"PersistentListEmployees": {
"Persistence": {
"TableName": "FactoryEmployees", // Intermediate table name
"ColumnName": "EmployeeId" // Id of the list item
}
}
```
List of Subentities where insertion order needs to be preserved - Persistent List

When the insertion order of entries on a list of Subentities is important and needs to be preserved, the list will be persisted to an intermediate table. The
insertion order of the list is maintained in the intermediate table.

Sample Model reference field: Step.PathSelectors(non existing)

```
Subentities, where insertion order is important
```
```
// In this example, the list of PathSelectors is on the Step, its Id will be stored in system generated Id
column
"PathSelectors": {
"Persistence": {
"TableName": "StepPathSelectors", // Intermediate table name
"ColumnName": "PathSelectorId" // Id of the List Item - PathSelector
}
}
```
List of non object types - Persistent List

Lists of non-objects are always persistent in a separate table and the insertion order is always preserved. The order of entries for retrieval can also be
additionally configured to be based on the value of the item instead of the insertion order as depicted in the sample config below. When optional sort config
is not present the system will default the the retrieval order to the insertion order.

Sample Model reference field: PersistentEntity.StringList

```
List of non-object types
```

// In this example, the list of strings on the current object is peristed on the Step
"StringList": {
"Persistence": {
"TableName": "PersistentEntityStringList",
"ColumnName": "Value",
"Sort": {
"Value": "Asc"
}
}
}


# Metadata Model Query Definitions

Queries defined using the designer tool will be stored in files with a ".momsql.json" extension. A definition is comprised of

```
Basic info
description
label
name is not included in the json, but rather designated in the file name
SQL text with possible database specific options
The "standard" option allows specification of a sql statement that doesn't require any database specific syntax, i.e. the ANSI/ISO/IEC
9075:2003 standard.
Expressions in the Select clause are limited to a single dot ".". At this time, there is no "implicit" joining.
Other attributes, if specified, will override the "standard" text.
Query parameter definitions with
Parameter name
The data type of the parameter
An indicator of whether the parameter is optional
A label
A description to help clarify the usage of the parameter if needed
Column header overrides, consisting of
the identifier (name) of the column definition. This is the value found in the "Select" clause.
A label for the column header text. This can be used to override the header pulled from the model, or to supply a value for calculated
fields.
```
## Model Query Features

```
Syntax Evaluated Example
```
```
$COName.FieldName TableName.ColumnName $Operation.UseQueue
```
```
$$CurObj Table for current object $$CurObj.Description
```
```
?Parameter Replacement Parameter ?Factory
```
```
<{?Parameter}>?<
{ifTrue}>
```
```
Conditional snippet. Makes coding the query simpler for times when the query
parameter is optional
```
```
<{?State}>?<{ AND Factory.State =?
State }>
```
```
$$CurObj.rbase RevisionBase associated with current object when it's revisioned $$CurObj.rbase
```
```
$$CurObj.owner Parent of subentity $$CurObj.owner
```
## Model Query Examples

Example: Siemens.MOM.PocModel.DispatchQuery.momsql.json

```
Query example
```
### {

```
"description": "Get the list of batches available to the current operation.",
"label": "Dispatch List",
"sqlTexts": {
"standard": "Select $TrackedObject.Name, $Product.Name, $Qty, UOMDef.Name,
$Qty*$Product.ItemCost, $$CObj
From $TrackedObject
Left Join $Product on $TrackedObject.Product = $Product.InstanceId
Left Join UOM as UOMDef on $Product.UOM = UOMDef.Id
Where $TrackedObject.inQueue = ?inQueue
AND $TrackedObject.Operation = ?Operation
<{?Factory}>?<{ AND $Operation.Factory = ?Factory}>
AND $TrackedObject.Resource = ?Resource",
"sqlServer": "",
"postgreSQL": ""
},
"parameters": [
{
"name": "Operation",
"expression": "Operation",
"dataType": "object"
},
{
"name": "Resource",
```

```
"expression": "Resource",
"dataType": "object",
"label": "Processing Resource"
},
{
"name": "Factory",
"dataType": "object",
"label": "Factory"
"optional": true
},
{
"name": "inQueue",
"expression": false,
"dataType": "boolean",
"description": "InQueue will be false if the material has been moved-in, or if the operation doesn't have
a processing queue"
}
],
"columnHeaders": [
{
"name": "$TrackedObject.Name",
"label": "Container"
},
{
"name": "$Qty*$Product.ItemCost",
"label": "Extended Cost"
},
{
"name": "UOMDef.Name",
"label": "UOM"
}
]
}
```
```
Query persistent list example
```
### {

```
"description": "Get a subset of the employees from the factory.",
"label": "Get Active users",
"sqlTexts": {
"standard": "Select $Factory.City, $Employee.FullName, $Employee.LastLogin
From $Factory
Left Join $Employee <using $Factory.Employees>
Where $Employee.CanLogin = ?canLogin
AND DateDiff(month, GetDate(), $Employee.LastLogin) <= 2",
},
"parameters": [
{
"name": "canLogin",
"dataType": "boolean",
"default": true
}
]
}
```
Executing the Query from Business Logic

Example: Siemens.MOM.PocModel.DispatchQuery.momsql.json

```
Query example
```
```
public void RunQuery()
{
MetamodelQuery qry = new MetamodelQuery("Siemens.MOM.PocModel.DispatchQuery");
qry.SetParameterValue("parameterName", value);
qry.OverrideParameterExpression("parameterName", "Expression");
```

```
qry.InitializeParameters();
QueryResult res = qry.Execute();
// QueryResult should have support for Linq functions.
//
}
```
Customer override

Customers can replace the definition by specifying the replacement in a file Siemens.MOM.PocModel.DispatchQuery@workspace.momsql.json

Query Overrides should still maintain the contract from the base query for output columns & the input parameters.


# Accessibility Configuration of CO Fields

Accessibility of CO Fields influences how the external components interact with the Metadata Models through the various interfaces such as REST API or
async Messaging through gRPC.

Possible values for the Accessibility of CO Fields:

```
None
Readable
Writeable
Both Readable/Writeable
```
Input DTO models should only generate properties for 'Writeable' fields to accept input from external clients of REST endpoints. 'Readable' only fields
should not have the property generated.

Similarly, Request/response models should only include fields that marked as 'Readable'. 'Writeable' only fields should not have the property generated.

Protobuf models for the gRPC Messaging will include only CO Fields that are both 'Readable' & 'Writeable'.

## Sidecar JSON Configuration

Read/Write accessibility of Configurable Object's Fields can be configured through the sidecar json config file similar to the various other field properties.

The following is a quick example of Accessibility configuration:

"SomeField": {
"Description": "<desc>",
"Label": "<label>",
"Persistence": {
"ColumnName": "<col_name>"
},
"Accessibility": {
"Readable": true,
"Writeable": true,
}
},

"SomeOtherField": {
"Accessibility": {
"Readable": true,
"Writeable": false,
}
},

## Configuration through Code

In cases when the sidecar json file are not available or not needed the the Accessibility of CO Fields can be configured through the C# code of the model.

The following is a quick example of Accessibility configuration:

[Accessibility(FieldAccessibilityFlag.Read)]
public StringField ReadOnlyField = null;
public string ReadOnly { get => ReadOnlyField.Value; }


# Required Fields on Persistent Entities

```
Introduction
Enabling an Entity for Persistence
Default Field Mappings
Platform Types and Interfaces
Objects extending BaseObject
Objects that implement INamed
Objects that implement ISubentityOf<TParent>
Objects that implement INamedSubentityOf<TParent>
Objects that implement IRevisioned<TRevisionBase>
Objects that implement IRevisionBase<TRevision> **
Composition Example
Override Example
```
## Introduction

MetaModel types by default are not persistent. To enable the persistent behavior for a MetaModel type, a Persistence configuration must be added to the
model.

Also, depending on the model type or interfaces it implements, certain fields are required in the persistence configuration.

These persistence configurations must be present prior to invoking the write model generation process.

## Enabling an Entity for Persistence

For each user defined MetaModel type, there exists a sidecar json file. The sidecar JSON file has the same name as the Model class by convention but
with a .json file extension.

For a MetaModel type named MyObject.cs, its sidecar JSON file is MyObject.json

The Persistence node in the JSON defines several attributes and fields.

The minimum required configuration must include TableName

### {...

```
"Persistence": {
"TableName": "MyObject"
}
}
```
When the above configuration is present, the a persistent write model and map for type MyObject are generated.

For System Object persistence, refer to this document: System Objects & Persistence

## Default Field Mappings

Once a TableName is defined, several fields are automatically mapped without any configuration.

The fields below are configured by default and cannot be changed.

```
Id - the unique identifier
Represents a globally unique that can be used on its own to lookup an entity of any type
COType id is embedded in its value
Stored in the database as char(18)
Required for every object implementing IEntity
Discriminator - field used to differentiate subtypes
Represents the fully qualified type name of the entity
Stored as varchar
Used by the ORM layer to map the database row into the proper runtime type
ChangeCount - field used to keep track of concurrent modifications
Represents the number of changes that have occurred. Newly inserted rows have a value of 1.
Stored as a 64bit integer
Used by the ORM to order changes performed by multiple requests
```
## Platform Types and Interfaces

The platform provides several interfaces and abstract objects from which all MetaModel types are derived.

A persistence configuration must minimally satisfy the contracts of objects that implement or extend platform interfaces and types.


When the minimal persistence configuration is not satisfied, write models will generate but won't compile.

The Write models parallel a subset of the platform types and interfaces and include:

```
Abstract Types
BaseObject
RevisionBase<T>
Interfaces
IEntity
IBaseObject
INamed
ISubentityOf<T>
INamedSubentityOf<T>
IRevisioned<T>
IRevisionBase<T>
```
Objects extending BaseObject

```
BaseObject is the root object of the type hierarchy and implements IEntity which exposes the property Id of type string
Most objects extend BaseObject
For objects that extend only BaseObject, the default field mappings are all that's required for a valid configuration.
When the sidecar json file contains a Persistence node as described above, a write model is generated for a given MetaModel type.
Additionally, Objects extending BaseObject will include the default mappings plus any additional persistent fields specified.
```
Objects that implement INamed

Persistent objects that implement the INamed interface, require the following persistent field configuration.

```
Name of type string
```
Objects that implement ISubentityOf<TParent>

Persistent objects that implement the ISubentityOf<TParent> interface, require the following persistent field configuration.

```
Parent of type TParent
```
Objects that implement INamedSubentityOf<TParent>

Persistent objects that implement the INamedSubentityOf<TParent> interface, require the following persistent field configurations.

```
Parent of type TParent
Name of type string
```
Objects that implement IRevisioned<TRevisionBase>

Persistent objects that implement the IRevisiond<TRevisionBase> interface, require the following persistent field configurations.

```
Base of type TRevisionBase
IsRevOfRec of type boolean
Revision of type string
```
Objects that implement IRevisionBase<TRevision> **

**Note: There is no such interface in the platform.

Instead there is an implicit implementation that the metamodel framework automatically adds when a user defined type implements
IRevisioned<TRevisionBase>.

To enable the RevisionBase functionality, simply create two JSON files for the IRevisioned type.

For the given Revisioned type 'MyRevisionType' three files are required.

MyRevisionType.cs - The MetaModel type definition C# file.

MyRevisionType.json - MetaModel type sidecar JSON file

MyRevisionType`RevisionBase`.json - MetaModel type definition RevisionBase file.

Persistent objects that implement the IRevisionBase<TRevision> pseudo interface, which means having a Type`RevisionBase`.json file, require the
following persistent field configurations.


```
RevOfRec of type TRevision
Revisions of type IEnumerable<TRevision>
```
For Custom Revision Base Types: See Revisioned & Revision Base Types

Composition Example

For MetaModels that implement multiple interfaces, minimum persistent field configurations must include the mappings for each interface.

For example consider the Revisioned Model Component / ComponentBase that implements: INamed and IRevisionBase<TRevision>

A persistent object of this type would require a file named Component`RevisionBase`.json and include persistent field configurations for both interfaces.

```
Name of type string (required by INamed)
RevOfRec of type TRevision (required by IRevisionBase<TRevision>)
Revisions of type IEnumerable<TRevision> (required by IRevisionBase<TRevision>)
```
The sidecar json file would look like the following.

### {

```
"Description": "Base Component Revision Object",
"Label": "RevisionBase<Component> Field Mappings",
"Persistence": {
"TableName": "ComponentBase"
},
"Fields": {
"Name": {
"Label": "Name, required by INamed",
"Persistence": {
"ColumnName": "ComponentName",
"Length": 128
},
"Required": "SystemRequired"
},
"RevOfRcd": {
"Label": "RevOfRcd, Required by IRevisionBase<TRevision>",
"Persistence": {
"ColumnName": "RevOfRcd"
}
},
"Revisions": {
"Label": "Revisions, Required by IRevisionBase<TRevision>",
"DynamicList": {
"LinkField": "Base",
"Sort": {
"Revision": "Asc"
}
}
}
}
}
```
Override Example

For MetaModels that override existing models, a specific convention must be followed to utilize the functionality.

For example consider the DinnerItem type in module Menu. This class defines a number of fields as shown below.

The filename DinnerItem.cs

```
[COTypeId("111111")]
public partial class DinnerItem: NamedObject
{
protected override void _RegisterFieldTypes()
```

### {

```
base._RegisterFieldTypes();
RegisterField(FieldInitializer.StringField<DinnerItem>.Get("DinnerItemTitle"));
RegisterField(FieldInitializer.StringField<DinnerItem>.Get("DinnerItemDescription"));
RegisterField(FieldInitializer.DecimalField<DinnerItem>.Get("DinnerItemPrice"));
}
```
```
public StringField DinnerItemTitleField = null;
public string DinnerItemTitle{ get => DinnerItemTitleField.Value; set => DinnerItemTitleField.Value =
value; }
```
```
// remaining field declarations omitted
}
```
It's corresponding json file is named DinnerItem.json

### {

```
"Description": "Dinner Item",
"Label": "CDOName_DinnerItem",
"Persistence": {
"TableName": "DinnerItem"
},
"Fields": {
"InstanceId": {
"Label": "Instance Id",
"Persistence": {
"ColumnName": "Id",
"Length": 128
},
"Required": "SystemRequired"
},
"Name": {
"Label": "Dinner Item"
},
"DinnerItemTitle": {
"Label": "Title",
"Persistence": {
"ColumnName": "DinnerItemTitle",
"Type": "String"
},
"Required": "UserInputRequired"
},
"DinnerItemDescription": {
"Label": "Description",
"Persistence": {
"ColumnName": "DinnerItemDescription",
"Type": "String"
}
},
"DinnerItemPrice": {
"Label": "Price",
"Persistence": {
"ColumnName": "DinnerItemPrice",
"Type": "Decimal"
},
"Required": "UserInputRequired"
}
}
}
```
The table data for this model would appear as shown below.


### 1.

### 2.

### 3.

### 4.

### 5.

```
a.
b.
i.
c.
i.
6.
a.
b.
```
To override the DinnerItem class, you define another module. In this example it's named: LimitedMenu

Declare a type of the same name as declared in the original module.

The overridden class declaration for DinnerItem has the following differences.

```
The class name must match the Menu module type DinnerItem
The class attribute [COTypeId] is removed
The class attribute [HasInterceptableMethods] is added
Any base classes are removed ( i.e. NamedObject)
The Field registration is done in a method named differently from the original class.
If the original field registration method is named _RegisterFieldTypes
The overridden method might be RegisterFieldTypes_<modulename>
This method name must be unique across all partial classes
The overridden method must be annotated with the attribute
[InterceptAfterMethod("_RegisterFieldTypes")]
The filenames are important:
They must be in the form Model@TargetModule (.cs / .json)
Where TargetModule is where the field is overridden.
```
In this example, the filename is DinnerItem@LimitedMenu.cs

```
[HasInterceptableMethods]
public partial class DinnerItem
{
[InterceptAfterMethod("_RegisterFieldTypes")]
protected override void RegisterFieldTypes_LimitedMenu()
{
RegisterField(FieldInitializer.IntegerField<DinnerItem>.Get("QuantityAvailable"));
}
```
```
public IntegerField QuantityAvailableField = null;
public int QuantityAvailable{ get => QuantityAvailableField.Value; set => QuantityAvailableField.Value
= value; }
}
```
The overridden type's corresponding json file is named DinnerItem@LimitedMenu.json

### {

```
"Description": "Limited Dinner Item",
"Label": "CDOName_LimitedDinnerItem",
"Fields": {
"InstanceId": {
"Label": "Instance Id",
"Persistence": {
"ColumnName": "Id",
"Length": 128
},
"Required": "SystemRequired"
},
"QuantityAvailable": {
"Label": "Price",
"Persistence": {
"ColumnName": "QuantityAvailable",
"Type": "Integer"
```

### },

```
"Required": "UserInputRequired"
}
}
}
```
When the the overriding module is built and deployed, the existing DinnerItem table will be updated to include the overridden field as shown.


# Selection Values

## Definition

Selection Values is a feature of the metamodel that provides a way to configure 'list of values' for any Configurable Object's Field. This is particularly used
in populating the UI screens with the potential list of values.

## Configuration

Selection Values are exposed on Configurable Object's Field configuration properties. The Selection Values for any field could be configured using the
json side-car config file similar to the various other field properties.

The following is a quick example of selection values configuration:

```
"State": {
"Description": "Factory State",
"Label": "Factory_State",
"Persistence": {
"ColumnName": "State"
},
"SelectionValues": {
"Mode": "Static"
}
},
```
## Modes of Selection Values

The metamodel provides various ways to produce selection values for a field for easy configurability. There are 4 different ways on how selection values
can be produced by a simple switch of a configuration along with another way to turn off the selection values all together.

The various modes of Selection Values are:

```
Static
ListExpr
ListExprOrStatic
Event
Query
None
```
Static Selection Values

Static Selection Values are the easiest way of configuring and producing the selection values. In this mode the platform runtime automatically produces the
list of values (selection values) for the field based on the field's meta properties, such as the object relationship information. For example, on a field that is
used to define an association relationship with another object, the 'Static' selection values are automatically produced by the platform runtime by inspecting
the corresponding associated objects' type and retrieving all the instance of that type as the result for selection values.

Example:

```
"PersistentListEmployees": {
"SelectionValues": {
"Mode": "Static"
}
},
```
This is a field in Factory class of the Sample Model - INamedObjectRefListField<Employee> PersistentListEmployees. In this case the field is of type
'Employee' so the auto populated selection values will be a list of 'Employee' instances.

Another Example:

```
"State": {
"SelectionValues": {
"Mode": "Static"
}
},
```
This is a field in Factory class of the Sample Model - FactoryStateEnum State. In this case the field is a special EnumField that represents the
enumeration of type 'FactoryStateEnum' so the auto populated selection values will be a list of the enumeration values for this enum definition.

ListExpr based Selection Values


Sometime the source for the selection values could be a list value such as another 'List' type of field or an some calculated List that's available/accessible
at runtime. So, this mode of Selection Values allows configuring an expression as the source that could supply the values for this selection values. This
expression would typically be a field expression, i.e. a path to target field starting from the current object's context or an in-memory variable expression that
is accessible & available from the current object's runtime execution context.

Example:

```
"FirstLocation": {
"SelectionValues": {
"Mode": "ListExpr",
"ListExpr": "Locations" // based on current object context
}
},
```
This is a field in Factory class of the Sample Model - Location FirstLocation. In this case the field is of type 'Location' and since the model is 'ListExpr', the
corresponding source list expression is evaluated at the runtime to auto populated selection values.

ListExprOrStatic based Selection Values

This mode is a combination of ListExpr & Static modes. The selection values are first evaluated based on the list expression and are returned if the
expression results into a non-empty result. Ortherwise, Static selection values are returned.

Example:

```
"FirstLocation": {
"SelectionValues": {
"Mode": "ListExprOrStatic",
"ListExpr": "Locations" // based on current object context
}
},
```
This is a field in Factory class of the Sample Model - Location FirstLocation. In this case the field is of type 'Location' and since the model is 'ListExpr', the
corresponding source list expression is evaluated at the runtime to auto populated selection values. If the list expression results into an emty list then the
Static selection values are evaluated and returned.

Event-based Selection Values

Sometimes, auto-populated selection values might not serve the purpose and there may some business logic involved to produce such selection values.
To provide for these instances, 'Event' mode is available to configure the custom logic to produce the selection values.

'OnSelectValues' is a field level event supported by the metamodel. This event is available on every field and this event could be used to configure an
event handler with custom logic to produce the selection values.

Example:

```
"DynListEmployees": {
"SelectionValues": {
"Mode": "Event"
},
```
```
protected override void _InitializeFieldEvents()
{
// Assign Selection Value events
DynListEmployeesField.OnSelectValues += GetDynListEmployeesSelectionValues(...);
```
In this mode selection values are produced by the custom event handler attached to this field's OnSelectValues event.

Query-based Selection Values

Sometimes, auto-populated selection values might not serve the purpose and there may some custom query that could produce such selection values. To
provide for these instances, 'Query' mode is available to configure the custom query to produce the selection values.

Example:

```
"AdministratorLocation": {
"Label": "AdministratorLocation",
"Persistence": {
"ColumnName": "AdministratorLocationId"
},
```

```
"SelectionValues": {
"Mode": "Query",
"Query": {
"Name": "FactoryWithLocationQuery",
"Parameters": {
"EmpName": "\"%\"",
"Qty": "90.0m"
}
}
}
}
```
Notes:

```
See REST API section on Selection Values for an example on how to request selection values and pass in values to the query parameters
See page Metadata Model Query Definitions for description on how to define Metadata Queries
```
Platform API

The Metamodel interface type of 'IField' that is implemented by every field in the model provides the following API to request for selection Values:

```
DataTable GetSelectionValues(IPagingOptions options);
```
Example code to invoke selection values:

```
// Request SelectionValues
var stateFld = factory.GetFieldMetadata("State");
var selVal = stateFld.GetSelectionValues(new PagingOptions());
```
For requesting the selection values through the REST API, see the API documentation in the 'Request' data section...


# System Objects & Persistence

## What are System Objects

System Objects are Configurable Objects defined inside the Metadata BCL/Framework but these objects are not visible & accessible to user models. The
System Objects are used for platform internal needs and are not accessible to user models for customizations or extensions unlike the regular configurable
objects. System Objects can only be created or accessed by platform developers.

## How to create System Objects

There is BCL Framework interface type 'ISystemObject' to tag objects as System objects. There is also a pre-defined base class called 'SystemObject'
which implements this interface and is used for defining additional system objects by the platform developers.

## How are System Objects Configured

System Objects can be defined in C# code like any other configurable object. All features of configurable objects apply to a System Object, except for
customization & extension mechanism. System Objects also do NOT support json-based sidecar configuration of the configurable objects.

Configuration options exposed by the Metamodel through the json-based sidecar file will have to be implemented differently for System Objects. Currently,
the persistence options of the configuration is implemented & supported on System Objects.

## Persistence Configuration for System Objects

A new C# Attributes 'PersistentObject' & 'PersistentField' are implemented to support for persistence configuration of the System Objects. These attributes
can be used to specify all the persistence options of the objects & the fields.

Example:

[PersistentObject("PersistentMessage")]
[COTypeId("0h7AwDp")]
internal class PersistentMessage : SystemObject
{
// State Field
[PersistentField("State")]
[PersistAs(PersistAsAttribute.EnumAs.Value)]
public EnumField<PersistenMessageStateEnum> StateField = null;
public PersistenMessageStateEnum State { get => StateField.Value; set => StateField.Value = value; }

// Last Activity Timestamp
[PersistentField("LastActivityTime")]
public DateTimeField LastActivityTimeField = null;
public DateTimeOffset LastActivityTime { get => LastActivityTimeField.Value; set => LastActivityTimeField.Value = value; }


# Framework for Trackable & Audit Trail Objects

## Trackable Objects

```
Add new platform interface to called ITrackableObject is available to tag any CO as a trackable object
Deletion of Trackable Objects will be based on App specific system configuration
Configuration is defined through AppSettings
Later, that could be configured and fed into the AppSettings through the Configurator
When enabled, ‘delete’ of Trackable Objects behave like any other CO instance delete (hard delete from database)
When NOT enabled, an error will be raised to indicate that Trackable Objects cannot be deleted
New Business capabilities should be added such as a state machine or something similar to take care of such business
requirements to support for instead of ‘deletes’
Default value is to allow deletes
```
## Configuration:

## - Add the following section to the appSettings.json or the ModelSettings.json under ModularMOM secti

## on

```
"MetadataEngine": {
```
```
"AllowTrackableObjectDelete": "$$ALLOW_TRACKABLEOBJECT_DELETE$$"
},
```
## Audit Trail

```
Introduce new interfaces for representing Audi Trail type of objects IAuditTrail, IModelingAuditTrail, ITrackableAuditTrail
Audit Trail type of objects can NOT be deleted, platform will validate and prevent deletion of such objects
```

# Message Model Extensions

## Extending Message Models:

To create a new message model M1_Messages_Extension model by extending the base message model M1_Messages:

```
Extend Model similar to domain model
See the following sample scenario/usage for extending Message Models: Example Scenario for Message Model Extension
```
## Extending Domain Models from a Base Domain Model that has a Message Model as a Binary

## Dependency:

To create a new domain model M1_Producer_Extension by extending the base domain model M1_Producer which has a binary dependency to
message model M1_Messages:

```
Extend Model
From the Base Message Model's modelconfig, for each binary dependency
Add dependency explicitly to the Extended domain Model
Add dependency with Action = 'Include' in modelconfig
Add Project Reference with 'Include' (XML Attr) in dependencies props
```
```
modelconfig.json
```
```
"binaryDependencies": [
{
"name": "M1_Messages",
"version": "1.0.1",
"Action": "Include"
}
]
```
```
dependencies.props
```
```
<ItemGroup Label="MMOM_Binary_Dependencies">
<PackageReference Include="M1_Messages" Version="1.0.1" />
</ItemGroup>
```
## Adding Extended Message Model as a Binary Dependency to a Domain Model:

To add a model M1_Messages_Extension model, which is an extension of the base message model M1_Messages, as new binary dependency:

```
Identify base Message Model, if any (composed message models), for each
Add 'Remove' dependency for the base Message Model
Add dependency with Action = 'Remove' in modelconfig
Add Project Reference with 'Remove' (XML Attr) in dependencies props
Add dependency for the Extended Message Model
Add dependency with Action = 'Include' in modelconfig
Add Project Reference with 'Include' (XML Attr) in dependencies props
```
```
modelconfig.json
```
```
"binaryDependencies": [
{
"name": "M1_Messages",
"version": "1.0.1",
"Action": "Remove"
},
{
```

```
"name": "M1_Messages_Extension",
"version": "1.0.1",
"Action": "Include"
}
]
```
```
dependencies.props
```
```
<ItemGroup Label="MMOM_BinaryDependencies">
<PackageReference Remove="M1_Messages" Version="1.0.1" />
<PackageReference Include="M1_Messages_Extension" Version="1.0.1" />
</ItemGroup>
```
Removing Extended Message Model as binary dependency:

To remove M1_Messages_Extension model, which is an extension of M1_Messages, from binary dependencies:

```
Identify base Message Model, if any (composed message models), for each
Add dependency for the Base Message Model
Replace 'Remove' with 'Include' (Action) in modelconfig
Replace 'Remove' with 'Include' (XML Attr) in dependencies props
Remove Extended Message Model dependency
Remove dependency from modelconfig
Remove Project Reference from dependencies props
```
```
modelconfig.json
```
```
"binaryDependencies": [
{
"name": "M1_Messages",
"version": "1.0.1",
"Action": "Include"
}
]
```

# Example Scenario for Message Model Extension

## Base Message Model

```
Create a message model 'M1_Messages'
Add a new Message type Configurable Object
```
```
Message Types
```
```
[COTypeId("2jpZsvF")]
public abstract partial class ResourceCommand : MessageObject
{
public string Name { get => NameField.Value; set => NameField.Value = value; }
public NameField NameField = null;
```
```
public ResourceStateEnum State { get => StateField.Value; set => StateField.Value = value; }
public StdEnumField<ResourceStateEnum> StateField = null;
```
```
public string Notes { get => NotesField.Value; set => NotesField.Value = value; }
public NameField NotesField = null;
```
```
protected override void _RegisterFieldTypes()
{
base._RegisterFieldTypes();
```
### //

```
RegisterField(FieldInitializer.NameField<ResourceCommand>.Get("Name"));
RegisterField(FieldInitializer.StdEnumField<ResourceCommand, ResourceStateEnum>.Get("State"));
RegisterField(FieldInitializer.NameField<ResourceCommand>.Get("Notes"));
}
```
```
protected override void _InitializeObjectEvents()
{
base._InitializeObjectEvents();
OnInitialize += InitializeDestination;
}
```
```
public virtual void InitializeDestination()
{
Destination.Add("<ConsumerApp>");
}
}
```
```
[COTypeId("2jp9CwG")]
public partial class CreateResourceCommand : ResourceCommand
{
protected override void _RegisterFieldTypes()
{
base._RegisterFieldTypes();
}
}
```
```
[COTypeId("2jpP5yD")]
public partial class UpdateResourceCommand : ResourceCommand
{
protected override void _RegisterFieldTypes()
{
base._RegisterFieldTypes();
}
}
```
```
public enum ResourceStateEnum : byte
{
Created = 0,
Up,
Down
}
```

Producer Model

This example demonstrates the scenario where changes to Modeling type of objects are needed to be propagated to other Apps.

```
Create a domain model 'M1_Producer'
Create a Modeling type Configurable Object
Create and customize the corresponding Modeling service
Configure the Service to propagate messages on Create & Update actions
```
```
Resource.cs
```
```
[COTypeId("2jl1K3S")]
public partial class Resource : NamedObject
{
protected override void _RegisterFieldTypes()
{
base._RegisterFieldTypes();
RegisterField(FieldInitializer.OptionsField<Resource, ResourceStateOptions, byte>.Get("State"));
}
```
```
[PersistAs(PersistAsAttribute.EnumAs.Value)]
public OptionsField<ResourceStateOptions, byte> StateField = null;
public ResourceStateOptions State { get => StateField.GetOptionsValue(); set => StateField.
SetOptionsValue(value); }
}
```
```
public partial class ResourceStateOptions : BaseOptions<ResourceStateOptions, byte>
{
public ResourceStateOptions(string name, byte value)
: base(name, value)
{ }
```
```
[EnumDisplayName("DN-Created")]
[EnumDescription("Desc for Created")]
public static readonly ResourceStateOptions Created = new ResourceStateOptions(nameof(Created), 0);
```
```
[EnumDisplayName("DN-Up")]
[EnumDescription("Desc for Up/Active")]
public static readonly ResourceStateOptions Up = new ResourceStateOptions(nameof(Up), 1);
```
```
[EnumDisplayName("DN-Down")]
[EnumDescription("Desc for Down/Closed")]
public static readonly ResourceStateOptions Down = new ResourceStateOptions(nameof(Down), 2);
}
```
```
Resource.json
```
### {

```
"Description": "A Resource defines a machine used to process WIP.",
"Label": "Resource",
"AssociatedServiceName": "<namespace_name>.ResourceMaintService",
"Persistence": {
"TableName": "Resource"
},
"Fields": {
"InstanceId": {
"Label": "ResourceId"
},
```

```
"State": {
"Description": "Resource Status",
"Label": "Resource Status",
"Required": "SystemRequired",
"Persistence": {
"ColumnName": "State"
}
}
}
}
```
```
ResourceMaintService.cs
```
```
[COTypeId("2jlKQXK")]
[MessageProducer(typeof(CreateResourceCommand))]
public partial class ResourceMaintService : NamedObjectService<Resource>
{
ResourceCommand _OutboundCommand = null;
public override bool Create_User()
{
_OutboundCommand = GetFeature<MessageProducer>().CreateMessage<CreateResourceCommand>();
return base.Create_User();
}
```
```
public override bool Update_User()
{
_OutboundCommand = GetFeature<MessageProducer>().CreateMessage<UpdateResourceCommand>();
return base.Update_User();
}
```
```
public override bool AfterExecute_User()
{
this.ApplyMapTo(_OutboundCommand, "ResourceMaint->ResourceMessage");
return base.AfterExecute_User();
}
}
```
```
ResourceMaintService_Maps.cs
```
```
[CDOMapName("ResourceMaint->ResourceMessage")]
public partial class ResourceMaintService_to_MessageObject<TSource, TTarget> : CDOMap<TSource, TTarget>
where TSource : ResourceMaintService where TTarget : ResourceCommand
{
protected override void InitializeFieldMaps()
{
base.InitializeFieldMaps();
```
```
mfieldMaps["Name"] = (src, trgt) => { trgt.Name = src.ObjectToChange.Name; };
mfieldMaps["State"] = (src, trgt) => { trgt.State = (ResourceStateEnum)src.ObjectToChange.State.
Value; };
mfieldMaps["Notes"] = (src, trgt) => { trgt.Notes = src.ObjectToChange.Description; };
}
}
```
Extended Message Model

```
Create new Message model 'M1_Messages_Entension' extending from the 'M1_Messages'
Override the Message types defined in the base Message model
```
```
Message Type Overrides
```

```
[HasInterceptableMethods]
public partial class CreateResourceCommand
{
public string Extn { get => ExtnField.Value; set => ExtnField.Value = value; }
public StringField ExtnField = null;
```
```
[InterceptAfterMethod("_RegisterFieldTypes")]
protected void My_RegisterFieldTypes()
{
RegisterField(FieldInitializer.StringField<CreateResourceCommand>.Get("Extn"));
}
}
```
Extended Producer Model

```
Create new Producre model 'M1_Producer_Extension' by extending from 'M1_Producer'
Override Modeling Object & its Service
```
```
Resource Overides
```
```
[HasInterceptableMethods]
public partial class Resource
{
[InterceptAfterMethod("_RegisterFieldTypes")]
private void My_RegisterFieldTypes()
{
RegisterField(FieldInitializer.StringField<Resource>.Get("Extn"));
}
```
```
public string Extn { get => ExtnField.Value; set => ExtnField.Value = value; }
public StringField ExtnField = null;
}
```
```
Resource Overrides json
```
### {

```
"Description": "Extended Resource - defines a machine used to process WIP.",
"Fields": {
"Extn": {
"Description": "Extended Field",
"Label": "Resource Extn",
"Required": "None",
"Persistence": {
"ColumnName": "Extn"
}
}
}
}
```
```
ResourceMaintService Overrides
```
```
[HasInterceptableMethods]
public partial class ResourceMaintService_to_MessageObject<TSource, TTarget> : CDOMap<TSource, TTarget>
where TSource : ResourceMaintService where TTarget : ResourceCommand
{
[InterceptAfterMethod("InitializeFieldMaps")]
private void MY_InitializeFieldMaps()
{
mfieldMaps["Extn"] = (src, trgt) => { trgt.Name = src.ObjectToChange.Extn; };
}
}
```


# Metamodel Override Features

```
Introduction
```
## Introduction

Metadata Objects represent a business objects by defining a collection of Metadata Fields and logic. These objects and fields are the fundamental data
structures and building blocks of the Framework. Metamodel objects are just like typical C# object in that they are extensible in the typical type hierarchy
sense. For example one object may extend another. However, a much more flexible feature of the platform is its Override mechanism. This platform
feature allows partial classes to be defined in a manner that override other class definitions. Each of the partial classes are maintained in their own
repositories and at compile time, the override infrastructure weaves the partial classes into a single composite compilation unit containing all field
definitions. This mechanism is powerful because it allows you to tailor new implementations by leveraging existing models. This approach follows the open-
closed principle by allowing for any variation of class customization, while maintaining a common shared code base that need not change.

With or without the override mechanism, the object and field framework provide a rich set of modeling capabilities. Each field provides strong typing and
encapsulates a single value or object reference. List type fields hold a collection of values or references. Fields have a type specific configuration that
defines attributes and behaviors such as if input is required, its precision or length, it's persistence, default value and current value expressions among
other settings. Metamodel objects and fields are defined as C# classes and the features or each are exposed via attributes and an accompanying .json
file. A link to an override example is below.

Override Code Example


# Configurable Object Maps (COMap)

Configurable Object Map, also known as a CO Map, is a feature that provides a configurable way of mapping information between any two Configurable
Objects. A CO Map has a source object and a target object. It contains a collection of Field Mappings, which are an evaluable expression bound to the
source object and mapped to a specific field on the target object. When a CO Map is applied/executed at runtime, the source field expression is evaluated,
and the corresponding result value is copied to the target field on the target object.

CO Maps support object-oriented features such as Inheritance and Polymorphism. With these features, the concept of CO Maps would be a very powerful
feature that can simplify a lot of simple assignment statements in the business logic definition.

Triple Dispatch: Multiple dispatch is a feature of some programming languages in which a function can be dynamically dispatched based on the run time
(dynamic) type or, in the more general case, some other attribute of more than one of its arguments [wiki].

With CO Maps, the dynamic dispatching is based on the three factors that are in play – the source object, the target object & the map. Since both the
Configurable Object & the CO Map support inheritance and polymorphism, the actual runtime type information of these instances will determine which map
is executed at runtime.

## CO Map - Field Mappings

CO Map Field Mappings are implemented as C# languages delegates and are designed to be configurable easily. These field mappings that are
configured at design time are executed or applied at runtime to actually copy data between objects.

## Field Map Strategies

There are 4 primary strategies that are employed for mapping field between objects:

Normal Map:

This is the simplest of all and need the source and target field name on the object types involved. This type of map copies over the value from the source
field to the target field overwriting the target field value if any.

Example Configuration:

```
// MapFields keyed by target field name
```
```
mfieldMaps["Field1"] = (src, trgt) => { trgt.Field1 = src.Field1; };
```
Accumulate Map:

This type of map is specifically useful on numeric type of fields where the target field is used as an accumulable field and values from multiple source fields
can be mapped into this target field and the target field keeps accumulating all source field values.

Example Configuration:

```
// MapFields keyed by target field name
```
```
mfieldMaps["DefaultQty"] = (src, trgt) => { trgt.DefaultQty += src.DefaultQty; }; // Accumulate
```
Object Hierarchy Map:

This type of map is applicable when a hierarchy of objects have to be replicated into target hierarchy of objects. It's almost equivalent to a deep-clone
except that the target types may have their own type hierarchy. This map type applies to both scalar (single-valued) or list (multi-valued) fields. (See
Sample Model for concrete example...)

Example Configuration:

```
// trigger auto map
```
```
mfieldMaps["auto"] = (src, trgt) =>
```

```
COMapper<SourceCOType, TargetCOType>.AutoMapObjectFields(src, trgt, "<SourceFieldName>",
"<TargetFieldName>")
```
```
.Invoke(src, trgt);
```
```
// trigger custom map for the nested hierarchy
```
```
mfieldMaps["auto"] = (src, trgt) =>
```
```
COMapper<SourceCOType, TargetCOType>.MapObjectFields(src, trgt, "<SourceFieldName>", "<TargetFieldName>", "<
COMapName>")
```
```
.Invoke(src, trgt);
```
Custom Map:

This type of map can be used to apply any custom logic. This is useful for complex mapping scenarios but makes this approach less configurable. This
type of mapping should be used sparingly.

Example Configuration:

```
// MapFields keyed by target field name
```
```
mfieldMaps["DefaultQty"] = (src, trgt) => { foreach ( var item in list) trgt.DefaultQty = src.DefaultQty * item.Qty; }; // Custom Logic
```

# [Partially Obsolete] Metadata Module Architectural Concept

Modular MOM is part of Opcenter leveraging the Xcelerator Cloud Architecture Strategy. The main objective of MOM Cloud Program is to define the next
generation strategy for ALL MOM applications that will ultimately support the entire product portfolio under the MOM umbrella. Modular MOM aims to
address high configurability capabilities with hybrid deployment strategy to support heterogeneous runtime infrastructures ranging from public cloud to
private cloud and even edge devices including hybrid scenarios.

## Modular MOM Architecture

The following image describes the technological concepts that form the core pillars to drive and influence the Modular MOM Architecture:

Based on the above technology pillars, the Modular MOM architecture is based on two key approaches:

```
Metadata-driven: The system behavior and business capabilities are defined through a configurable set of data covering all their aspects such as
data model, business logic, communication interfaces, user interfaces, deployment characteristics, etc.
Modular: The functional capabilities of the system are decomposed into a set of granular parts where each part represents a piece of the overall
application allowing the customers to pick and choose from an available set of parts that satisfy their business needs.
```
These key architectural approaches drive the Modular MOM architecture to fulfill the requirements of the pillars like Modular, Configurability, Extensibility,
etc


# Use Case View


# Example Business Scenario

Consider an example business scenario where Weigh & Dispense is a business module that as part of its business functionality frequently requires
materials that are supplied by a business module dealing with inventory and material management called Material Flow.

The Weigh & Dispense module puts in a request for the required materials and the Material Flow module manages the requests and fulfils them by
sending the required materials. Both the modules need to keep track of the open requests and the status of their corresponding material deliveries.
Another such business module Additive Manufacturing (MOM Business capabilities dealing with the functionalities of additive manufacturing, see https://e
n.wikipedia.org/wiki/3D_printing) that also requires materials supplied by the Material Flow module, and, the same flow of business process exists
between this and the Material Flow module.

## Use Cases for Example Business Scenario

The following diagram and the table below illustrate the high-level sample use cases for the loosely coupled modules or parts of the Modular MOM.

```
Key Use Cases Description
```
```
Weigh & Dispense A business module that deals with the weighing & dispensing business
features. As part of its business processes it may need raw materials, so
it depends on Material Call to request the required raw materials.
```
```
Material Call A set of common business capabilities for requesting materials when
needed. Material Call uses Material Flow to request and track the status
of the materials flow. Material Call can be used by any module that
wants to request materials from Material Flow.
```
```
Material Flow A business app module that is responsible for the inventory and stock
keeping of materials and dispatching of the materials when requested.
```
```
Additive Mfg. A business module that specializes for the Additive Manufacturing business
features. As part of its business processes it may need raw materials, so
it depends on Material Call to request the required raw materials.
```

# Logical View


# Modular MOM Design Approach

Traditional approaches take a model defined in a modelling language and transform it into an executable model in a standard programming language or
interpret the model at runtime. By inverting the control, we could start from a standard programming language and build abstractions atop to make it
suitable to serve as a modelling language. Since the basis is already an executable language, the model built on this modelling abstraction is executable
as is, without any further transformation or interpretation.

```
Build Custom Metamodel to fulfill the full spectrum of configurability aspects such as customizability, extensibility and upgradeability.
Leverage on a popular ubiquitous language to gain support from community in terms of tooling, resources, personnel, etc.
```
## Key Design Conclusions

```
No explicit code generation necessary but with all the benefits of executable code.
No new language to learn for the end-users.
Common programing constructs and techniques could be used within the boundaries set by the Custom Metamodel.
Allows for leveraging from a lot of commercial & community libraries.
What you see is what you debug, unlike both the code-generation and model-interpretation approaches.
Commodity & Commercial tools could be leveraged for troubleshooting, collaborating, etc.
```

# What Is Modularity?

Modularity is the Modular MOM architecture approach that is the base of the technology pillars defined by the business. The functional capabilities of the
system are decomposed into a set of granular parts where each part represents a piece of the overall application allowing the customers to select it from
an available set of parts that satisfy their business needs.

## Representation

The top-down representation of Modularity displays at first the Product that can contain one or more Apps that are composed of Models:


# What Are Metadata Models?

A Metadata model, also called Metadata Model or Business domain model, is an independent composition of related business features related to a
domain, subdomain or a bounded context and configured using the Modular MOM Metamodel. These models can be composed of other models
(composable) and extended (extensible). The grouping of the related business features together should follow the DDD (see Domain Driven Design)
principles such as bounded contexts to keep the models independent (see Asynchronous Messaging Patterns and Inter-App Communications for more
information).

The models can fall into two categories:

```
App model (Top-level model)
Reusable model (composable model) that can also be composed of other reusable models.
```
## Model Relationships

The Metadata Models created using the Modular MOM metamodel provides 2 distinct relationship modes between models to facilitate the usage of the
functionality provided by the models:

```
Composition: Also called Reusable Models. It facilitates using the functionality exposed by other models by composing these other models into
it like in a typical Object-Oriented Composition relationship. Metamodel Framework facilitates the composed models to be extended in the context
of the composing models to support customization and extensibility of the models.
```
```
The key aspects to keep in mind with this reuse pattern:
```
```
Deployment Aspects: The composed model becomes part of the composing model which results into a single binary or a deployment
package.
Data Sharing Aspects: This results in having a single store for data persistence that is shared by all the models hosted by the app which
enables easy data share across models internally.
Association: Also called Runtime dependent Models. It facilitates using the functionality exposed by other models like in a typical Object-
Oriented Association relationship. This is like a shared binary pattern of typical software engineering where the associated models in this
relationship only needs to be available in its binary form at design time. Hence the associated model cannot be customized or extended in the
context of the associating model, but it could be used as-is.
```
```
The key aspects to keep in mind with this reuse pattern:
```
```
Deployment Aspects: Each app has its own deployment package.
Data Sharing Aspects: This results in each app having its own independent store for data persistence and the data sharing across
models is possible only with runtime exchange of messages.
```
## Model Dependencies

When one or more Models are in relationship, a dependency between these models arise based on the type of relationship between them:

```
Design time: Arises when one or more models are related by composition. This is like the shared code pattern of typical software engineering.
With this relationship type, both the related models should co-exist at the design or development time, hence the term Design Time Dependency.
Modular MOM's Model customization techniques facilitate customizing and extending of the models without having to touch the models being
customized or extended. All customizations are stored as new artifacts, and this ensure independence between the extensions and the base
model. In the context of Model composition, the composed models could be extended in the context of the composing model. Modular MOM's
Metamodel and Model Customization ensures the independence of these models for individual evolution.
```
```
Example scenario with reusable models : an example of reusable models could be for common business capabilities like 'Electronic
Signatures' or 'Data Collection' which are common building block type functionalities that could be applied in the context of a typical
MOM app as depicted in the diagram below. Reusable Models could be leveraged with design time dependency pattern that allows for
the customization and extension of the composed models.
```

Runtime: When one or more models do not have any direct dependency between them but could have an indirect loose binding with another
Model with which they have a simple association type relationship (binary reference) at design time. The dependency arises from the fact that the
shared common model could be leveraged to exchange information at runtime since both the Models are aware of the shared common model.
This requires the Models and their corresponding Apps should coexist at runtime hence the term Runtime Dependency. This pattern enables
configuring interactions between Apps. The Apps participating in a conversation have a runtime dependency (Model Association relationship)
through the binary reference to a common model that defines the contracts/messages for these interactions.

```
Example scenario with the use of Message Models: Messages are the contracts of interactions between apps. Message models facilitate
the configuration of these app interactions. Message models could be reused across apps/models that participate in the inter-app
communications. Message Models facilitate runtime dependency.
```
```
As illustrated in the diagram above, the runtime dependency between the W&D Model and MF Model is because they interact with each
other at runtime by exchanging information. The contract for these interactions is defined in a common Message Model which both the
Models share. This runtime exchange of information causes the runtime dependency between the W&D Model and the MF Model.
```

# What Are Apps?

An App, also called Module, is a loosely coupled, isolated and independently deployable unit. In the context of this app you can customize every model
you bring into this app.

By requirement, Modular MOM Apps should be loosely coupled and should exist and run independently in isolation or work alongside of other apps
communicating and exchanging information between these apps. For the independence of these apps and to maintain loose coupling between them an asy
nchronous messaging pattern is employed for inter-app communications.

Each app is composed of:

```
A top-level app model called Metadata Model that provides app-specific customization of composed models (reusable models). It can be
independent.
One app model (the Metamodel associated to it) that is composed of zero or more reusable models.
```

# What are Products?

A product is an offering package composed of a logical collection of loosely couple modular Apps that contain configurable units called Models and
provide overarching solutions to a specific business domain or industry.

A product has one or more mandatory apps and zero or more optional apps.


# Development View


# Design Time Dependency with Composed Models

As explained in the Logical View section, Design time dependency facilitates usage of the models by composition. The composed models become part of
the composing model and hence produce a single binary unit for the combined models. This creates a dependency between the related models. To
facilitate independent evolution of these models and to manage the model dependencies Modular MOM’s Metamodel takes the following strategy to
provide a collaborative configuration experience:

```
Leverage the distributed source control system– ‘git’ for the metadata configuration management.
Leverage multi-repo strategy to manage multiple Metadata Models – individual repo for each Model.
Composed models, which are by themselves represented by its own repo, are embedded into the composing models by leveraging the git
submodule feature.
Composed Models could be referenced by a specific point in time concept like git tag. This allows for the models to evolve independently since
upstream changes do not mandate the dependent model to change (see What Are Metadata Models? and Development View sections )
```
As depicted with an example in the figure below:

```
Producer Model has a binary reference to Message Model
Consumer Model has a binary reference to Message Model
Both Producer and Consumer Models sets the dependency explicitly to version v1 of Message Model
This create a Runtime dependency between Producer Model & Consumer Model since they exchange messages defined in the common
Message Model
As all models evolve, Producer Model depends on v1 of Message Model even though vX is available
Version vX of Producer Model explicitly chooses to upgrade the new Message Model vX on demand
```

# Runtime Dependency

As explained in the Logical View section, Runtime dependency helps configurating of Inter-App communications. Messages are the contracts of interaction
between apps and Message models facilitate the configuration of these app interactions. Message models could be reused across apps/models that
participate in the inter-app communications. The configuration of these interactions involves defining a ‘message’, configuring what business services
produce this message and what business services consume this message. The actual framework of configuring messages are described in detail in Inter-
App Communications.
To maintain loose coupling between apps, the apps that use these messages have a binary dependency on the message model assembly. This reference
provides all the message types defined in the Message Model to be available for both the app business domain models to use while configuring the
services that produce/consume these messages.
To facilitate independent evolution of the involved models and to manage the model dependencies Modular MOM’s Metamodel takes the following strategy
to provide a collaborative configuration experience:

```
The metadata artifacts for a Model are represented as a repo under the distributed source control system– ‘git’, as mentioned above in Design
Time Dependency with Composed Models
The dependency between the models is set to binary dependency leveraging underlying Metamodel
The dependencies between the models are managed by leveraging the MS Build’s properties file concept. This allows for the binary
dependencies to be carried forward without additional configurations when these models are composed inside of another model.
Binary dependency is set to specific version of the binary. This also allows for the models to evolve independently (see What Are Metadata
Models? and Development View sections)
```
As depicted with an example in the figure below:

```
Producer Model has a binary reference to Message Model.
Consumer Model has a binary reference to Message Model.
Both Producer and Consumer Models sets the dependency explicitly to version v1 of Message Model.
This create a Runtime dependency between Producer Model and Consumer Model since they exchange messages defined in the common
Message Model.
As all models evolve, Producer Model depends on v1 of Message Model even though vX is available.
Version vX of Producer Model explicitly chooses to upgrade the new Message Model vX on demand.
```

# Model Dependencies : Example Scenarios

The image below depicts a common scenario where:

```
Material Call Model encapsulates the business capabilities dealing with managing and requesting the required materials.
Weigh & Dispense Model (W&D), which is a consumer of materials, uses the Material Call model by composition pattern and reuses its
functionality for managing requests of materials.
Material Flow Model (MF), which is an inventory or a supplier of materials, uses the Material Call model by composition pattern and reuses its
functionality for tracking and fulfilling the requests of materials
Material Call Messages Model defines the contract for inter-app communications is reused with binary dependency from the Material Call model.
```
## Model Relationships/Dependencies exhibited in the Example Scenario

```
Material Call Model is composed inside of Weigh & Dispense Model
Material Call Model is composed inside of Material Flow Model
Material Call Model has an association relationship to the Material Call Messages Model (binary reference)
Weigh & Dispense Model has a Design time dependency with Material Call Model
Material Flow Model has a Design time dependency with Material Call Model
Weigh & Dispense Model & Material Flow Model exchange information through the common shared Material Call Messages Model, they have an
indirect dependency between them. Hence, Weigh & Dispense Model & Material Flow Model has Runtime Dependency between them.
```

# Model Dependency : Risk Management

Modular MOM’s Metamodel and Modularity architecture, by design, facilitates Model relationships and dependencies to provide usage of the Models
across Apps and other Models. But models which are tied together because of shared models and messages cause conflicts with the loose coupling
requirements.

So, to better manage dependencies and mitigate any risks, Model Versioning and Release Management will be introduced into the Metadata Configuration.

## Model Versioning

Models will be versioned using Semantic Versioning concept. As an example, Message revisions will be backwards compatible within the context of the
Semantic Versioning strategy. Semantic Versioning strategy helps Models to evolve independently and maintain loose coupling.

## Release Management

Semantic Versioning and the Release Management will provide governance to manage the model dependencies and mitigate the risks to fulfil loose
coupling.

```
Note
```
```
Both Semantic Versioning and Release Management topics will be covered extensively in Configuration Management architecture.
```

# Runtime View


# High Level View

Modular MOM Apps adhere to the microservices style architecture and hence each App has its own dedicated data store and no data is shared directly
through the data store. Any information exchange between apps always happens through inter-process communications using asynchronous messaging.

The following diagram depicts the Runtime View of Modular MOM Apps:

The following diagram depicts some sample messages that are exchanged at runtime for the example scenario from the section Use Case View.


# Physical View


# Application Layout - Composed Models

Modular MOM Apps built by leveraging the design time dependency patterns result into a single running process (App instance). Each app will be hosted
in a separate process and all the models that the app is composed of are part of this single deployment package as you can see in the following diagram:


# Deployment Configuration

Modular Apps are based on the configurable metadata and are deployed by ingesting the metadata models into the Modular MOM’s Metadata Runtime at
the time of deployment. The necessary deployment configuration is ingested by using a manifest file as shown in the diagram below. For more information
about deployment, see the Modular MOM Software Architecture document.


# Deployment Architecture

Modular MOM is a distributed and scalable application platform that allows one or more runtime instances of an app to be deployed as per the load and
scalability needs of the app.
One of the primary architectural pillars for Modular MOM is the capability to be platform agnostic and be able to run anywhere – on-prem, cloud (private
/public), edge, etc. To fulfil this requirement Modular MOM leverages the Application Containerization technology and the Kubernetes everywhere strategy
for managing and orchestrating the application containers.

The following diagram is a simplistic depiction showing more than one instance of an app running and the possibility to deploy these instances on any
node in a distributed fashion.
For more information about deployment architecture, see the Modular MOM Software Architecture document.


# [To Remove] Metadata Concepts & How to Guides for

# Defining the Models


# How To Define Logs and Traces in the MetaModel

Modular MOM provides tracking tools to analyze the system behavior during the execution of manufacturing operations through relevant application
process. Each process functions in a unique way and provides a different outcome to improve the overall functionality:

```
Logging: To track error reporting and collect data of all the operations that have been carried out on a specific entity. Log files collect discrete
events to indicate which change caused the error, this information is displayed in log messages arranged in levels to better describe the
application activity.
For information about logs, see section Log Levels of Enabling Structured Logging.
Tracing : To follow a program flow and data progression in order to identify performance bottlenecks and find out how did get there. In general,
trace files store the activity history, function durations, passed parameters and how far did get into the function.
For information about tracing, see Instrumenting Applications.
```

# Enabling Structured Logging

Modular MOM logs provide information about the operation performed. The important aspect of logging is the log content since there are various structures
(fields inside a log entry that vary depending on the application ) for the entire application: some log entries need more fields than others and other log
entries do not need all the extra fields.

The structure of the log varies depending on the application required to distinguish the type of information logged as well as the log levels that are used to
convey the type of information logged.

A log entry contains two major parts: context and message, together with contextual properties repeated across log entries, specific log messages for
each log entry and the necessary fields to better describe the application activity.

## Log Levels

Depending on the information provided, the common log levels which specify the log severity are described below.

```
Verbose: Anything and everything you might want to know about a running block of code. At this level, sensitive application data can also be
logged. This level should never be enabled in a production environment.
Debug: Internal system events that are not necessarily observable from the outside, but useful for determining how something happened. Logs at
this level are used for interactive investigation during development. These logs should primarily contain information useful for debugging and have
no long-term value.
Information: Information events describe things happening in the system that correspond to its responsibilities and functions. Generally these are
the observable actions the system can perform. Logs at this level track the general flow of the application. These logs should have long-term
value.
Warning: When service is degraded, endangered, or may be behaving outside of its expected parameters, Warning level events are used. Logs
at this level highlight an abnormal or unexpected event in the application flow, but do not otherwise cause the application execution to stop.
Error: When functionality is unavailable or expectations broken, an Error event is used. Logs at this level highlight when the current flow of
execution is stopped due to a failure. These should indicate a failure in the current activity, not an application-wide failure.
Fatal: The most critical level. Fatal events demand immediate attention. Logs at this level describe an unrecoverable application or system crash,
or a catastrophic failure that requires immediate attention.
```
## Overall Design

The figure below depicts the future architecture for structured logging within the platform.

```
Exceptions Level
```
```
The exception message , the stack trace and all other useful information inside this level should be logged. The correct log level should be used to
distinguish critical, erroneous and informational exceptions.
```
```
It is recommended to enable only high-severity log levels at production, for example, Warning and above or Error and above. For more information on
log levels, see https://docs.microsoft.com/en-us/aspnet/core/fundamentals/logging/?view=aspnetcore-5.0.
```


# Using Metamodel Logs

The table below displays examples of how to add logging in all Metamodel classes both in services and models by leveraging the extension methods
available for the Logger property.

```
To Enter
```
```
Add information logs
Information log sample
```
```
Logger.LogInformation("Created ProductionOrderHeader
'{ProductionOrderHeaderName}'", ProductionOrderHeader.Name);
```
```
Add logs for error/warning paths
Error log sample
```
```
Logger.LogError("Operation Name '{B2MML_ProductionOrderOperationName}'
occurred more than once.", name);
```
```
Use LogDebug to add logs needed for
debugging the application Debug log sample
```
```
Logger.LogDebug($"Validating ImportProductionOrderService...");
```
```
Use LogTrace to add detailed logs
Trace log sample: detailed information on the received is logged
```
```
Logger.LogTrace("Import B2MML document: {B2MMLDocument}", B2MMLDocument);
```
```
Note: If a log message contains sensitive data, use the log trace to log it.
```
## Adding contextual properties of local logging to a Metamodel

Within Metamodel business logic it is possible to add local logging contextual properties in order to provide further information. (see Example of Log
Output below)

The example below describes the ProcessB2MMLProductionSchedule shopfloor service, this service may be issued to import multiple ProductionRequest
from B2MML. While importing data for each ProductionRequest, it is useful to add contextual information regarding the current ProductionRequest.

To accomplish it, you can simply leverage on Logger.BeginScope API and pass to it a message that contains the required contextual properties.

```
foreach (var productionRequest in dataArea.ProductionRequest)
{
using var logScope = Logger.BeginScope("Import B2MML for Production Request
'{B2MML_ProductionRequestId}'",
productionRequest.ID.Value);
// Here add logic to import the current ProductionRequest
} // At the end of the scope, local contextual information will no more be present in logs
```
## Example of Log Output

The following example is the result of adding contextual properties of local logging explained above.

```
Sample log output using a custom console formatter for metamodel unit tests
```
```
2021-06-14 13:55:19.945 ### Debug ### {OriginalFormat}=Validating ImportProductionOrderService...
###
```

2021-06-14 13:55:19.946 ### Trace ### B2MMLDocument=<?xml version="1.0" encoding="utf-8"?
><ProcessProductionSchedule releaseID="0.0.2" xmlns="http://www.mesa.org/xml/B2MML-V0600" xmlns:xsi="http://www.
w3.org/2001/XMLSchema-instance" xsi:schemaLocation ="http://www.mesa.org/xml/B2MML-V0600 B2MML-V0600-
ProductionSchedule.xsd" xmlns:Extended="http://www.mesa.org/xml/B2MML-V0600-AllExtensions"
><ApplicationArea><Sender><LogicalID>ModularMOM</LogicalID></Sender><Receiver><LogicalID>ModularMOM</LogicalID><
/Receiver><CreationDateTime>2020-06-21T13:20:00.000-05:00</CreationDateTime><
/ApplicationArea><DataArea><Process></Process><ProductionSchedule><ProductionRequest><ID>Z2-PO-Demo-0911<
/ID><StartTime>2020-09-10T03:20:00.000-05:00</StartTime><EndTime>2020-09-11T13:20:00.000-05:00<
/EndTime><Priority>5</Priority><SegmentRequirement><ID>0010<
/ID><MaterialRequirement><MaterialDefinitionID>Steel</MaterialDefinitionID><MaterialUse>Consumable<
/MaterialUse><Quantity><QuantityString>500</QuantityString><UnitOfMeasure>Unit</UnitOfMeasure></Quantity><
/MaterialRequirement><MaterialRequirement><MaterialDefinitionID>Mat-1<
/MaterialDefinitionID><MaterialUse>Consumable</MaterialUse><Quantity><QuantityString>100<
/QuantityString><UnitOfMeasure>Unit</UnitOfMeasure></Quantity></MaterialRequirement><Extended:Type>Assembling<
/Extended:Type><Extended:WorkCenterName>WC-Demo911</Extended:WorkCenterName><Extended:Milestone>milestone-1<
/Extended:Milestone></SegmentRequirement><SegmentRequirement><ID>0020<
/ID><MaterialRequirement><MaterialDefinitionID>Copper</MaterialDefinitionID><MaterialUse>Consumable<
/MaterialUse><Quantity><QuantityString>600</QuantityString><UnitOfMeasure>Unit</UnitOfMeasure></Quantity><
/MaterialRequirement><MaterialRequirement><MaterialDefinitionID>Mat-1<
/MaterialDefinitionID><MaterialUse>Consumable</MaterialUse><Quantity><QuantityString>200<
/QuantityString><UnitOfMeasure>Unit</UnitOfMeasure></Quantity></MaterialRequirement><Extended:Type>Painting<
/Extended:Type><Extended:WorkCenterName>WC-Demo912</Extended:WorkCenterName><Extended:Milestone>milestone-2<
/Extended:Milestone></SegmentRequirement><Extended:Type>Manufacturing</Extended:Type><Extended:
AutomaticLaunch>false</Extended:AutomaticLaunch><Extended:LocationName>Delhi</Extended:LocationName><Extended:
Material><Extended:ID>P003</Extended:ID><Extended:Revision>P2020</Extended:Revision></Extended:
Material><Extended:Quantity><Extended:QuantityString>100</Extended:QuantityString><Extended:UnitOfMeasure>Each<
/Extended:UnitOfMeasure></Extended:Quantity></ProductionRequest></ProductionSchedule></DataArea><
/ProcessProductionSchedule> ### {OriginalFormat}=Import B2MML document: {B2MMLDocument}
###
2021-06-14 13:55:19.947 ### Information ### ProductionOrderHeaderName=Z2-PO-Demo-0911 ### {OriginalFormat}
=Created ProductionOrderHeader '{ProductionOrderHeaderName}'
### B2MML_ProductionRequestId=Z2-PO-Demo-0911


### 1.

### 2.

# Instrumenting Applications

Instrumenting an application indicates the creation of Activity primitives in specific code locations to monitor the application's performance and capture
information about application behavior at runtime to be used for diagnosis.

This operation can be performed only in specific parts of the code (libraries, frameworks, methods inside the libraries) that must be identified, keeping in
mind that code coverage and tracing scenarios in production must be monitored.

## Workflow

```
Identify the parts of the code to be instrumented
Set the configuration level of the instrumentation
```

# Identifying the Code Parts to be Instrumented

To identify the code parts you have to add attributes in a platform library or a Model library in order to produce the traces to be analyzed.

This operation can be performed by using attributes (available at class level and method level) to model Observability , or manually by using the Observabili
ty wrapper.

## Setting Observability to a Class

```
Type Attribute Inherited attributes Description
```
```
No attribute No attributes in hierarchy No methods are Observable
```
```
[Observable] No attributes / [Observable] attribute / [NotObservable] attribute in hierarchy All methods are Observable
```
```
No attribute Inherits directly or indirectly from type marked as [Observable] All methods are Observable
```
```
[NotObservable] No attributes / [Observable] attribute / [NotObservable] attribute in hierarchy No methods are Observable
```
```
No attribute Inherits directly or indirectly from type marked as [NotObservable] No methods are Observable
```
```
No attribute Both attributes in hierarchy, [Observable] is the most immediate All methods are Observable
```
```
No attribute Both attributes in hierarchy, [NotObservable] is the most immediate No methods are Observable
```
This is a graphical example of Observability configuration in a class hierarchy:

```
Class observability code example
```
```
[Observable]
public class Factory
{
```

### 1.

### 2.

```
// This class is Observable
}
```
```
[NotObservable]
public class SpecialFactory : Factory
{
// This class is not Observable
}
```
```
public class SuperSpecialFactory : SpecialFactory
{
// This class is not Observable because it has SpecialFactory as immediate base class
}
```
```
public class AnotherFactory : Factory
{
// This class is Observable because it has Factory in its own hierarchy chain
}
```
Setting Observability to a Method belonging to an Observable class

```
Attribute Description
```
```
No attributes on method Method is Observable and trace level set to Informational (default value)
```
```
[DoNotTrace] Method is not Observable
```
```
[Trace(ActivityLevel.Debug)] Method is Observable and trace level set to the one specified. Allowed values: Informational, Debug
```
```
Method observability code example
```
```
[Observable]
public class Factory
{
// This class is Observable
```
```
public void DefaultMethod()
{
// This method is Observable, as it belongs to an Observable class. It has default
ActivityLevel.
}
```
```
[DoNotTrace]
public void HiddenMethod()
{
// This method is not Observable
}
```
```
[Trace(ActivityLevel.Debug)]
public void DebugMethod()
{
// This method is Observable, as it belongs to an Observable class. It has a configured
ActivityLevel.
}
}
```
Instrumenting code platform using the Observability wrapper

```
Add references to Siemens.MOM.Platform.Observability.
On each class to be instrumented, define a field of type ActivityWrapper and instantiate its value in the constructor or during the initialization by
using the Factory implemented in the class ObservabilityManager as displayed in the example below.
```
```
private readonly ActivityWrapper _activitySource;
```

### 2.

### 3.

### ...

```
_activitySource = ObservabilityManager.GetActivityWrapper(this);
```
```
Do either of the following:
Pass the object to GetActivityWrapper to define it as information source of the activity name.
If the object instance is not available, use the GetActivityWrapper overload and replace 'Type' with 'Object'.
Example:
```
```
_activitySource = ObservabilityManager.GetActivityWrapper<classType>();
```
Additional Operations

```
To instrument all methods, place an instruction at the beginning of the method to call one of the overloads of StartActivity in the ActivityWrapper
class as follows:
```
```
using var activity = _activitySource.StartActivityInformation();
```
```
To instrument a part of a method, use the statement below:
```
```
//outside span
```
```
using(var activity = _activitySource.StartActivityDebug())
```
### {

```
//instrumented
```
### }

The following code is an example of the available operations:

```
public class MyClass
{
private IGovernanceGateway _governanceGateway;
protected readonly ActivityWrapper _activitySource;
```
```
public MyClass (IGovernanceGateway governanceGateway)
{
_governanceGateway = governanceGateway;
_activitySource = ObservabilityManager.GetActivityWrapper(this);
}
```
```
public void MyMethod()
{
using var activity = _activitySource.StartActivityInformation();
```
```
// all body instrumented
}
```
```
public void Method2()
{
```
```
Notes
```
```
All overloads of StartActivity return an Activity as defined in https://docs.microsoft.com/en-us/dotnet/api/system.diagnostics?view=net-5.0
All overloads of StartActivity can support the same argument as the Microsoft method https://docs.microsoft.com/en-us/dotnet/api/system.
diagnostics.activitysource.startactivity?view=net-5.0 in addition to the level indication.
If it is required a reference different from the one used creating the activity source wrapper, pass an object to startActivity.
```

//outside span

using(var activity = _activitySource.StartActivityDebug())
{
//instrumented
}

//outside span
}
}

public static MyStaticClass
{
//Initialization of source wiyh type
private static ActivityWrapper _activitySource = ObservabilityManager.GetActivityWrapper<MyStaticClass>();

public static void MyStaticMethod()
{
using var activity = _activitySource.StartActivity(TraceActivityLevel.Informational, ActivityKind.
Internal);

// all body instrumented
}
}

public class DerivedClass : MyClass
{
public void MySpecilizedMethod()
{
//using base class wrapper (same assembly) but activity take name from this
using var activity = _activitySource.StartActivity(this, TraceActivityLevel.Debug);

// all body instrumented
}
}


# Configuring the Application for the Deployment

The following procedure allows you to set the available configuration levels for the instrumentation configurability:

```
Disable/enable instrumentation of the entire library/framework
Filter instrumentation of some methods inside the library/framework
```
These operations are performed by modifying specific parameters of the appsettings.json file

## Code Example

Depending on the operations to perform, modify the Observability parameters in the appsettings.json file as described in the example below:

### {

```
"ModularMOM": {
"Observability": {
"Instrumentation": {
"DefaultLevel": "Informational",
"Providers": [
"Siemens.MOM.Platform.Application",
"Siemens.MOM.MetaModel.Framework",
"Siemens.MOM.Platform.Information"
],
"Siemens.MOM.Platform.Application": {
"Level": "Debug"
}
},
"ModelInstrumentation": {
"Providers": [
"Camstar.Core.SampleModel"
]
},
"3rdPartyInstrumentation": {
"Providers": [
"AspNetCore",
"HttpClient",
"SqlClient",
"GrpcClient"
],
"SqlClient": {
"SetDbStatementForText": true,
"SetDbStatementForStoredProcedure": false,
"EnableConnectionLevelAttributes": false,
"RecordException": false
},
"GrpcClient": {
"SuppressDownstreamInstrumentation": false
},
"AspNetCore": {
"EnableGrpcAspNetCoreSupport": true,
"RecordException": false
},
"HttpClient": {
"SetHttpFlavor": false
}
},
//"Filters": {
// "Activity": {
// "Kind": "Internal"
// }
//},
"UseExporter": "jaeger",
"Jaeger": {
"Host": "localhost",
"Port": 6831
},
"Zipkin": {
"Endpoint": "http://localhost:9422/api/v2/spans"
},
```

"otlp": {
"Endpoint": "http://localhost:49165"
},
"Resources": {
"Service": "MOM Module"
},
"UseSampler": "AlwaysOn",
"TraceIdRatioBased": {
"rate": 0.5
}
}
}
}


# Working with Metadata


# What Is Metadata?

Metadata is a configurable set of data defined using the Domain Specific Language (DSL) that can determine some aspects of behavior in an application.

Metadata-Driven Architecture is an architectural style for building enterprise application where the system behavior is defined through this configurable set
of data called 'Metadata'. In this style of software architecture, Metadata creates a logical and self-describing framework for allowing the data to drive all or
most of the application features and functionality. Such metadata includes structural as well as behavioral metadata, at a minimum.

The main functionalities of Metadata are:

```
Commodity Language: Can leverage existing industry tools
High Performance: Pre-compiled Metadata, no runtime overhead
Quick to market: Can leverage on OcCore’s pre-built Industry Solutions and can be migrated from OcCore Metadata.
```
## Abstraction Levels of Metadata

The Object Management Group (OMG) defines 4 levels of Metadata abstractions or Model hierarchies, where items at each level being dependent upon
the items at the level above.

```
Level Description Example
```
```
M3 Meta-Metamodel
```
```
Defines a language to specify the Metamodel.
```
```
Meta Object Facility
```
```
M2 Metamodel, Meta-Metadata, Metadata Model (DSL)
```
```
Schema for Metadata, it defines the structure of the Metadata on the semantic model.
```
```
UML, any proprietary
frameworks
```
```
M1 Model, Metadata
```
```
Structural, descriptive and behavioral data about the data. It defines the semantic domain model and
represents business model.
```
```
Composition of business features related to a domain, subdomain or a bounded context.
```
```
Could be independent or be dependent on other models.
```
```
Model from a problem
domain
```
```
M0 Instance Model, User Domain
```
```
Contains runtime element of the model
```
```
Instances of the semantic
model
```


# What Are Executable Models?

An executable model is a model that is complete enough to be executed, that means it contains all the elements of the application domain including the
functionality, that is, the business logic. The actual executability of the model itself depends on the underlying tool used to execute the model, also called
Metadata Execution Engine.

## Strategies to implement the execution tools

```
Code-Generation: it involves using a model compiler (many times defined as a model-to-text transformation) to generate a lower-level
representation of the model using existing programming languages and platforms.
Model-Interpretation: it relies on the existence of a virtual machine able to directly read and run the model.
```

# What Are Business Logic Templates

Business Logic Templates are a feature of the Metadata built into the M1 abstraction space leveraging the concepts and features provided by the
Metamodel, M2 level. These templates provide a structure to the design of logic flow to configure the business specific functionalities.

The following basic Business Logic Templates can be used to define other templates or customize the existing ones:

```
Service templates such as Modeling and Shopfloor
Feature templates such as Where Used and Electronic Signature.
```

# How to work with Metadata


# Metadata Design

The following image depicts the Metadata building blocks to better understand the logical view of the Metadata design. These building blocks are built into
the M1 abstraction space leveraging the concepts and features provided by the Metamodel, M2 level, and add another abstraction layer on top of the
Metamodel features to provide a way for simpler configuration experience for the end users.

Example of building blocks:

```
Object Model Families: Define base classes for the various types of the Object categories supported by the Metamodel, such as ‘Named’,
‘Revisioned’ objects etc.
Service Templates: Define behavioral templates for various ‘Service’ types supported such as ‘Modeling’, ‘Shopfloor’, ‘Inquiry’, etc.
Business Logic Feature Templates: Provide structure to the design of logic flow to configure the business specific functionalities.
Workspaces: Provide configuration isolation technique for the ability to customize models and still be easily upgradeable.
```
## Metadata Components

Object Model

```
Domain model: Modelling Objects define the business domain model. This model can be defined using the following object categories provided
by the system: like Named Objects, Revisioned Objects, Subentity Objects. Examples of domain model objects include: Product, Work Order,
Operation, etc. The business model would be based on what actual business functionality will be implemented in Modular MOM.
Workflow model: In Opcenter Execution Core, the unit of work is controlled and configured through a concept called Workflow model. A Workflow
is a series of sequential tasks that are carried out based on user-defined rules or conditions, to execute a business or a manufacturing process. It
is a collection of data, rules, and tasks that need to be completed to achieve a certain business outcome. A Workflow visually depicts a detailed
sequence of business activities and information flows needed to complete a process. Actual flow of the unit of work through this workflow is
controlled by various design time and runtime parameters such as the Product being produced or the corresponding Manufacturing Order, or what
piece of equipment is being used and even such parameters as process control measurements or any other information associated with the
current unit of work.
Operational model: Manufacturing operations revolves around tracking the manufacturing processes and the units of work flowing through these
processes. A Traceable Object category represents a unit of work that is intended to be tracked through the various manufacturing processes. In
Opcenter Execution Core, this concept is represented by an object type called ‘Container’. Most of the Manufacturing Business operations are
modelled around this Traceable object. To help simply the definition of the actual business functionality and the configurability of the system,
various business logic templates are created that are centered around the Traceable object.
Audit Trail model: An audit trail is a log of the path that a unit of work has taken and all the collected information during such flow of work. Audit
trail captures key details such as: date and time the unit of work was started, name of user who started, operations performed, additional
parameters collected etc. Modular MOM Metadata provides a structured and configurable way to track and collect the necessary information
towards the audit trail during the manufacturing operations. This configuration is implemented as part of Business Logic templates of the system
that tie in various such key functional features together and provide a unified mechanism for configurability and customizability.
```
Service Model

Business operations are modelled using the Service type of objects and these objects become the interface between the external world and this Metadata-
based system’. And the Business Logic Templates provide a framework for easier and simpler configurable way to define business operations.
At the Metamodel level, the following are the basic types of service models, and a specific Business Logic Template is defined for these service types in
the Metadata layer:

```
Update : Services that create or manipulate the state of the persisted data. The Modeling services provide a template for the creation and
manipulation of the Engineering model, that is, Factory model instances. They provide the basic CRUD (Create, Read, Update, Delete)
operations on the corresponding objects.
Inquiry : Services that do not have any effects on the persisted data and they provide capabilities for reporting like functionality.
Compound: Services that are composed of more than one service and they provide a mechanism to arrange the execution of these composed
services.
```


# Model Execution

## Metadata Engine

Metadata Engine is an embeddable engine that understands and processes the Metadata. It is an engine that is also responsible for taking the user's input
and executing the business transaction as defined in the Metadata

## Interfaces

Interfaces Exposed

```
Invocation API: Contains Interfaces and Classes to support the creation of the Metadata Engine and as well as create ‘Service’ type objects and
execute them.
Introspection API: Provides API for the discovery of the metadata model at the runtime.
```
Interfaces Consumed

```
Persistence API: This platform API is provided by the embedding layer (Execution Engine) and is consumed by the embedded Metadata Engine
to handle object persistence.
Integrations/Communications API: This platform API is provided by the embedding layer (Execution Engine) and is consumed by the embedded
Metadata Engine to handle any interactions/communication with external components such as for pub/sub of messages or any other supported
communication patterns.
Governance/Observability API: This platform API is provided by the embedding layer (Execution Engine) and is consumed by the embedded
Metadata Engine to provide an ability to govern and monitor the metadata engine by the outer execution engine.
```
## Engine Factory

Metadata Engine provides 'Factory' style API for the creation of the Metadata Engine instances. Classes and interfaces provide runtime context for the
Model execution (business logic).

```
BizLogicEngineFactory: This class implements the 'Factory' pattern and provides the API to create instances of the Metadata Engine.
IMetadataEngine: This interface defines the contract exposed by the Metadata Engine.
```
```
public interface IMetadataEngine
{
7 references
ICollection<KeyValuePair<string, IObjectMetadata>> ConfigurableTypes {get; }
2 references
Type GetActualTypeFromGeneric(string genericClassName, string genericParameterClassName);
4 references
Type GetActualTypeFromGeneric(Type genericType, Type genericParameterType);
2 references
Type GetActualTypeFromGeneric(Type genericType, Type [] genericParameterTypes);
```
```
8 references
IService CreateService(string genericClassName, string genericParameterClassName);
5 references
IService CreateService(string ServiceTypeName);
3 references
IService CreateService(Type serviceType);
1 reference
```

```
ClassType CreateService<ClassType>() where ClassType : class, IService;
}
```
Runtime Context

Object Runtime Execution Context: Base classes and interfaces to provide runtime context for the business logic execution.

ServiceExecutionContext: This class provides the runtime context to the invocation of a 'Service'. Every instance of a Service gets a new instance of the
context of this Service invocation. An instance of this type is available through a property ‘_ExecutionContext’ that is available on every Configurable
Object.
Sample usage of this property:

_ExecutionContext.GetByName<ClassType>(name);

_ExecutionContext.GetById<ClassType>(id.AsObjectId());

_ExecutionContext.GetAll<ClassType>();


# Working with Modular MOM Metamodel


# What Is Modular MOM Metamodel?

Modular MOM Metamodel is a programming language that allows to configure the business model, the domain model, the business logic and other
aspects of the business model.

In Metadata-driven systems/architectures, the Metadata is the base of the business solution the system provides. The structure and the form of the
Metadata is governed by the Metadata DSL or the Metamodel. As Modular MOM is built using the Metadata-driven architecture, there is a specific
Metamodel that is used to define the Metadata for Modular MOM.

## Metamodel Features

Modular MOM Metamodel is inspired from the architecture and Metamodel of Opcenter Ex Core.. The main features of the Modular MOM Metamodel are:

```
Object Oriented: Enables the definition of metadata that adopts the Object Oriented principles: Encapsulation, Data Abstraction, Polymorphism
and Inheritance.
Imperative: Functional logic expressivity of the metamodel follows the imperative programming paradigm.
Strongly Typed: Provides a Strong Type System for the primary components of the Metadata, and also supports dynamic types in the logic
definitions.
Relational: Enables the definition of an Object Model based on Relational Model and supports relationships as Association, Composition and
Aggregation.
Aspect-Oriented: Enables the addition of cross-cutting functionality based on annotations like in an aspect-oriented programming paradigm.
Persistence: Enables configurable Object-Relational Mapping based persistence mechanism for the user data.
```

# What Are Configurable Objects?

The Configurable Object contains a collection of Fields and can be represented in the C# class of the Object. It is the core of the Metamodel design
because it enables the creation of classes or object types to describe an object instance, the central idea of the Metadata.

In the Metadata Abstraction Level, the Configurable Object represents an instance at M2 level so its instances represent the instance at M1 level, and
consequently the type definitions of the object instances represent an instance at M0 level.

## Structure

The configurable object is composed of:

```
Fields: To define the properties of the object. See Field Types for a list of predefined types of Field definitions that can be used to define the
properties on an object.
Methods: To define the logic associated with the objects like in any Object-Oriented programming language.
Events: A set of pre-defined object life-cycle actions automatically fired by the system.
Event Handlers: Every Configurable Object has a set of predefined object life-cycle related events automatically fired by the system during the
life of an object based on some external or internal triggers. Event handlers are the logic blocks to be associated to the events that get executed
when the events are fired.
```
```
Field Event Handlers: Also the Fields have events fired by the system and then event handlers to be associated to these events to
configure any logic to be executed when the events are fired.
```
```
Attributes: They define additional characteristics to be related to the aspects that are either used at the ‘design’ time by the configuration tools or
behavioral aspects that could take effect at the system runtime. These Attributes can be defined both at the Configurable Object level and at the
Field level.
Examples of attribute: Description to describe the item, Category to put the object into a particular group which could be used by the configuration
tools, Cache-able to define if this the instances of this object could be cached by the system at runtime.
The attributes can be:
Overridable: The attributes can be overridden by the sub-classes or by customizations
Non-Overridable: The attributes are NOT allowed to be overridden by the sub-classes or by customizations
Persistence Options: They provide the Object-Relational Mapping to facilitate the object instance to be persisted into a database storage.
```
## Object Categories

Configurable Objects are classified into the following high-level types based on the intended purpose of the objects:

```
Business Objects: They represent the concepts from the Business domain model.
Service Objects: They model Business operations and become the interface between the external world and this Metadata-based system. Service
definitions act like an API to the business services provided by this Metadata. Every interaction with the external world other than just querying the
model is always through the execution of a ‘Service’ type of object.
Persistent Objects: The state of the Business objects which represent the concepts from the Business domain model are typically persisted in
some form of storage. This Metamodel provides a framework through ORM to facilitate the persistence of these objects. The Business Objects
that are configured to be persistent are called Persistent objects.
Transient Objects: Service objects and some Business Objects that do not require the state to be persisted. The main purpose of these transient
objects is to aid during the execution of business functionality to keep and track some internal state but are not required to be persisted.
Named Objects: Business objects within the business domain model identified by a unique name assigned automatically by the system that
accomplishes an alternate logical key for the object.
Revisioned Objects: Objects that support revision control, that is, they have multiple revisions and are able to be managed as one object. Within
this Metamodel, revisioned objects implement the following pattern for revision control mechanism:
Revisioned objects are standard objects like any other Business object
Revisioned objects can have one or more ‘Revisions’
Revisioned objects will have one revision marked as Revision of Record (RoR)
```

```
Revisioned objects have 2 alternate logical keys of identification:
Name only, which always refers to the current RoR object
Name and Revision, which uniquely identifies a specific single revision
```
```
Subentity Objects: Composable objects.
Named Subentity Objects: Subentity objects that can be uniquely identified by a name within the context of its composing object.
```
Relationships

Relationship between objects are explicitly expressed through the Object type fields listed below:

```
Inheritance: This Metamodel implements Single Inheritance Model only with full support for polymorphism. Navigation between the object and its
super classes is through the keywords "this" or "base", as defined by the underlying language.
Association: An association is a “using” relationship between two or more objects in which the objects have their own lifetime and there is no
owner. Association can also be defined as a semantically weak relationship between otherwise unrelated objects. Associated objects can be
referenced by Id or any of the alternate logical keys to uniquely identify the object.
```
```
Characteristics of the Association relationship within this Metamodel:
```
```
Navigation between related objects are implicitly expressed by the Fields defined
One-way navigation is implied in the direction defined by the user in the form of a Field
The user must explicitly express back-navigation and, if needed, back-navigation can automatically be configured through the tool
automations.
```
```
Composition: A strong type of relationship like in whole/part or parent/child relationship. It defines ownership between the composing and
composed objects, such that, if the lifecycle of the composed objects is linked to the composing object, for example, when the composing object
is destroyed, the composed object cease to exist.
Within this Metamodel, a composed object is called Subentity and the creation of these Subentity objects are always within the context of the
parent/owner/composing object.
The navigation aspects of this relationship are:
Forward navigation expressed explicitly by the user when defining composition
Back-navigation is provided automatically through ‘Parent’ field
Aggregation: A specialized form of relationship to represent a collection - One-to-Many multiplicity dimension. Aggregation supports both
Association and Composition types with navigation aspects being same as the underlying relationship type.
```

# What Are Configurable Object Maps?

The Configurable Object Map (CDO Map) is a feature that provides a configurable way of mapping information between any two configurable objects. A
CDO Map has a source object and a target object and contains a collection of field mappings, an evaluable expression bound to the source object and
mapped to a specific field on the target object. When a CDO Map is executed at runtime, the source field expression is evaluated, and the corresponding
result value is copied to the target field on the target object.
CDO Maps support object-oriented features such as Inheritance and Polymorphism to simplify simple assignment statements in the business logic
definition.
With CDO Maps, the dynamic dispatching is based on the three factors: the source object, the target object and the map. Since both the Configurable
Object and the CDO Map support inheritance and polymorphism, the actual runtime type information of these instances will determine which map is
executed at runtime.


# What Are Field Types?

Field types are custom fields to define the several types of field in a configurable object such as native types, in addition to types to represent several types
of objects. In general, fields can be divided in two types: Scalar (single-valued) and List (multi-valued) and within each of these categories there are other
data types supported by the Metamodel type system:

```
Primitive Types: Basic data types like String, Boolean, Datetime and numeric types like Integer and Decimal.
Complex Types: They are used to represent object relationships. The supported relationships are: Association, Composition and Aggregation.
```

# What Are Workspaces?

A workspace provides configuration isolation technique for the ability to isolate and customize the solutions because each customization is performed in a
separate workspace in order to simplify the upgrade operations.

In the Modular MOM Metamodel, a workspace supports the following customizations:

```
create new objects and logic
extend existing objects using inheritance
override out-of-the-box objects and logic
build Industry or Partner solutions on top of the out-of-the-box Solution.
```

# What Are Usages?

Usages allow to configure some feature functionalities on Configurable objects and Fields such as:

```
Specify Object Category: Type of usage that defines some aspects of the configurable object such as 'Named' and 'Revisioned'.
Represent Special Fields: Type of usage that defines specific functionalities for the fields, i.e. the 'Name' property of a Named object and the
'Revision' field of a Revisioned object.
Toggle Caching: Type of usage that defines whether an object can be cached at runtime. It gives a mechanism to toggle this feature per Object
definition.
Enable WIP Messages: Type of usage that defines the 'Work in progress' messages for specific modeling objects in order to show it to a user
when a traceable material with specific attributes reaches a specific processing point.
```

# Inter-App Communications

## Business Context

Modular MOM is based on two key approaches:

```
Metadata-driven: the system behavior and business capabilities are defined through a configurable set of data covering all their aspects such as
data model, business logic, communication interfaces, user interfaces, deployment characteristics, etc.
Modular: the functional capabilities of the system are decomposed into a set of granular parts where each part represents a piece of the overall
application.
```
## Modular MOM Messaging - Overall Design

Modular MOM is a metadata-driven configurable platform and the app’s business functionality is configured as metadata that is governed by the Modular
MOM’s Metamodel (Conceptual DSL). So, unlike many traditional systems the interactions between apps should also be configurable like the rest of the
business functionality. For the ease of this configurability, a simplistic programming model is crucial.

Remote Procedure Call (RPC) is a form of client–server interaction (caller is client, executor is server), typically implemented via a request–response
message-passing system. This makes the two sides of the communication dependent on each other more tightly, typically.

After considering the top requirements of loose coupling and simple programming model, and to maintain the independence of the Modular Apps, an async
hronous message-based pattern over RPC style communication is developed for the inter-app communications of Modular MOM.

The asynchronous messages between the apps are exchanged outside of any transaction context of a business operation in order the maintain the
independence of the apps and still maintain data consistency in case of network or app availability issues.

Modular MOM’s app runtime, aka. Metadata Runtime provides support for the underlying RPC style communications needed for the inter-app
communications. In RPC terminology, an RPC client denotes a Producer/Sender App of a message and an RPC Server denotes a Consumer/Receiver
App of the message. Since inter-app communications is between metadata-based Modular Apps, each such App can play a role of a Producer/Sender and
a Consumer/Receiver according to the context of the business functionality.


# Use Case View

The following diagrams and the table below illustrate the high-level sample use cases for the inter-app communication.

## Sample Use Cases illustrating the need for Inter App Communications

```
Key Use
Cases
```
```
Description
```
```
Dispatch
App –
Launch
Production
Order
```
```
A business app service of the Dispatch App that initiates the launch of a Production Order. When a Production Order is launched a
corresponding Work Order for this Production Order should be initiated in the TnT App to track the processing of the manufacturing
operations. This use case uses the TnT Launch Work Order use case to initiate the creation of a Work Order.
```
```
TnT App –
Launch
Work Order
```
```
A business app service of the TnT App that initiates the launch of a Work Order to track manufacturing operations performed for an
order.
```
```
TnT App –
Process
Work Order
```
```
A TNT App’s business service to execute some manufacturing operation on a Work Order
```
```
TnT App –
Complete
Work Order
```
```
A TNT App’s business service to indicate the completion of all manufacturing operations on a given Work Order
```
```
On completion of processing on a Work Order the Dispatch App should be notified using the Dispatch App Work Order Completed use
case.
```
```
A Dispatch App’s business service to notice the completion of a Work Order in the TnT App and to update the status of the
corresponding Production Order
```

Dispatch
App – Work
Order
Completed


# Logical View

Modular MOM’s metadata-based Apps are composed of one or more configurable metadata Models. Each app should be independently deployable. By
requirement, these apps should be loosely coupled and should exist and run independently in isolation or work alongside of other apps communicating
and exchanging information between these apps.

For the independence of these apps and to maintain loose coupling between them an asynchronous messaging pattern has be selected to implement
the inter-app communications.


# Loosely Coupled Modular Apps

Modular MOM’s metadata-based Apps are composed of one or more configurable metadata Models. Each app should be independently deployable. By
requirement, these apps should be loosely coupled and should exist and run independently in isolation or work alongside of other apps communicating
and exchanging information between these apps.

For the independence of these apps and to maintain loose coupling between them an asynchronous messaging pattern has be selected to implement
the inter-app communications (See: Example of Inter-App Communications for the Sample Use Cases)


# Example of Inter-App Communications for the Sample Use

# Cases

## Component View

The following diagram illustrates the interactions between Modular MOM apps for the sample use case referenced in the Inter-App Communications - Use
Case View. The sequence numbers on the actions indicate the order in which these actions occur. The actions depicted in green color are the actions
explicitly trigger by a user. The actions depicted in the brown color are the actions that are automatically triggered messages between the 2 apps.

In general, there is a User Interface component in-between the actor and the App’s backend component (business logic), something like the following
diagram. But for brevity, we’ll skip the interactions with the UI component, here and in the rest of the document, since the focus of this architecture is more
on the inter-app communications – app backend to app backend communication. The interactions with the UI components will be addressed with UI
architecture.

## Activity View

The following activity diagram expands more on illustrating the above example business use case indicating the sequence of activities that occur. Note that
this flow only represents a happy path where everything works as expected. There would be slightly different flows to handle errors such as, at step 5, a
different message could be sent as part of the error flow - not so happy path.



# Asynchronous Messaging

Asynchronous messaging is fundamentally a pragmatic reaction to the problems of distributed systems. Sending a message does not require both systems
to be up and ready at the same time. Furthermore, thinking about the communication in an asynchronous manner forces developer to recognize that
working with a remote application is slower, which encourages design of components with high cohesion (lots of work locally) and low adhesion (selective
work remotely). For more information, see Asynchronous Messaging.

Messaging helps with transfer of data frequently, immediately, reliably, and asynchronously.

## Concepts

```
Message channel: A logical connection between the two communicating applications and that can be used to transfer information as messages.
Message: A data record that a system can transmit through a message channel. Thus, any data that is to be transmitted via a messaging system
must be converted into one or more messages that can be sent through messaging channels. The message is a single unit of data, a single
object or data structure which may decompose into smaller units.
```
For more information about messages, see Enterprise Integrations Patterns by Gregor Hohpe and Bobby Woolf

## Message Types

There are several types of Messages and messaging patterns in use today. Some of such types are:

```
Command Message: a regular message that happens to contain a command – an instruction to invoke some functionality on the receiving side.
Document Message: it passes data and lets the receiver decide what, if anything, to do with the data
Event Message: A message to notify or announce an occurrence of an ‘event’. The message would be delivered to all receivers that expressed
interest to receive this type of message. There could be no interested receiver as well
```
For more information about messages, see Enterprise Integrations Patterns by Gregor Hohpe and Bobby Woolf


# Message Communication Styles

## Unicast style Messaging

In Unicast style Messaging, a message is delivered to a single destination process, which is uniquely addressed by the sender. That is, the message
contains the information of the destination process.

## Broadcast Style Messaging

In Broadcast style Messaging, a message is delivered to all destination processes of a specific group. In this style of messaging, the message contains the
information about the destination such a group or topic, etc.


# Message Communicating Parties

In a typical messaging style communication there are, at least, 2 parties involved. These components of the communication are commonly referred to as
Producer/Sender and Consumer/Receiver.

## Producer/Sender

A Producer or a Sender of the message is a component/process that initiates the communication to send a message.

In Unicast and as well as Broadcast style of messaging, there is exactly 1 Sender for every communication of a message.

## Consumer/Receiver

A Consumer or a Receiver of the message is a component/process that is the destination of the message.

In Unicast style of messaging there is exactly 1 Receiver for every communication of a message. For Broadcast style of messaging there can be zero-to-
many number of Receivers or Consumers.

```
Note
```
```
The terms Producer and Sender are used interchangeably in the rest of this document.
```
```
Note
```
```
The terms Consumer and Receiver are used interchangeably in the rest of this document.
```

# Asynchronous Messaging Patterns

## Fire & Forget Message Pattern

When two applications communicate via Messaging, the Fire & Forget pattern is where the sender fires a one-way unicast style message to towards the
receiver and forgets about it, meaning the sender does not expect any response back to such messages. For more information, see Remoting Patterns
by Völter, Kircher, Zdun

## Request-Reply Message Pattern

Some applications may want a two-way conversation. Request-Reply messaging pattern helps applications model a 2-way conversation utilizing a pair of
one-way messages, 1 in each way, to represent the request and reply for the request.

Correlation Identifier

In a Request-reply style of communication, the reply message should contain information to uniquely identifier the original request message for which this
reply has been generated. Every message has a unique ID , and the ‘reply’ type messages should include this unique identifier of the original ‘request’
message. The forms Correlation Identifier on the ‘reply’ messages. For more information, see Enterprise Integrations Patterns by Gregor Hohpe and
Bobby Woolf

## Event Notification Message Pattern

Event Notification Message Pattern is like the broadcast style messaging where a message producer sends a message to a specific group (more than
one receiver) to notify about an event. This can be implemented as more common Publish-Subscribe Model, where the consumers express interest
(subscribe) to receive certain types of messages with the underlying messaging platform. The producer then sends (publish) the message and all the
subscribed consumers will receive the message.

For more information, see Enterprise Integrations Patterns by Gregor Hohpe and Bobby Woolf


# Modular MOM Messaging Overall Design

Modular MOM is a metadata-driven configurable platform and the app’s business functionality is configured as metadata that is governed by the Modular
MOM’s Metamodel (Conceptual DSL). So, unlike many traditional systems the interactions between apps should also be configurable like the rest of the
business functionality. For the ease of this configurability, a simplistic programming model is crucial.

Remote Procedure Call (RPC) is a form inter process communication where a program/process causes a procedure to execute in another program
/process, which is coded as if it were a normal (local) procedure call, without the programmer explicitly coding the details for the remote interaction. The
RPC model implies a level of location transparency, namely that calling procedures are largely the same whether they are local or remote. This simplistic
programming model of RPC style makes it very conducive for the configurability of inter-app interactions.

RPC is typically, a form of client–server interaction (caller is client, executor is server), typically implemented via a request–response message-passing
system. This makes the two sides of the communication dependent on each other more tightly, typically.

After considering the top requirements of loose coupling and simple programming model, and to maintain the independence of the Modular Apps, an async
hronous message-based pattern over RPC style communication is developed for the inter-app communications of Modular MOM. (See: Example of
Inter-App Communications for the Sample Use Cases )

The asynchronous messages between the apps are exchanged outside of any transaction context of a business operation in order the maintain the
independence of the apps and still maintain data consistency in case of network or app availability issues.

Modular MOM’s app runtime, aka. Metadata Runtime provides support for the underlying RPC style communications needed for the inter-app
communications. In RPC terminology, an RPC client denotes a Producer/Sender App of a message and an RPC Server denotes a Consumer/Receiver
App of the message. Since inter-app communications is between metadata-based Modular Apps, each such App can play a role of a Producer/Sender and
a Consumer/Receiver according to the context of the business functionality.


# Modular Apps Asynchronous Messaging over RPC style

# Communication

There are several types of Messages and messaging patterns in use today. Based on the current set of high-level requirements, the following few
messaging patterns have been identified for use with Modular MOM and support more type or patterns could be added as and when new requirement
arise. For more information, see Modular MOM Messaging - Overall Design.

```
One-way Message Pattern – for fire & forget type of situations
Two-way Message Pattern – for request-response type of situations
Event Notification Pattern – for publish/subscribe type of situations with external systems
Eventual Consistency Pattern – for distributed business transaction type of situations
```

# One-way Message Pattern with Command or Document

# Type Message

One-way conversation is implemented using a ‘Command Message’ or a ‘Document Message’ sent from a sender to a specific receiver.

The following Sequence Diagram illustrates the asynchronous one-way message pattern implemented over RPC style communication.

This style of communication is used in fire and forget kind of situations where the sender is concerned only about the message being sent is accepted by
the receiver. The sender will not expect any response after the message is processed on the receiver side. If the business process requires such
communication of the result of the message processing be sent to the original sender then another 1-way message communication could be configured
(through the business model) to initiate the message from the receiver to the original sender – in the reverse direction. This pattern is based on ‘Sync with
Server’ pattern which is an extension to the Fire & Forget pattern as defined in Remoting Patterns by Völter, Kircher, Zdun.


# Two-Way Message Pattern with Request-Reply Messages

Request-Reply messaging pattern for a 2-way conversation is implemented using a ‘Command Message’ as the request and a ‘Document Message’ as the
reply going through 2 separate and individual channels. For more information see: Enterprise Integrations Patterns by Gregor Hohpe and Bobby Woolf.

The following Sequence Diagram illustrates the asynchronous request-reply message pattern implemented over RPC style communication.

In Modular MOM, to maintain the simplicity of the message interaction configuration, the two-way message pattern for the Request-Reply type scenarios
are to be implemented by configuring 2 separate one-way messages – request from the sender to the receiver & the reply from the receiver to the original
sender. See One-way Message Pattern and Request-reply Message Pattern for concepts and Development View for configuration details.


# Event Notification Pattern with Event Messages

Modular MOM Metadata-based apps use one of the two message patterns for communicating with other Modular MOM Metadata-based apps. This Event
Notification pattern will be supported for interactions between Modular MOM Metadata-based apps and external non-Modular MOM metadata-based apps.

A conscious decision has been made to not use event notification style pattern for inter-app communications between Modular MOM apps for the following
reasons:

```
For maintaining the simplicity of the message interaction configuration since both the parties of communication are based on configurable
Metadata
Since these communications are coded interactions, the contract, the senders and the receivers are well known during the configuration phase
Event Notification Pattern is complex to implement and requires depending on third-party commoditized software to have a robust implementation
which could make the deployment options complex and expensive for the small on-prem customers
```
For scenarios where a message needs to be sent to multiple receivers, like in the Event notification pattern, One-way message pattern with support to
send to multiple receivers could be employed.

Event Notification Pattern comes in handy and is crucial for supporting external integrations, interactions with non-metadata-based apps or external apps,
since the interaction can be configured only on one side and the other side is an opaque black box. Event Notification pattern based on the Publish
/Subscribe model or any other such similar models will be part of the robust messaging platform - Modular MOM Orchestration & Integrations Platform
(former MOM Connect Platform) and, it is not in the scope for this architecture document. (See: Modular Apps Asynchronous Messaging over RPC style
Communication)


# Distributed Business Transactions with Runtime

# Dependency

## CAP Theorem

In theoretical computer science, the CAP theorem, also named Brewer's theorem after computer scientist Eric Brewer, states that it is impossible for a
distributed data store to simultaneously provide more than two out of the following three guarantees:

```
Consistency: Every read receives the most recent write or an error
Availability: Every request receives a (non-error) response, without the guarantee that it contains the most recent write
Partition tolerance: The system continues to operate despite an arbitrary number of messages being dropped (or delayed) by the network
between nodes.
```
## Eventual Consistency

Eventual consistency is a consistency model used in distributed computing to achieve high availability that informally guarantees that, if no new updates
are made to a given data item, eventually all accesses to that item will return the last updated value.
Modular MOM is a set of self-contained Apps with each app defining a bounded context that manages all its data internally. These Apps also use a
message-based approach to interact between apps. Self-contained Apps addresses partitioning, the messaging (plus resilience) address the availability.
Thus, Modular MOM becomes a PA type system that eases Consistency in order to be loosely coupled (network partitions) and be highly available, hence
the system will achieve Best Effort Consistency or Eventual Consistency for its data. Hence, strict data consistency across apps by distributed
transactions cannot be achieved in Modular MOM Apps.
Business Data Consistency for business processes that span multiple Apps can be achieved by employing Enterprise Applications Integration patterns like
the Saga Design Pattern which is a way to manage data consistency across distributed app scenarios.

A saga is a sequence of transactions that updates each app and publishes a message or event to trigger the next transaction step.

Choreography is a way to coordinate sagas where participants exchange events without a centralized point of control.

Orchestration is a way to coordinate sagas where a centralized controller tells the saga participants what local transactions to execute.


# Modular App Decomposition

The following decomposition view of the Modular MOM’s App Runtime (Metadata Runtime) illustrates both the RPC Client component, which is
responsible for sending messages, and the RPC Server component which is responsible for receiving messages. Note that the components depicted in col
or in the diagram identify the focus points in the context of the messaging architecture.

### T

he
fol
lo
wi
ng
ta
ble describes the components depicted in the above diagram.

```
Component Description
```
```
RPC Server
```
```
(RPC API
Server)
```
```
External facing listener to serve RPC style requests
```
```
RPC Client Modular App is both an RPC Server as well as an RPC Client since each app is capable of sending messages to other apps and are
also capable of receiving messages from other apps. This RPC Client component is responsible to send messages to other apps
```
```
Rest API
Server
```
```
External facing listener to serve REST style requests
```
```
Metadata
Engine
```
```
Internal component that is responsible to process the user input and execute the corresponding business service configured as per
Modular MOM’s Metamodel
```
```
Application
Layer
```
```
Internal component that bridges and maps the user requests received by the API layers and passes it to the Metadata Engine for
execution
```
```
Persistence
Engine
```
```
Component responsible to facilitate data persistence support that is utilized by the Metadata Engine
```
```
Observability Component responsible for the overall observability of the system
```

# Development View


# Communication Technology

gRPC, which is a low-latency, high-throughput, lightweight opensource remote procedure call (RPC) framework is the choice from technology perspective
(see: Modular MOM Messaging - Overall Design).

## gRPC Key Advantages

```
Performance: messages are serialized using Protobuf, an efficient binary message format which serializes very quickly and results in small
message payloads. gRPC is designed for HTTP/2, a major revision of HTTP that provides significant performance benefits over HTTP 1.x
Runs Anywhere: this is another crucial aspect of the Modular MOM’s architectural requirement.
Strong Typed Contract Specification and Code Generation Tools: Shared ‘.proto’ file and the code generation provide strong typed contracts
between the client and the server
```
Note that the advantages like Performance, Strict Contracts and Run Anywhere outweigh some of the drawbacks and since these drawbacks are not really
significant requirements for Modular MOM.

## gRPC Disadvantages

```
Not Human Readable: Since the message format is not textual it is not human readable and requires additional tooling analyze requests or
compose by hand
No Browser Support: gRPC heavily uses HTTP/2 features and no browser provides the level of control required over web requests to support a
gRPC client
No Broadcasting Support: The concept of broadcasting a message out to registered connections doesn't exist in gRPC
```

# Metamodel Framework for Communications Configuration


# Inter-App Communication Configuration Overview

Modular MOM’s apps are configured as Metadata models. The messages to be exchanged between apps are also configured in a similar way like the rest
of the business model. The configuration of these interactions involves defining a ‘message’, configuring what business services produce this message
and what business services consume this message. This configuration of the schema of the message and the ‘Producers’ and ‘Consumers’ of the
message are configured during the design phase of the business models (for more information see Modular MOM Messaging - Overall Design and Inter-
App Communication Interface )

To maintain loose coupling between apps, the messages to be exchanged between apps are configured as a separate model, referred as ‘Message’
Model in the diagram below. This message model assembly is then referenced by the metadata models of the both apps similar to how ‘.proto’ file is
shared between the gRPC client and the gRPC server. This reference provides all the message types defined in the Message Model to be available for
both the app business domain models to use while configuring the services that produce/consume these messages (for more information see Inter-App
Communication Interface )

For more information about Metadata model configuration, see Modular MOM Development and Configuration Guide.

For more information about Metadata model dependencies, see the Modular MOM Modularization architecture documentation.


# Configuration of Messages

A message is at the heart of the inter-app communications. Modular MOM's Metamodel Framework provides the following types for messaging in order to
configure the messages that can be exchanged between apps.

```
IMessage - Interface : The framework interface IMessage has all the basic data elements need on a message to be able communicate between
apps. The table below describes the data properties on this interface:
```
```
Property
Name
```
```
Description
```
```
MessageID Unique identifier of the message
```
```
MessageT
ype
```
```
Indicates the type of the message (Command, Document, etc.)
```
```
Origin Name of the App where this message is originated
```
```
Destination A list of destination App names where this message needs to be sent to
```
```
Timestamp Timestamp of when this message is created
```
```
Version A string representing the version of the message
```
```
Correlation
ID
```
```
A value uniquely identifying another message which is the source/basis for this message, Ex: in a request-reply style messages the reply
message’s correlation ID should point to the request message for which this message serves as a reply message
```
```
Message Type - Enumeration :Enumeration listing all the supported types of messages.
```
```
MessageObject -Abstract Base Class: The abstract base class provided as framework's built-in type that helps model a message that is being
exchanges. Any user configured message should be extended from this base class type.
```
```
The following diagram shows an example of a user-configured message (depicted in green color) from the sample use cases mentioned in the
section Example of Inter-App Communications for the Sample Use Cases
```

### 1.

### 2.

Workflow

```
Configure a Message producer
Configure a Message consumer
```

### 1.

### 2.

# Configuration of Message Producer

A message producer is a configurable business service that produces messages or initiates send of messages when the service is executed. This
execution of a business service configured to produce messages trigger or imitate a communication between apps.

Metamodel Framework’s built-in C# Attributes help configure ‘Producer’ type of business services. These ‘Producer’ services should add corresponding
business logic to properly create and initialize the messages that intends to produce.

The following class diagram illustrates the Metamodel’s built-in framework types available for helping with configuring ‘Producers’ for the messages. The
classes depicted in green are the examples of user-configured types from the Example of Inter-App Communications for the Sample Use Cases

## Configuring a Service to Send Messages

```
Add the ‘MessageProducer’ attribute to a service type object and specify the types of messages that this service will produce.
In the business logic of the service, create an instance of this message and initialize the fields in the object as needed
```
The figure below describes the sample Metadata Model showing how to configure a service to produce messages


### 1.

### 2.

### 3.

### 4.

### 1.

### 2.

### 3.

# Configuration of Message Consumer

A message consumer is a configurable business service that consumes messages. When the app runtime receives a message, it looks up for the services
that consume that specific type of message and invoke/execute them. This execution of a business service processes/consumes the received message.

The following class diagram illustrates the Metamodel’s built-in framework types available for helping with configuring ‘Consumers’ for the messages. The
classes depicted in green are the examples of user-configured types from the Example of Inter-App Communications for the Sample Use Cases.

## Configuring a Service to Consume Messages

```
Add the ‘MessageConsumer’ attribute to a service and specify the types of messages that this service will consume through the Framework’s
interface ‘IMessageConsumer<>’
Implement a method ‘Consume’ to receive and process this message as needed for each type of message
Derive from a base service that implements the IUpdateService interface, such as ShopfloorService
If you don't want ot expose this service to API, add ExposeToApi attribute and set it to false.
```
The figure below describes the sample Metadata Model showing how to configure a service to produce messages

```
MessageConsumer Example
```
```
[COTypeId("xxxxxxx")]
[MessageConsumer]
[ExposeToAPI(false)]
public class WorkOrderCompletedConsumer : ShopfloorService, IMessageConsumer<WorkOrderCompleted>
{
public bool Consume(WorkOrderCompleted message)
{
NamedObjectService<ProductionOrderHeader> productionOrderheaderService =
Create<NamedObjectService<ProductionOrderHeader>>();
ProductionOrderHeader po = productionOrderheaderService.Create(message.ProductionOrderName);
// Handle message data
return true;
}
}
```
## Enhance a Message Consumer Service to handle Message Consume failures

```
Specify the types of messages for which this service will handle failures through the Framework’s interface ‘IMessageConsumerFailure<>’.
Implement the method 'OnConsumeFailure' to handle the message consumption failure.
If needed, send a reply message back to the originator, implementing the Message Producer pattern
```

Enhance Message Consumer to handle failures

[COTypeId("xxxxxxx")]
[MessageConsumer]
[MessageProducer(typeof(ServiceReply))]
[ExposeToAPI(false)]
public class WorkOrderCompletedConsumer : ShopfloorService, IMessageConsumer<WorkOrderCompleted>,
IMessageConsumerFailure<WorkOrderCompleted>
{

public bool Consume(WorkOrderCompleted message)
{
// Add here your consume logic
return true;
}

public bool OnConsumeFailure(WorkOrderCompleted message, Exception failureReason)
{
Logger.LogInformation($"Consume WorkOrderCompleted from {message.Origin} FAILED: {failureReason.
Message}");
if (message.ReplyFailureToOrigin)
{
var reply = GetFeature<MessageProducer>().CreateMessage<ServiceReply>();
reply.Destination.Add(message.Origin);

switch (failureReason)
{
case MOMException:
reply.ErrorCode = (failureReason as MOMException).ErrorCode;
reply.Reason = (failureReason as MOMException).Key;
break;

case MOMUserException:
reply.ErrorCode = (failureReason as MOMUserException).ErrorCode;
reply.Reason = (failureReason as MOMUserException).Key;
break;
default:
reply.ErrorCode = -1;
reply.Reason = failureReason.Message;
break;
}
}

return true;
}


# Generation of Model-driven Interfaces for Communication

gRPC uses protocol buffers as both its Interface Definition Language (IDL) and as its message payload format. The required protocol buffers for the
communication are created by extracting the Message models from the Metadata using the Metadata Engine’s Introspection API.

The image below shows how the Proto Generator generates the required protocol buffers for each of the Message models defined in the Metadata
assembly. The generated protocol buffers are generated into a ‘.proto’ file which is then used to generate the message and interface contracts and the
client and server stubs using the tooling provided by gRPC. These generated stubs are then built into a standalone dotnet assembly, aka. Proto Assembly

. (for more information see Modular MOM Messaging - Overall Design and Inter-App Communication Interface )

The image below shows the process of generating protocol buffers from the Message Model:

The image below shows the sample Message Model and corresponding generated artifacts:


# Inter-App Communications Sequence for Example Use Case

The following diagram represent a gRPC based communication sequence and it illustrates a 2-Way Request-Reply style inter-app communication for the
sample use case mentioned in Inter-App Communications - Use Case View and Example of Inter-App Communications for the Sample Use Cases.

This sequence starts from the user triggered action on the Dispatch App - Launch Work Order, which is represented by the action #3 in the Component
View section of Example of Inter-App Communications for the Sample Use Cases. This user action triggers a CreateWorkOrder (#4) message to be sent
to the TnT App. When the TnT App processes the received message, the Create Work Order business service that is configured to consume the CreateW
orkOrder message sends a reply, back to the Dispatch App by means of sending a WorkOrderCompleted (#5) message.


# Runtime View


# High Level View

In any RPC style communications, there are 3 components is play at runtime:

```
A server that listens and receives the messages
A client that initiates and sends messages to a server
The interface contract between the server & the client that defines the message structures and the call signatures
```
Modular MOM’s Metadata Runtime which is a host process for the business model configured as metadata, acts as both the client and server for the gRPC
based inter-app communications.


# Inter-App Communication Interface

Modular MOM is a metadata-driven platform and since the business functionality of apps are configured through the metadata models, the following
strategy is used to define the gRPC based interface between the client and the server components.

```
Define a generic interface for the call with the following signature using the generic message structures
```
```
Define high-level generic message data structures which would later be used to wrap the user configured message structures
```
## Google.Protobuf.WellKnownTypes.Any

Any contains an arbitrary serialized protocol buffer message along with a URL that describes the type of the serialized message.

The generic data structures used in this RPC interface leverage the concept of ‘Any ’, Protobuf’s well-known type that helps to wrap/pack any arbitrary
information to be included as part of this communication.

The strategy here is to dynamically generate the user-defined message types for the protocol buffer definitions by extracting the metadata information from
the Message Models. Then use these dynamically generated protocol message structures from the Proto Assembly to pack into the generic structures of
the interface when making remote calls.

For more information, see Generation of Model-driven Interfaces for Communication.


# gRPC based Communication Processing Sequence

The following communication sequence is an example of Two-Way Message Pattern with Request-Reply Messages implemented as 2 separate
messages using the One-way Message Pattern with Command or Document Type Message. This diagram expands on the generic sequence from section
Modular MOM Messaging - Overall Design by including the flow information specific to the gRPC technology contextualized in to the Modular MOM's
Metadata Runtime.


# gRPC Server

Modular MOM’s Metadata Runtime is based on the ASP.NET Core framework-based application server. This framework defines patterns and has hookups
for gRPC based services like that of the standard REST based services. Based on the server stubs generated, as mentioned in the previous sections, a
simple API Controller is configured to implement the contract as defined in the generated ‘.proto’ file. The server stubs are part of the generated Proto
Assembly and is ingested into the server through deployment configuration. This Proto Assembly also contains all the message structures of all the user
configured ‘Message’ objects.

When a client request comes in, the gRPC API Controller inspects the ‘Any’ type field on the request to determine what type of message structure was
used to pack into that value. The type information of the message object packed into this ‘Any’ type field is available by inspecting the Proto Assembly.
Once the actual type of the message structure is determined the value from that field can be unpacked.

## Determination of the Consumers of an incoming message

The consumers for any given message type that is part of the loaded Proto Assembly are mapped to its source metadata object type from which the proto
message was originally generated. Once the metadata message object type information is available, the consumers (services that consume this type of
message) are looked up. As per the Metamodel specification, all services that can consume any specific type of message are required to implement a
specific framework interface type (IMessageConsumer<>) to indicate its intent. Metadata Engine uses this information to compile a list of consumers for
every message type and it is served to the callers through the Metadata Engine’s Introspection API.

After the actual message object is unpacked, and corresponding consumers are determined, the proto message is mapped to the Metadata object
instance and then is passed on to the consuming service for further processing as configured in the metadata using the Metadata Engine’s invocation
APIs. The Metadata Engine executes the business logic as configured to consume the incoming message


# gRPC Client

Initiation of communication from an app to another app begins during the execution of business logic for any service within the Metadata Engine as part of
some user request. The trigger for such communication is when a ‘Message’ type of object is created as part of the business execution. The Metadata
Engine tracks these kinds of objects and then after the service logic execution is complete and the service is successfully committed, Metadata Engine
triggers the communication using the gRPC Client implemented as part of the Integration layer of the Metadata Runtime.

The gRPC Client is based on the client stubs that are part of the Proto Assembly loaded into this runtime. The gRPC client first constructs the request
based on the server contract interface. As part of the request building, the gRPC client maps the message object to its corresponding proto message type
and packs the request with value into the ‘Any’ type field.

## Determination of the Remote Server

As part of the Metamodel specification, all messages should include the destination of the message through the ‘Destination’ field of the MessageObject.
The value for this Destination field is the name of the App to which this message should be sent to. Once the destination app name is available, it can be
looked up using the name resolution strategy implemented. Once the request is constructed a remote call is initiated to the remote server using the gRPC
framework.

## Name Resolution Strategy

```
Current strategy: To lookup the name from a preconfigured list of apps and its URLs, which are configured as part of the deployment
configuration.
Long Term strategy: Leverage external DNS/Gateway/Load Balancer/Service Mesh etc. to automatically resolve the remote apps’ address by its
name.
```

# gRPC Security

gRPC is designed to work with a variety of authentication mechanisms. gRPC provides simple authentication API to pass all the necessary Credentials
when creating a channel or making a call. The following authentication mechanisms are built-in to gRPC: SSL/TLS, ALTS, Token-based authentication,
etc. The ASP.NET Core implementation of gRPC supports authentication and authorization through most of the standard ASP.NET Core mechanisms. [gR
PC Authentication]


# Reconfiguration of Inter-App Communications

Modular MOM Apps are configurable apps and the business functionality is expressed through Metadata configuration. As such, any reconfiguration needs
for the inter-app communications can easily performed by updating the configuration as needed and on both the ‘Sender’ app and as well as the ‘Receiver’
app.


# Communications Resiliency


# Communication Problems

In any distributed application, transient faults are not uncommon, and an application should be designed to handle them elegantly and transparently. This
minimizes the effects faults can have on the business tasks the application is performing.

Clients may encounter network problems during the communication with their peer server (the server is down, the intermediate network infrastructure
present some faulty apparatus, load balancer, so on and so forth).

The server may encounter intermittent failure during execution of a service depending on local conditions (load, local resources exhaustion, problems to
access data).

Dynamic deployments may result in temporary failures on communication impacting one or more services (upgrades while other services are still working).

Resiliency of a system is:

```
its capacity to recover quickly from difficulties; toughness
its ability to spring back into normalcy after certain types of error conditions; elasticity
```
Modular MOM’s App (Metadata Runtime) and its inter-app communication framework are built with such resilience, by design. Since an App serves as
gRPC client and a gRPC server, both components have different resiliency requirements. The following sections describe the resiliency of these
components:

```
Resiliency of the gRPC client
Resiliency of the gRPC server
```

# Resiliency of the gRPC Client

The gRPC framework has built-in connection resiliency and supports several other techniques to build resilient clients such as providing timeouts,
deadlines or supporting cancellation requests. These are a good start but for a system to be more resilient additional mechanisms need to be implemented
to mitigate the problem and avoid downtime. Main strategies from an RPC client perspective are:

```
Persistency & Recovery strategy
Retries with Exponential back off strategy
Circuit Breaker strategy
```
The detailed gRPC Client flow diagram in the page Client Resiliency Flow illustrates how these strategies work together to improve the resilience of the
gRPC Client.


# Persistence & Recovery Strategy of the gRPC Client

Inter-app communication messages are sent outside of any business transaction, by design. So, while sending a message to another app and if any failure
occurs after the commit but before the message is sent, then that message will be lost and there will not be any way to recover it. To handle these
transient errors and make the gRPC client component resilient, persisting the message and tracking its state is essential to recover from these errors.

The state machine depicted below is a simple state machine to track the various states of the message being sent out. The Created state indicates that
the message is just created and the InProcess state indicates that the message is picked up for being sent. The Sent state indicate that the message is
successfully sent out.


# Retry with Exponential Back Off Strategy

Retry strategy helps with faults caused by one of the more commonplace connectivity or busy failures such as the network or service might need a short
period while the connectivity issues are corrected, or the backlog of work is cleared. The application should wait for a suitable time before retrying the
request. The number of retries could also be configured along with delay between the retries using an exponential back off strategy to not overload the
system or the network in these critical times.

This strategy is better explained by the following simplified activity flow diagram.


# Circuit Breaker Strategy

This strategy helps handle faults that might take a variable amount of time to recover from, when connecting to a remote service. This can improve the
stability and resiliency of an application.

There can also be situations where faults are due to unanticipated events, and that might take much longer to fix. These faults can range in severity from a
partial loss of connectivity to the complete failure of a service. In these situations, it might be pointless for an application to continually retry an operation
that is unlikely to succeed, and instead the application should quickly accept that the operation has failed and handle this failure accordingly to prevent any
cascading failure subsequently. For more information, see https://docs.microsoft.com/en-us/azure/architecture/patterns/circuit-breaker

The Circuit Breaker is implemented as a state machine with the following states that mimic the functionality of an electrical circuit breaker:

```
Closed: In this state the request is routed to the operation
Open: In this state the request fails immediately, and an exception is returned
Half Open: In this state a limited number of requests are allowed to pass through and invoke the operation to test the remote service status. If the
requests succeed the state is transitioned into a Closed state for normal flow otherwise it will transition into an Open state to prevent further
failures
```
The Circuit Breaker States and its transitions are better expressed by the following diagram.


# Client Resiliency Flow

The following diagram illustrates 2 separate flows that implement outgoing message framework for resiliency built into it.

```
The first flow illustrates a standard business operation being executed that initiates an outgoing message. These messages are persisted into a
persistent store as part of the business process commit cycle.
The second process illustrates a background process that’s polling the persistent store for new messages and picks up any new message and
sends them out using the Circuit Breaker pattern and the Retry pattern for resiliency.
```
## Implementation Notes

```
The background polling process should also be invokable on-demand as part of the business service execution after a successful commit to keep
the message transmission to near real time and not having to wait for the background process for its next iteration
The implementation team could be free to choose persistent store and the poll/pull mechanism while still being aligned with the overall
deployment and configurability aspects – options like: DB store, Persistent distributed cache store, etc.
Resilience strategies like circuit breaker and retry could leverage open source hardened frameworks and libraries like Polly for robust configurable
implementation and to keep out technical debt to a minimum.
```

# Resiliency of the gRPC Server

The gRPC framework has built-in connection resiliency and supports several other techniques to build resilient servers such as providing timeouts,
deadlines or supporting cancellation requests. These are a good start but for a system to be more resilient additional mechanisms need to be implemented
to mitigate the problem and avoid downtime. Main strategies from an RPC server perspective for the Modular MOM’s inter-app communications are Persist
ence & Recovery Strategy of the gRPC Server


# Persistence & Recovery Strategy of the gRPC Server

When messages from other Modular apps are received by the gRPC Server they are validated before sending an acknowledge back to the client and the
messages are then processed asynchronously in the background to trigger the consume actions as configured. But, if any non-business failure occurs
before the message is consumed by a consuming business service, then that message will be lost and there will not be any way to recover it. To handle
these errors and make the gRPC server component resilient, persisting the message and tracking its state is essential to recover from these errors.

```
Note
```
```
The state machine for the message processing on the server has a configurable time-based auto transition to recover from runaway threads. This
auto-transition should occur after the message is in the InProcess state for the preconfigured amount of time and there are no active tasks
processing this message.
```

# Server Resiliency Flow

The following diagram illustrates 2 separate flows that implement the resilient mechanism for processing messages of the server:

```
The first flow illustrates the gRPC server receiving the message which then validates and stores the received message into a persistent store.
The second flow is a background process that’s polling the persistent store for new messages and picks up any new received message and
processes them by invoking the corresponding consuming services.
```
```
Note
```
```
The implementation team could be free to choose persistent store and the poll/pull mechanism while still being aligned with the overall deployment
and configurability aspects – options like: DB store, Persistent distributed cache store, etc.
```

# Physical View


# Deployment Configuration

Modular apps are based on the configurable Metadata and are deployed by ingesting the Metadata models into the Modular MOM’s Metadata Runtime at
the time of deployment. For supporting the gRPC interfaces the Proto Assembly generated from the metadata models also need to be ingested to expose
the model-driven gRPC interface. The necessary deployment configuration is ingested by using a manifest file as shown in the diagram below.

For more information, see the Modular MOM Metadata Architecture documentation.


# Deployment Architecture

Modular MOM is a distributed scalable application and any number of runtime instances of an app could be deployed as per the load and scaling needs of
the app. (for more information see Modular Apps Asynchronous Messaging over RPC style Communication, Modular MOM Messaging - Overall Design,
and Event Notification Pattern with Event Messages)

One of the primary architectural pillars for Modular MOM is the capability to be platform agnostic and be able to run anywhere – on-prem, cloud (private
/public), edge, etc. To fulfil this requirement Modular MOM leverages the Application Containerization technology and the Kubernetes everywhere strategy
for managing and orchestrating the application containers.

The following diagram is a simplistic depiction showing more that one instance of an app running and the possibility to deploy these instances on any node
in a distributed fashion.


# Metadata Runtime and App Deployment Configuration


### 1.

### 2.

### 3.

# Modular MOM Deployment Configuration

Modular MOM apps are based on the configurable Metadata and are deployed by ingesting the Metadata models into the Modular MOM’s Metadata
Runtime at the time of deployment. The key requirements of the deployment architecture to be considered in the deployment configuration are:

```
Containerized Applications
Productize-able deployment configuration:
Any certified K8s distro
On-prem
On-cloud
Role-oriented deployment configuration:
to enable management of different components by different role contexts, that is, cluster (external) versus Application (internal).
Portable across many implementations
Expressive enough to support core concepts
Extensible for more advanced use cases
Run Anywhere Stack, that is:
On-Premises
On Cloud (Public/Private)
Hybrid Cloud (Private and On-Prem)
```
## Operational Context

The system lifecycle of Modular MOM is composed of 3 phases:

```
Setup: this phase involves environments, database and Metadata Management.
Configure: Metadata, engineering/ configuration and UI
Operate: Publish Metadata, business apps, Runtime and Metadata customizations.
```
Based on both requirements and context, the chosen strategy of Modular MOM is the Kubernetes Deployment Strategy :

```
In case of on-premise or private cloud there is a compliant Kubernetes cluster to deploy anywhere (Kubernetes certified cluster)
The Kubernetes operator is the Modular MOM unit of deployment.
Kubernetes orchestration based Application stack
External dependencies can exist in cluster or out so any external managed service can be used where practical.
```
For more information about Kubernetes and apps, see Kubernetes Concepts for Applications.

## Deployment Architecture Strategy

This image depicts the technology selections of the deployment architecture strategy that cause the adoption of Kubernetes Application Stack to fulfill the
requirement "Run anywhere stack".


# Modular MOM Deployment And Packaging

The Modular MOM Kubernetes Operator for on-prem and Private cloud is a method of packaging, deploying, and managing a Kubernetes application. On
Kubernetes, an application is both deployed and managed using the Kubernetes API tooling. It is also an application-specific controller that extends he
functionality of the Kubernetes to create, configure, and manage instances of complex applications.

The Modular MOM Kubernetes Operator is the sum of Modular MOM Application Stack, Modular MOM Operational Patterns and Kubernetes Operator
SDK.


# Modular MOM Application Environments

The image below shows the Kubernetes Cluster Diagram - On-Prem and Private Cloud.

## Notes

```
Environments are optional and perform isolated deployment of the Application Stack
Namespaces are logically isolated, can be shared across Nodes and Node Pools, can span across multiple Node Pools and can have resource
quotas/limits.
Node Pools can exist in different availability zones.
Dependencies can exist in/out and can also be cloud native services.
```

# Modular MOM Design Decisions

Based on the requirements of Modular MOM deployment, the following design decisions have been adopted.

## Ingress, Gateway and Load Balancer Options

In any certified K8S cluster:

Key Decisions:

```
Ingress Controller as Baseline Service or Dependency with Customer’s responsibility
Customer’s choice of Ingress Controller
SSL Termination at Ingress
Ingress as App managed resource:
Rules Strategy – sub-domain per namespace/env or URL path?
```
- Are there any other considerations?
API Gateway as App managed resource
Simplifies Ingress configuration by the application
built-in Load Balancer

## Kubernetes Gateway API Key Design Decisions

A new set of Kubernetes APIs for Advanced Traffic Routing


Key Decisions:

```
Gateway Class & Gateway as Baseline Service or Dependency with Customer’s responsibility
Customer’s choice of Gateway Controller
SSL Termination at Gateway is Customer’s responsibility
Routes configured by application:
HTTP or TCP Routes
Route Strategy – URL path
API Gateway as an optional & App managed resource:
Simplifies handling additional cross-cutting concerns specific to an env (ex: production)
built-in Load Balancer for better
balancing strategy
```

# Kubernetes Concepts for Applications

The key design considerations of Kubernetes are:

Containerized applications:

```
Deployments
Pods
Sidecars
```
Service:

```
Abstraction over a set of application instances
DNS
Discovery
Load Balancing
```
Ingress:

```
Exposes services
Gateway pattern, reverse proxy, load balancing
SSL termination, AuthN/AuthZ
```
Namespaces:

```
Scope of resources
Isolation technique
Resource constraints
Resource quotas
Security policies
```
Auto scaling:

```
Horizontal Por Autoscaler (HPA)
Cluster Auto Scaler
Disruption Budgets
```
Resource constraints:

```
Memory & CPU constraints
Resource quotas
Per namespace
```
Health probes:

```
Readiness probe
Leveliness probe
```
Rolling updates:

```
CI/CD with Zero downtime updates
Zero downtime upgrades
```
Secrets management:

```
Sensitive information management - Credentials / Passwords
```
Storage/Persistent data:

```
Volumes
Storage classes
```

# Modular MOM Application Stack Components

The following images show the logical view of the Modular MOM components.

## Application Stack Components

## Application Stack Components with dapr


# Modular MOM Deployment Types

The following images describe the types of Modular MOM deployment.

## On-Prem Small Deployment and Edge

Target:

```
Cheap, simple single node installation
Can scale to multi-node HA Configuration
```
Takeaways:

```
Explore MicroK8s
Has great lightweight single-node, low barrier to entry that supports Windows, Linux, and Mac.
Can also scale to High Availability multi-node mode with configuration changes
By using Operator + MicroK8s, our codebase (and even operations configuration)) will be nearly the same as our other targets.
```
## Multi-Deploy Single Node / Edge to Multi-Node HA


# Modular MOM - Kubernetes Application

The following image depicts the runtime services logical view of SaaS deployment - cluster diagram.


# Modular MOM Metamodel Design

Modular MOM's Metamodel is inspired from the Metamodel of Opcenter Ex Core. Based on its salient features and the management of the Configurable
Object (the core of the Metamodel design) the Metamodel design is focused in the Metadata as code design as described below:

```
Bootstrap Custom Metamodel (DSL) by building onto a popular commodity language in such a way that the metadata could be expressed as code.
A multi-paradigm imperative programming commodity language could be considered for this, since widely known opensource languages have
community and commercial support and tools.
Custom Metamodel (DSL) prescribes the structure of the metadata and its associated behaviors, that is, business logic.
The Metadata components such as the structural definitions are married naturally with the behavioral definitions using the object-oriented
concepts provided by the language itself :
Cross-cutting features could be toggled using the aspect-oriented features of the language.
Functional programming concepts could be used in expressing the business logic, as seem fit.
Abstract models could be built on top of the metadata-as-code to represent the components of the Custom Metamodel, these abstract models
help build modeling tools without having to deal with the underlying code directly.
```

# Modular MOM Metadata Design

The Metadata building blocks are built into the M1 abstraction space leveraging the concepts and features provided by the Metamodel, M2 level and add
another abstraction layer on top of the Metamodel features to provide a way for simpler configuration experience for the end user.


# Modular MOM Metadata Engine

Metadata Engine is the key component for the metadata execution strategy and is responsible for processing and executing the business functionality
expressed as Metadata.

For more information about Metadata Engine, see Model Execution

## Interfaces

The image below shows the available interfaces:

```
Interfaces Exposed:
Invocation API: Contains Interfaces and Classes to support the interaction with the Metadata Engine to instantiate Metadata Objects
(‘Service’) and execute them.
Introspection API: Provides API for the discovery of the metadata model at the runtime.
Interfaces Consumed:
Persistence API: This platform API is provided by the embedding runtime to handle object persistence.
Communications API: This platform API is provided by the embedding runtime to help with communications with other Apps.
Observability API: This platform API is provided by the embedding runtime to enable monitoring and make metadata engine observable.
```
## Components Logical View


Design Time Framework

```
Base Class Library
```
```
Abstract Base Classes is a collection of fundamental types and interfaces used to define a configurable object.
Field Types are custom types to define the various types of fields in a Configurable Object such as native types like String, Integer, etc.,
along with types to represent various types of objects.
```
Runtime Framework

```
Model Execution:
Invocation API: Provides support for creating and executing Configurable Objects (Metadata)
Runtime Execution Context: Provides runtime context for the business logic execution.
Lifecycle Management: Manages object state and execution of system built-in life-cycle events.
Model Introspection
```
```
Introspection API: Provides API for the discovery and introspection of the metadata models at the runtime.
```
Object Lifecycle Events

```
Every Configurable Object and Fields within them have a set of predefined events such as
OnInitialize & After_Execute on the Objects
OnGetValue & OnSetValue on the Fields
Events are fired by the Metadata Engine at runtime based on the Configurable Objects Instance's state and lifecycle.
```
The image below shows an example of firing these events when certain actions are performed on the Configurable Object instances:

```
Creating a new instance triggers ‘OnInitialize’ even on that object instance
Requesting or accessing the value of a field triggers ‘OnGetValue’
```

Metamodel and Metadata Sensitivity Points

The table below describes the sensitive point and the corresponding risk mitigation

```
Sensitivity Point Risk Mitigation
```
```
Learnability of the Metamodel concepts Documentation, Education and Configuration Tools
```
```
Learnability of the Metadata concepts Documentation, Education, Simple Abstractions and Configuration Tools
```
```
Complexity of configuration tools development Metamodel Abstractions and .NET Foundation Tools
```
```
Data properties as C# objects may induce performance overheads Compute intensive algorithms could mix in native C# mechanisms
```
```
Intellectual Property Protection Shared library with native implementations and binary distribution
```

# Metamodel and Metadata Sensitivity Points

The following table describes the sensitive point and the corresponding risk mitigation.

```
Sensitivity Point Risk Mitigation
```
```
Learnability of the Metamodel concepts Documentation, Education and Configuration Tools
```
```
Learnability of the Metadata concepts Documentation, Education, Simple Abstractions and Configuration Tools
```
```
Complexity of configuration tools development Metamodel Abstractions and .NET Foundation Tools described in the section
below.
```
```
Data properties as C# objects may induce performance
overheads
```
```
Compute intensive algorithms could mix in native C# mechanisms
```
```
Intellectual Property Protection Shared library with native implementations and binary distribution
```
## Complexity of Configuration Tools Development

The Complexity of Configuration Tools Development is mitigated by leveraging model abstractions & .NET Foundation Tools:


# Metadata Runtime

Metadata Runtime is a host application server that embeds the Metadata Engine and provides all platform level support needed for executing the Metadata
by the Metadata Engine.

The architectural requirements for Metadata Runtime are:

```
Model Driven
Run Anywhere
Database Vendor Agnostic
Multi-Model API
Observable
```
The technology selections have been decided accordingly to create a platform agnostic stack:

```
ASPNET Core based Platform Agnostic Runtime
nHibernate ORM based Database Persistence
Container based Application Virtualization
```
## Database Persistence Strategy

The adopted strategy for database persistence is a Relational data model based ORM. ORM stands for Object-Relational Mapping and is a
programming technique for converting data between relational databases and object-oriented programming languages.

ORM Benefits

```
SQL abstractions: optimized SQL generation and prevents SQL injection.
Database abstractions: Provides vendor agnostic abstractions and typically supports multiple vendors.
DRY Principle: Models can leverage OO concepts and some Model can be used for all CRUD operations.
Improved developer productivity: easier to code and maintain
```
ORM Key Features Considered

```
Data Access Features
Transactions and Isolation levels
Data Concurrency Control
Data Change Tracking
Lazy loads
Schema Management
Automatic creation/migration of database schema
Programmatic configuration
Explicit constraint management
Table per type and Table per hierarchy patterns
Performance
Optimized SQL generation
Concurrent transactions
Asynchronous and non-blocking calls
Other Features
Custom ID strategy
LINQ to SQL
Raw SQL query capability
Support for multiple database providers
```


# Metadata Runtime Components

This page describes the logical view of Metadata Runtime components and the request processing sequence as well as the design principles and the
requirements to decisions mapping.

## Logical View

## Design Principles

```
Separation of concerns
Strict contracts between components
Keeping it simple.
```
## Requirements to Decisions Mapping

```
Model Driven: embedded Metadata Engine
Multi Model APIs: dynamic generation of REST, gRPC APIs
Database Vendor Agnostic: Commodity ORM based Persistence Engine
Observable: Configurable Observability
```
## Request Processing Sequence



# Metadata Runtime Decisions

As described in [Partially Obsolete] Metadata Module Architectural Concept, the Modular MOM architecture is based on two key approaches: Metadata-
driven and Modularity. In order to fulfill the Model-driven requirements, Modular MOM has adopted the Configurable Model Ingestion as depicted in the
following graphical representations.

Modularity

The image below depicts the concept of Modularity and the role of Metadata Runtime in the creation of apps. The Modularity sensitivity point is
Deployment Configuration.

Model-Driven

The system behavior and business capabilities are defined through a configurable set of data covering all their aspects as showed below. The Model-
driven sensitivity point is Deployment Configuration.

Observability


In order to fulfill the Observability requirement, Metadata Runtime uses Open Telemetry based Observability as depicted in the Overall strategy image.

The Observability sensitivity points are Deployment Configuration and Integration with Mendix Services.

Governance

To fulfill the requirement of "run anywhere", the decision adopted is Container Optimized and Application Configuration (see below the overall strategy
of the application configuration).

The Governance sensitivity point is Deployment Configuration that is mitigated by the Configuration Design.

.


The Governance sensitivity point is Deployment Configuration that is mitigated by the Container Optimized Application Configuration Design


# Metadata Runtime Sensitivity Points

The following table describes the sensitive point and the corresponding risk mitigation of Metadata Runtime.

```
Sensitive Point Risk Mitigation
```
```
Container Orchestration Kubernetes
```
```
ORM Performance Careful design and slim wrappers around ORM as a
tradeoff between performance & functional needs
```
```
Deployment Configuration Container/Kubernetes Optimized Application
Configuration Design
```
```
Integration with Mendix Services Open Telemetry Exporter for Mendix Services - Build if
not provided by Mendix
```

# Setting File Fields Details


# Configuring the Platform Application Settings

All the common settings of the application are generally stored in appsettings.json files and managed when configuring the application for the deployment

. These files are separated on the basis of environments and used to store configuration settings such as database connection strings, any application
scope global variables, logging, host filtering, etc. For example, the settings for production environment are stored in appsettings.production.json and
the settings for acceptance environment are stored in appsettings.acceptance.json.

The following pages describe specific parameters of Modular MOM appsetting.json files:

```
Appsettings.json file: Messaging Parameters
Appsettings.json file: Observability Parameters
```

# Appsettings.json Messaging Parameters

This section of the appsettings.json file describes the parameters to configure the messaging subsystem, in particular, the resiliency capability of the
platform.

## Messaging

This section in the "appsettings.json" file describes the parameters to configure the messaging subsystem, in particular, the resiliency capability of the
platform.

The example below shows the default value for each parameter:

### {

```
"ModularMOM": {
"Messaging": // Applied for each destination app / message
{
"MaximumResponseTime": 45000, // max time to wait the remote response, after that the message will be
recovered
"MaximumConsumeTime": 45000, // max time to wait the handling of the message, after that the message will
be recovered
"Retry": // retry delay interval = ScaleFactor * 2 ^ RetryNumber + RND(0, JitterRange)
{
"RetryNumber": 6, // 0 disable the retry mechanism
"ScaleFactor": 1.0,
"JitterRange": 0 // in millisec. 1000 = 1s
},
"CircuitBreaker": {
"AllowedConsecutiveFailures": 20, // 0 disable the circuit breaker mechanism
"DurationOfBreak": 20000
},
"Recovery": {
"CycleTime": 30000,
"BatchMessageNumber": 10
"MaxMessageLifetime": 10800000, // 3 hours, after that the message is put in a failed state and never
recovered again.
"MaxPersistenceTime": 86400000 // 1 day, after that the messages consumed or sent successfully are
deleted }
```
### }

### }

### }

## Retry

The retry is performed when some conditions are met, in particular when the communication subsystem encounters a gRPC error of these types:

```
Failure on gRPC Exceptions with the following state codes:
StatusCode.DeadlineExceeded,
StatusCode.Internal,
StatusCode.ResourceExhausted,
StatusCode.Unavailable,
StatusCode.DataLoss,
StatusCode.Unknown
```
Note that connection problems with the remote peer are mapped into gRPC states such as "Unavailable".

When such kind of problems are detected, an exponential back off is performed to recover the not delivered messages which are sent to the remote
application: this allows for retries to be made initially quickly, but then at progressively longer intervals, to avoid hitting a subsystem with repeated frequent
calls if the subsystem may be struggling.

The calculation of the delay for each retry is the following:

```
retry delay interval (RetryNumber) = ScaleFactor * 2 ^ RetryNumber + Random(0, JitterRange)
```
So that, with the default parameters, we have the following sequence:


```
Retry Count Delay for the retry
```
### 0 N.A.

```
1 2 seconds
```
```
2 4 seconds
```
```
3 8 seconds
```
```
.... 2 ^ RetryNumber
```
The ScaleFactor is a multiplicative term to rescale this sequence.

The Jitter is a variation on the sequence to spread the retry randomly. In very high throughput scenarios it can be beneficial to add Jitter to prevent retries
bunching into further spikes of load.

CircuitBreaker

The Circuit Breaker is a mechanism to avoid to send too many messages when the remote peer is in failure.

A circuit breaker detects the level of faults in calls placed through it, and prevents calls when a configurable fault threshold is exceeded.

While retrying plays for success, faults do arise where retries are not likely to succeed or may be counter-productive - for example, where a subsystem is
completely offline, or struggling under load.

In such cases additional retries may be inappropriate, either because they have no chance of succeeding, or because they may just place additional load
on the called system.

The circuit initially starts closed. When the circuit is closed:

```
The circuit-breaker executes actions placed through it, measuring the faults and successes of those actions.
If the faults exceed a certain threshold, the circuit will break (open).
```
While the circuit is in an open state:

```
Any action placed for execution through the policy (sending a message) will not be executed through the circuit and this will set the messages
sent in a failed state.
The circuit remains open for the configured DurationOfBreak. After that time span, on a new request, a probe message is placed through the
circuit in order to update its state periodically.
If the probe message confirm the failure the circuit will remain open, otherwise the circuit will be closed and the normal communication activity
recovers.
```
Recovery

The Modular MOM Platform is capable to store each and every message sent or received by its applications. When a undelivered message is stored and
the sending process dies, the messaging subsystem recovers that message and send it back to the defined destination.

In the same fashion, the Recovery is applied also to the Consuming mechanism to recover each message arrived and not processed by the Business
Logic when the process dies during processing.

In this way the data is not lost even when the network or virtual infrastructure is behaving erratically.

Each process has its own Recovery cycle that monitors the state of each message recorded, regulated by the following parameters: CycleTime, BatchMes
sageNumber (See Parameter Description).

In order to do so Message Recovery acts on the persisted messages recorded into the Database during the execution to keep track of the current state
and the expirations. Each message sent and received is persisted and its state is updated at every step (see Persistent Messages)

Persistent Messages

The messages are persisted into the Database in the following tables: PersistentMessage, PersistentMessagePerConsumer.

PersistenMessage is the main table where the messages in/out are recorded.

PersistentMessagePerConsumer is the secondary table to keep track of the consume operation for each consumer of the same message type, this is
necessary for the Message Recovery in order to restart only those consumers that have not consume the message yet, skipping the ones that have
already consumed it previously.

The state of the messages are:

```
Value Message
State
```
```
Description
```
```
0 ClientMessage The message is sent from the application to a remote peer, but the remote peer has not sent back any confirmation yet.
```

```
Sent
```
```
1 ClientMessag
eAcked
```
```
The remote peer confirmed the arrival, recording the incoming message on its side. This is a terminal state for the
messages.
```
```
2 ClientMessag
eFailed
```
```
The message is not sent successfully. This may happen whether all retries have failed or if the circuit breaker policy
establish that a destination is in failure. This is a terminal state, recovery will ignore failed messages.
```
```
3 ServerMessag
eRetreived
```
```
The server application received the message and it is trying to execute the Consume().
```
```
4 ServerMessag
eConsumed
```
```
The server application finished the Consume operation successfully. This is a terminal state for the received messages.
```
```
5 ServerMessag
eFailed
```
```
The server application encontered a non recoverable error. This is a terminal state, recovery will ignored failed messages.
```
Recovery will monitor each message considering its expiration parameter (MaximumResponseTime for the send, MaximumConsumeTime for the
consumers).

Automatic cleanup:

Troubleshooting

You can see the persistent messages with the following SQL query:

```
SELECT * FROM [<ModMomDb>].[dbo].[PersistentMessage]
select * from [<ModMomDb>].[dbo].[PersistentMessagePerConsumer]
```
In certain circumstances, its useful to clean up these tables manually with the following SQL statements:

```
delete from [<ModMomDb>].[dbo].[PersistentMessage]
delete from [<ModMomDb>].[dbo].[PersistentMessagePerConsumer]
```
Failed messages:

As already mentioned, the retry mechanism is going to retry to send any messages which are not completed. However, after the retry sequence failed, or
because of the circuit breaker caused the message to fail, this set the client message in a failed state which it is a final state.

In case some messages have failed during a server down phase for the message Destination, and you need to re-send them after the infrastructure has
been recovered, they could be restarted changing the State to 0 and waiting for the next Message Recovery cycle to complete. They will be recovered
sending the messages again after the original Expiration time (likely already expired).

```
-- Restart any failed message again
UPDATE [<ModMomDb>].[dbo].[PersistentMessage] SET State = 0 WHERE State = 2 AND Destination = '<remote module>'
```
```
The expiration for every client message is extended to cover the delay at each retry. If the application crashes in the middle of a retry sequence,
the message will be recover at the next expected retry time + MaximumResponseTime, restarting the retry count from 0.
```
```
The PersistenMessage and the PersistentMessagePerConsumer tables are automatically cleaned up after 1 day, leaving the messages in an
not completed state untouched (ClientMessageSent, ServerMessageRetreived ) and keeping messages in a failure state (ClientMessageFailed,
ServerMessageFailed) for further root-cause analysis.
```
```
Use it carefully.
```
```
This operation will delete every messages from the tables. Any persistent messages will be deleted and any tracks of the message content will
be lost.
```
```
This could also cause errors during any in-progress network operations or any in-progress execution of the Consume method during the
message consumers transactions.
```
```
This operation will restart any messages in a failed state. It is still possible that the confirmation did not arrived but the message had been
previously received and processed by the remote peer. However, the server has an anti-duplication mechanism to distinguish already
processed messages, so it will just complete them setting the State to 1, without executing them again.
```

Parameter Description

```
Parameter Description Default UoM Notes
```
```
Retry:
RetryNumber
```
```
The overall number of attempts to perform when
the communication is broken
```
```
6 int 0 disable the retry mechanism.
```
```
Each retry is counted and applied to the message instance sent.
```
```
Retry:
ScaleFactor
```
```
A multiplicative term to apply to the delay of the
next retry
```
```
1.0 double
```
```
Retry:
JitterRange
```
```
A slight variation to apply to the delay of the
next retry
```
```
0 millisec
onds
```
```
The Jitter is a variation on the sequence to spread the retry randomly. This
parameter is a random peek between 0 and JitterRange milliseconds.
```
```
CircuitBreaker:
AllowedConsec
utiveFailures
```
```
CircuitBreaker will break after N consecutive
failures executed through the policy, where N is
the AllowedConsecutiveFailures.
```
```
20 int 0 disable the circuit breaker mechanism.
```
```
Consider that this parameter must be higher than the RetryNumber
parameter in order to avoid conflicts with the Retry Policy: each retry will be
counted also for the Circuit Breaker Policy.
```
```
All aggregated failures on each destination are counted in the Circuit Breaker
Policy, thus, it is advisable to configure a value high enough to sustain an
adeguate parallel bandwidth during a failure scenario.
```
```
CircuitBreaker:
DurationOfBreak
```
```
The time during the circuit is broken and all the
actions executed through it, will fail without
being attempted.
```
```
20000 millisec
onds
```
```
After this time, a new message is sent over the network to update the circuit
state. If this probe is completed successfully the circuit will close again,
otherwise it will stay opened and broken.
```
```
Recovery:
CycleTime
```
```
The time period used to check the state of each
persisted message and apply the Recovery
Policy.
```
```
30000 millisec
onds
```
```
Each message is checked against a computed expiration time which is
updated during normal operations. When failures occur all the expired
messages are recovered (see MaximumResponseTime and MaximumConsu
meTime).
```
```
Recovery:
BatchMessage
Number
```
```
The number of grouped messages recovered
together in a single operation.
```
```
10 int
```
```
MaximumResp
onseTime
```
```
The maximum time to wait the message
response and consider the message delivered.
```
```
45000 millisec
onds
```
```
The response is the confirmation sent from the server that it has taken the
responsibility of the message. This response is sent immediately after
receiving the message, without executing the "Consume".
```
```
MaximumCons
umeTime
```
```
The maximum time to wait the consume of the
message by the business logic.
```
```
45000 millisec
onds
```
```
The business logic is expected to be very fast and asynchronous, consuming
the message accordingly. In case of the processing duration is higher than
this parameter, the message is recovered even without a failure.
```
```
MaxMessageLif
etime
```
```
The maximum time allowed to recover from a
consume failure.
```
```
10800000 millisec
onds
```
```
3 hours, after that the message is put in a failed state and never recovered
again.
```
```
MaxPersistenc
eTime
```
```
The maximun time to keep completed
messages.
```
```
86400000 millisec
onds
```
```
1 day, after that the messages consumed or sent successfully are deleted
```

# Appsettings.json Observability Parameters

This section of the appsettings.json file describes the parameters to configure Observability tracing, in particular, the provider parameter that specifies
the instrumentation configuration.

## Example

### {

```
"ModularMOM": {
"Logging": {
"CycleMinimumLogLevel": "Warning",
"LogLevel": {
"Default": "Information",
"Microsoft": "Warning",
"Siemens": "Information",
"Siemens.MOM.Model.WriteBase.Session.SessionProvider": "Debug",
"Microsoft.Hosting.Lifetime": "Information"
},
"Console": {
"UseTemplate": "false",
"Template": "[{Timestamp:HH:mm:ss.fff} {Level:t3}] [{SourceContext}:{methodName}] {Scope} {NewLine}
{Message:lj} {NewLine}{Exception}",
"LogLevel": {
"Default": "Information"
}
},
"ElasticSearchURI": "$$MODMOM_EKS_URL$$"
},
```
```
"Observability": {
"Instrumentation": {
"DefaultLevel": "Information",
"Providers": [
"Siemens.MOM.Platform.Application",
"Siemens.MOM.MetaModel.Framework",
"Siemens.MOM.Platform.Information"
],
"Siemens.MOM.Platform.Application": {
"Level": "Debug"
}
},
"ModelInstrumentation": {
"Providers": [
"Camstar.Core.SampleModel"
]
},
"3rdPartyInstrumentation": {
"Providers": [
"AspNetCore",
"HttpClient",
"SqlClient",
"GrpcClient"
],
"SqlClient": {
"SetDbStatementForText": true,
"SetDbStatementForStoredProcedure": false,
"EnableConnectionLevelAttributes": false,
"RecordException": false
},
"GrpcClient": {
"SuppressDownstreamInstrumentation": false
},
"AspNetCore": {
"EnableGrpcAspNetCoreSupport": true,
"RecordException": false
```
```
Microsoft Logging section is no longer supported.
```

### },

```
"HttpClient": {
"SetHttpFlavor": false
}
},
//"Filters": {
// "Activity": {
// "Kind": "Internal"
// }
//},
"UseExporter": "jaeger",
"Jaeger": {
"Host": "localhost",
"Port": 6831
},
"Zipkin": {
"Endpoint": "http://localhost:9422/api/v2/spans"
},
"otlp": {
"Endpoint": "http://localhost:49165"
},
"Resources": {
"Service": "MOM Module"
},
"UseSampler": "AlwaysOn",
"TraceIdRatioBased": {
"rate": 0.5
}
}
}
}
```
Parameter Description: Logging

The Logging parameters of log level can have the following allowed values:

```
Trace
Debug
Information
Warning
Error
Critical
None
```
```
Parameter Description Default
```
```
ModularMOM:Logging:
CycleMinimumLogLevel
```
```
Sets the minimum level of log in a cyclic call.
```
```
This parameter reduces the log numbers in the cyclic call (for example, health check and
message recovery).
```
```
ModularMOM:Logging:LogLevel:
Default
```
```
Sets the default level of log.
```
```
ModularMOM:Logging:LogLevel:
Source
```
```
(Optional) Sets the level of log for the classes within the specific namespace.
```
```
ModularMOM:Logging:Console:
LogLevel:Default
```
```
Sets the level to the messages sent to the Console. This level is applied only after
setting the Default and Source levels.
```
```
ModularMOM:Logging:
ElasticSearchURI
```
```
Defines the URI of the currently used ElasticSearch. $$MODMOM_EKS
_URL$$
```
Parameter Description: Instrumentation

```
Parameter Description Allowed Values Default
```
```
ModularMOM:Observability:
Instrumentation:DefaultLevel
```
```
Sets the default level of instrumentation. "Informational", "Debug",
"None"
```
```
If not present, "Informational"
```

```
ModularMOM:Observability:
Instrumentation:Providers
```
```
Defines the available source providers. Can contain the array values:
```
```
"Siemens.MOM.Platform.
Application",
```
```
"Siemens.MOM.MetaModel.
Framework",
```
```
"Siemens.MOM.Platform.
Information"
```
```
If not present, all available
sources are enabled.
```
```
ModularMOM:Observability:
Instrumentation:ProviderName:Level
```
```
Sets the level for the selected provider "Informational", “Debug”, "None"
```
```
ModularMOM:Observability:
ModelInstrumentation:Providers
```
```
Defines the available source providers for the Model
instrumentation
```
```
Can contain the array value
"Model assembly name"
```
```
If not present, all available
sources are enabled.
```
```
ModularMOM:Observability:
ModelInstrumentation:ProviderName:Level
```
```
Sets the level of the provider for the Model
instrumentation
```
```
"Informational",
“Debug”, "None"
```
```
ModularMOM:Observability:
3rdPartyInstrumentation:providers
```
```
Defines the available source providers for the third-
party instrumentation
```
```
Can contain the following array
values:
```
```
"AspNetCore",
```
```
"HttpClient",
```
```
"SqlClient",
```
```
"GrpcClient".
```
```
If not present, all supported
libraries are enabled.
```
```
ModularMOM:Observability:Resources:
Service
```
```
Defines the current service name Any non empty string
representing the service name.
```
```
If not present, the default
value is "MOM Module".
```
```
ModularMOM:Observability:
TraceIdRatioBased:Rate
```
```
Sets the ratio basis for the trace sampler in case of
use TraceIdRatioBased sampler.
```
```
0, 1
```
```
ModularMOM:Observability:UseSampler Defines the sampler to be used. AlwaysOn,
```
```
AlwaysOff,
```
```
TraceIdRatioBased
```
```
If not present, "AlwaysOn"
```
Parameter Description: 3rd-Party Instrumentation

```
Parameter Description Default Notes
```
```
AspNetCore:
EnableGrpcAspN
etCoreSupport
```
```
Adds RPC attributes to an Activity. True
```
```
AspNetCore:
RecordException
```
```
Records the exception as ActivityEvent on the span during which it occurred. False Instrumentation automatically sets
Activity Status to Error if the Http
StatusCode is >= 400.
```
```
Additionally, RecordException f
eature may be turned on, to store
the exception to the Activity itself
as ActivityEvent.
```
```
HttpClient:
SetHttpFlavor
```
```
Includes the HttpFlavor attribute in order to specify the kind of HTTP protocol used (e.g. 1.1 for
HTTP 1.1)
```
```
False The SetHttpFlavor option can
be used to include the http.
flavor attribute.
```
```
SqlClient:
SetDbStatement
ForText
```
```
Captures stored procedure commands. False If false, it prevents accidental
capture of sensitive data that
might be part of the SQL
statement text.
```
```
If true, the instrumentation will set
db.statement attribute to the
text of the SQL command being
executed.
```
```
SqlClient:
SetDbStatement
ForStoredProced
ure
```
```
Captures stored procedure command names. True
```
```
SqlClient:
EnableConnectio
nLevelAttributes
```
```
Parses the DataSource on a SqlConnection into server name, instance name, and/or port
connection-level attribute tags.
```
```
False
```

```
SqlClient:
RecordException
```
```
Instructs the instrumentation to record SqlExceptions as Activity events. False
```
```
GrpcClient:
SuppressDownst
reamInstrumenta
tion
```
```
Prevents downstream instrumentation from being invoked. Since Grpc.Net.Client is built on top of
HttpClient, when instrumentation for both libraries is enabled, it prevents the HttpClient
instrumentation from generating an additional activity and propagates context.
```
```
False
```
Setting 3rd-Party Libraries Details

In addition to the parameter description of third-party instrumentation above, you can also set the details of 3rd-party libraries as described in this example:

```
ModularMOM:Observability:3rdPartyInstrumentation "SqlClient": {
"SetDbStatementForText": true,
"SetDbStatementForStoredProcedure": false,
"EnableConnectionLevelAttributes": false,
"RecordException": false
},
```
```
ModularMOM:Observability:3rdPartyInstrumentation "GrpcClient": {
"SuppressDownstreamInstrumentation": false
},
```
```
ModularMOM:Observability:3rdPartyInstrumentation "AspNetCore": {
"EnableGrpcAspNetCoreSupport": true,
"RecordException": false
},
```
```
ModularMOM:Observability:3rdPartyInstrumentation "HttpClient": {
"SetHttpFlavor": false
}
```

# Internal Release Notes

This section will contain notes on what changes went into the Platform's internal releases starting from version 1.0.3

Also, changes going into M1_Common metadata model


# Platform Version 2.2.0

The following table lists all high-level features and bug fixes that went into this version.

They are listed in more or less the order they were committed.

```
Feature or Bug Description Work Item Link
```
```
Authorization for extension modules: MOM Permission group override in extension
MOMPermissionGroups file.
If the extension module overrides a Permission Group already available from the base module, then
such Permission Group is entirely replaced. Hence, the extension module is enabled both to allow
and to restrict permissions.
```
```
Bug 46026 [Authorization] Publish API apply
wrong merge strategy when the same
PermissionGroup is overridden
```
```
When access management database contains some inconsistencies in the configuration of some
permissions, also correct permissions are discarded. The desired behavior is that only wrong
permissions are discarded.
```
```
Bug 44420: [Access Management] Permission
inconsistencies in the model prevents other
permissions to load
```
```
Metadata Runtime - Message Consume Logic Enhancements
```
```
Consume logic:
```
- Transaction separation on each consumer (eliminates the snapshot isolation problem).
- Deferred completion of the PersistentMessage in a separate transaction.
- Parallel consumers execution on the same incoming message type.
Error Handling:
- add exception type evaluation on the consumers results (Recoverable | Unrecoverable errors)
- add a final failed state on persistent messages for consumers resulting in an unrecoverable state.
Recovery:
- Recovery (server) enhancement to skip messages in one of the final states (Consumed|Failed).
- Recovery (server) enhancement to stop recovering unconsumed messages older than a finite time
threshold (default: 3 hours), declaring them as failed afterwards.
- Delete old Messages enhancement: skip messages in the final Failed state (keeping a persistent
dead letter)
- Delete old Messages enhancement: add a configurable time threshold to delete successfully
completed messages (default: keep 1 day log).
Tests:
- Sample model extensions to support multiple consumers on the same message type and a
dedicated shopfloor service to remotely control triggers on the Consume() execution behavior.
- Add Integration Tests using the dedicated shopfloor.

```
Related work items: #45018, #45019, #45020
```
```
Story 45018 [Message Consume] Recovery
Server - Extend the RetryCount concept and
related configuration
```
```
Story 45019 [Message Consume] Enhance
Consume logic to handle a new failed state
```
```
Story 45020 [Message Consume] Delete Old
Message Cycle - Handling of failed messages
```
```
Metadata model query configuration.
Sql statements can be configured using separate momsql.sql files and using multiple lines.
The old configuration using SqlTexts section is deprecated.
Example of metadata model query configuration with separate momsql.sql files:
```
- queryName.momsql.json
- queryName.Standard.momsql.sql
- queryName.SqlServer.momsql.sql
- queryName.PostgreSQL.momsql.sql

```
Story 45099 Metadata model query configuration:
Configure sql statements in separate momsql.sql
files
```
```
Remove direct reference to Grpc.Core
```
```
Grpc library update final tests fix
```
```
Task 46670 Remove unused reference to grpc.
Core
```
```
Story 44554 Grpc library update final tests fix
```
```
Metamodel: Add support for expressing the intent to allow writing NULLs to database when native
/enum field values are not specified
```
```
Add capability in the Metamodel to express the intent of writing NULL in database when a field
value is not specified (native & enum)
When fields tagged with this intent should NOT write the type's default value into the database and
instead leave it as NULL
API Response processing should respect this new intent and should NOT return default value but
return NULL
```
```
Story 31266 Enum & Native fields to support
unspecified value in the database
```
```
Cannot delete revision through modeling api Bug 46108 Cannot delete revision through
modeling api
```
```
Fix search of MOMPermisionGroup file in subfolders Bug 45765 [Access Management]
MOMPermissionGroups.json file not found in
submodule
```
```
Authorization for extension modules: MOM Permission group resources definition for extension
modules.
MOMPermissionGroups file defines granted resources including the application name (for
```
```
Bug 46024 [Authorization] Publish API provides
inconsistent resource names for extended
modules
```

```
example: "Resource": "OrderManagement.ProductionOrderHeader").
Extension Model develop can use any placeholder for the application name in such resource
definition (in the example above "OrderManagement").
Whichever placeholder is used, the system will replace it with the deployed ApplicationName
retrieved from appsettings.json.
```
```
Log platform version and module version Bug 43063 Log platform version and module
version
```
```
Implement base classes for class-based Metamodel enums Story 45195 Implement base classes for class-
based Metamodel enums
```
```
Fix loading of Neutral Cultures Story 44600 Fix loading of Neutral Cultures
```
```
Framework support for Trackable Objects & Audit Trail
```
```
Deletion of Trackable Objects could be based on App specific system configuration
Audit Trail type of objects can NOT be deleted
```
```
Feature 46028 - Framework support for
Trackable Objects & Audit Trail
```
```
EnumField Object Type Cast Issue Bug 47350 EnumField Object Type Cast Issue
```
```
Preliminary SBOM computation added to metadata runtime Story 45914 Preliminary SBOM computation
added to metadata runtime
```
Notes

```
The Message Consumers execution has been refactored to handle better the outcome of the operations in case of specific failures. Even if no
breaking changes are expected, please verify whether the Consume logic handling changes may affect the overall behavior of your business
logic. You can find details and guide-lines here: Message Consume Handling in the Platform#UserDocumentation&Guidelines
```
Use version 2.2.0 or greater of M1_Common with this release


# Platform Version 2.1.10

The following table lists all high-level features and bug fixes that went into this version.

They are listed in more or less the order they were committed.

```
Feature or Bug Description Work Item Link
```
```
[MetadataRuntime-Authz] Authorization module is able to authorize at
maximum 10 modules
```
```
Bug 44421 [MetadataRuntime-Authz] Authorization module is able to authorize
at maximum 10 modules
```
```
Implement new CO Map capabilities for specialized use cases User Story 13779 Implement new CO Map capabilities for specialized use
cases
```
```
[LOCALIZATION] Support a new standard language - preliminary work User Story 43172 [LOCALIZATION] Support a new standard language
```
## Notes

```
Swagger UI is removed on production image releases. Swagger UI is available only when the platform is compiled in debug on development
machines.
```
Use version 2.1.15 or greater of M1_Common with this release

DO NOT USE Platform Version 2.1.9.


# Platform Version 2.1.8

The following table lists all high-level features and bug fixes that went into this version.

They are listed in more or less the order they were committed. Built with commit

```
Feature or Bug Description Work Item Link
```
```
SonarQube Finding in MetadataRuntime: 'queryOption' is null on at least
one execution path.
```
```
Bug #41417 SonarQube Finding in MetadataRuntime: 'queryOption' is null
on at least one execution path.
```
```
Upgrade pipeline templates to 1.1.4 Story #41563 Upgrade pipeline templates to 1.1.4
```
```
Publish MetadataRuntime nugets to Artifactory feed Story #42134 Publish MetadataRuntime nugets to Artifactory feed
```
```
Using platform intercept attribute ReplaceMethodFor produces invalid IL
code
```
```
Bug #41905 Using platform intercept attribute ReplaceMethodFor produces
invalid IL code
```
```
"Publish as stable" fails in digital signature step Issue #41577 "Publish as stable" fails in digital signature step
```
```
Removed variable affects condition Issue #42558 Removed variable affects condition
```
```
Handle authorization for non-standard rest APIs Story #37980 Handle authorization for non-standard rest APIs
```
```
Add API support for IObject & IObjectList fields Story #41979 Add API support for IObject & IObjectList fields
```
```
[OM] CreateWorkOrders message is not sent immediately after the
consume of the MaterialCreated message, causing a long delay until the
final WorkOrderCreated message is processed (Launching state).
```
```
#42974 [OM] CreateWorkOrders message is not sent immediately after the
consume of the MaterialCreated message, causing a long delay until the
final WorkOrderCreated message is processed (Launching state).
```
## Notes

```
Swagger UI is removed on production image releases. Swagger UI is available only when the platform is compiled in debug on development
machines.
```
CORRECTION Use version 2.1.10 or greater of M1_Common with this release

Use version 2.1.9 or greater of M1_Common with this release


# Platform Version 2.1.7

The following table lists all high-level features and bug fixes that went into this version.

They are listed in more or less the order they were committed. Built with commit 820032fc

```
Feature or Bug Description Work Item Link
```
```
Create package to install IdGenerator User Story 37924 Create package to install IdGenerator
```
```
Remove the usage of 'dynamic' from Information Gateway & ORM
Repository
```
```
User Story 37984 Remove the usage of 'dynamic' from Information Gateway &
ORM Repository
```
```
Increment platform build Bug 39822 Increment platform build
```
## Notes

Use version 2.1.6 or greater of M1_Common with this release

Build Log


# Platform Version 2.1.6

The following table lists all high-level features and bug fixes that went into this version.

They are listed in more or less the order they were committed. Built with commit 698d88fc

```
Feature or Bug Description Work Item Link
```
```
Add and Get SubEntity performance degradation with high amount of
items
```
```
Bug 37532 Add and Get SubEntity performance degradation with high amount
of items
```
```
Check Dependencies for Clearing Task 36570 Check Dependencies for Clearing
```
```
Remove garbage collection calls from shopfloorservice Bug 37818 Remove garbage collection calls from shopfloorservice
```
```
After Open event causes NullReferenceError Bug 22493 After Open event causes NullReferenceError
```
## Notes

2022-04-11 - Use version 2.1.7 instead of this release.

Use version 2.1.6 or greater of M1_Common with this release


# Platform Version 2.1.5

The following table lists all high-level features and bug fixes that went into this version.

They are listed in more or less the order they were committed.

```
Feature or Bug Description Work Item Link
```
```
Expose function evaluation in execution context User Story 37204 Expose function evaluation in execution context
```
```
Service accounts Name is not properly handled in the user context
logs and responses
```
```
Bug 36564 Service accounts Name is not properly handled in the user context
logs and responses
```
## Notes

Use version 2.1.5 or greater of M1_Common with this release


# Platform Version 2.1.4

The following table lists all high-level features and bug fixes that went into this version.

They are listed in more or less the order they were committed.

```
Feature or Bug Description Work Item Link
```
```
Make CheckProperty case insensitive Bug 37318 Make CheckProperty case insensitive
```
```
Create a template for digital-sign task US 36959 Create a template for digital-sign task
```
```
CheckAuthorization returns 403 for /api endpoints which don't exist Bug 36970 CheckAuthorization returns 403 for /api endpoints which don't exist
```
## Notes

Use version 2.1.4 of M1_Common with this release


# Platform Version 2.1.3

The following table lists all high-level features and bug fixes that went into this version.

They are listed in more or less the order they were committed.

```
Feature or Bug Description Work Item Link
```
```
Sort on Material column is not working in Material Definitions page Bug #36597 Sort on Material column is not working in Material Definitions page
```
```
[MetadataRuntime] Get endpoint for Material broken Bug #36650 [MetadataRuntime] Get endpoint for Material broken
```
```
Fix bug 35190 User Context is not validated in message Consumer
when consuming recovered messages
```
```
Bug #35190 Fix bug 35190 User Context is not validated in message
Consumer when consuming recovered messages
```
```
Add support for configuration of return format of enum fields.
```
```
Integration TCs for localization Enum - Part 2
```
```
US #32291 Add support for configuration of return format of enum fields.
```
```
Task #36141 Integration TCs for localization Enum - Part 2
```
## Notes


# Platform Version 2.1.2

The following table lists all high-level features and bug fixes that went into this version.

They are listed in more or less the order they were committed.

```
Feature or Bug Description Work Item Link
```
```
Specify the sdk docker image used to build the platform US 36360: Specify the sdk docker image used to build the platform
```
```
[.NET6] Metadataruntime: improve package references to fody
packages
```
```
US 35655: [.NET6] Metadataruntime: improve package references to fody
packages
```
```
CheckAuthorization null reference exception handling /api endpoints
which don't exist
```
```
Bug 35856: CheckAuthorization null reference exception handling /api
endpoints which don't exist
```
## Notes


# Platform Version 2.1.1

The following table lists all high-level features and bug fixes that went into this version.

They are listed in more or less the order they were committed.

```
Feature or Bug Description Work Item Link
```
```
Enable Platform Authorization Feature Task 36376: Enable Platform Authorization Feature
```
```
[GetAll] Localize enum field return value US 32291 [GetAll] Localize enum field return value
```
## Notes

## AuthZ (Authorization) applied to any requests

In this release the Authorization feature is officially enabled: it requires a proper admin user and all the relevant permissions assigned to any users logged
in the system.

Rest APIs: the Platform will reply with HTTP status code 403 (Forbidden) in case the user is not authorized to access a specific resource, even when the
token provided is valid. Back-end API calls and UI depends on the availability and the configuration of the Access Management module to complete
successfully, please follow instructions to configure the Open Shift environments here: How to initialize Access Management.

Limitations: for development and debugging, the authorization is still disabled. Please make sure to use a local Metadata Runtime instance compiled in
'Debug' to avoid messaging exchange between your module and the remote deployed AM: those messages are internal gRPC and they cannot cross the
network boundary between remote workstations and the running deployments for security reasons.

In case you need to Debug with the authorization feature enabled, please define the environment variable
"MODMOM_ENABLE_AUTHORIZATION_DEBUG" = "true" and run a local copy of AM properly configured for your local environment, overriding the
"usermanagement" destination with [http://localhost:<port>,](http://localhost:<port>,) where "port" is the running local instance.


# Platform Version 2.1.0

The following table lists all high-level features and bug fixes that went into this version.

They are listed in more or less the order they were committed.

```
Feature or Bug Description Work Item Link
```
```
US #29718 Update build pipelines to use DotNet Version variable group Update build pipelines to use DotNet Version variable group
```
```
Us #34514 Capability to define custom fields on a Revision Base type just like
any other Configurable Type
```
```
Capability to define custom fields on a Revision Base type just like
any other Configurable Type
```
```
Bug #35793 Metadata Runtime: Fix list item remove API handler code Metadata Runtime: Fix list item remove API handler code
```
```
Bug #35890 Loopback message to Publish the Securables for AM is failing
during startup in OS/K8s environments
```
```
Loopback message to Publish the Securables for AM is failing during
startup in OS/K8s environments
```
```
US# 32291 Add support for configuration of return format of enum fields. Add support for configuration of return format of enum fields.
```
```
Bug #36113 Platform modelsettings broken for running with SampleModel Platform modelsettings broken for running with SampleModel
```
## Notes

Wiki on extending Revision Base Types

Use with Version 2.1.0 of M1_Common


# Platform Version 2.0.0

This is the first 2.x version of the MetatdataRuntime based on Asp.Net Core 6.0

You must also upgrade all M1_Common references to version 2.0.0

To build this version of the platform, you must have the Netcore 6.0 SDK installed in your development environment.

https://dotnet.microsoft.com/en-us/download/dotnet/6.0

The following table lists all high-level features and bug fixes that went into this version.

They are listed in more or less the order they were committed.

```
Feature or Bug Description Work Item Link
```
```
Bug #31227 Problem in analysis/parsing of raw log entries in Sumologic due to
formatting
```
```
Problem in analysis/parsing of raw log entries in Sumologic due to
formatting
```
```
Task #33964 Return list count in response to always have value irrespective of
list pagination options in the input...
```
```
Return list count in response to always have value irrespective of list
pagination options in the input...
```
```
Bug #35189 [MetadataRuntime] NamedSubentity synchronization fails [MetadataRuntime] NamedSubentity synchronization fails
```
```
Task #35187 Reproduce NamedSubentity bug on sample model Reproduce NamedSubentity bug on sample model
```
```
US #29718 Update build pipelines to use DotNet Version variable group Update build pipelines to use DotNet Version variable group
```
```
Bug #35588 [MetadataRuntime-Authz] InquiryService that contains 'Get' in the
url is not authorized
```
```
[MetadataRuntime-Authz] InquiryService that contains 'Get' in the url
is not authorized
```
```
US #32383 Metadataruntime NET6 upgrade Metadataruntime NET6 upgrade
```
```
Task #33368 Update ModularMOM repository Update ModularMOM repository
```
```
US #32383 Fix nuget package version and references Fix nuget package version and references
```
```
Bug #35599 Selection values fails when IObjectRef result is empty Selection values fails when IObjectRef result is empty
```
## Notes


# Platform Version 1.0.14

The following table lists all high-level features and bug fixes that went into this version.

They are listed in more or less the order they were committed.

Use version v1.0.8 of M1_Common with this platform version.

```
Feature or Bug Description Work Item Link
```
```
Bug #31228 Confusing fields in the logs from integration layer/missing logs
from grpc library
```
```
Confusing fields in the logs from integration layer/missing logs from
grpc library
```
```
Bug #33703 Fix 5 Sonarqube bugs: 'mpa' is null on at least one execution path. Fix 5 Sonarqube bugs: 'mpa' is null on at least one execution path.
```
```
Task #33706 Add a mechanism to request selection values drilling down into
sub-entity type fields when no input/key is passed in
```
```
Add a mechanism to request selection values drilling down into sub-
entity type fields when no input/key is passed in
```
```
Task #33706 Fix response processing when selection values requested
without input/key
```
```
Fix response processing when selection values requested without
input/key
```
```
#33859 [MetadataRuntime] Publish API treats Inquiry endpoints as Service
endpoints
```
```
[MetadataRuntime] Publish API treats Inquiry endpoints as Service
endpoints
```
```
#33963 Services that implement MessageConsumer can only be called via
grpc interface.
```
```
Services that implement MessageConsumer can only be called via
grpc interface.
```
```
Task #33964 Return list count in response to always have value irrespective of
list pagination options in the input...
```
```
Return list count in response to always have value irrespective of list
pagination options in the input...
```
```
#33963 Move POC services to dev Move POC services to dev
```
```
#34037 AM:GetUserAssignedPermissionGroups inquiry service should be
called by any authenticated users
```
```
AM:GetUserAssignedPermissionGroups inquiry service should be
called by any authenticated users
```
```
#33599 Multiple Concurrent Collection generates concurrency issues on
database and retries are not handled
```
```
Multiple Concurrent Collection generates concurrency issues on
database and retries are not handled
```
```
#33985 Model generator problem with BooleanField Model generator problem with BooleanField
```
## Notes

## appsettings.Debug.json

The file "appsettings.Debug.json" is now part of the official release. It is used to enable the debug level logs on pods that are deployed with the information
level by default, setting the environment variable on demand: ASPNETCORE_ENVIRONMENT = Debug.

Please do not put in this file any local environment settings such as the Network Services endpoints.

In case you need to use these overrides to debug, change a local copy of "appsettings.Debug.json" in the bin folder (not the source one) or use local
environment variables in the form:

ModularMOM__Network__Services__<name_of_the_service>="http://localhost:<port>"


# Platform Version 1.0.13

### 2022-02-16

The following table lists all high-level features and bug fixes that went into this version.

They are listed in more or less the order they were committed.

```
Work Item Link
```
```
US #32820 Add new API endpoint for requesting metadata information of the
configurable types
```
```
Add new API endpoint for requesting metadata information of the
configurable types
```
```
US #32290 Need ability to request static selection values without any
references to instances (request static meta information where input is empty)
```
```
Need ability to request static selection values without any references to
instances (request static meta information where input is empty)
```
```
Task # 33502 Update implementation to handle empty 'entityKey' sections or
the absence of it for 'RequestDataUsingKey' route on modeling objects
```
```
Update implementation to handle empty 'entityKey' sections or the
absence of it for 'RequestDataUsingKey' route on modeling objects
```
```
Bug #33498 Fix Postgres DateTimeOffset column compatibility Fix Postgres DateTimeOffset column compatibility
```
```
Bug # 33613 UserManagement Service returns error 500 when calling GET on
/api/MOMUser/User1
```
```
UserManagement Service returns error 500 when calling GET on /api
/MOMUser/User1
```
Note: A new version of M1_Common v1.0.8 was also created and is compatible with this platform version.


# Platform Version 1.0.12

### 2022-02-14

The following table lists all high-level features and bug fixes that went into this version.

They are listed in more or less the order they were committed.

```
Feature or Bug Description Work Item Link
```
```
Bug #31226 Creation of 1.0.12 Prerelease adds unnecessary build artifacts Creation of 1.0.12 Prerelease adds unnecessary build artifacts
```
```
Bug #31348 Database dialect not correctly selected when DbKind not
specified in integration tests.
```
```
Database dialect not correctly selected when DbKind not specified in
integration tests.
```
```
US #31539 Add platform support for persistent list fields of heterogeneous
types
```
```
Add platform support for persistent list fields of heterogeneous types
```
```
US # 31538 Implement lazy loading of object/ref lists in the Metadata engine Implement lazy loading of object/ref lists in the Metadata engine
```
```
Bug #27934 InstanceId when used in query returns Field is not persistent InstanceId when used in query returns Field is not persistent
```
```
US #28596 Enhance Publish API to expose Permission Groups Enhance Publish API to expose Permission Groups
```
```
US #32465 Remove the capability to request Selection Values using the
RequestData options
```
```
Remove the capability to request Selection Values using the
RequestData options
```
```
US #32289 Selection Values request on a list field returns SV for every item
of the list...should be changed to return only once per field
```
```
Selection Values request on a list field returns SV for every item of the
list...should be changed to return only once per field
```
```
Bug #32411 MetadataRuntime: PersistentMessages are never consumed MetadataRuntime: PersistentMessages are never consumed
```
```
Task #32225 Add DateTimeOffset support for PostgreSQL Add DateTimeOffset support for PostgreSQL
```
```
Us # 28596 Enhance Publish API to expose Permission Groups Enhance Publish API to expose Permission Groups
```
```
Bug #32922 [MetadataRuntime] When Publish API is consumed by security
model, permission cache is updated only after cycle time is expired
```
```
[MetadataRuntime] When Publish API is consumed by security model,
permission cache is updated only after cycle time is expired
```
```
Bug #32683 Support for wildcards doesn't work in Authorization Middleware Support for wildcards doesn't work in Authorization Middleware
```

# Platform Version 1.0.11

### 2022-01-26

The following table lists all high-level features and bug fixes that went into this version.

They are listed in more or less the order they were committed.

```
Feature or Bug Description Work Item Link
```
```
Bug #29799 Clean up build files committed by stable version build Clean up build files committed by stable version build
```
```
Bug #30076 Module pipeline not working with platform version 1.0.10 Bug 30076: Module pipeline not working with platform version 1.0.10
```

# Platform Version 1.0.10

The following table lists all high-level features and bug fixes that went into this version.

They are listed in more or less the order they were committed.

Use M1_Common version 1.0.7 (or greater) with this version of the platform

```
Feature or Bug Description Work Item Link
```
```
User Story #27838 Expose Platform metrics on different port than REST APIs. Expose Platform metrics on different port than REST APIs.
```
```
User Story #26847 Automatic TFS-build tags for modules Automatic TFS-build tags for modules
```
```
Task #27411 Integrated Mom Exception fix Integrated Mom Exception fix
```
```
User Story #26304 [Auth-z] Publish all the module resources on startup of
module
```
```
[Auth-z] Publish all the module resources on startup of module
```
```
Feature #25227 Query and Sync permissions between Apps and User
Management service
```
```
Query and Sync permissions between Apps and User Management
service
```
```
Feature #25225 Metadata Runtime: Implement Authorization Module Metadata Runtime: Implement Authorization Module
```
```
Feature #25224 Metadata Runtime: Implement Authorization Middleware Metadata Runtime: Implement Authorization Middleware
```
```
Bug #28812 Business models failing with latest FieldInitializer changes Business models failing with latest FieldInitializer changes
```
```
Task #28956 SonarQube Fixes SonarQube Fixes
```
```
User Story #26627 Enable SubentityMaintenance Enable SubentityMaintenance
```
```
User Story #28921 Add a controller endpoint to expose information about
Model and Platform
```
```
Add a controller endpoint to expose information about Model and
Platform
```
```
Feature #28937 Localization of Enums Localization of Enums
```
```
User Story #28939 Update the Swagger UI with Model and Platform
information
```
```
Update the Swagger UI with Model and Platform information
```
```
User Story #28021 Add support for some parallelism inside the BL Execution
Engine
```
```
Add support for some parallelism inside the BL Execution Engine
```
```
User Story #27924 Add support to retrieve unmodified values Add support to retrieve unmodified values
```
```
User Story #27923 Create new configuration option for CO Fields to persist
CVE values
```
```
Create new configuration option for CO Fields to persist CVE values
```
```
Bug #23671 Could not use SortingFields from revisionedBase obj Could not use SortingFields from revisionedBase obj
```
```
Bug #27919 Retry and Recovery Management does not work correctly on OS
with OrderManagement->MaterialManagement (missing deadline in
communication channel).
```
```
Retry and Recovery Management does not work correctly on OS with
OrderManagement->MaterialManagement (missing deadline in
communication channel).
```
```
Bug #29211 Fix the Sonar Issues in MetadataRuntime Fix the Sonar Issues in MetadataRuntime
```
```
Bug #27606 COTypeId Length should be checked by ModuleWeaver COTypeId Length should be checked by ModuleWeaver
```
```
Task #26323 SecurablesMessagePublishService: unit test and integration test SecurablesMessagePublishService: unit test and integration test
```
```
Bug #29771 Consume Message on a faulty consumer service does not trace
any useful failure information
```
```
Consume Message on a faulty consumer service does not trace any
useful failure information
```
```
Bug #26006 Advanced Query configuration for Enum must respect PersistAs
Attribute value
```
```
Advanced Query configuration for Enum must respect PersistAs
Attribute value
```

# Platform Version 1.0.9

The following table lists all high-level features and bug fixes that went into this version

Use M1_Common version 1.0.7 (or greater) with this version of the platform

This release is 100% functionally equivalent to 1.0.8 with reduced logging and tracing.

Some NFR testing was impacted by the high volume of logging and tracing events and this update corrects this.

```
Feature or Bug Description Work Item Link
```
```
#27024 27024 - While executing NFR test on OS 18 for Importing 2000 material getting Time out exception
```
Note:

An additional effort was made to update the following modules to platform version 1.0.9 and the details are found here.

```
M1_Factory
M1_TrackAndTrace
M1_MaterialManagement
M1_OrderManagement
M1_UserManagement
```

# Platform Version 1.0.8

The following table lists all high-level features and bug fixes that went into this version

Use M1_Common version 1.0.4 (or greater) with this version of the platform

Use M1_Common version 1.0.7 (or greater) with this version of the platform

Also see previous platform release notes for Platform Version 1.0.7

```
Feature or Bug
Description
```
```
Work Item Link
```
```
Fixed bug #27706 27706 - PersistentMessagePerConsumer cannot store message bodies > 8K in size
```
```
Changes for Material
Management import NFR
```
```
Lowered default tracing / logging setting which are causing NFR tests to fail.
```
```
On local laptop and logging / tracing set to lowest levels, 2000 materials import in 10-20 seconds. With moderate
logging enabled, the import takes ~2 minutes.
```
```
More investigation is needed but the changes will most likely allow the NFR to pass for material import. If not, we need
to look into the memory and CPU provisioned to these pods.
```

# Platform Version 1.0.7

The following table lists all high-level features and bug fixes that went into this version

```
Feature or Bug Description Work Item Link
```
```
Bug 24689: Ensure the api message returned does not contain
irrelevant fields.
```
```
24689 - Ensure the api message returned does not contain irrelevant fields
```
```
MOMExceptionBase and User Configurable HTTP Status Code
#27006, #27007
```
```
27006 - Add method for manual HTTPStatusCode feature (Authentication)
```
```
27007 - Make MOMUserException and MOMException depend on common base
class
```
```
User Story 26802 26802 - Additional platform apis for subentity maint and compound services.
```
```
Bug 27040 27040 - Integration tests should use COTypeId from model - not call IdGenerator.
GenerateCOTypeId()
```
```
Misc Set ObjectRef when lazy loading
```
```
Fix Message token substitutions
```
Use M1_Common version 1.0.4 (or greater) with this version of the platform

Also to ensure a module does not depend on the platform version defined by M1_Common, add the following XML nodes to the <model>/dependencies.
props

Where <PlatformVersion> is the version of the platform required by your module.

```
<PropertyGroup>
<PlatformVersion>1.0.7</PlatformVersion>
<ObservabilityVersion>1.0.0</ObservabilityVersion>
</PropertyGroup>
```
Example of a module using Platform Version 1.0.7

Breaking Changes in ( M1_MaterialManagement , M1_InspectionPlanningShopfloor, M1_OrderManagement, M1_TrackAndTrace) for 1.0.7

The Exception framework was refactored and the ThrowException() method has been removed.

Wherever MOMUserException.ThrowException(...) is used as below

```
MOMUserException.ThrowException(...)
```

That code must be refactored to a throw new MOMUserException(...) clause as shown.

```
throw new MOMUserException(...)
```
Below is an example of the change.

The following Modules / classes are affected

M1_InspectionPlanningShopfloor\Code\Service\CharacteristicGroupService.cs(20): MOMException.ThrowException(100, "Can't delete Characteristic
Group '{0}' because of linked Characteristic(s) '{1}'.", new string[] { ObjectToChange.Name, string.Join("', '", characteristics) });
M1_InspectionPlanningShopfloor\Code\Service\CharacteristicService.cs(45): MOMException.ThrowException(200, "Can't {0} Characteristic '{1}' because
of mssing mandatory field(s) '{2}'.", new string[] { objectAction.ToString().TrimEnd('d').ToLower(), ObjectToChange.CharacteristicName, string.Join("', '",
invalidFields.Select(x => x.FieldName)) });
M1_InspectionPlanningShopfloor\Code\Service\OperationService.cs(30): MOMException.ThrowException(200, "Can't {0} Operation '{1}' because of
mssing mandatory field(s) '{2}'.", new string[] { objectAction.ToString().Trim('d').ToLower(), ObjectToChange.OperationName, string.Join("', '", invalidFields.
Select(x => x.FieldName)) });
M1_MaterialManagement\Code\Service\ProcessB2MMLMaterialObject.cs(46): MoMUserException.ThrowException(100, "Error caught while reading xml
string in XMLHelper", "");
M1_MaterialManagement\Code\Service\ProcessB2MMLMaterialObject.cs(79): MoMUserException.ThrowException(200, "Material definition name can
not be null or empty", "");
M1_MaterialManagement\Code\Service\ProcessB2MMLMaterialObject.cs(82): MoMUserException.ThrowException(200, "Material definition revision can
not be null or empty", "");
M1_MaterialManagement_Test\Code\Service\ProcessB2MMLMaterialObject.cs(46): MoMUserException.ThrowException(100, "Error caught while
reading xml string in XMLHelper", "");
M1_MaterialManagement_Test\Code\Service\ProcessB2MMLMaterialObject.cs(77): MoMUserException.ThrowException(200, "Material definition name
can not be null or empty", "");
M1_MaterialManagement_Test\Code\Service\ProcessB2MMLMaterialObject.cs(80): MoMUserException.ThrowException(200, "Material definition revision
can not be null or empty", "");
M1_OrderManagement\Code\Service\ChangePOPriorityService.cs(50): MOMUserException.ThrowException(errKey, 400, ProductionOrder.Name,
ProductionOrder.Status.ToString());
M1_OrderManagement\Code\Service\ChangePOPriorityService.cs(57): MOMUserException.ThrowException(errKey, 401, ProductionOrder.Name,
OldPriority.ToString(), ProductionOrder.ProductionOrderPriority.ToString());
M1_OrderManagement\Code\Service\ChangePOPriorityService.cs(64): MOMUserException.ThrowException(errKey, 402, NewPriority.ToString());
M1_OrderManagement\Code\Service\ProductionOrderQuantityUpdateService.cs(48): MOMUserException.ThrowException($"Prouction Order
'{ProductionOrderName}' was not found.", 1);
M1_OrderManagement\Code\Service\ProductionOrderQuantityUpdateService.cs(53): MOMUserException.ThrowException($"Production Order
‘{ProductionOrderName}’ has the state ‘{productionOrder.Status}’. This state is not supported.", 2);
M1_OrderManagement\Code\Service\ProductionOrderQuantityUpdateService.cs(58): MOMUserException.ThrowException($"The Quantity cannot be set
because it was changed in the meantime. Please refresh the screen.", 3);
M1_OrderManagement\Code\Service\ProductionOrderQuantityUpdateService.cs(63): MOMUserException.ThrowException($"The new Quantity must be
less than the old value '{OldQuantity}', but greater than zero.", 4);
M1_OrderManagement\Code\Service\Common\ProductionOrderMapper.cs(54): MOMUserException.ThrowException(errorKey, 300);
M1_OrderManagement\Code\Service\Common\ProductionOrderMapper.cs(113): MOMUserException.ThrowException($"Operation Name is mandatory
information", 300);
M1_OrderManagement\Code\Service\Common\ProductionOrderMapper.cs(129): MOMUserException.ThrowException($"Operation Name '{name}'
occurred more than once.", 300);
M1_OrderManagement\Code\Service\Common\ProductionOrderMapper.cs(175): MOMUserException.ThrowException($"Material Name is mandatory
information", 300);
M1_TrackAndTrace\Code\Service\Collect.cs(58): MoMUserException.ThrowException(errCode, errMessage);
M1_TrackAndTrace\Code\Service\Collect.cs(65): MoMUserException.ThrowException(errCode, errMessage);
M1_TrackAndTrace\Code\Service\Collect.cs(72): MoMUserException.ThrowException(errCode, errMessage);

For more information see the Exception Framework documentation.


# Platform Version 1.0.6

The following table lists all high-level features and bug fixes that went into this version

Use M1_Common version 1.0.4 (or greater) with this version of the platform

```
Feature or Bug Description Work Item Link
```
```
System fields are not coming due some some recent changes
of audit trail model
```
```
23577 - System fields are not coming due some some recent changes of audit trail
model
```
```
Add standard tfs tags to builds USER STORY 26724 - Add standard tfs tags to build runs
```
```
Integrate Exception framework into platform USER STORY 24069 - Integrate Exception Framework into the Platform and
accessory Modules codebase
```
```
Launch Production Order USER STORY 24548 - Launch Production Order
```

# Platform Version 1.0.5

The following table lists all high-level features and bug fixes that went into this version on 2021-Nov-22

Use M1_Common version 1.0.4 (or greater) with this version of the platform

```
Feature or Bug Description Work Item Link
```
```
RevisionBase COTypeId should be configurable for all its subtypes so that the COTypeId is
unique across all RevisionBase subtypes
```
```
26296 - RevisionBase COTypeId should be configurable
```
```
Add IModelingData interface to tag CO types that should get Modeling style CRUD API 26627 - Add ModelingObject interface to enable
SubentityMaintenance
```
```
Object References in Subentities having non persistent parent were not generated properly. 26705 - Object Reference Field in Subentity does not
generate write models properly
```
```
Persistent Messages were not being saved if their message body exceeded the default byte
[] length of 8000
```
```
26626 - 10 POs cannot be Imported
```
```
Exception Framework changes - See Documentation Exception Framework
```

# Platform Version 1.0.4

The following table lists all high-level features and bug fixes that went into this version 1.0.4 since the version 1.0.3

```
Feature or Bug Description Work Item Link
```
```
Add support for Compound Services in the Metadata Engine
Runtime
```
```
26420 - Metadata Runtime: Add Support from Compound Service in the Metadata
Engine
```

# Platform Version 1.0.3

The following table lists all high-level features and bug fixes that went into this version 1.0.3 since the version 1.0.2 (Oct 17).

```
Feature or Bug Description Work Item Link
```
```
Fix more memory issues associated with CSharp script execution 23255 - Memory issue in pod
```
```
Add support for Object type of fields that are not Subentities 24640 - Support for Object Field
```
```
Prevent duplicates being added to the Object Ref lists... 24415 - Adding duplicate entry to list used to throw, but now just adds the
duplicate
```
```
Trigger field validations on all dirty objects of a service 24421 - When using a ShopFloorService, Field validations are not
evaluated on objects created within the service
```
```
Fix triggering of OnRemoveItem event on Subentity list fields... 23325 - OnRemoveItem issue
```
```
Separate User & System Required validations and perform System
required check only after service logic execution
```
```
24381 - 'UserInputRequired' doesn't work for non-nullable input
parameters
```
```
Implement support for SubentityOf<> to support non-persistent parent types 23623 - Persistent SubEntity cannot reference a non-persistent Parent
base class (BaseObject)
```
```
Add support for generic Field types, ex: Fields for types like
NamedObjectGroup<WorkCenter>
```
```
23311 - Generic field definition causes model generation errors
```
```
Check of the presence of [Flags] attribute on enum definitions when
generating write models.
```
```
25380 - Enum flags not properly generated in write models
```
```
Implement support for 'Query' mode on Field Selection Values
```
```
Wiki: Selection Values
```
```
25146 - Extend Selection Values to support 'Query' mode
```
```
Enhance Metamodel framework to add new capability to configure read
/write accessibility of CO Fields.
```
```
Wiki link: Accessibility Configuration of CO Fields
```
```
25590 - Metamodel: Add capability to enable configuring read/write
accessibility of CO Fields
```
```
Fix for Dynamic list configuration not being propagated from non-persistent
superclass to its persistent subclasses
```
```
23621 - Dynamic list configuration is not propagated from non-persistent
superclass
```
```
Rename Modelling to Modeling (remove extra "L") Bug 26114: Refactor: Change spelling of Modelling to Modeling in platform
```
```
Rename *.properties files to *.props 26115 - Rename properties files to props
```
```
Fix resolution of Revisioned Object Ref fields when retrieving Revisioned
object by name & using ROR instead of a revision
```
```
26301 - Metadata Runtime: Referencing Revisioned object by name only
&amp; using ROR is failing to resolve reference
```
```
Refactor localizable MOMException and MOMUserException to the
framework with uniform required signatures
```
```
21280 -Metadata Runtime: Refactor errors to use Exception Framework
```

# Platform Version 2.2.1-prerelease

The following table lists all high-level features and bug fixes that went into this version.

They are listed in more or less the order they were committed.

```
Feature or Bug Description Work Item Link
```
## Notes

Use version 2.2.0 or greater of M1_Common with this release


# SaaS Architecture

## Summary

The pages bellow will detail our SaaS architecture, the deployment model and all the architectural requirements


# Failure points and Recover Strategy

## Picture

## Details

General Approach:

Log in SUMO Logic the HTTP Response, add retry mechanism. à Alert

Key Decision 1: in case of a failure, we store the message in a separate queue\database so that a Developer\Support Team can take the massage and re-
queue it after the environment is fixed.
We delete the message in SQS but we don’t communicate to ARM, so, for ARM provisioning is still in progress.

Key Decision 2: Developer\Support Team should be able, if possible, to revert any step.

à we keep a provision status in database, so we can recover from it

1.SAM Communication: Check with PL PaaS why SAM\SAMAuth are not properly answering.

2.ARM Communication: Check if Onboard Manager is running. Message will be enqueue again (bye KeyDecision1).

```
We want 3 replicas of On-Board Manager Service in production for July.
```
3.Database Communication: Check the error and depending on it verify the status of RDS.

4.Operator not responding properly: Check if Operator is running, contacts Central DevOPS Team.

5.Failure in the Operator Logic: Contacts Central DevOPS Team

6.Failure in HelmChart installation: Contacts ModMOM DevOPS Team

```
For 4-5-6, for 2204, we just delete tenant and re-invoke the operator. So, we consider those 3 failures point as a single 1.
```
7.Failure in new user provisioning (so either failure in keycloak or AM communication): Check if both services are running on the tenant scenario, and if
they are, contact ModMOM Dev Team
à Using Postman collection, Add User In keycloack and in AM.

8.Notifier Communication: Check with PL PaaS why Notifier is not properly answering
à Ask PRM to send email? (save an email template and share it)

9.SQS Queue not provisioned: Contacts Central DevOPS Team

10.Database not provisioned: Contacts Central DevOPS Team

11.Failure in Reply ARM: Check with PL PaaS why ARM is not properly answering

## Statuses

In case we re-push the message, we use statuses in the on-board manager to avoid replicating items already done:

A: SAM provisioned and data stored on it. (we cannot split those 2 things becasue they are related, we store in the database what SAM is answering us)


B: after Operator replies us the that the tenant has been successfully provisioned, when we store the tenant information.

C: after we create the "Admin" user

Proposed failure scenario


### 1.

### 2.

### 3.

### 4.

### 5.

### 6.

```
i.
```
### 7.

# Policy configuration in SAM

```
Log in to SAM console using SAM Console.
Select the relevant account from the drop down.
Create a non admin user in Sam console.
Create a user group and add the user in user group.
Create a SAM policy and update below JSON section.
```
### {

```
"version": "2017-05-11",
"rules": [
{
"effect": "Permit",
"actions": [
"samauth:registerApp",
"samauth:unregisterApp",
"samauth:listApps",
"samauth:getAppInfo",
"samauth:updateApp",
"samauth:submitAppForApproval",
"samauth:registerTemplate",
"samauth:unregisterTemplate",
"samauth:updateTemplate",
"samauth:addTemplateScopes",
"samauth:submitTemplateForApproval",
"samauth:listTemplates",
"samauth:createClient",
"samauth:registerAppFromTemplate",
"samauth:deleteClient",
"samauth:getClientInfo",
"samauth:listClients"
],
"resources": [
"urn:siemens:samauth::template::*",
"urn:siemens:samauth::app::*",
"*"
]
}
]
}
```
```
Create a ARM policies and update below JSON section
```
### {

```
"version": "2017-05-11",
"rules": [
{
"effect": "Permit",
"actions": [
"arm:registerProduct",
"arm:unregisterProduct",
"arm:updateProduct",
"arm:listProducts",
"arm:addProductTier",
"arm:removeProductTier",
"arm:updateProductTier",
"arm:listProductTiers",
"arm:provisionProduct",
"arm:provisionProductUser",
"arm:unprovisionProductUser",
"arm:unprovisionProduct",
"arm:listProvisionedProducts"
],
"resources": [
"urn:siemens:arm::product::*"
]
}
]
}
```
```
Refer below section to create policy
```

Policies

Permissions - Delivery

1. Register Product
{
"policyContent": {
"rules": [
{
"effect": "Permit",
"actions": [
"arm:registerProduct"
],
"resources": ["urn:siemens:arm::product::*"]
}
]
}
}

--------------

2. Add Product Tier
{
"policyContent": {
"rules": [
{
"effect": "Permit",
"actions": [
"arm:addProductTier"
],
"resources": ["urn:siemens:arm::product::*"]
}
]
}
}

--------------

3. Register a new Template

{
"policyContent": {
"rules": [
{
"effect": "Permit",
"actions": [
"samauth:registerTemplate"
],
"resources": ["urn:siemens:samauth::template::*"]
}
]
}
}

--------------

4. Add Template Scopes

{
"policyContent": {
"rules": [
{
"effect": "Permit",
"actions": [
"samauth:addTemplateScopes"
],
"resources": ["*"]
}
]
}
}

--------------

5. Submit Template for Approval


### {

"policyContent": {
"rules": [
{
"effect": "Permit",
"actions": [
"samauth:submitTemplateForApproval"
],
"resources": ["*"]
}
]
}
}

--------------------------------------------------------------------------------------------------

Permissions - Internal

Sam Auth

1. RegisterApp from Template

{
"policyContent": {
"rules": [
{
"effect": "Permit",
"actions": [
"samauth:registerAppFromTemplate"
],
"resources": ["urn:siemens:samauth::template::*"]
}
]
}
}

--------------

2. Unregister App

{
"policyContent": {
"rules": [
{
"effect": "Permit",
"actions": [
"samauth:unregisterApp"
],
"resources": ["*"]
}
]
}
}

--------------

3. Create Client

{
"policyContent": {
"rules": [
{
"effect": "Permit",
"actions": [
"samauth:createClient"
],
"resources": ["*"]
}
]
}
}

### --------------

4. List Apps

{
"policyContent": {
"rules": [
{


"effect": "Permit",
"actions": [
"samauth:listApps"
],
"resources": ["urn:siemens:samauth::app::*"]
}
]
}
}

### --------------

### ARM

1. Provision Product

{
"policyContent": {
"rules": [
{
"effect": "Permit",
"actions": [
"arm:provisionProduct"
],
"resources": ["urn:siemens:arm::product::*"]
}
]
}
}

### --------------

2. Provision Product for User

{
"policyContent": {
"rules": [
{
"effect": "Permit",
"actions": [
"arm:provisionProductUser"
],
"resources": ["urn:siemens:arm::product::*"]
}
]
}
}

### --------------

3. List Products

{
"policyContent":
{
"rules": [
{
"effect": "Permit",
"actions": [
"arm:listProvisionedProducts"
],
"resources": ["urn:siemens:arm::product::*"]
}
]
}
}

### --------------

4. Process Product Provisioning Event

{
"policyContent": {
"rules": [
{


"effect": "Permit",
"actions": [
"arm:provisionProductUser"
],
"resources": ["urn:siemens:arm::product::*"]
}
]
}
}


# Product Definition

## Product Tier: Basic

```
Order Management
Order Management Db
Material Management
Material Management Db
Factory
Factory Db
Track and Trace
Track and Trace Db
User Management
User Management Db
Keycloak [Configured]
Ambassador Emissary Gateway
Authentication service
UI Business Module
```

### 1.

### 2.

```
a.
i.
ii.
3.
4.
a.
b.
c.
d.
i.
ii.
1.
2.
iii.
e.
```
```
f.
```
Deployment Sequence:

```
New tenant event from ARM for first time provisioning.
Deployment of SamAuth Template [Create a Client app configuration in SAMAuth]
Thru SAM endpoints,
we have to create app using template
Using that app id, create client to get client id and secret
Extract customer admin user info from event response
Invoke k8s operator
Invoke XCR tenant provisioning [Creation of tenant namespace]
Deploy RDS [where will these be?]
Deploy SUMO LOGIC collector
Deployment and configuration of Keycloak
configure service-client-(Client Credential flow)
configure and add Identity provider(samAuth Configuration)
samauth Idp details like Auth, Token, logout urls
ClientId and secretId of App registered in samAuth
Add app admin user( TBD..)
Deploy and configure Ambassador
```

### 4.

```
f.
i.
g.
```
h.
5.
6.

```
Deploy and configure AuthN service
Add ClientId and secret
Deploy OcMod apps and WebUI for apps [Creation of databases, already provided as a sidecar by ModMOM DevOps, might need
update]
Data feeding of Admin User in Access Management [SQL script]
Update route table for tenant [TBD: Strategy] [AWS Parameter store for storing tenant name to id map]
Send the event back to ARM
```

# SaaS Project Status for Limited Preview 2207

TRX22

Milestones populated from: here

General Activities

```
XCR on US region, how for Zug? - in EU from 2207 on?
XCR can also be enabled for EU
Durga - How about running SumoLogic in EU
```
```
Schmidt, Jan Identify expert from Siemens AG for factories to discuss requirements (Meeting on 23rd May)
Ensure that basic costing dashboard is available and reviewed periodically
Yogesh
```
SaaS Readiness Milestones

```
Requirement_ID Title Item Actions Lead MM Status* Evidence
```
```
1,01 Pricing
and
Packaging
```
```
Business
Model
Validation
```
```
Singh,
Abhishek
```
```
1,02 Solution
Definition and
Strategy
Review
```
```
Singh,
Abhishek
```
```
1,03 Pricing
&Packaging
Validation
```
```
Singh,
Abhishek
```
```
Teams Sharepoint
```
```
2,01 SaaS
Platform
Adoption
```
```
XCP SSO: Use
SAM Auth
Identity Broker
to Sign in
Steps
considered:
```
```
basis for
SaaS
integrati
on,
registere
d the
Applicati
on in
Samauth
List.
Procedur
e
defined
to add
Users
Automat
e to get
User in
our
Access
Manage
ment
```
```
De
Pascale,
Mauro
```
```
2,02 XCP Service-to-
Service
Authorization:
Use Security
Access
Management
(SAM) to
authorize
incoming
service API
requests
Check
with
Ramesh
```
```
2,03 Register SaaS
application
product with
Entitlement
Service
```
```
?
```
```
2,04 Require SAM
Tenancy for
ECA: Register
```
```
?
```

```
SaaS
application
product with
Account
Resource
Management
(ARM)
```
```
Included
in
Backlog
(Check
if
existing
and
which
scope) S
chmidt,
Jan
```
2,05 Adopt XCP
Provisioning for
Services:
Register
backend
domain service
definition and
resources as a
product with
ARM
Included
in
Backlog
(Check
if
existing
and
which
scope) S
chmidt,
Jan

```
?
```
2,06 Engage with
the UX Board
to register plan
and obtain a
maturity rating
(Target for
XaaS is M3)
checked
with
Julia:
Need to
be
answere
d if
complian
ce
assessm
ent is
only
within
our
product
or does
it start
even
earlier?
Is this
covered
by
XCR?

2,07 Onboard with
DISW DevOps
Tooling: GitLab
Azure
DevOps
being
used in
ModMOM

2,08 Onboard with
DISW DevOps
Tooling:
Artifactory


```
Azure
Devops
internal
feed for
develop
ment
cycle
Artifactor
y used
to share
with
other
teams
for
released
compon
ents
```
2,09 Onboard with
DISW DevOps
Tooling:
SonarQube
In use

2,1 Use Xcelerator
Cloud Runtime
(Harbor &
Kubernetes)
In use

2,11 Manage
Product
License
Clearing via
DISW Product
Clearing (PCM
+ BlackDuck)

```
Isabella
Blackduc
k
Automati
on for
Clearing
```
```
Clear
statement
about
tooling
availability
check
with
Barney (R
oncagliolo
, Isabella )
```
```
De
Pascale,
Mauro
```
2,12 Onboard to
CApS RunOps
Dashboards
and
Notifications
Jeremy
Alerts
need to
be
defined
by us
and
therefore
in
SumoLo
gic
SumoLo
gic
second
priority
as it will
not run
on prem
for
2207...
Maintaini
ng 2
Observa
bilty
stacks
is not
sufficient

```
Henage,
Jeremy
De
Pascale,
Mauro
```
```
FEATURE 41058
```
```
Integrate our product to RunOps Dashboard StatusPage
```
2,13 Onboard to
the DISW
SaaS
Experience
Platform (https:/
/cloud.sw.
siemens.com
/<appname>)

Pottigar,
Durgapra
sad
Anand
We need
a
deviation
for this
2207.
Roberts
statement
was we
need to
stick to
the URL.
2,14 Onboard to
common Log
Management

2,15 Onboard
application with
Cloud Security
Office (AWS
Accounts with
CSO)


2,16 Cloud
Environment
Audit and
Remediation

2,17 SaaS
Penetration
Testing - Web
and
Infrastructure

```
Frances
co
Russo
and
Isabella
```
```
Isabella
```
2,18 Comply with
SISW ISMS
process
requirements
and obtain
certifications

```
Isabella
```
3,01 SaaS
Business
Process
&
Systems

```
BizOps
Validation
```
```
Operatio
ns
Strategy,
Incident
Manage
ment
RunOPs,
Devops
and our
Team
O
b
s
er
v
a
bi
lit
y
of
o
u
r
a
p
pl
ic
at
ion
fo
r
in
te
rn
al
c o n s u m
```
```
pt
io
n
```
-
th
is
is
w
h
at
S u m o L o

```
gi
c
is
fo
r
c
ur
re
nt
ly
.
T
hi
s
w
o
ul
d
al
s
o
in
cl
u
d
e
d
e
```
```
Making
sure we
*know*
how our
system
is
running,
alerts,
measure
operatio
nal
throughp
ut and
efficienc
y, what
trouble
shoots
tools
will be
used by
develop
ers
J e r e m y p
```
```
ut
to
g
et
h
e
r
w
o
rk
s
h
o
p
at
E
n
d
of
w
e
e
k
of
6t
h
of
M
a
y
to
g
et
h
e
r
w
it
h
Y
o
g
e
sh
F o c u s d
```
```
ef
in
in
g
o
n
```
```
Durga,
Yogesh
```
```
Yogesh shared the Observability guidelines and requirements with the
team DONE
```
```
PagerDuty License procured for 5 team member
DONE
```
```
Incident Management RACI Chart for Modular MOM. Yogesh working with
Federico. WIP
```
```
Modular MOM Observability Kickoff meeting scheduled on 05 Jul 2022
WIP
```
```
Yogesh to start setting up observability dashboards in SumoLogic. Start
Date :- 06 Jul 2022 NOT STARTED
```
```
Yogesh to Configure users and roles in PagerDuty
NOT STARTED
```
```
Yogesh to plan Game day after setting up observability dashboards in
SumoLogic and integrating SumoLogic with PagerDuty.
NOT STARTED
```
```
DM SaaS Operations Model
```

v
el
o
p
er
u
s
a
g
e,
IE
d
e
b
u
g
gi
n
g
th
ei
r
a
p
pl
ic
at
io
n
in
pr
o
d
u
ct
io
n,
o
p
er
at
io
n
al
m
et
ri
c
s
(I
E:
al
er
ts
&
p
er
fo
r m a n c e m

et
ri
c
s)
, a n d o v

er
al
l
a
n
yt
hi
n
g
th
at
w e c a n m o

ni
to
r
/o
b
s
er
v
e
th
at
w e a s R & D w o

ul

```
al
e
rt
s
in
S u m o L o
```
```
gi
c
```
```
How
does
RunOps
receive
ModMO
M data
for their
dahsboa
rd?
SumoLo
gic?
How
does
ModMO
M Status
/availabil
ity goes
to
RunOps
dashboa
rd?
How
critical
Logs
from
pagerdut
y
comes
to
ModMO
M R&D
team.
Execute
a game
day and
see
how
Modular
MOM
reacts
```

d
fi
n
d
u
s
ef
ul
.
T
hi
s
is
a
n
ar
e
a
m
y
te
a m h a s b e e n

le
a
di
n
g,
a
n
d
pr
o
vi
di
n
g
to
ol
in
g a n d a w

ar
e n e s s o

n.
O
b
s
er
v
a
bi
lit
y
fo
r o u r c u

st
o
m
er
s
y
st
e
ms

-
T
hi
s
is
a
re
c
e
nt
e
m
er
g
e
n
c
e:

H
o
w
ar
e
w
e
pr


o
vi
di
n
g
c
er
ta
in
ty
p
e
s
of
in
si
g
ht
s
to
o
ur
c
u
st
o
m
er
s
?
T
hi
s c a n b e e v e n

br
o k e n d o w n

fu
rt
h
er
(a
n
d
w
ill
h
a
v
e
to
b
e)
b
et
w e e n w h

at
w
e
ar
e
pr
o
vi
di
n
g
o
ur
o
n-
pr
e
m
c
u
st
o
m
er
s
O
O
T
B,
a
n
d
w
h
at
w
e
ar
e


e
x
p
o
si
n
g
to
o
ur
S a a S c u

st
o
m
er
s.

T
hi
s
c
o
n
si
d
er
at
io
n h a s a n u m b

er
of
c
o
n
c
er
n
s
re
g
ar
di
n g p a c k a

gi
n
g
(
w
h
at
’ s c o m m

er
ci
al
ly
vi
a
bl
e
to
s
u
p
p
or
t
to
c
u
st
o
m
er
s)
,
a
s
w
el
l a s a w h

ol
e
lo
t
of


st
ic
k
y
is
s
u
e
s
s
ur
ro
u
n
di
n
g
d
at
a
pr
iv
a c y a n d s e c

ur
it
y w h e n w e

ta
lk
a
b
o
ut
S
a
a
S.

T
hi
s
w
o
ul
d
al
s o b e s q u

ar
el
y
a
pr
o
d
u
ct
R
&
D
pr
o
bl
e
m
,
th
o u g h s o m e

of
th
e
to
ol
s
ar
e
th
e s a m e a n d m y

te
a


```
m
g
e
n
er
al
ly
h a s b e e n h
```
el
pi
n
g
fa
ci
lit
at
e
h
er
e
d
u
e
to
d
o
m
ai
n
e
x
p
er
ie
n
c
e.
Observa
bility:
S u m o L o

```
gi
c
is
it
th
e
ri
g
ht
to
ol
?
Al
s o o n c u
```
```
st
o
m
er
/s
u
p
p
or
t
si
d e? ( M a
```
```
ur
o)
O
b
s
er
v
a
bi
lit
y
St
a c k n e e d e d
```
```
al
s
o
fo
r
```

```
th
e
c
u
st
o
m
er
pr
o
p
o
s
al
--
>
El
a
st
ic
S
e
ar
c
h
(
m
or
e e x p e n
```
```
si
v)
or
s o m e O p e n S o
```
```
ur
c
e
to
ol
(h
tt
p
s:
//i
n
vi
si
bl
.
io
/b
lo
g
/k
u
b
er
n
et
e
s-
o
b
s
er
v
a
bi
lit
y-
lo
ki
```
-
c
or
te
x-
te
m
p
o-
pr
o
m
et
h
e
u
s-
gr
af
a
n
a/
).
Business
Continua
tion,
How do


```
we
handle
disaster?
Updates,
Versioni
ng and
compatib
ility
```
3,02 Mock Quote??
Need to be
checked what it
is

3,03 BizTech
Validation
Integrati
on with
Xcelerat
or
Services
,
Dashboa
rds,
Xcelerat
or
Console

```
RunsOP
S in
TFS
already,
check
with
Item
```
3,04 Entitlement and
Licensing
Onboarding

```
Licensi
ng still
to be
done (
Check
with
Yoges
h)
Entitle
ment
done
```
```
Yogesh
```
3,05 Admin Console
Onboarding

```
Onboar
d
Manag
er
needs
to be
finishe
s for
provisi
oning
flow
```
3,06 Trial and Demo
Onboarding

Check With
Abhi and in
documents,
technically
enabled
3,07 ALRT for SaaS

```
Check with Abhi
```
4,01 Quality Defect and
enhancement
tracking

4,02 Test coverage:
CX Journey
Need to
be
checked
what
really
needs
to be
covered
for
2207?

4,03 Test coverage:
Portfolio
compatibility
4,04 Scalability
testing

4,05 Security testing

```
Security
Require
ments
```
```
Security
Requirem
ents for
2207,
Check
with
Francesco
```

```
Russo
what will
be
mandator
y for
2207 (Sch
midt, Jan )
```
5,01 Custome
r
Success

```
Customer
Support
Operatio
ns
Manage
ment on
XCR
(CST &
SLA)
Incident
Respons
e /
Custome
r
Support
/ On
Call Set
Up
B
o
u
n
d
ar
ie
s
b
et
w e e n M o d
```
```
ul
ar
M O M M O M D A T e a m , S y s O p
```
```
s,
X
C
R
,
et
c)
to
b
e
d
ef
in
e
d
in
W
or
k s h o p w
```
```
it
h
J
er
e
m
y.
W
h
o
is
re
s
p
o
n
si
bl
e
fo
r
w
h
at
?
```
```
Work w/
SysOps
team &
SRE
group to
get
onboarde
d. This
will give
at least
basic
triage
coverage,
and align
Incident
response
tooling.
Du
rga
```
-
Wil
l
put
tog
eth
er
Po
C
to
co
nta
ct
the
se
tea
ms
an
d
get
ov
erv
iew
(M
ay
6th
to
get
me
eti
ng
w/
Kis
hor
e
&
tea
m)
sh
are
d
res
po
nsi
bilit
y
mo
del
bet
we
en
Jer
em
y
an
d
XCR
Mo
dM
O
M
Te
am
def
ine
d
fro
m
Du
rga

```
Jeremy
put
together
work
shop at
End of
week of
6th of
May
```
```
Yogesh/
Jan
/Jeremy
```

```
W
or
k w / S y s O p s
```
```
te
a m & S R E
```
```
gr
o
u
p
to
g
et
o
n
b
o
ar
d
e
d.
T
hi
s
w
ill
gi
v
e
at
le
a
st
b
a
si
c
tri
a g e c o v
```
```
er
a
g
e,
a
n
d
al
ig
n
In
ci
d
e
nt
re
s p o n s e
```
```
to
ol
in
g.
Who is
the
Modular
MOM
support
team?
Girgio's
team
(MOM
Support
) and
Mike
Bosoms
Teams
(SW
Support)
```
```
together
with
Yogesh
on-call
person
knows
where
debug
data is for
ModMom,
DB,
Kubernete
s layer &
incoming
web traffic
How it
works,
how to
handle
support
issues,
timelines,
etc. They
will need
proper
onboardin
g to do
this. Pottig
ar,
Durgapras
ad Anand
```
5,02 Product
Telemetry

```
Onboarding with
Siemens PLM
Analytics is in
TFS and
planned for
```
2210. 46200
Deviation
needed.

```
Jan
```
5,03 In-App
guidance


```
5,04 Customer
Listening
```
```
5,05 Learning
```
```
5,06 Community
```
```
5,07 Content
readiness
```
```
5,08 Customer
Success
Management
```
```
5,09 CSM Analytics
```
```
5,1 Transactional
Tech Touch
Journey
```
*Status legend:

```
Completed
```
```
In Progress
```
```
Not Started
```
```
Blocked
```

# User Workflows

## Customer Onboarding

## Sub-user Onboarding


Simplified Authentication Flow



# User Journey (Simplified)


# Configuration Layer (Former Governance Layer)

## Introduction

This section will describe the architecture of the Configuration Layer, detailing the main functionalities

## Diagram

## Notes

The Platform Configuration centralizes all the information in a single place, capturing them from:

```
Application Settings: it flows in the aspnetcore service via IConfiguration.
Environment Variables: can override Application Settings
Manifest: an XML File (To be updated as JSON file in the near future), it contains the path of the models (dtos, messages and entities)
```
```
The Governance Layer have been renamed in Configuration Layer, focusing the scope in only handling the Configuration
```
```
This diagram is a draft, the current Governance Layer is not organized in this manner
```

Code

We use the DI of AspNetCore for injecting the "layer" specific configuration in each pieces it's needed:

```
//Old version
public class IntegrationGateway: IIntegrationGateway
{
```
```
private string messageModelPath;
```
```
public IntegrationGateway(<...>, IGovernanceGateway governanceGateway)
{
<...>
this.messageModelPath = governanceGateway.GatewaySpecificPaths.MessageModelPath;
}
```
### <...>

### }

```
//New version
public class IntegrationGateway: IIntegrationGateway
{
private string messageModelPath;
```
```
public IntegrationGateway(<...>, IIntegrationConfiguration integrationConfiguration)
{
<...>
this.messageModelPath = integrationConfiguration.MessageModelPath;
}
```
### <...>

### }

Manifest Configuration example

Example of configuration:

### {

```
"ModularMOM": {
"Metadata":
{
"domainmodellocation": "..\InputMetadataModel\SampleModel\bin\debug\net5.0\Camstar.Core.
SampleModel.dll",
"inputmodellocation": "..\GeneratedModels\bin\debug\net5.0\Siemens.MOM.SampleModel.Dto.dll"
```
### <...>

### }

### }

### }

Discovery Settings Structure

Example of the new portion in appsettings.Development.json for the discovery portion:

### {

```
"ModularMOM": {
"Network": {
"Services": {
"dispatch": "http://localhost:5001",
"tracentrack": "http://localhost:5002"
}
}
}
```

Not to be added in the container, because we want to leverage k8 discovery capability.


### 1.

### 2.

### 3.

### 4.

# Application Configuration and Deployment

```
Introduction
Deployment strategy
Kubernetes configuration
Appsettings.json Schema
Database Configuration
Integration with Kubernetes
Online Configuration Updates
Scenario 4:
Conclusions
References
```
## Introduction

Here we analyze a deployment strategy of an application service through the use of a configuration file in json format in a Kubernetes environment and
evaluate some options to change some value at runtime.

## Deployment strategy

A ConfigMap is an API object used to store non-confidential data in key-value pairs. Pods can consume ConfigMaps as environment variables, command-
line arguments, or as configuration files in a volume.

There are four different ways that you can use a ConfigMap to configure a container inside a Pod:

```
Inside a container command and args
Environment variables for a container
Add a file in read-only volume, for the application to read
Write code to run inside the Pod that uses the Kubernetes API to read a ConfigMap
```
The 3rd is more in line with our current configuration reading an "appsettings.json" file containing a large amount of parametric keys and values.

The 2nd may be used if the 3rd is not suitable to be deployed in different distribution of Kubernetes. However, when this option is used it is still possible to
define a number of parametric environment variables directly into a defined appsettings.json file, using the integrated reader in the Governance Gateway.
This option has the cost to have a large number of Environment Variables to be defined, and also present limitations on structured/nested configurations
with multiple sections (See "Appsettings.json schema" paragraph).

The 1st and the 4th are excluded because they imply a great deal of coding, maintainability or dependency issues.

Example flow describe for the option 3 is:

```
Once the Governance Gateway will be removed in favor of the Platform Configuration, please consider Platform Configuration whenever you
read Governance Gateway
```

Kubernetes configuration

A sample pod using the appsettings-config:

```
echo-pod.yaml
```
### ---

```
kind: Pod
apiVersion: v1
metadata:
name: pod-using-configmap
```
```
spec:
# Add the ConfigMap as a volume to the Pod
```
```
containers:
```
- name: test-container
image: busybox
command: ["/bin/sleep", "1d"]
# Mount the volume that contains the configuration data
# into your container filesystem
volumeMounts:


- name: example-configmap-volume
# `name` here must match the name
# from the volumes section of this pod
mountPath: /config

```
volumes:
# `name` here must match the name
# specified in the volume mount
```
- name: example-configmap-volume
# Populate the volume with config map data
configMap:
# `name` here must match the name
# specified in the ConfigMap's YAML
name: appsettings-config
items:
- key: appsettings.json
path: appsettings.json

ConfigMaps:

For each environment you will have a single yaml file defining the appsettings-config ConfigMap, like the following example:

```
appsettings-production.yaml
```
```
apiVersion: v1
kind: ConfigMap
metadata:
name: appsettings-config
data:
appsettings.json: |
{
"Logging": {
"LogLevel": {
"Default": "Information",
"Microsoft": "Warning",
"Microsoft.Hosting.Lifetime": "Information"
}
},
"AllowedHosts": "*",
"Kestrel": {
"EndpointDefaults": {
"Protocols": "Http1AndHttp2"
}
},
"ModularMOM": {
"ApplicationName": "TestApp",
"Network": {
"SwaggerPort": 5004,
"GrpcPort" : 5002
},
"Database": {
"DbHostName": "dbhost_production",
"DbUserName": "Trusted_Connection"
},
"Observability":
{
}
}
}
```
And another for example for development:


```
appsettings-development.yaml
```
```
apiVersion: v1
kind: ConfigMap
metadata:
name: appsettings-config
data:
appsettings.json: |
{
"Logging": {
"LogLevel": {
"Default": "Information",
"Microsoft": "Warning",
"Microsoft.Hosting.Lifetime": "Information"
}
},
"AllowedHosts": "*",
"Kestrel": {
"EndpointDefaults": {
"Protocols": "Http1AndHttp2"
}
},
"ModularMOM": {
"ApplicationName": "TestApp",
"Network": {
"SwaggerPort": 5004,
"GrpcPort" : 5002
},
"Database": {
"DbHostName": "dbhost_development",
"DbUserName": "Trusted_Connection"
},
"Observability":
{
}
}
}
```
you can apply the configuration at runtime using standard kubectl commands:

```
kubectl apply -f appsettings-<environment>.yaml
```
Appsettings.json Schema

The json application settings described is used to configure Modular MOM runtime with the following example parameters.

```
appsettings.json
```
### {

```
"Logging": {
"LogLevel": {
"Default": "Information",
"Microsoft": "Warning",
"Microsoft.Hosting.Lifetime": "Information"
}
},
"AllowedHosts": "*",
"Kestrel": {
"EndpointDefaults": {
```
```
Please consider the structure of the file and not the values that are there just to discuss the general structure of the configuration. When the
configuration model will be finalized, an official documentation of the file will be provided.
```

"Protocols": "Http1AndHttp2"
}
},
"ModularMOM": {
"ApplicationName": "SampleModelApp",
"Network": {
"SwaggerPort": 5001,
"GrpcPort": 5002
},
"Database": {
"DbName": "$$MODMOM_DBNAME$$",
"DbHostName": "$$MODMOM_DBHOSTNAME$$",
"DbUserName": "$$MODMOM_DBUSERNAME$$",
"DbPassword": "$$MODMOM_DBPASSWORD$$",
"DbKind": "$$MODMOM_DBKIND$$",
"DbIsSecure": "$$MODMOM_DBISSECURE$$"
},
"Observability": {
"Instrumentation": {
"DefaultLevel": "Information",
"Providers": [
"Siemens.MOM.Platform.Application",
"Camstar.Core.SampleModel",
"Siemens.MOM.MetaModel.Framework",
"Siemens.MOM.Platform.Information"
],
"Siemens.MOM.Platform.Application": {
"Level": "Debug"
}
},
"3rdPartyInstrumentation": {
"Providers": [
"AspNetCore",
"HttpClient",
"SqlClient",
"GrpcClient"
],
"SqlClient": {
"SetDbStatementForText": true
},
"GrpcClient": {
"SuppressDownstreamInstrumentation": false
}
},
"Filters": {
"CustomOtProcessor": {
"Batch": {
"exporter": "Jaeger",
"scheduledDelayMillis": 5000
},
"Activity": {
"Kind": "Internal"
}
}
},
"Sampler": {
"AlwaysOn": true
},
"Exporter": {
"Jaeger": {
"RemoteEndpoint": {
"Protocol": "https",
"Host": "host",
"Port": 561
},
"Options": {}
},
"OpenCollector": {
"RemoteEndpoint": {
"Protocol": "https",
"Host": "host",


```
"Port": 561
},
"Options": {}
}
},
"Logging": {},
"Resources": {
"Service": "Dispatch Module"
},
"Processors": {}
}
}
}
```
An example of the schema is this:


The Governance Gateway is able to access this configuration file and apply the configuration at runtime by query the name space of the json structure,
such as:

```
gov.GetValue<string[]>("ModularMOM:Observability:3rdPartyInstrumentation:Providers",..);
```

Database Configuration

There are several configurable parameters for the database.

DbName - Which is the database or instance name.

```
An example is: ModMomDb
```
DbHostName - This is the host or IP address of the database server. Any host format can include optional a port in the form <hostname>[:<port>]

```
Some examples are:
```
```
localhost when database server and app server are on the same host
```
```
server.company.com fully qualified domain name
```
```
server.company.com:1455 fully qualified domain name with specific port
```
```
192.168.10.100 Ip address format.
```
DbKind - This value indicates the database vendor type and configures the underlying provider and driver. There are currently 4 supported values:

```
SqlServer Microsoft SQL Server 2012 and greater.
```
```
SqlServerEdge Microsoft SQL Server Edge 2012 and greater
```
```
PostgreSQL PostgreSQL Server version 14
```
```
SQLite SQLite database both file and in memory
```
DbUserName - The database username. There are some static usernames that affect the configuration.

```
Memory_Database When the username is set to this value, the configuration DbKind is overridden and an in memory SQLite database is used.
```
```
Trusted_Connection When the username is set to this value, the connection is made using Windows Integrated Authentication.
```
```
<null> When the username is null, the configuration DbKind is overridden and an in memory SQLite database is used.
```
DbPassword - The database user password.

DbIsSecure - Boolean value to enable TLS connection to the database. ** Currently only supported with SqlServer / SqlServerEdge using Windows
Integrated Authentication. **

Integration with Kubernetes

In order to have a better configurable option, integrating with containerized application deployments, it is beneficial to take the appsettings.json file with the
full path from the command line parameters:

```
Platform.Api.exe /config/appsettings.json
```
In case this option is specified from the command line parameter, the governance gateway will be able to initialize itself using this configuration file, and if
this is not the case, it will take the appsettings.<environment>.json from the standard folder of the host project.

In the latter the environment name may be specified from an environment variable such as ASPNETCORE_ENVIRONMENT="development" before
starting the process, so that the appsettings.development.json located at the standard root project path is used.

Note on Kubernetes deployments

Because we have multi-level environment options supported with both strategies it is possible to work with just one deployed a single file into the pod,
selecting the appsettings.<environment>.json to use in the yaml configuration, and loading the <appsettings.json> single file by the command line
parameter of the process. Conversely, we can have the same level of abstraction in the general case, when we do not have kubernetes deployed images, i.
e. virtual machines with a running kestrel application service using the environment variable to select the file from the local file system folder.

In this way, using the environment in Kubernetes is just a matter of change the pod yaml definition to use the correct ConfigMap for that specific
environment and applying it to a running cluster. The target file will be updated into the pod for each instance in the cluster and the application host
process may read the new configuration from there on. Examples could be: re-apply a "appsettings.debug.json" to a running configuration at runtime to
increase the log level for every instance of the container without restarting the process.

Online Configuration Updates

During the runtime of an App hosted by a container in a Pod, there is the possibility to change the configuration online, without restarting/recreating the
Pod in the cluster with a different configuration to reload.


### 1.

### 2.

### 3.

### 4.

For changing the configuration during normal runtime operation, when the app is already started and pod are running, three main approaches are
considered:

```
Using the input configuration file, updating the associated ConfigMap
Calling an internal Rest API (available on all applications) to change the internal configuration.
Calling an external service providing the configuration state to the application.
Per-request configurable tracing levels.
```
Scenario 1:

In this scenario the important thing to notice is that appsettings.json configuration file is the same of the one configured in startup mode, it will just be
updated applying a new ConfigMap containing a new key/value pairs for ModMom sections.

The appsettings.json file is then propagated at runtime for all the pod in the replica set, like in a distributed file system the volume seen by the container
sees the updated values because it will be mounted in place of the previous one for all the containers referencing it, in this way the containers will see a
change in the configuration file simultenously.

This powerful mechanism, availabale on all k8s distribution, can be used with the standard ASP.NET Core Configuration object that opens and initializes
the configuration from the json file: inside the containers a new configuration is loaded when it changes and a notification is propagated by the framework
to the app on values change.

```
Configuration loading Code snippet
```
```
var host = new WebHostBuilder()
.UseContentRoot(Directory.GetCurrentDirectory())
.UseUrls("http://*:" + (AppConfig.Port ?? 80))
.UseKestrel()
// this will not clone the value in conf and support reload capability
.ConfigureAppConfiguration((builder, conf) =>
{
conf.SetBasePath(Directory.GetCurrentDirectory())
//Note: reloadOnChange flag
.AddJsonFile("config.json", optional: false, reloadOnChange: true)
.AddEnvironmentVariables();
})
.UseIISIntegration()
//.UseConfiguration(config)
.UseStartup<Startup>()
.Build();
```
### //[...]

```
//Values can be notified with the IOptionsMonitor pattern:
// Is used to retrieve options and manage options notifications for TOptions instances.
// Is registered as a Singleton and can be injected into any service lifetime.
// Supports:
// Change notifications
// Named options
// Reloadable configuration
```

```
// Selective options invalidation (IOptionsMonitorCache<TOptions>)
// See: https://andrewlock.net/reloading-strongly-typed-options-when-appsettings-change-in-asp-net-core-rc2/
```
```
services.Configure<MyOptions>(Configuration.GetSection("MyOptions"), trackConfigChanges: true); // registers a
configuration instance which MyOptions binds against.
// the following example uses IOptionsMonitor<TOptions>:
public class TestMonitorModel
{
private readonly IOptionsMonitor<MyOptions> _optionsDelegate;
```
```
public TestMonitorModel(IOptionsMonitor<MyOptions> optionsDelegate )
{
_optionsDelegate = optionsDelegate;
}
```
```
var value = _optionsDelegate.CurrentValue;
_optionsDelegate.OnChange(Action<MyOptions> listener ...);
```
### //[...]

When a new pod for the specific app is instanciated by the user it will get the updated value accordingly, this is important using horizontal scalability and
autoscale policies because the new instances are automatically consistent with the running ones and there is no need to synchronize them externally.

Scenario 2:

In this scenario the integrators change the online configuration calling a rest API, exposed by the Runtime Platform for all applications, in which the internal
configuration is changed in memory.

This present three major drawbacks:

```
Updating the value is somewhat difficult and less integrated with k8s cluster: the integrator has to create and run a script that is capable to detect
the internal topology of the clustered Pod instances (that is, the internal IPs of each instance) and call the rest API for each one. This breaks the
standard abstraction of a k8s cluster and require accessing/parsing the online configuration of the cluster either by parsing the cmd line output or
by using kubernetes integration APIs.
Even if the script is really tailored and well integrated into a deployment, there is no guaranties that it will work consistently on all container
instances. This could lead to consistency problems where some instances are updated and others are not that has to be managed somehow by
the integrator script.
Do not support automatic scaling: each time a new container instance is deployed the user has to update the configuration loaded, deviating from
the default values read from the json file.
```
Scenario 3:


In this scenario the integrators change the online configuration calling a rest API, exposed by an external "configuration service", in which the configuration
is kept for all the instances of a running application.

This present two major challanges:

```
More complex deployment requiring a "configuration service" capable to notify each container instance by subscription on values.
Complexity synchronizing the actual configuration values: the platform has to read the configured value from the file during start-up and then has
to applies the value change from the external online configuration service, reading the current values and subscribing to them for value changes.
```
Also:

```
Relationship with the configuration file is unclear and has to be defined: is it an "online" configuration subsection for an app in which the app itself
has to create the model for these types of keys on the remote service (by writing it) the first time the application starts? How about the next
launched instance? Is the configuration persistent across pod redefinition/restart?
```
Scenario 4:

Operations to change the configuration at runtime, without restarting services, have the limited scope to specific changes, such as observability options.

This type of options should not be strictly bound to a specific application service configuration, instead it should be scoped to the action involved in a
request. In this scenario it is more meaningful to change a specific behavior on the root call perfomed, for example, by the UI and propagate these
information to the call chain triggered across different services in the runtime deployment.

For example: the user may call a service from the UI enabling a specific level of verbositiy or filtering on the log and traces produced by its own call. This
should span across al the services in a contextual way during the propagation of the call chain.

The question about what can be changed during the online phase is then crucial to distinguish the use cases.


### 1.

### 2.

### 3.

Because in Modular MOM, at least at this stage, the online parameters that is required to be changed during online operations involve only the
observability feature, it is beneficial to extend the Platform to accomodate such a use case implementing this fine grained contextualized approach. This
has some pros and cons that has to be analyzed:

Pros:

```
The user is capable to override the default configuration for a logical action at runtime, in a end-to-end way, using the call stub itself to invoke the
serivce, without additional configuration steps. In this way the complixity is low. The fine grained (per-request) approch may also be extended in a
grouped way for all the requests a web client or a UI service may produce, using a local configuration and enabling the client to decide in which
mode to operate during runtime.
Without a global configuration requirement this approach is more streamlined because it does not imply to deploy an additional specific
configuration service; to provide high availability without introducing single-point of failures and interface each module to it. In this way the
deployment and configuration of the platform is more lightweight and simple for the end-user.
```
Cons:

```
Global online parameters may not be changed outside the domain of one call. Until a specific requirement of this type arise (that is, changing a
value online for all the services globally or for a specific service deployed) the additional complexity is evaluated as not necessary to cover only
the observability feature, that is contextual in its own nature.
The complexity is moved to the Platform side: the runtime Platform must be extended to transport an additional and optional "context" thoughout
the call chain, despite the protocol used. This has to be standardize in the protocol defition for both Rest API and gRPC message exchanges,
across backend application services and platform layers (to be always available).
This could pose some challenge about the nature of a call vs the message exchange pattern (such as the event driven asynchronous pattern)
that not always requires a specifc input from the end user to start an action, for example: a state machine could trigger a configured workflow of
tasks from an external system event (or a timer). However, this may be mitigated with configurable options during the design of the module
implementing such an use case: the definition of the business logic in a work flow may expose parameters to run with options at runtime. The
user then may change them and re-apply the new configuration online.
```
Conclusions

The scenario 4 is preferred at this stage and considered a useful communication mechanism to have inside the platform. This may be further extended in
the future for additional "contextualized" per-call information, such as authentication, user custom contexts, etc. The design should take this options into
account, defining a generic context bag without making the expected parameters exposed directly and too specifically for the observability options (given
the possible burden involved to add new ones).

The context has to be optional during the call and works as an override: the default behavior is to rely on the configuration parameters read during startup
of the service, using the standard appsettings.json file. This act as a "global" default configuration option for every call handled by a service, even if it is not
globally available for all service in the deployment. The support personnel and dev-ops may rely on this option to debug and analyze specific problem that
could arise on a specific service deployment.

A supporting Spike US is needed to analyze the flow technically inside the Platform.

References

```
Kubernetes Configuration Best Practices
Kubernetes ConfigMaps documentation
Configuration options in asp.net core 5
```

# Appsettings.json - Messaging Configuration

```
This page has been moved under the section Metadata Runtime and App Deployment Configuration , see Appsettings.json Messaging
Parameters
```

# Information Layer

## Introduction

This page define the architecture of the information Layer, so it's gateway and the below services needed to make it work.
Also the picture will show the interaction between the information gateways and other layer.
Finally, the colored areas are meant to explain when the different items will be ready on the platform.

## Diagram



# Cache Strategy


# Cache Flow Diagram


# Cache Setup

There are five main areas of configuration for Redis

```
Docker installation and configuration
Installing / Running Redis image in docker
Connecting to Redis with a 3rd party client for element inspection
Configuring NHibernate to use Redis
Running multiple api processes that share one Redis instance
```
## Docker Configuration

Most windows 10 installations have Docker Desktop installed. Find it from the Program menu and click.

If it is not running, you're presented with the following dialog.


# Implementation of Redis Distributed Cache

## Application configuration for Redis Distributed Cache

```
Attribute Name Details Example
```
```
Enable Cache This attribute is used for enabling Redis Cache "EnableCache": true
```
```
Redis Url This url is used for connection to Redis Database "RedisUrl": "127.0.0.1:6379"
```
```
Cache Expiration
Time
```
```
This attribute is used for set Cache expiration time for Redis database and
Memory Cache
```
```
"CacheExpirationTime" : 600(in seconds)
```
```
Redis Connection
Timeout
```
```
This attribute is used for setup connection timeout for Redis Database "RedisConnectionTimeout": 5000(in
miliseconds)
```
## Approach

In our platform, we can retrieve entity object by its name or by its Id. So to manage this type of request in Redis Distributed Cache two type of cache is
created for an object. One is Named cache where combination of entity type name and object Name is key and the Id is stored as value. And the second
one where combination of Entity Type name and Id is key and whole object is stored as value.

Whenever an object is requested by its name, then in cache layer we will have to make two get call to Redis server, one to fetch Id value by it's name and
then another redis call to fetch whole object value based on Id we fetched in previous call.


In Id specific cache , key will be combination of name of Entity Type and Entity Id. We store the whole entity in binary format against this key.

Name specific cache is available for Named Entity objects where key will be combination of name of Entity Type and name of the entity. In this case value
will Id specific cache's key.

Example: - We have Factory entity where Id is some alphanumeric value like 'ax567up' and Name is ' Fact1'. So our first level key will be 'Factoryax567up'
.In this case our second level key will be 'FactoryFact1.'

Working Flow

In our platform when we create or update an entity, we called method 'SetCache' from Information layer. This method take list of entities as an input
parameter. Responsibility of this method is to create redis cache and memory cache for these entities.

We are maintaining Memory Cache for each entity along with it's Redis cache. Scope of this Memory Cache is limited to instance. Memory cache is not
worked as a distributed cache among multiple instances of same application. Memory Cache has also expiration time for each entity, same like redis
cache. Whenever an object is requested by it's Id or Name, we first check in Memory Cache of respected instance. If we don't find the entity in Memory
Cache , then we check in Redis Cache for this entity.


# Redis- Lua Scripting

Redis provides a way to extend its functionality on the server side by providing support for Lua Scripting. If you are coming from a relational database
world, you already know that you can use Stored Procedures to extend functionality of your relational database. Now, you may also know that some
people do frown upon using stored procedures, I think one could also think of using scripting in Redis sort of belongs in the same category.

Why do we need Lua Scripts?

```
Lua Scripts may result in better performance
All steps within script are executed in atomic way. No other redis command can run while a script is executing.
```
In our platform, we can retrieve entity object by its name or by its Id. So to manage this type of request in Redis DistributedCache two type of cache is
created for an object. One is Named cache where combination of entity type name and object Name is key and the Id is stored as value. And the second
one where combination of Entity Type name and Id is key and whole object is stored as value.

Whenever an object is requested by its name, then in cache layer we will have to make two get call to Redis server, one to fetch Id value by it's name and
then another redis call to fetch whole object value based on Id we fetched in previous call.

Lua Script is useful in this situation, where sequence of steps to fetch data can be created as script and this script can be evaluated at once on redis
server by passing relevant parameters. Moreover this script can be loaded on server during bootstrap of Redis once and subsequent call to evaluate script
will use the cached version of script. Thus it helps in gaining performance by minimizing roundtrips to Redis server. If we need to update script, then it can
be flushed out of cache and new version can be loaded.

```
Example of Lua Script in Redis
```
```
//Lua Script string _getByNameScript = @$"local id= redis.call('get',@key)
return redis.call('get',
tostring(id))";
//Steps to load and evaluate scriptvar server = _redis.GetServer("openshift03-cg5wc-worker-fcjqt.openshift03.
swqa.tst:31471");
var db = _redis.GetDatabase(0);
var prepared = LuaScript.Prepare(_getByNameScript);
var loaded = prepared.Load(server);
var res = loaded.Evaluate(db, new { key = (RedisKey)key });
```

# Spike on NHibernate Second Level Cache

```
Concept Description
```
```
Second Level Cache We can enable second level cache using fluent hibernate configuration. This cache works on across session.
We can enable Second Level cache with the help of different cache provider. For our spike code we use
Distributed Cache provider.
```
```
Query Cache We can also enable QueryCache and it can be done on choice basis , when to cache the query depending
upon the requirement. We can pass flag for this purpose in Generic Repo.
```
```
Cache Expiration We can configure cache expire duration through fluent hibernate configuration. It can be done region specific
also.
```
```
Entity Caching We can enable caching for all entities through fluent hibernate configuration. For per entity level caching we
have to perform it on entity mapping file.
```
```
Each entity needs it's own caching
configuration
```
```
We can enable caching for all entities through fluent hibernate configuration. For per entity level caching we
have to perform it on entity mapping file.
```
```
Each query need to indicate if it
needs to be cacheable or not.
```
```
We can also enable QueryCache and it can be done on choice basis , when to cache the query depending
upon the requirement. We can pass flag for this purpose in Generic Repo.
```
```
Are there settings that need to be
exposed in the
InformationConfiguration?
```
```
No. We have to do the changes in SessionProvider class.
```
```
Where is the caching (per entity)
enabled? Is it a parameter in the
GenericRepository?
```
```
We can enable caching for all entities through fluent hibernate configuration. For per entity level caching we
have to perform it on entity mapping file.
```
```
It is not a parameter in the GenericRepository.
```
```
Does the caching need to be written
during the generation process?
```
```
Yes. If we want to enable it for specific entity, not for all the entities.
```
NHibernate L2 Cache constraints

```
Concept Description
```
```
Dependen
cy on
Session
Factory
object
```
```
In our API application on startup we create an object of Session Factory(Singleton). L2 Cache created by this Session Factory , will be
accessed by all the session created by this Session Factory. This cache will not be accessed by any other Session Factory of other
instance of same application. As a result we are going to create duplicate entry of same object in Redis Cache Server as per multiple
instances of same Application
```


# Database Schema Strategy


# Overview

The ModMom platform provides a facility for updating a the database schema for a model. The process of updating a schema will follow the logic below.


# Query Strategy


# LINQ Query Implementation Details

## Class Overview

```
Name Description Example
```
```
QueryO
ption
```
```
This class is used to provide information regarding dynamic sorting, skip and fetch.
```
```
User can provide valid column names for sorting along with sorting order information for each column. User
can also provide paging related information through Skip and Limit attribute.
```
```
QueryOption queryOption =
new QueryOption();
SortField field = new
SortField();
field.FieldName = "Name";
field.
Direction=SortDirection.
Descending;
queryOption.SortFields.Add
(field);
queryOption.Skip = 0;
queryOption.Limit = 2;
```
## Method Overview

```
Name Description
```
```
Query This method is provided by Generic Repo. We can pass the QueryOption object to this method as a parameter. Using this object's
value we perform dynamic ordering and paging.
```
## Example

Example 1:-

QueryOption queryOption = new QueryOption();
SortField field = new SortField();
field.FieldName = "Name";
field.Direction=SortDirection.Descending;
queryOption.SortFields.Add(field);
queryOption.Skip = 0;
queryOption.Limit = 2;

informationGateway.QueryAll(typeof(Location),queryOption);

This method will call Query method internally and return list of two Location objects order by Name descending.

Example 2:-

QueryOption queryOption = new QueryOption();
SortField field = new SortField();
field.FieldName = "Name";
field.Direction=SortDirection.Descending;
queryOption.SortFields.Add(field);
queryOption.Skip = 0;
queryOption.Limit = 2;

var strFilter = $"i => i.Parent.Id.Equals(new Guid(\"{factory.Id}\"))";

QueryOption queryOption = new QueryOption();
SortField field = new SortField();
field.FieldName = "Name";
field.Direction = SortDirection.Descending;
queryOption.SortFields.Add(field);
queryOption.Skip = 0;
queryOption.Limit = 2;
var locations = _locationRepo.QueryAllByExpression(strFilter, queryOption).GetAwaiter().GetResult();

This method will call Query method internally and return list of two Location objects order by Name descending.



# NHibernate raw Sql Query implementation details

## Method Information

```
Method
Name
```
```
Infrastructure
Layer
```
```
Description Return
Type
```
```
QueryAllBy
RawSql
```
```
Information Layer This method is used to execute raw sql query on specific entity. We can write parameterized "Data Query Language" a
nd can also perform joins. This method returns IEnumerable<IEntity>.
```
```
IEnumerable
<IEntity>
```
## Input Parameter

```
Parameter
Name
```
```
Data Type Mandatory Description
```
```
writeModelType Type Yes This is the Entity Type on which query will be performed
```
```
query string Yes It represents the actual Sql query
```
```
param Dictionary<string,
dynamic>
```
```
No Here we add the column name as key and parameter value as value. This dictionary is required to provide
items which are required for where condition.
```
```
queryOption QueryOption No This parameter is required for provide skip and take value while returning entity list
```
## Method Information

```
M
et
h
o
d
N
a
me
```
```
Inf
ra
st
ru
ct
ur
e
La
yer
```
```
Description R
e t u r n T y
```
```
pe
```
```
Q
u
er
y
Al
lB
y
R
a
w
S
ql
```
```
Inf
or
m
ati
on
La
yer
```
```
This method is used to execute raw sql query on specific entity. We can write parameterized "Data Query Language" and can also perform joins. This
method take input of query as string, dictionary of input parameters for where condition. Here key will be parameter name and value will be
parameter value. QueryOption class is used for dynamic sorting on the basis of some parameters and it also take input for skip and take parameter for
paging.. This method is used for return DataTable as per our query, where we can have a special facility to give column name for the datatable. For this
purpose we have to pass Dictionary<string,type>scalarParam where key is column name and type is column datatype.
```
```
D
at
a
T
a
ble
```
## Input Parameter

```
Parameter
Name
```
```
Data Type Mandatory Description
```
```
writeModelTy
pe
```
```
Type Yes This is the Entity Type on which query will be performed
```
```
query string Yes It represents the actual Sql query
```
```
scalarParam Dictionary<string,
type>
```
```
Yes Here we add the column name as key and column type as value. This dictionary is required to provide items which
are required for Data Table columns to be returned.
```
```
param Dictionary<string,
dynamic>
```
```
No Here we add the column name as key and parameter value as value. This dictionary is required to provide items
which are required for where condition in sql query.
```
```
queryOption QueryOption No This parameter is required for provide skip and take value while returning entity list
```

Example

string Query = "Select * from Factory where FavEmployeeName=:FavEmployeeName";

*** ":" is the syntax for NHibernate to express parameter***

Dictionary<string, Type> scalarparam = new Dictionary<string, Type>();
scalarparam.Add("FavEmployeeName",typeof(string));
scalarparam.Add("Name", typeof(string));

Dictionary<string, object> param = new Dictionary<string, object>();
param.Add("FavEmployeeName", "Joe");
var result = informationGateway.QueryAllByRawSql(typeof(Factory), Query, scalarparam, param);
string favEmployeeName = string.Empty;
foreach(DataRow dr in result.Rows)
{
favEmployeeName = dr["FavEmployeeName"].ToString();

### }


# Query Execution Flow

```
This is deprecated
```

### 1.

```
a.
i.
ii.
iii.
iv.
v.
vi.
vii.
viii.
ix.
x.
2.
a.
i.
ii.
iii.
iv.
3.
a.
b.
i.
4.
a.
b.
i.
ii.
```
# Search Criteria

In order to perform queries, a formal structure for storing and grouping conditions is needed.

A Search Criteria defines a single or set of expressions.

The Search Criteria is simple struct type model having four basic values.

```
Operation - Enum value that defines how to evaluate the expression
Examples
And Logical And
Or Logical Or
Eq Equal
Neq Not Equal
Lt Less Than
Lte Less Than or Equal
Gt Greater Than
Gte Greater Than or Equal
In In a set
Nin Not in a set
Left - String value defining a property name or subentity property name
Examples
'Name' primitive string member
'Quantity' primitive integer member
'Locations.Name' Subentity
'Locations[]' Collection *
Right - The value expression
This can be a scalar value, for example 'Acme' or 99
This can be an Array value (in terms of In / Nin)
If operation is not In/Nin arrays are not allowed
SearchTerms - List of SearchTerms used for grouping terms
A grouped collection of Terms
When this collection is non empty there are some requirements.
Left and Right must be null because the
Only And/Or are allowed when SearchTerms exist otherwise exception shall be thrown.
```
* Feature being evaluated

```
This is deprecated
```

# Integration Layer

```
Pseudo-Code of Factory
Pseudo Code of Channels and Registration
```
Log Message fields

In green parts mean the target implementation with vanilla grpc client, the orange ones are related to dapr integration.

## Pseudo-Code of Factory

```
class Factory : IDisposable
{
private IntegrationConfiguration cfg;
ConcurrentDictionary<stirng, IMessageChannel> channels;
IServiceProvider ioc;
```
```
IMessageChannel GetChannel(string module)
{
if(channels.containsKey(module)){
return channels[module];
}
```
```
//here create new one
if(cfg.IsDaprEnabled){
var channel = ioc.GetService<IDaprCommunicationChannel>();
channels[module] = channel;
```
```
return channel;
}
else{
var channel = ioc.GetService<IGrpcMessageService>();
channels[module] = channel;
```
```
return channel;
```

### }

### }

```
Dispose(){
foreach( v in channels.Values) {v.Dispose();}
}
}
```
The goal of this is to generate a single channel for each module, so opening only a single socket (now we open and release socket every request)

Reason for recycling channel, taken form Microsoft Doc:

Source

Pseudo Code of Channels and Registration

```
interface IDaprCommunicationChannel: IMessageChannel, IDisposable {}
interface IGrpcMessageService: IMessageChannel, IDisposable {}
```
```
class DaprCommunicationChannel : IDaprCommunicationChannel {}
class GrpcMessageService : IGrpcMessageService{}
```
```
//Register service
services.AddTransient<IDaprCommunicationChannel, DaprCommunicationChannel>();
services.AddTransient<IGrpcMessageService, GrpcMessageService>();
```
The goal is to have interfaces specific for the communication technology also, in order to improve testability (mockability).

Log Message fields

```
Name Description Notes
```
```
CommunicationMode The name of the communication channel type used. "Grpc" is the only one supported until now
```
```
CommMessageDesti
nation
```
```
The target destination for a message. Message.Destination specified by the user
```
```
CommUrl The resolved Url for the CommMessageDestination.
```
```
CommMessageType The type of the message being sent. User Message Type
```
```
CommMessageId The message Identification number.
```

CommGrpcStatus The Grpc status code in case of a rpc failure.

CommGrpcDetails The Grpc details causing the CommGrpcStatus code.

CommReason The cause of a failure for communication. Circuit Breaker policy message

CommTimeSpan The amount of time until the next retry. Retry policy

CommRetryCount The retry number currently executed. Retry policy

CommRetryNumber The amount of retry configured. Retry configuration

CommScaleFactor The scale factor for the exponential retry delay
(CommTimeSpan).

```
Retry configuration
```
CommJitterRange The random amount of time for the exponential retry delay
(CommTimeSpan)

```
Retry configuration
```
CommAllowedCons
ecutiveFailures

```
The allowed maximum number of consecutive failure for a
specific destination (CommMessageDestination).
```
```
Circuit Breaker configuration
```
CommDurationOfBr
eak

```
The duration of the Circuit Breaker window before a probe
message is allowed again.
```
```
Circuit Breaker configuration
```
CommMaximumRes
ponseTime

```
The maximum time to wait to receive a confirmation from the
remote peer.
```
```
Messaging configuration
```
CommMaximumCon
sumeTime

```
The maximum time allowed to consume a message until the
expiration.
```
```
Messaging configuration
```
CommRecoveryCycl
eTime

```
The client and server recovery polling time. Message Recovery configuration
```
CommBatchMessag
eNumber

```
The number of messages recovered at one time. Message Recovery configuration
```
CommRecoveryCou
nter

```
The number of the recovery cycle from the startup. Message Recovery
```
CommRecoveryMes
sageNumber

```
The number of messages to be recovered. Message Recovery
```
CommMessageInsta
nceId

```
The persistent message Instance Id. Message Recovery
```
CommConsumers The list of the model Consumers registered for a Message Type. Message Recovery

CommGrpcTypeUrl The fully qualified Grpc type name of a message. Message handling service

CommNumberOfCo
nsumers

```
The number of Consumers registered for a Message Type. Message consuming service. If this value is 0, the
message is not consumed and completed as is.
```
CommConsumerNa
me

```
The type name of each consumer Message consuming application.
```

# Dapr Analysis

```
Topics
General overview
Topology and Configuration
Service Invocation
Communication
Aspnet.core Integration
Resiliency
Name Resolution
Observability
Distributed Tracing
Logging
Metrics
Security
Cross service communication
Local communication
Authorization
Namespaces
Example service invocation security
Monitoring and Support tools
Licensing and Support
Conclusion
References
```
## Topics

```
General overview
What and Why
Topology and Configuration
Define a strategy to deploy using Dapr and Modular MOM
On prem and Cloud, mixed scenario.
Kubernetes integration
Specific topic regarding kubernetes integration (pod structure, etc...)
Security
How do you configure Dapr in a secure way to communicate across services
Authentication, encryption and access control
Service Invocation Building block (Communication and Resiliency)
What a container should take care and what is demanded to Dapr
Can we implement the metadata based messaging (using dynamically generated and instantiated message models)?
3rd party integration
Enterprise Service Buses (kafka, rabbitmq, mqtt, etc..)
Monitoring and Support tools
Service Mesh capability
Performance impacts (memory, cpu, latency)
Multi-tenancy?
Discovery
Service discovery , how it is different from kubernetes service locator.
State Store/ Distributed Caching
Testing
Ability to perform end to end testing
Licensing & costs
```
## General overview

Dapr (Distributed Application Runtime) is a free and open source runtime system designed to support cloud native and serverless computing. Its initial
release supported SDKs and APIs for Java, .NET, Python, and Go, and targeted the Kubernetes cloud deployment system. https://dapr.io/

Dapr is a portable, event-driven runtime that makes it easy for any developer to build resilient, stateless and stateful applications that run on the cloud and
edge and embraces the diversity of languages and developer frameworks.

There are many considerations when architecting microservices applications. Dapr provides best practices for common capabilities when building
microservice applications that developers can use in a standard way and deploy to any environment. It does this by providing distributed system building
blocks.

Each of these building blocks is independent, meaning that you can use one, some or all of them in your application.

```
This document is a work in progress, some topics may be further expanded and integrated with the design.
```

Topology and Configuration

Dapr configurations are settings that enable you to change the behavior of individual Dapr application sidecars or globally on the system services in the
Dapr control plane.


Dapr uses a sidecar architecture, running as a separate process alongside the application and includes features such as, service invocation, network
security and distributed tracing. An example of a per Dapr application sidecar setting is configuring trace settings. An example of a Dapr control plane
setting is mutual TLS which is a global setting on the Sentry system service.

This often raises the question - how does Dapr compare to service mesh solutions such as Linkerd, Istio and Open Service Mesh (OSM)?

While Dapr and service meshes do offer some overlapping capabilities, Dapr is not a service mesh where a service mesh, is defined as a networking
service mesh. Unlike a service mesh which is focused on networking concerns, Dapr is focused on providing building blocks that make it easier for
developers to build applications as microservices. Dapr is developer-centric versus service meshes being infrastructure-centric.

Dapr however does work with service meshes. In the case where both are deployed together, both Dapr and service mesh sidecars are running in the
application environment.

Dapr can be configured to run on any Kubernetes cluster. To achieve this, Dapr begins by deploying the dapr-sidecar-injector, dapr-operator, d
apr-placement, and dapr-sentry Kubernetes services. These provide first-class integration to make running applications with Dapr easy.

Service Invocation

Here we explored the possibility of using Dapr for communication between Apps in the Metadata Runtime.

As with any distributed system, the Modular MOM platform faces several challenges with inter-app communication, such as Service Discovery,
Communication Security, Communication Resiliency, Tracing etc. The Service Invocation Building Block provides several out-of-the-box functionalities to
address these challenges.

Here we specifically explored the aspects of Service Invocation Building Block of Dapr.


Communication

In the Metadata Runtime the interactions between the Apps are configured as metadata. This metadata constitutes the domain models for the Message,
the Producer and the Consumer.

At runtime this metadata is used to exchange data between Apps.

Dapr implements both an HTTP and a gRPC API for local calls. gRPC is useful for low-latency, high performance scenarios and has language integration
using the proto clients.

Aspnet.core Integration

Client invocation:

There are several differences to call a service method with gRPC or with Dapr.

gRPC lets define a proto spec with all the exposed service methods, then it generates all the client and server classes into your project. The auto-
generated client class is a stub than can be used as is to invoke each service method through the gRPC channel, initialized within a gRPC client.

Example:

### //...

```
services
.AddGrpcClient<NameOfTheService.NameOfTheServiceClient>(o =>
{
o.Address = new Uri("http://localhost:5002"); // Target address for NameOfTheService service
})
//...
NameOfTheService.NameOfTheServiceClient client; // Client stub for the service
MessageResponse resp = await client.NameOfTheMethod(new MessageRequest); // Invoke the SomeServiceMethod method
```
Dapr instead has a more generic Invoke semantics that you can use with input parameters that lets you specify the ServiceName and the ServiceMethod
at runtime, you can still use the auto-generated IMessage (requests and responses) from the gRPC proto file to use it in the Dapr Invoke methods.

To invoke a remote gRPC service Dapr exposes these APIs:

```
Dapr Client APIs to invoke a request with gRPC
```
### //

```
// Summary:
// Perform service invocation using gRPC semantics for the application idenfied
// by appId and invokes the method specified by methodName with an empty request
// body. If the response has a non-success status code an exception will be thrown.
//
// Parameters:
// appId:
// The Dapr application id to invoke the method on.
//
// methodName:
// The name of the method to invoke.
//
// cancellationToken:
// A System.Threading.CancellationToken that can be used to cancel the operation.
//
// Returns:
// A System.Threading.Tasks.Task that will return when the operation has completed.
public abstract Task InvokeMethodGrpcAsync(string appId, string methodName, CancellationToken
cancellationToken = default);
//
// Summary:
// Perform service invocation using gRPC semantics for the application idenfied
// by appId and invokes the method specified by methodName with a Protobuf serialized
// request body specified by data. If the response has a non-success status code
// an exception will be thrown.
//
// Parameters:
// appId:
// The Dapr application id to invoke the method on.
```

### //

// methodName:
// The name of the method to invoke.
//
// data:
// The data that will be Protobuf serialized and provided as the request body.
//
// cancellationToken:
// A System.Threading.CancellationToken that can be used to cancel the operation.
//
// Type parameters:
// TRequest:
// The type of the data that will be Protobuf serialized and provided as the request
// body.
//
// Returns:
// A System.Threading.Tasks.Task that will return when the operation has completed.
public abstract Task InvokeMethodGrpcAsync<TRequest>(string appId, string methodName, TRequest data,
CancellationToken cancellationToken = default) where TRequest : IMessage;
//
// Summary:
// Perform service invocation using gRPC semantics for the application idenfied
// by appId and invokes the method specified by methodName with an empty request
// body. If the response has a success status code the body will be deserialized
// using Protobuf to a value of type TResponse; otherwise an exception will be thrown.
//
// Parameters:
// appId:
// The Dapr application id to invoke the method on.
//
// methodName:
// The name of the method to invoke.
//
// cancellationToken:
// A System.Threading.CancellationToken that can be used to cancel the operation.
//
// Type parameters:
// TResponse:
// The type of the data that will be Protobuf deserialized from the response body.
//
// Returns:
// A System.Threading.Tasks.Task`1 that will return the value when the operation
// has completed.
public abstract Task<TResponse> InvokeMethodGrpcAsync<TResponse>(string appId, string methodName,
CancellationToken cancellationToken = default) where TResponse : IMessage, new();
//
// Summary:
// Perform service invocation using gRPC semantics for the application idenfied
// by appId and invokes the method specified by methodName with a Protobuf serialized
// request body specified by data. If the response has a success status code the
// body will be deserialized using Protobuf to a value of type TResponse; otherwise
// an exception will be thrown.
//
// Parameters:
// appId:
// The Dapr application id to invoke the method on.
//
// methodName:
// The name of the method to invoke.
//
// data:
// The data that will be Protobuf serialized and provided as the request body.
//
// cancellationToken:
// A System.Threading.CancellationToken that can be used to cancel the operation.
//
// Type parameters:
// TRequest:
// The type of the data that will be Protobuf serialized and provided as the request
// body.
//


```
// TResponse:
// The type of the data that will be Protobuf deserialized from the response body.
//
// Returns:
// A System.Threading.Tasks.Task`1 that will return the value when the operation
// has completed.
public abstract Task<TResponse> InvokeMethodGrpcAsync<TRequest, TResponse>(string appId, string
methodName, TRequest data, CancellationToken cancellationToken = default)
where TRequest : IMessage
where TResponse : IMessage, new();
```
Example:

### //...

```
services.AddDaprClient(); // for aspnet.core service integration resolving the DaprClient with DI
// OR
DaprClient daprClient = new DaprClientBuilder().Build();
//...
MessageRequest messageRequest = new MessageRequest();
//...
var messageResponse = await daprClient.InvokeMethodGrpcAsync<MessageRequest, MessageResponse>("RemoteAppId",
"NameOfTheMethod", messageRequest);
```
The communication is redirected to the localhost:<DAPR_PORT> (default 50001 for dapr-grpc) in order to talk with the local sidecar process deployed with
the Dapr container along with your app service.

Then a name resolution of the appId is performed by Dapr through the Kubernetes DNS depending on the registration of app-id name used to start the
Dapr instance. See notes.

The messageRequest is encapsulated within the Dapr protocol and will arrive on the remote sidecar process, that will dispatch it to the local service app
handling logic.

```
Notes and limitations:
```
```
Because the app-id is the configured name to start a Dapr instance, you have to be sure to use this assigned named in the invocation block when
you call a remote app. This name has to be known and assigned accordingly in the configuration.
Because of a limitation on the Invocation API at the time of writing, there are no ways to have more then one gRPC service registered under the
same app: you can map more than one using the endpoints but it is unclear how you would address the different dapr derived grpc concrete
classes from the invocation block. The service name is simply missing so there is an implicit one to one relation between the appId and the
mapped service, consequently the app is considered a single service. No separation for different types of methods inside the same app is
possible (flatted methods).
```
Server side handling:

```
public void ConfigureServices(IServiceCollection services)
{
services.AddGrpc();
```
```
services.AddDaprClient(); // to be used for service invocation, state store, etc... with DI
}
```
Substitute the grpcservice class mapped in the Grpc endpoints with the one derived from AppCallback.AppCallbackBase (see later).

```
app.UseEndpoints(endpoints =>
{
endpoints.MapGrpcService<NameOfTheService>();
```
### ...

### });

MapGrpcService() exposes NameOfTheService grpc service into ASP.NET Core route endpoints.


To implement it you need to inherit AppCallback.AppCallbackBase that will be called by the Dapr runtime to invoke method, register for pub/sub topics and
register bindings.

```
using Dapr.AppCallback.Autogen.Grpc.v1;
```
```
public class NameOfTheService: AppCallback.AppCallbackBase
{
...
}
```
And you can override the OnInvoke (and ListTopicSubscriptions called during dapr initialization) method of this class to handle message requests and
responses:

```
//Example:
public override async Task<InvokeResponse> OnInvoke(InvokeRequest request, ServerCallContext context)
{ var response = new InvokeResponse();
IMessage input, output;
_logger.LogInformation($"Incoming Message arrive from Dapr, invoke for {request.Method}");
```
```
switch (request.Method)
{
case "RpcCallFast":
input = request.Data.Unpack<RpcFastRequest>();
output = await _backingService.RpcCallFast(input as RpcFastRequest, context);
response.Data = Any.Pack(output);
break;
case "RpcCallSlow":
input = request.Data.Unpack<RpcSlowRequest>();
output = await _backingService.RpcCallSlow(input as RpcSlowRequest, context);
response.Data = Any.Pack(output);
break;
default:
break;
}
return response;
}
public override async Task<ListTopicSubscriptionsResponse> ListTopicSubscriptions(Empty request,
ServerCallContext context)
{
return new ListTopicSubscriptionsResponse();
}
```
You can use the backing gRPC service class to map the methods processed to the already implemented ones in the original gRPC service and use the
OnInvoke as a simple wrapper to them, taking care to register the old class as a Singleton in ConfigureServices() in place of the
MapGrpService<OldGrpcService>.

```
//Startup.cs
public void ConfigureServices(IServiceCollection services)
{
services.AddSingleton<OldGrpcService>(); // used to DI into NameOfTheService Dapr service
}
```
And then assigning it in the contructor of NameOfTheService() via Dependency Injection

```
public NameOfTheService(DaprClient daprClient, ILogger<NameOfTheService> logger, OldGrpcService service)
{
_daprClient = daprClient;
_logger = logger;
_backingService = service;
}
```
Resiliency


Name Resolution

Name resolution in Dapr is performed after service-to-service invocation (point 1) by either this two specific internal component implementations (point 2):

```
mDNS
Kubernetes DNS
```
mDNS is multicast DNS protocol "Zeroconf" and it is only used in self-hosted mode (the one deployed on the bare bone docker). This protocol allows to
discover services within the local network without contacting the per-configured standard DNS server which is set up in the networking interfaces at system
level. This possibility is not discussed here for brevity.

Kubernetes DNS is used when Dapr is integrated in Kubernetes: it uses the name of the app as the key specifier, along with the namespace, with this
resolvable Kubernetes FQDN format:

<appid>-dapr.<namespace>.svc.cluster.local

The default DNS suffix on any container deployed in kubernetes is <namespace>.svc.cluster.local so if anyone tries to reach an app specifying just the
appid, the default is to resolve looking it up in the same namespace of the calling container, but containers are allowed to talk across namespaces
reaching the respective target services.

In order to contact the remote http endpoint Dapr has the following input parameters for the full service resolution:

Appid: the name of the remote app registered (the pod/service name)

Namespace: the namespace of the destination app deployed into the cluster

Port: the remote port number for the sidecar communication

Ref: https://github.com/dapr/components-contrib/tree/master/nameresolution/kubernetes

The protocol used for points 1,7 and 4,5 of the diagram depends on the Deployment specs used with the following annotations :

```
Dapr Deployment
```
```
apiVersion: apps/v1
kind: Deployment
metadata:
name: myapp
namespace: default
labels:
app: myapp
spec:
replicas: 1
selector:
matchLabels:
app: myapp
template:
metadata:
labels:
app: myapp
annotations:
dapr.io/enabled: "true"
dapr.io/app-id: "myapp"
dapr.io/app-protocol: "grpc"
dapr.io/app-port: "5005"
```
The protocol used between sidecars (points 3,6) is grpc.

TODO:Clarify port mappings, local and destination, from the prospective of the invoker code.

Observability

Dapr can be configured to emit tracing data. Therefore, when an application is leveraging Dapr to perform service-to-service calls and pub/sub messaging,
the inter-service communication which flows through Dapr sidecar can be traced via Dapr observability buidling block. Dapr uses W3C tracing for tracing


context and can generate and propagate the context header itself or propagate user provided context headers. Dapr can be configure to work with
OpenTelemetry collector.

Distributed Tracing

The spans (activities) which are generated inside different layers of MMOM applications such as Metadata engine, Information layer, ... for tracing can be
integrated with distributed traceing of Dapr for HTTP and gRPC calls. However, the spans inside the application should be generated inside an HTTP or
gRPC call initiated by Dapr. An example is given in the picture belown. (All the tests were done locally using Zipkin as backend, so OpenTelemetry
Collector was not configured as backend.)


Logging

Dapr produces structured logs to console out either as a plain text (default) or JSON formatted (should be configured). When running Dapr in a Kubernetes
cluster, logs then can be collected using a container log collector such as Fluent Bit. (see Dapr logs)

Metrics

Dapr exposes a Prometheus metrics endpoint that can be scraped to understand the behavior of the Dapr sidecar and system services. For example, the
metrics between a Dapr sidecar and the user application show call latency, traffic failures, error rates of requests etc. For a list of Dapr metrics see Dapr
metrics. Since in MMOM, OpenTelemetry collector is used for collecting telemetry data (trace, metric, in future also log), it seems that Prometheus Receiver
can be used for receiving metric data in Prometheus format (scraping a Prometheus endpoint).

Security

Here we analyzed some aspects of the security for a deployment using Dapr, as the infrastructure communication gateway across the application services.

Cross service communication

In Dapr communication security is established using TLS, in particular mTLS (mutual TLS), enforcing both client and server TLS authentication during the
handshake of the TCP connection. In this way a sidecar process can establish a secure channel to the destination service (remote sidecar) across the real
network, validating the peer in a secure and standard way. This has two advantages:

```
Authentication is performed very early on and with a well understood and widespread standard over the internet, enforcing non-repudiation (the
ability to validate the peer through digital signature) and data-integrity very well.
During runtime operations, application services communicating between each other use an encrypted channel, so that confidentiality is also
guaranteed.
```
Dapr leverages a system service named Sentry which acts as a Certificate Authority (CA) and signs workload (app) certificate requests originating from
the Dapr sidecar.

Dapr also manages workload certificate rotation, and does so with zero downtime to the application.

Sentry, the CA service, automatically creates and persists self signed root certificates valid for one year, unless existing root certs have been provided
by the user.

When root certs are replaced (secret in Kubernetes mode and filesystem for self hosted mode), the Sentry picks them up and re-builds the trust chain
without needing to restart, with zero downtime to Sentry.

When a new Dapr sidecar initializes, it first checks if mTLS is enabled. If it is, an ECDSA private key and certificate signing request are generated (CSR)
and sent to Sentry via a gRPC interface. The communication between the Dapr sidecar and Sentry is authenticated using the trust chain cert, which is
injected into each Dapr instance by the Dapr Sidecar Injector system service.


By default, a workload cert is valid for 24 hours and the clock skew is set to 15 minutes.

Local communication

The communication between the Application Service and the Dapr Sidecar is confined within the same Pod in a typical Kubernetes deployment. This is
equal to talk to a local service on the same machine (localhost) and constitutes a trusted security boundary whenever this type of configuration is
applied (there is less needs to replicate the same amount of protections locally).

However, a particular use case to deal with is the use of a gRPC HTTP/2 channel from the application to the sidecar process. In fact, this type of
connection can go through a TLS channel as well (https://localhost as the destination URI). The sidecar process validates itself with a certificate presented
to the client application (us), so it is important to understand which type o certificates and which configurations are required to establish the connection
successfully.

TODO: clarify the certificate requirements to open the connection locally to the side car process: it would require a common CA certificate installed on the
app container instance, in case it is signed by the trusted Sentry CA. Is all of these steps automatically resolved by the Dapr Client API accessing that
information from the configuration/filesystem somehow? or do we need to retrieve it from some secrets store (even if it is not a secret)? The nature of the
dynamic allocation of the sidecar prevents the use of a certificate installed on the image but it is not clear from where the CA certificate is exposed.

Authorization

Authorization is the ability to validate a user token in order to determine whether it will be able to call a specific service endpoint or not, restricting the
access through a configurable access control list.

In Dapr this is used to restrict what operations the calling applications can perform, via service invocation, over the called target application.

In order to limit the access to a given application for specific operations and HTTP verbs, you can define an access control policy specification in
configuration.

The authorization is typically implemented for user logins, but it must be noticed that, for service-to-service communication, each user in the system should
be able to call a downstream service, after being successfully validated and authorized in the first place by the HTTP front-end. In case this consistency
principle is not guaranteed, the business logic will partially fail in a non-intended way (incomplete executions).

So the authorization treated here is referring the one needed between two application services in order to communicate correctly and securely between
each other, with a scope of access rights mapped to the service application level (example: can an Application Service A communicate to an Application
Service B?) so that it is possible to define and apply policies between two sidecar restricting access to specific application service endpoints or methods
whenever is needed :

ServiceAServiceB is ok.

The service identity of ServiceA is accepted and authorized by ServiceB sidecar.

Or:

ServiceAServiceC is not ok/planned by the deployment. The service identity of ServiceA is not accepted nor authorized by ServiceC sidecar.

This is enforced trough 3 levels of configuration:

Global;

App level;

Method level;

applying the policy from the most to less specific case.


The required type of trust between execution services is automatically and securely provided by the mTLS authentication and by applying authorization
policies, conversely, the user identity invoking and starting the call chain, should be transported and verified by the ModMOM protocol for non-repudiation
purposes (examples are: logs, audit & trails, etc...), the latter is outside the scope of this document.

Trusted Domains are logical separation between deployments and may span across different namespaces (in Kubernetes) using the same Dapr
infrastructure: for example, an App1 deployed on premise, may be put in the same trusted domain with a remote App2 service in the cloud. As long they
share the Dapr infrastructure and the same Root certification authority (Sentry) the connection between this two parties is secure and authorized.

At the heart, the service-to-service authorization is performed using the "Spiffe" standard which is used to establish the service identity. Spiffe ID is
provided as an extension into the X509 workload cert issued by the Sentry process during the sidecar initialization and it is spent during the authorization
process in the dapr sidecar.

Note: Spiffe is different from the OAuth tokens, the main difference is that OAuth protocols have to be implemented and supported in the application code
itself: they are typically used for user credentials (issued by google, facebook or other identity providers). They have "scopes" so that your application logic
will be the judge on what it is allowed and what not. Conversely, Spiffe is commonly used in service meshes, with authorization policies you can get native
mutual authentication drives all of that for you, without the need for an external auth provider. Also with Trusted domains and namespaces, Spiffe is
tailored for service to service invocation inside the same cluster or something that it can be even spent across different non local clusters in a standard and
a secure way.

Once the service identities of invoking applications are established, they are used to create the required level of trust defining the access control in the
yaml configuration for Dapr, example:


Note: Grpc is supported specifying the methods name instead of the http/1 verb, without the full namespace path of the grpc classes.

The level of granularity that is possible to achieve arrives to the method called on a target service. For a simple and a more streamlined configuration,
however, it is suggested to limit this level on a service-to-service basis, so that the configuration will be more in line with the micro-service nature of the
implementation which implies a certain level of isolation and confinement of features for the back-end services (in contrast with the monolithic services
which do everything), leaving the option to specify a full custom configuration, down to invoked methods, only when really needed.

The default configuration is a subject to clarify: the deployment should place all the services in the same trusted domain and set the defaultAction to "allow"
for services in the same domain, and "deny" otherwise. In case some services in the deployment are somewhat different in nature (for example, they have
been placed into a different network, on prem or on edge) they may be set into a different domain, specifying specific exception rule to follow the least
privilege principle with only specific services able to communicate with them.

Namespaces

Dapr fully supports namespaces, it will allow you to assign one per dapr instance.

When Dapr is running on Kubernetes, it inherits all the characteristics of a Kubernetes namespace:

```
You can separate Pods and Containers and Components to run in a separate namespace where you can assign different resources to the
namespace.
You can assign different RBAC permissions to the namespaces.
```
This is particularly useful in a real deployment where you can have different requirements for different groups of services and to partition the network of
relationships across different namespaces to model the security and the isolation needed in such environments. For example: a group of services can be
dedicated to the shop floor and others, more centralized, may be dedicated to interface with the ERP management of the plant, or, you can have different
environment modeled with a namespace, one for staging and one for the production, ans so on and so forth. They may need to use different
characteristics having different security requirements and resources allocation.

During service-to-service invocation, Dapr will address the namespace specified in the URI of the request:

```
http://localhost:<dapr_port>/v1.0/invoke/<app_id>.<namespace>/method/somemethod
```
where

app_id: is the name of the remote service.

namespace: is the id of the remote namespace, in case it is not specified the default is to use the same namespace of the caller service.

An important thing to notice is that you can have different application instances, with the same app_id, running inside different namespaces.

Also, Dapr Components are "scoped" per namespace, so for example if you have a redis state store component, you can define one store for one
namespace and a different redis state store with different credentials to another namespace and only Dapr applications that are within the same
namespace will be able to use it.

Having Dapr be able to reverse proxy calls between namespaces opens up the ability to overcome an inherent limitation of Kubernetes itself.

For example, using just Kubernetes you could have an Ingress component, which is a load balancer that does TLS termination for you (like nginx or
haproxy), and you might want to send traffic from the namespace that the ingress controller is installed in to the back end which might sit in a different
namespace, Kubernetes has a limitation where ingress rules cannot target services from a namespace that is different from the actual ingress controller.


So these components need to be coupled with your application and deployed within the same namespace (this is a k8s design decision) but still a lot of
people are pretty unsatisfied about it, because a reverse proxy may be deployed, serving different backends, as a single entity able to provide the ingress
rules for all of them in a more consistent and secure way, separating the backends into different namespaces. For example, you might have different
devops in your organization using the resources and allocations to the namespace, such as a dedicated QA namespace to be used like a canary with
different permissions, or in general, other scenarios with different working groups managing only a portion of the deployment.

Dapr, supporting cross-namespace communications, may be use to overcome also this problem improving the security of the deployment.

This can be done injecting into the Ingress controller pod a Dapr sidecar.

Example service invocation security

The diagram below is an example deployment on a Kubernetes cluster with a Daprized Ingress service that calls onto Service A using service
invocation with mTLS encryption and an applies access control policy. Service A then calls onto Service B also using service invocation and mTLS.
Each service is running in different namespaces for added isolation.


### ...

See Dapr Security Concept for further information.

Monitoring and Support tools

Dapr has a simple dashboard to monitor the current state of the deployment. However, the same information may be seen also on other Kubernetes
dashboards selecting the dapr-system or the target namespace. The dashboard in the Kubernetes mode seems to be a little redundant and too much
simplified, it is probably most used in Self-Hosted mode.

Example:

Dapr has also its own command line tool, called dapr. You can use it for example to run an app in the self-hosted mode with "dapr run", or to monitor
configurations with commands such as "dapr status -k" or "dapr configurations -k" in a Kubernetes cluster deployment.

Note that in the Kubernetes mode the dapr command line seems to need access at the cluster level scope and may fail for unauthorized permissions.


Licensing and Support

Dapr is distributed with the very permissive MIT License (basically, AS IS software).

Dapr is an open source project started and backed by Microsoft. It is unclear the kind of commercial support Microsoft would give, but for sure is a good
thing that this project is backed by a commercial entity for recognition and acceptance with the customers and partners.

Conclusion

Dapr is a way to implement a richer runtime platform capable to run on different target environments such as on prem, on edge, on cloud.

Dapr runtime goes into the direction of the Multi-Runtime pattern for Kubernetes that many see as the "post microservice" architecture.

It extends Kubernetes with 3 main functionalities exposing them to the Business Logic applications:

```
Networking, with resiliency features such as retry handling and circuit breakers along with the services discovery, inherited from k8s.
Binding, with many connectors to different 3rd party soft-wares.
State Management, with the persistency option provided to implement distributed application across microservices
```
All of these areas are typically basic to non-existent in a standard Kubernetes reference implementation. Furthermore, it implements also other interesting
features such as secrets integration and actors based patterns which, in turn, can provides to the applications a way to handle timed actions, with
configurable timers, in a simple and distributed way.

On the other hand, Dapr has very limited extensions in the realm of the Life-Cycle management, but this is where Kubernetes shines: k8s popularized this
concept in the first place, so the decision to rely on the standard Kubernetes capabilities should not been seen as a poor design decision, but instead a
way to build upon and fill the gaps when really needed.

The main cons that Dapr has (like others Kubernetes runtimes) is that it tries to do a little bit of everything and its sidecar implementation is not transparent
to the application layer: the Dapr Sidecar is a sort of "broker" that exposes these features to the application logic by specific protocols and APIs. It provides
independence from the underlying runtime platform with pluggable components which can be swapped out. However, to provide this level of abstraction,
the APIs end up on the lower common denominator: having different components of the same type still requires to expose just a limited set of
functionalities available on all of these implementations, with simpler, albeit in some cases trivial, interfaces.

This may pose a challenge in our evolving platform when we will need to implement sophisticated features that will require a fairly advance support
underneath, or mix and match features bypassing Dapr for some specific area when needed (possibly on Observability).

Dapr, with its out-of-the-box and cloud native nature, is a way to have the cake and eat it too, but sometimes the cake you will get may not be the one you
have always needed, or wanted.

Apart from these limitations, Dapr is a great tool to abstract the runtime and it provides a very good way to improve substantially the time to market,
leveraging its declarative and plugable integrations with the cloud native service. This will help a great deal in the still evolving landscape of microservice
deployments: Dapr can be used as a decoupling runtime layer providing many features otherwise required to be supported directly in the code, for each
specific service.

References

Dapr Docs

Dapr Security Concept


# Dapr Integration Issues

Here we collect any issue and problem encountered to share the knowledge and coordinates the effort to solve any impediments.

States:

```
RESOLVED
```
```
OPEN
```
```
BLOCKING
```
```
ID Status Description Explanation Assigned
to
```
```
CC Notes
```
```
#1 OPEN First installation on OpenShift cluster We need it to know how dapr can be used in the current CI/CD
pipeline integrated in the OS environment
```
```
Lini, Gianluca Ticket was
opened
Issues rises
in cluster 02
(independent
from dapr)
```
```
A test cluster
is available
below.
#2 RESOLVED Test DAPR in Openshift (Test Enviroment) In the dapr-test namespace, we need to test dapr in terms of
installation, configuration an main uses.
```
```
Trubini,
Piergiorgio
```
```
This analysis
is performed
on a test
environment
while we are
waiting for
the ticket to
be solved.
Simple dapr
dashboard
accessible
from here
(SWQA
network only):
https://dapr-
dashboard-
dapr-system.
apps.
openshift-dev-
ossdn.swqa.
tstWh/
#3 BLOCKING When the remote dapr instance is down,
sending messages with the Dapr invoking
block from a client service, causes a crash in
the Dapr sidecar container (daprd). Details
below.
```
```
The Invoke method using grpc protocol binding for the Dapr API
(ClientApp->darpd) causes a segmentation fault inside the daprd
sidecar when the remote server is unavailable. This happens
only in a Kubernetes deployment (not in self-hosted mode).
```
```
The ClientApp is a aspnet.core service, using dapr api
integration with an instance of a DaprClient object to invoke the
methods. Almost immediately the daprd process exits with
always the same callstack.
```
```
This behavior was observed on the OpenShift test cluster and
also on a local Kubernetes deployment with a single node cluster
(installed on a VDI) with Dapr runtime-verision = 1.1.1 so it
seems unrelated in respect to the installation on OS.
```
```
However, a new version of Dapr is available: 1.1.2 so we have to
upgrade the cluster to be sure to use the last version released.
```
```
See the expected behavior tested on Self-Hosted mode.
```
```
Lini, Gianluca
```
```
Trubini,
Piergiorgio
```
```
Bardini
,
Matteo
Naga
malli,
Rame
sh
```
```
It should be
possible to
upgrade Dapr
on a running
cluster using
Helm to 1.1.2.
```
```
Ticket was
open https://m
omis.
industrysoftwa
re.automation.
siemens.com
/tickets/10387.
```
## Additional Details

## Issue #3

Logs from the daprd container after the boot (suppressing debug for dapr-placement server) with kubectl logs deployments/grpcresiliencyclient daprd -f --
since=30s | grep -v "placement"


Daprd crash

time="2021-04-30T09:10:37.9196308Z" level=info msg="starting Dapr Runtime -- version 1.1.2 -- commit 3148f36"
app_id=grpcresiliencyclient instance=grpcresiliencyclient-5f4b759b64-zkvrs scope=dapr.runtime type=log ver=1.1.2
time="2021-04-30T09:10:37.9197428Z" level=info msg="log level set to: debug" app_id=grpcresiliencyclient
instance=grpcresiliencyclient-5f4b759b64-zkvrs scope=dapr.runtime type=log ver=1.1.2
time="2021-04-30T09:10:37.9201416Z" level=info msg="metrics server started on :9090/"
app_id=grpcresiliencyclient instance=grpcresiliencyclient-5f4b759b64-zkvrs scope=dapr.metrics type=log ver=1.1.2
time="2021-04-30T09:10:37.9202044Z" level=info msg="loading default configuration" app_id=grpcresiliencyclient
instance=grpcresiliencyclient-5f4b759b64-zkvrs scope=dapr.runtime type=log ver=1.1.2
time="2021-04-30T09:10:37.9203792Z" level=info msg="kubernetes mode configured" app_id=grpcresiliencyclient
instance=grpcresiliencyclient-5f4b759b64-zkvrs scope=dapr.runtime type=log ver=1.1.2
time="2021-04-30T09:10:37.9204215Z" level=info msg="app id: grpcresiliencyclient" app_id=grpcresiliencyclient
instance=grpcresiliencyclient-5f4b759b64-zkvrs scope=dapr.runtime type=log ver=1.1.2
time="2021-04-30T09:10:37.9207537Z" level=info msg="mTLS enabled. creating sidecar authenticator"
app_id=grpcresiliencyclient instance=grpcresiliencyclient-5f4b759b64-zkvrs scope=dapr.runtime type=log ver=1.1.2
time="2021-04-30T09:10:37.9210602Z" level=info msg="trust anchors and cert chain extracted successfully"
app_id=grpcresiliencyclient instance=grpcresiliencyclient-5f4b759b64-zkvrs scope=dapr.runtime.security type=log
ver=1.1.2
time="2021-04-30T09:10:37.9211541Z" level=info msg="authenticator created" app_id=grpcresiliencyclient
instance=grpcresiliencyclient-5f4b759b64-zkvrs scope=dapr.runtime type=log ver=1.1.2
time="2021-04-30T09:10:37.9669955Z" level=info msg="Initialized name resolution to kubernetes"
app_id=grpcresiliencyclient instance=grpcresiliencyclient-5f4b759b64-zkvrs scope=dapr.runtime type=log ver=1.1.2
time="2021-04-30T09:10:37.9673855Z" level=debug msg="loading component. name: kubernetes, type: secretstores.
kubernetes/v1" app_id=grpcresiliencyclient instance=grpcresiliencyclient-5f4b759b64-zkvrs scope=dapr.runtime
type=log ver=1.1.2
time="2021-04-30T09:10:37.9689842Z" level=info msg="component loaded. name: kubernetes, type: secretstores.
kubernetes/v1" app_id=grpcresiliencyclient instance=grpcresiliencyclient-5f4b759b64-zkvrs scope=dapr.runtime
type=log ver=1.1.2
time="2021-04-30T09:10:38.0788554Z" level=debug msg="found component. name: statestore, type: state.redis/v1"
app_id=grpcresiliencyclient instance=grpcresiliencyclient-5f4b759b64-zkvrs scope=dapr.runtime type=log ver=1.1.2
time="2021-04-30T09:10:38.0788998Z" level=info msg="waiting for all outstanding components to be processed"
app_id=grpcresiliencyclient instance=grpcresiliencyclient-5f4b759b64-zkvrs scope=dapr.runtime type=log ver=1.1.2
time="2021-04-30T09:10:38.0789114Z" level=debug msg="loading component. name: statestore, type: state.redis/v1"
app_id=grpcresiliencyclient instance=grpcresiliencyclient-5f4b759b64-zkvrs scope=dapr.runtime type=log ver=1.1.2
time="2021-04-30T09:10:38.207921Z" level=info msg="component loaded. name: statestore, type: state.redis/v1"
app_id=grpcresiliencyclient instance=grpcresiliencyclient-5f4b759b64-zkvrs scope=dapr.runtime type=log ver=1.1.2
time="2021-04-30T09:10:38.2079751Z" level=info msg="all outstanding components processed"
app_id=grpcresiliencyclient instance=grpcresiliencyclient-5f4b759b64-zkvrs scope=dapr.runtime type=log ver=1.1.2
time="2021-04-30T09:10:38.2080493Z" level=info msg="enabled gRPC tracing middleware"
app_id=grpcresiliencyclient instance=grpcresiliencyclient-5f4b759b64-zkvrs scope=dapr.runtime.grpc.api type=log
ver=1.1.2
time="2021-04-30T09:10:38.2080643Z" level=info msg="enabled gRPC metrics middleware"
app_id=grpcresiliencyclient instance=grpcresiliencyclient-5f4b759b64-zkvrs scope=dapr.runtime.grpc.api type=log
ver=1.1.2
time="2021-04-30T09:10:38.208123Z" level=info msg="API gRPC server is running on port 50001"
app_id=grpcresiliencyclient instance=grpcresiliencyclient-5f4b759b64-zkvrs scope=dapr.runtime type=log ver=1.1.2
time="2021-04-30T09:10:38.2086594Z" level=info msg="enabled metrics http middleware"
app_id=grpcresiliencyclient instance=grpcresiliencyclient-5f4b759b64-zkvrs scope=dapr.runtime.http type=log
ver=1.1.2
time="2021-04-30T09:10:38.2087154Z" level=info msg="enabled tracing http middleware"
app_id=grpcresiliencyclient instance=grpcresiliencyclient-5f4b759b64-zkvrs scope=dapr.runtime.http type=log
ver=1.1.2
time="2021-04-30T09:10:38.2087271Z" level=info msg="http server is running on port 3500"
app_id=grpcresiliencyclient instance=grpcresiliencyclient-5f4b759b64-zkvrs scope=dapr.runtime type=log ver=1.1.2
time="2021-04-30T09:10:38.2087342Z" level=info msg="The request body size parameter is: 4"
app_id=grpcresiliencyclient instance=grpcresiliencyclient-5f4b759b64-zkvrs scope=dapr.runtime type=log ver=1.1.2
time="2021-04-30T09:10:38.2087972Z" level=info msg="enabled gRPC tracing middleware"
app_id=grpcresiliencyclient instance=grpcresiliencyclient-5f4b759b64-zkvrs scope=dapr.runtime.grpc.internal
type=log ver=1.1.2
time="2021-04-30T09:10:38.2088081Z" level=info msg="enabled gRPC metrics middleware"
app_id=grpcresiliencyclient instance=grpcresiliencyclient-5f4b759b64-zkvrs scope=dapr.runtime.grpc.internal
type=log ver=1.1.2
time="2021-04-30T09:10:38.2088188Z" level=info msg="sending workload csr request to sentry"
app_id=grpcresiliencyclient instance=grpcresiliencyclient-5f4b759b64-zkvrs scope=dapr.runtime.grpc.internal
type=log ver=1.1.2
time="2021-04-30T09:10:38.4443499Z" level=info msg="certificate signed successfully"
app_id=grpcresiliencyclient instance=grpcresiliencyclient-5f4b759b64-zkvrs scope=dapr.runtime.grpc.internal
type=log ver=1.1.2


time="2021-04-30T09:10:38.4446052Z" level=info msg="internal gRPC server is running on port 50002"
app_id=grpcresiliencyclient instance=grpcresiliencyclient-5f4b759b64-zkvrs scope=dapr.runtime type=log ver=1.1.2
time="2021-04-30T09:10:38.4446209Z" level=info msg="application protocol: grpc. waiting on port 5001. This
will block until the app is listening on that port." app_id=grpcresiliencyclient instance=grpcresiliencyclient-
5f4b759b64-zkvrs scope=dapr.runtime type=log ver=1.1.2
time="2021-04-30T09:10:38.4449991Z" level=info msg="starting workload cert expiry watcher. current cert expires
on: 2021-05-01 09:25:38 +0000 UTC" app_id=grpcresiliencyclient instance=grpcresiliencyclient-5f4b759b64-zkvrs
scope=dapr.runtime.grpc.internal type=log ver=1.1.2
time="2021-04-30T09:10:38.445333Z" level=info msg="application discovered on port 5001"
app_id=grpcresiliencyclient instance=grpcresiliencyclient-5f4b759b64-zkvrs scope=dapr.runtime type=log ver=1.1.2
time="2021-04-30T09:10:38.4455274Z" level=info msg="actor runtime started. actor idle timeout: 1h0m0s. actor
scan interval: 30s" app_id=grpcresiliencyclient instance=grpcresiliencyclient-5f4b759b64-zkvrs scope=dapr.
runtime.actor type=log ver=1.1.2
time="2021-04-30T09:10:38.4455894Z" level=info msg="dapr initialized. Status: Running. Init Elapsed 525.1772ms"
app_id=grpcresiliencyclient instance=grpcresiliencyclient-5f4b759b64-zkvrs scope=dapr.runtime type=log ver=1.1.2
time="2021-04-30T09:10:39.5345599Z" level=debug msg="retry count: 1, grpc call failed, ns: default, addr:
grpcresiliency-dapr.default.svc.cluster.local:50002, appid: grpcresiliency, err: rpc error: code = Unavailable
desc = last resolver error: produced zero addresses" app_id=grpcresiliencyclient instance=grpcresiliencyclient-
5f4b759b64-zkvrs scope=dapr.runtime.direct_messaging type=log ver=1.1.2
time="2021-04-30T09:10:39.5346148Z" level=debug msg="retry count: 1, grpc call failed, ns: default, addr:
grpcresiliency-dapr.default.svc.cluster.local:50002, appid: grpcresiliency, err: rpc error: code = Unavailable
desc = last resolver error: produced zero addresses" app_id=grpcresiliencyclient instance=grpcresiliencyclient-
5f4b759b64-zkvrs scope=dapr.runtime.direct_messaging type=log ver=1.1.2
panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x1 addr=0x18 pc=0x7b92ed]

goroutine 43 [running]:
google.golang.org/grpc/internal/resolver.(*SafeConfigSelector).SelectConfig(0xc000b98628, 0x3c15eb8,
0xc000ba39b0, 0x36cdafb, 0x34, 0x0)
/home/runner/go/pkg/mod/google.golang.org/grpc@v1.34.0/internal/resolver/config_selector.go:92 +0x6d
google.golang.org/grpc.newClientStream(0x3c15eb8, 0xc000ba39b0, 0x54dc540, 0xc000b98380, 0x36cdafb, 0x34,
0xc000aa1660, 0x2, 0x2, 0x0, ...)
/home/runner/go/pkg/mod/google.golang.org/grpc@v1.34.0/stream.go:178 +0x1b5
google.golang.org/grpc.invoke(0x3c15eb8, 0xc000ba39b0, 0x36cdafb, 0x34, 0x339b4e0, 0xc0009126e0, 0x339b5c0,
0xc000ac9c70, 0xc000b98380, 0xc000aa1660, ...)
/home/runner/go/pkg/mod/google.golang.org/grpc@v1.34.0/call.go:66 +0x99
github.com/dapr/dapr/pkg/diagnostics.(*grpcMetrics).UnaryClientInterceptor.func1(0x3c15eb8, 0xc000ba39b0,
0x36cdafb, 0x34, 0x339b4e0, 0xc0009126e0, 0x339b5c0, 0xc000ac9c70, 0xc000b98380, 0x3774d18, ...)
/home/runner/work/dapr/dapr/pkg/diagnostics/grpc_monitoring.go:213 +0x1a4
google.golang.org/grpc.(*ClientConn).Invoke(0xc000b98380, 0x3c15eb8, 0xc000ba39b0, 0x36cdafb, 0x34, 0x339b4e0,
0xc0009126e0, 0x339b5c0, 0xc000ac9c70, 0xc000aa1660, ...)
/home/runner/go/pkg/mod/google.golang.org/grpc@v1.34.0/call.go:35 +0x109
github.com/dapr/dapr/pkg/proto/internals/v1.(*serviceInvocationClient).CallLocal(0xc000ba12e8, 0x3c15eb8,
0xc000ba39b0, 0xc0009126e0, 0xc000aa1660, 0x2, 0x2, 0x2, 0xc000ba39b0, 0x0)
/home/runner/work/dapr/dapr/pkg/proto/internals/v1/service_invocation_grpc.pb.go:46 +0xd4
github.com/dapr/dapr/pkg/messaging.(*directMessaging).invokeRemote(0xc00064d130, 0x3c15eb8, 0xc000ba39b0,
0xc000914170, 0xe, 0xc00005400a, 0x7, 0xc0003dcac0, 0x33, 0xc00000e120, ...)
/home/runner/work/dapr/dapr/pkg/messaging/direct_messaging.go:171 +0x3d0
github.com/dapr/dapr/pkg/messaging.(*directMessaging).invokeWithRetry(0xc00064d130, 0x3c15eb8, 0xc00067f0b0,
0x3, 0x3b9aca00, 0xc000914170, 0xe, 0xc00005400a, 0x7, 0xc0003dcac0, ...)
/home/runner/work/dapr/dapr/pkg/messaging/direct_messaging.go:125 +0xcf
github.com/dapr/dapr/pkg/messaging.(*directMessaging).Invoke(0xc00064d130, 0x3c15eb8, 0xc00067f0b0,
0xc000914170, 0xe, 0xc00000e120, 0xc5aa45, 0xc01b1043de2d684c, 0x6ce3c47d)
/home/runner/work/dapr/dapr/pkg/messaging/direct_messaging.go:99 +0x1d2
github.com/dapr/dapr/pkg/grpc.(*api).InvokeService(0xc0004ee900, 0x3c15eb8, 0xc00067f0b0, 0xc00055a540,
0xc0004ee900, 0x2f69aa0, 0xc0001b9628)
/home/runner/work/dapr/dapr/pkg/grpc/api.go:333 +0x17b
github.com/dapr/dapr/pkg/proto/runtime/v1._Dapr_InvokeService_Handler.func1(0x3c15eb8, 0xc00067f0b0, 0x331cc20,
0xc00055a540, 0x29, 0x9e, 0xc01b1043de2d684c, 0x6ce3c47d)
/home/runner/work/dapr/dapr/pkg/proto/runtime/v1/dapr_grpc.pb.go:404 +0x89
github.com/dapr/dapr/pkg/diagnostics.(*grpcMetrics).UnaryServerInterceptor.func1(0x3c15eb8, 0xc00067f0b0,
0x331cc20, 0xc00055a540, 0xc00092a0a0, 0xc000936138, 0x0, 0x0, 0x0, 0x0)
/home/runner/work/dapr/dapr/pkg/diagnostics/grpc_monitoring.go:199 +0x136
github.com/grpc-ecosystem/go-grpc-middleware.ChainUnaryServer.func1.1.1(0x3c15eb8, 0xc00067f0b0, 0x331cc20,
0xc00055a540, 0x0, 0x0, 0x0, 0x0)
/home/runner/go/pkg/mod/github.com/grpc-ecosystem/go-grpc-middleware@v1.2.2/chain.go:25 +0x63
github.com/dapr/dapr/pkg/diagnostics.GRPCTraceUnaryServerInterceptor.func1(0x3c15eb8, 0xc00067f0b0, 0x331cc20,
0xc00055a540, 0xc00092a0a0, 0xc00092a0c0, 0xc79a3a, 0x330f680, 0xc00092a0e0, 0xc00092a0a0)
/home/runner/work/dapr/dapr/pkg/diagnostics/grpc_tracing.go:56 +0x2eb
github.com/grpc-ecosystem/go-grpc-middleware.ChainUnaryServer.func1.1.1(0x3c15eb8, 0xc00067f050, 0x331cc20,
0xc00055a540, 0xc000077400, 0x0, 0xc0001b9b30, 0x40e3d8)


```
/home/runner/go/pkg/mod/github.com/grpc-ecosystem/go-grpc-middleware@v1.2.2/chain.go:25 +0x63
github.com/grpc-ecosystem/go-grpc-middleware.ChainUnaryServer.func1(0x3c15eb8, 0xc00067f050, 0x331cc20,
0xc00055a540, 0xc00092a0a0, 0xc000936138, 0xc000941ba0, 0x4c1806, 0x33a79c0, 0xc00067f050)
/home/runner/go/pkg/mod/github.com/grpc-ecosystem/go-grpc-middleware@v1.2.2/chain.go:34 +0xd7
github.com/dapr/dapr/pkg/proto/runtime/v1._Dapr_InvokeService_Handler(0x358a7c0, 0xc0004ee900, 0x3c15eb8,
0xc00067f050, 0xc000094d80, 0xc0008f73b0, 0x3c15eb8, 0xc00067f050, 0xc000942000, 0x9e)
/home/runner/work/dapr/dapr/pkg/proto/runtime/v1/dapr_grpc.pb.go:406 +0x150
google.golang.org/grpc.(*Server).processUnaryRPC(0xc000454fc0, 0x3c412f8, 0xc0003e2780, 0xc000938100,
0xc0008f7500, 0x54be520, 0x0, 0x0, 0x0)
/home/runner/go/pkg/mod/google.golang.org/grpc@v1.34.0/server.go:1210 +0x52b
google.golang.org/grpc.(*Server).handleStream(0xc000454fc0, 0x3c412f8, 0xc0003e2780, 0xc000938100, 0x0)
/home/runner/go/pkg/mod/google.golang.org/grpc@v1.34.0/server.go:1533 +0xd0c
google.golang.org/grpc.(*Server).serveStreams.func1.2(0xc000a437e0, 0xc000454fc0, 0x3c412f8, 0xc0003e2780,
0xc000938100)
/home/runner/go/pkg/mod/google.golang.org/grpc@v1.34.0/server.go:871 +0xab
created by google.golang.org/grpc.(*Server).serveStreams.func1
/home/runner/go/pkg/mod/google.golang.org/grpc@v1.34.0/server.go:869 +0x1fd
```
Note: the version here is 1.1.2 upgraded on my VDI machine, I need to know if the problem persists on the upgrade Open Shift cluster as well.

Note: the first requests are sent only after a 15 s delay after the startup of the Client App to give time to the daprd process to start and to initialize.

Brief analysis:

On line 34 daprd process regognized the client application grpc service is up and running and completes the initialization (10:38.445), on lines 35,36 (10:
39.5345) the first fail of the two remote outstanding invocations are reported with a resolver error on the destination service (the remote service was in fact
down).

After that a segmentation fault on memory address 0x18 is reported (this value is always 0x18 or 0x17) the callstack is always the same.

Expected behavior in this situation is this:

```
Client App + daprd in Self-Hosted mode
```
```
INFO[0001] application discovered on port 5001 app_id=GrpcResiliencyClient instance=vm-vdip46-031
scope=dapr.runtime type=log ver=1.1.2
INFO[0001] actor runtime started. actor idle timeout: 1h0m0s. actor scan interval: 30s
app_id=GrpcResiliencyClient instance=vm-vdip46-031 scope=dapr.runtime.actor type=log ver=1.1.2
== APP == info: Microsoft.Hosting.Lifetime[0]
```
```
== APP == Now listening on: http://[::]:5001
```
```
== APP == info: Microsoft.Hosting.Lifetime[0]
```
```
== APP == Application started. Press Ctrl+C to shut down.
```
```
== APP == info: Microsoft.Hosting.Lifetime[0]
```
```
== APP == Hosting environment: Production
```
```
== APP == info: Microsoft.Hosting.Lifetime[0]
```
```
== APP == Content root path: /mnt/c/Users/ITR00295/source/repos/GrpcResiliency/GrpcResiliencyClient/bin
/Release/net5.0/publish
```
```
INFO[0001] placement tables updated, version: 0 app_id=GrpcResiliencyClient instance=vm-vdip46-031
scope=dapr.runtime.actor.internal.placement type=log ver=1.1.2
INFO[0002] dapr initialized. Status: Running. Init Elapsed 1544.1574999999998ms app_id=GrpcResiliencyClient
instance=vm-vdip46-031 scope=dapr.runtime type=log ver=1.1.2
== APP == Error - Dapr Exception: An exception occurred while invoking method: 'RpcCallFast' on app-id:
'GrpcResiliency' - Inner:Status(StatusCode="Internal", Detail="fail to invoke, id: GrpcResiliency, err:
couldn't find service: GrpcResiliency")
```
```
== APP == Error - Dapr Exception: An exception occurred while invoking method: 'RpcCallFast' on app-id:
'GrpcResiliency' - Inner:Status(StatusCode="Internal", Detail="fail to invoke, id: GrpcResiliency, err:
couldn't find service: GrpcResiliency")
```
```
== APP == Error - Dapr Exception: An exception occurred while invoking method: 'RpcCallFast' on app-id:
'GrpcResiliency' - Inner:Status(StatusCode="Internal", Detail="fail to invoke, id: GrpcResiliency, err:
```

```
couldn't find service: GrpcResiliency")
```
```
== APP == Error - Dapr Exception: An exception occurred while invoking method: 'RpcCallFast' on app-id:
'GrpcResiliency' - Inner:Status(StatusCode="Internal", Detail="fail to invoke, id: GrpcResiliency, err:
couldn't find service: GrpcResiliency")
```
The invoke method should fail with the proper DaprException stating the unavailability of the remote service without crashing the sidecar.

Instead the Client app will get a reset/abort IO exception because the local communication channel is reset from the crashing process.

```
Application logs
```
```
[2021-04-30 09:10:16.549] info: GrpcResiliencyClient.Services.ClientService[0]
INFO - Dapr mode enabled
[2021-04-30 09:10:16.565] info: GrpcResiliencyClient.Services.ClientService[0]
INFO - Dapr Client initializing...
[2021-04-30 09:10:16.845] info: Microsoft.Hosting.Lifetime[0]
Now listening on: http://[::]:5001
[2021-04-30 09:10:16.846] info: Microsoft.Hosting.Lifetime[0]
Application started. Press Ctrl+C to shut down.
[2021-04-30 09:10:16.846] info: Microsoft.Hosting.Lifetime[0]
Hosting environment: Production
[2021-04-30 09:10:16.847] info: Microsoft.Hosting.Lifetime[0]
Content root path: /app
[2021-04-30 09:10:31.596] info: GrpcResiliencyClient.Services.ClientService[0]
INFO - Dapr Client done.
[2021-04-30 09:10:32.630] info: GrpcResiliencyClient.Services.ClientService[0]
Sending 2 requests...
[2021-04-30 09:10:34.939] info: GrpcResiliencyClient.Services.ClientService[0]
Error - Dapr Exception: An exception occurred while invoking method: 'RpcCallFast' on app-id:
'grpcresiliency' - Inner:Status(StatusCode="Unavailable", Detail="Error starting gRPC call.
HttpRequestException: An error occurred while sending the request. IOException: The request was aborted.
IOException: The response ended prematurely while waiting for the next frame from the server.", DebugException="
System.Net.Http.HttpRequestException: An error occurred while sending the request.
---> System.IO.IOException: The request was aborted.
---> System.IO.IOException: The response ended prematurely while waiting for the next frame from the
server.
at System.Net.Http.Http2Connection.<ReadFrameAsync>g__ThrowMissingFrame|48_1()
at System.Net.Http.Http2Connection.ReadFrameAsync(Boolean initialFrame)
at System.Net.Http.Http2Connection.ProcessIncomingFramesAsync()
--- End of inner exception stack trace ---
at System.Net.Http.Http2Connection.ThrowRequestAborted(Exception innerException)
at System.Net.Http.Http2Connection.Http2Stream.CheckResponseBodyState()
at System.Net.Http.Http2Connection.Http2Stream.TryEnsureHeaders()
at System.Net.Http.Http2Connection.Http2Stream.ReadResponseHeadersAsync(CancellationToken
cancellationToken)
at System.Net.Http.Http2Connection.SendAsync(HttpRequestMessage request, Boolean async,
CancellationToken cancellationToken)
--- End of inner exception stack trace ---
at System.Net.Http.Http2Connection.SendAsync(HttpRequestMessage request, Boolean async,
CancellationToken cancellationToken)
at System.Net.Http.HttpConnectionPool.SendWithRetryAsync(HttpRequestMessage request, Boolean async,
Boolean doRequestAuth, CancellationToken cancellationToken)
at System.Net.Http.RedirectHandler.SendAsync(HttpRequestMessage request, Boolean async,
CancellationToken cancellationToken)
at Grpc.Net.Client.Internal.GrpcCall`2.RunCall(HttpRequestMessage request, Nullable`1 timeout)")
[2021-04-30 09:10:34.967] info: GrpcResiliencyClient.Services.ClientService[0]
Error - Dapr Exception: An exception occurred while invoking method: 'RpcCallSlow' on app-id:
'grpcresiliency' - Inner:Status(StatusCode="Unavailable", Detail="Error starting gRPC call.
HttpRequestException: An error occurred while sending the request. IOException: The request was aborted.
IOException: The response ended prematurely while waiting for the next frame from the server.", DebugException="
System.Net.Http.HttpRequestException: An error occurred while sending the request.
---> System.IO.IOException: The request was aborted.
---> System.IO.IOException: The response ended prematurely while waiting for the next frame from the
server.
at System.Net.Http.Http2Connection.<ReadFrameAsync>g__ThrowMissingFrame|48_1()
at System.Net.Http.Http2Connection.ReadFrameAsync(Boolean initialFrame)
at System.Net.Http.Http2Connection.ProcessIncomingFramesAsync()
--- End of inner exception stack trace ---
```

at System.Net.Http.Http2Connection.ThrowRequestAborted(Exception innerException)
at System.Net.Http.Http2Connection.Http2Stream.CheckResponseBodyState()
at System.Net.Http.Http2Connection.Http2Stream.TryEnsureHeaders()
at System.Net.Http.Http2Connection.Http2Stream.ReadResponseHeadersAsync(CancellationToken
cancellationToken)
at System.Net.Http.Http2Connection.SendAsync(HttpRequestMessage request, Boolean async,
CancellationToken cancellationToken)
--- End of inner exception stack trace ---
at System.Net.Http.Http2Connection.SendAsync(HttpRequestMessage request, Boolean async,
CancellationToken cancellationToken)
at System.Net.Http.HttpConnectionPool.SendWithRetryAsync(HttpRequestMessage request, Boolean async,
Boolean doRequestAuth, CancellationToken cancellationToken)
at System.Net.Http.RedirectHandler.SendAsync(HttpRequestMessage request, Boolean async,
CancellationToken cancellationToken)
at Grpc.Net.Client.Internal.GrpcCall`2.RunCall(HttpRequestMessage request, Nullable`1 timeout)")


# Grpc Message serialization changes (US 45509)

## Possible breaking changes with platform > v2.2.0

platform version <= v 2.2.0: in case of errors on serializing a field from a mom message to google proto, the field is skipped and the serialization goes on.

platform version > v 2.2.0: in case of errors on serializing a field from a mom message to google proto, the message serialization throws an exception that
is not caught by the integration layer.

For an overview of the supported fields, please read the link Supported Fields - Modular MOM - MOM Wiki (siemens.com)

## Related links

Supported Fields - Modular MOM - MOM Wiki (siemens.com)


# List of Fields and related support in grpc messages

## Introduction

The tables below shows which Field types are supported (cell in green), which one has been fixed in a branch to be merged to the master (cells in yellow)
and the fields not supported or to yet to be tested (with an automatic test). The cells in red are related to fields with known bugs yet to be fixed. Each table
is related to the file where the Field class is defined.

The kind of fields are

```
Scalar / List (multi valued)
Primitive / Complex
```
For more info about the kind of field types::

What Are Field Types? - Modular MOM - MOM Wiki (siemens.com)

## Column and their meaning

```
Type: the field type.
Supported: field successfully tested by an integration test in the platform. Some fields are supported in an ongoing development on a separate
branch (not master).
Issue: issue description (serialization exception, generator crash etc)
Fixed in development branch: for field fixed in an ongoing development on a separate branch (yet to be pushed to the master)
To be supported?:
Note: some notes
Field Type cs: the file where the class is defined
Used in mes messages: yes if the field is used in a mes message (factory message, tnt message, material management message,
ordermanagement message)
```
## NativeFieldTypes.cs

Kind: scalar - primitive

```
Type Supported Issue (if
not
supported)
```
```
Kno
wn
bug
```
```
Fixed in development branch To be supported? notes Field
```
```
Type cs
```
```
Used in
MessageObject or
MES messages
StringFi
eld
```
```
yes yes (used in MES
messages)
```
```
NativeFie
ldTypes.
cs
```
```
yes
```
```
Integer
Field
```
```
yes yes (used in MES
messages)
```
```
NativeFie
ldTypes.
cs
```
```
yes
```
```
LongInt
Field
```
```
yes NativeFie
ldTypes.
cs
```
```
yes
```
```
Decima
lField
```
```
yes yes (used in MES
messages)
```
```
NativeFie
ldTypes.
cs
```
```
yes
```
```
Boolea
nField
```
```
yes yes (used in MES
messages)
```
```
NativeFie
ldTypes.
cs
```
```
yes
```
```
DateTi
meField
```
```
Serialization
exception
```
```
Bug
44722
```
```
gi/US_45509_MessageModelMapping_Refactoring yes (used in MES
messages)
```
```
NativeFie
ldTypes.
cs
```
```
yes
```
```
IsRefOf
RcdField
```
```
? NativeFie
ldTypes.
cs
Instanc
eIdField
```
```
No (workaround for
InstanceId field
name)
```
```
Serialization
exception
```
```
gi/US_45509_MessageModelMapping_Refactoring
```
```
:Workaround for InstanceId namefield: skip in
serialization (it was already skipped in
deserialization)
```
```
Yes? (InstanceId
namefield is a common
field)
```
```
Generated in
the proto as
string
```
```
NativeFie
ldTypes.
cs
```
```
yes
```
```
Named
Field
```
```
?
```
```
Revisio
nField
```
```
?
```
## NativeListTypes.cs

Kind: list - primitive


```
Type Supported Issue (if not
supported)
```
```
Known
bug
```
```
Fixed in development branch To be supported? not
es
```
```
Field
```
```
Type cs
```
```
Used in mes
messages
```
```
ListField of
scalar types
```
```
Serialization
exception
```
```
Bug
27231
```
```
gi
/US_45509_MessageModelMappin
g_Refactoring
```
```
(updated with a fix from old
Monviso branch)
```
```
yes (at least StringListField used in
MES messages)
```
```
NativeListTy
pes.cs
```
```
StringListField Yes with the fix yes NativeListTy
pes.cs
```
```
yes
```
```
IntegerListField To be tested with the fix NativeListTy
pes.cs
LongIntListField To be tested with the fix NativeListTy
pes.cs
BooleanListFieldTo be tested with the fix NativeListTy
pes.cs
DecimalListFieldTo be tested with the fix
```
```
(possible further issues on
serialization)
```
```
NativeListTy
pes.cs
```
```
DateTimeListFi
eld
```
```
To be tested with the fix
```
```
(possible further issues on
serialization)
```
```
NativeListTy
pes.cs
```
BlobFieldType.cs

Kind: scalar or list? - primitive

```
Type SupportedIssue (if not
supported)
```
```
Known bug Fixed in development
branch
```
```
To be supported? notesField
```
```
Type cs
```
```
Used in mes
messages
```
```
BlobField? no: for internal use only for
now
```
```
BlobFieldType.cs
```
```
StdEnumFieldno Message generator
crash
```
```
Bug 33764
```
```
(also Bug
22750?)
```
```
? EnumFieldTypes.
cs
```
OptionsFieldTypes.cs

Kind: scalar or list? - primitive

```
Type SupportedIssue (if not supported) Known bugFixed in development branchTo be supported? notesField
```
```
Type cs
```
```
Used in mes messages
```
```
OptionsField? OptionsFieldTypes.cs
FlagOptionsField? OptionsFieldTypes.cs
```
ObjectFieldTypes.cs

Kind: scalar - complex

The ObjectRefField types are skipped during the message model generation and runtime serialization. To be tested if all the other "ref" types are skipped
as well

```
Type Supported Issue (if not
supported)
```
```
Known
bug
```
```
Fixed in development
branch
```
```
To be
supported?
```
```
not
es
```
```
Field
```
```
Type cs
```
```
Used in mes
messages
```
```
SubentityField yes ObjectFieldTyp
e.cs
NamedSubentityFie
ld
```
```
X (missing automatic test) ObjectFieldTyp
e.cs
ObjectRefField No: skipped in generation and serialization
/deserialization)
```
```
ObjectFieldTyp
e.cs
ParentRefField Skipped? ObjectFieldTyp
e.cs
```
```
Yes
```

```
NamedObjectRefFi
eld
```
```
No: skipped in generation and serialization
/deserialization)
```
```
ObjectFieldTyp
e.cs
RevisionBaseRefFi
eld
```
```
Skipped? ObjectFieldTyp
e.cs
RevisionObjectRef
Field
```
```
skipped? ObjectFieldTyp
e.cs
SubentityRefField skipped? ObjectFieldTyp
e.cs
NamedSubentityRe
fField
```
```
skipped? ObjectFieldTyp
e.cs
```
ObjectListTypes.cs and FieldInitializer.cs

Kind: list - complex

The ObjectRefField types are skipped during the message model generation and runtime serialization. To be tested if all the other "ref" types (also the list
types) are skipped as well

```
Type Supported Issue (if not
supported)
```
```
Known
bug
```
```
Fixed in development
branch
```
```
To be supported? not
es
```
```
Field
```
```
Type cs
```
```
Used in mes
messages
```
```
SubentityListField x yes (used in MES
messages
```
```
ObjectListType
s.cs
```
```
yes
```
```
NamedSubentityListField X (missing
automatic test)
```
```
ObjectListType
s.cs
ObjectListField? ObjectListType
s.cs
ObjectRefListField Skipped? ObjectListType
s.cs
NamedObjectRefListField Skipped? ObjectListType
s.cs
RevisionObjectRefListField Skipped? ObjectListType
s.cs
SubentitRefListField Skipped? ObjectListType
s.cs
```
```
NamedObjectListField
(IObjectListField)
```
```
? FieldInitializer.
cs
RevisionObjectListField
(IObjectListField)
```
```
? FieldInitializer.
cs
```

# Message Consume Handling

```
Current state up to now
Error Handling
Proposed changes for Feature 28019 - Refinement
Refactor of the Server Recovery
Refactor the Delete Old Message hosted service
Refactor the Consume logic loop for each message per consumer service in order to isolate them in separate transaction.
Model
State Machine for Persistent Messages
Dead-Letter handling
User Documentation & Guidelines
Consume logic behavior changes
Recovery and Cleanup
Platform Error Handling on Message Consumers
Compensation Logic in case of failures
Documentation and guidelines
```
## Current state up to now

Incoming framework messages, are handled with two entrypoints:

```
GrpcMessageService class
Server Recovery
```
GrpMessageService is the gRPC service class derived from the main proto message (base) and implementing the platform protocol (HandleMessage).
This is the main entrypoint used with point-to-point communication when a remote peer send a message to a specific destination.

Server Recovery is the Hosted Service designated to handle messages which are not delivered yet to the business logic, in event of a crash of the hosting
process. This service is used to avoid the possible data loss due to the asynchronous communication nature of the protocol, like in the fire&forget pattern
implemented by the Platform.

The incoming messages are stored in a PersistentMessage table, after persisting the message an ack is returned to the sender to provide the confirmation
that the message will be handled by the consuming side. This happens in a dedicated transaction, not shared with the business logic (which executes
later on).

Server Recovery service resumes messages that are expired in a non-consumed state and executes the Consumer service on those.

The BL Consumer service is called in both cases and, whenever a successful execution is finished, it updates the PersistentMessage entity to exclude that
from the recovery mechanism.

This update is performed within the same transaction of the business logic, through the ExecuteService base method, which executes the PreCommit
event, updating the PersistentMessagePerConsumer and, finally, the PersistentMessage (when all the business logic are executed for that message type).

Reason: if a Consumer completes its own transaction (modifying its state correctly) the PersistentMessage(PerConsumer) is updated accordingly.
Conversely, if the transaction is rolled back, the persistent message is not consumed, so it must be recovered.

Right now, the Consumer service transaction is the same transaction for all the consumers of the same message type this is a bug because different
business logics should have different transactional context, not shared between each others.

Implementation/Class diagram for GrpcMessageService and MessageRecovery Service classes:


Error Handling

The Recovery service will restart the Consume logic each time a message is aborted (along with its own transaction) due to possible infrastructural issues
(process crashed, network errors, etc...).

However, there is no server "Failed" status for a PersistentMessage: just ServerMessageRetrieved, ServerMessageConsumed.

Any exceptions occurring during execution are treated as the message was aborted during the Consume phase (in fact, it is, because the transaction is
rolled back) but this is too indiscriminate, and it causes an endless-loop of recovery when the BL crashes for other reasons, either because of errors in
the data input format (message incompatible with the current implementation or state) or because of bugs in the BL, for example where the state of its data
in the database is inconsistent with the logic assumptions (duplicated keys and such alike), along with more "recoverable" infrastructural errors, such as
snapshot violations in the transaction (other business logics have updated the same records at the same time and the transaction has to be restarted from
scratch).

Proposed changes for Feature 28019 - Refinement

Feature 28019: Metadata Runtime - Message Consume Logic Enhancements

Here is a list of changes to be done to fulfill a better consumer logic in the platform:

Refactor of the Server Recovery

```
DONE Extend the enum status for messages to include a server failure state (failure during consume). This status is terminal one
and the recovery must not recover any messages in this state any further (they are completed with an error)
```
```
This will be linked to non-recoverable errors such as (business logic raised errors, other terminal exceptions to be better defined) (see
later)
The Persistent Message state is in failed if any of its consumers failed.
Keep the logic of the recovery to restart the message for any not completed messages (!consumed, !failed)
DONE Extend the configuration to support a limit on the recovery cycles for a given failing message.
After that limit the message will be put in a Failed state automatically.
NOT DONE Extend the RetryCount handling applying it for server messages and link it to the number of recovery cycles in which the
message has been already recovered (possibly per consumer)
Should each consumer be independent from each others or could we keep a global count per message here (a round)? Preferred way:
Message lifetime on the top message.
DONE Decoupled the condition on the final Update of the top level message away from the Consumers transaction (__PreCommit)
(this could cause collisions and snapshot exceptions among consumers)
Recovery can handle it in its own cycle (the aggregate state on the Top level message is a simple deterministic query on the message-
per-consumers records).
Keep only the single entry (message per consumer) in the same transaction of the business logic, because when the user transaction is
aborted for any reason, the message has not been consumed yet for that consumer.
```
Refactor the Delete Old Message hosted service

```
DONE Keep the Failed state messages, even if older than the default 1 day lifespan.
```

```
This is a dead-letter like feature. Some terminal failures may be recovered manually by the user, with a specific UI browsing/handling
system feature in the Platform (out of scope for now).
Be aware that, even if the purpose of this service is to cleanup the database from historically completed records, the failure state must
be kept indefinitely because this is a possible data-loss point.
Have a look if this is consistent with the already implemented logic for outbound messages (after the retries). If no, change it to comply to
this statement.
Configurable threshold instead of hard-coded 1 day lifespan.
NOT DONE Evaluate the possibility to clean message-per-consumers (in state consumed) for a top level message in a failure state this is
not really mandatory, just an optimization if the code is not too complex to modify in this fashion.
```
Refactor the Consume logic loop for each message per consumer service in order to isolate them in separate

transaction.

```
DONE Message Application entry point ConsumeMessageAsync() should initiate a new BizLogicEngine for each consumer.
The first service created in that instance is the Top Service corresponding to the registered Consumer Service and will start a separate
transactional unit.
DONE Error Handling: move the try/catch per consumer (into the inner loop) and handle the resulting exceptions as follow:
MOMUserException (raised explicitly by the business logic) Put the message in a failed state (it is completed with an error).
Evaluate a programmable exception filter mechanism on MOMException to distinguish/configure exceptions categorized as rec
overable or non-recoverable (example: snapshot isolation exceptions are recoverable, even if they should not be handled
from the server recovery: they should be retried immediately by the Engine/Information Layer, but as a measure of last resort,
we could recover these consumers exceptions).
Be aware that Consume(IMessage) is just a "trigger", but the real deal is happening during the ExecuteService().
An error on a single consumer must not affect the others (the loop should continue).
DONE Parallelize the consumers loop (Parallel.ForEach)
DONE Remove the Update on the Top level message in CompleteMessage() and move the logic in the recovery server cycle (see
above).
Message per consumers are ok to be updated in the same user transaction as single entries.
The Update of the main message is performed on a separate transaction after the consumer loop (and by the recovery service in case
this update fails).
```
Model

```
NOT DONE Evaluate to add a "Failure Reason" column in the PersistentMessage table for "failed" messages, to be easily recorded and
seen outside the logs. It is often a missing information about what happened.
```
State Machine for Persistent Messages

PersistenMessageStateEnum:

```
State Value Description
```
```
ClientMessa
geSent
```
```
0 The communication layer on the client side has started sending the message. Notes: RetryCount keeps track of the retry
attempts in case of unavailability of the server. In case the attempts exceed the maximum threshold, the message is set in
state 2.
```
```
ClientMessa
geAcked
```
```
1 The server has received the message correctly and persisted it on its side. This indicates a transfer of responsibility to the
target application in order to complete the execution.
```
```
ClientMessa
geFailed
```
```
2 The communication layer on the client side has not been able to deliver the message to the server. Also, the server was
unable to de-serialize the message or to invoke any consumers given the current deployed model (model inconsistencies).
Fatal.
```
```
ServerMess
ageRetreived
```
```
3 The communication layer on the server side has persisted the incoming message correctly and it has started the execution of
the associated Consumers.
```
```
ServerMess
ageConsum
ed
```
```
4 All the consumers have executed correctly consuming the incoming message.
```
```
ServerMess
ageFailed
```
```
5 One or more consumers failed with an unrecoverable error or, the recovery cycle exceeded the maximum threshold trying re-
executing the failing consumers. Fatal.
```

Dead-Letter handling

In case of undelivered failed client messages, the support personnel can restart them with the following SQL statement over the client application db:

```
---- Restart any failed client messages again
UPDATE [<client application db>].[dbo].[PersistentMessage] SET State = 0 WHERE State = 2 -- AND Destination =
'...' ...
```
Notes: messages already received (and processed) by the target application will be discarded by the server whenever the messages are already present
in the server application db (de-duplication), but be aware not to change the State = 1 to 0 because the messages are kept for a limited amount of time
(default: 1 day) and this command may cause duplicated records on the target business logic.

In case of unconsumed failed server messages, the support personnel can restart them with the following SQL statement over the server application db:

```
---- Restart any failed server message again (to consume: update the state of the consumers also)
UPDATE [<target application db>].[dbo].[PersistentMessage] SET State = 3 WHERE State = 5 -- AND
OriginalMessageType = '...' ...
UPDATE [<target application db>].[dbo].[PersistentMessagePerConsumer] SET State = 3 WHERE State = 5 -- AND
OriginalMessageType = '...' ...
```
User Documentation & Guidelines

Consume logic behavior changes

```
The Consume method implemented by the Business Logic must return a positive boolean. In case the Consume() returns a negative result, the
message is considered explicitly rejected and it is set in a failed state: no further attempts will be made by the platform to consume it again.
Furthermore, if the Consume() entry-point method returns a negative result, no execution of BL implemented events is triggered by the platform.
Any MOMUserException, raised explicitly by the Business Logic execution, is considered as the BL rejected the message: the message is set in a
failed state and no further attempts will be made by the platform to consume it again.
Other errors cases: see Platform Error Handling.
Multiple consumers on the same message type are now supported. Each Consumer has its own transaction context and it is isolated from the
others. They are executed in a parallel fashion whenever a message instance is received.
```
Recovery and Cleanup

The Message Recovery service is now tuned up to recover only infrastructural errors (due to transient problems in the infrastructure). The Business Logic
implementations are expected to consume their message and they should not rely on the recovery mechanism to retry the consumption.

In case any unpredictable exceptions is raised, the Recovery service is still capable to retry the consumer's execution for a limited amount of time,
regulated by a configurable threshold defined in appsettings(.*.).json: ModularMOM:Messaging:Recovery:MaxMessageLifetime (default: 3 hours), after
which, the message is automatically set in a failed state and no further attempts will be made to consume it again.

Given the asynchronous nature of the business logic implementations, any failed messages are kept in the persistent storage for each receiving module
([PersistentMessage] and [PersistentMessagePerConsumer] DB tables). This is a sort of dead letter of any unsuccessful executions, those messages may
be reviewed and manually retried later, in case of data losses due to business errors. No cleanup is attempted on any failed messages (State 5).


Automatic cleanup is made only on completed messages (States: 1, 4) after reaching a configurable amount of time, regulated by the setting appsettings(.
*.).json: ModularMOM:Messaging:Recovery:MaxPersistentTime (default: 1 day).

Platform Error Handling on Message Consumers

A failed state on a message means that the platform considered the execution outcome as a fatal, non-recoverable state. No further attempts will be made
to retry the execution of the message consumer afterwards. Conversely, if the outcome is considered recoverable, the message is rescheduled by the
platform for another execution, until the failure threshold MaxMessageLifetime is reached.

This situation is determined in these cases:

```
Negative results on either consumer.Consume() or consumer.ExecuteService() are considered fatal errors and they immediately set the message
in a failed state.
Any MOMUserException is fatal and set the message in a failed state.
Any MOMException is recoverable, except in these cases:
SysFieldValidationFailed, ErrorCode 0011605. Errors on UserRequired Field Validation or SystemRequired Field Validation.
ObjectWithKeyNameAlreadyExist ErrorCode 0011422: duplicate key.
ErrorCode 0041699: SqlExceptions. Errors returned by the SQL service, like constraints violation, lengths exceeded, etc...
Any MOMConcurrentUpdateException is recoverable.
Any un-handled exceptions is fatal and set the message in a failed state, except in these cases:
TaskCanceledException or OperationCanceledException
```
Compensation Logic in case of failures

User Story 23435: Return error from async process (Message)

This Us implements the Option 2B in the following diagram:


Documentation and guidelines

The message consumers can now implement the following interface (Siemens.MOM.MetaModel.Interfaces.Interfaces.Messaging):

```
IMessageConsumerFailure interface
```
```
public interface IMessageConsumerFailure<in TMessage>
where TMessage : IMessage
{
/// <summary>
/// Stateless compensation handler invoked when the consume of the message has failed.
/// Reasons may be independent from the Consume code and outside of the consumer process instance.
/// </summary>
/// <remarks>
/// Be aware that the OnConsumeFailure handler is called on a new Consumer service instance,
independent from the call to Consume.
/// Any local field or property set during the Consume will not be available during OnConsumeFailure.
/// If needed, please define the original message fields to create a correlation between the original
message and a message reply.
/// </remarks>
/// <param name="message">the original message</param>
/// <param name="failureReason">the exception causing the failure</param>
/// <returns></returns>
```

```
public bool OnConsumeFailure(TMessage message, Exception failureReason);
}
```
The method OnConsumeFailure is called by the platform when the following conditions are met:

Immediately:

```
The Consume(Message) has raised an unrecoverable exception.
The ExecuteService on a consumer has raised an unrecoverable exception.
```
Deferred:

```
The Recovery service has failed to recover the message calling the faulting consumer for the MaxMessageLifetime period, exceeding this
threshold.
```
Notes:

The method OnConsumeFailure is a stateless compensation handler invoked whenever the consume of the message has failed. This method is called on
a new consumer instance, unrelated from the one used by the Consume(), with a fresh state and a new transaction context (top service).

In case further errors occur during the compensation execution, no further attempts will be made to re-execute this method.

The OnConsumeFailure returns a boolean true, in case the return is false the consumer compensation execution is aborted (ExecuteService is not called).

Examples:

```
OnConsumeFailure Example
```
```
public bool OnConsumeFailure(TestServiceInvoked message, Exception failureReason)
{
Debug.WriteLine($"Consume TestServiceInvokedConsumer_1 from {message.Origin} FAILED: {failureReason.
Message}");
if (message.PublishMessages)
{
var reply = GetFeature<MessageProducer>().CreateMessage<TestServiceReply>();
reply.Destination.Add(message.Origin);
reply.OriginalCOName = message.OriginalCOName;
```
```
switch (failureReason)
{
case MOMException:
reply.ErrorCode = (failureReason as MOMException).ErrorCode;
reply.Reason = (failureReason as MOMException).Key;
break;
```
```
case MOMUserException:
reply.ErrorCode = (failureReason as MOMUserException).ErrorCode;
reply.Reason = (failureReason as MOMUserException).Key;
break;
default:
reply.ErrorCode = -1;
reply.Reason = failureReason.Message;
break;
}
}
```
```
var testObject = Create<TestObject>();
testObject.Name = Guid.NewGuid().ToString();
testObject.CreatedBy = "TestServiceInvokedConsumer_1";
testObject.Output = $"Consumer1Failed";
return true;
}
```


# Message Extensibility

```
Use Cases
Contraints
Google Protobuf / Grpc proto
Repos Source Version handling
Message repos and model repos version configuration
Model configuration file
Project <Version> element
Nuget Package Version Handling for messages
Current status
Requirements and development needs
Requirements from the Feature Teams
Useful links and hints from the Feature Teams
Message package dependencies
Current status
Development on a new message repo version
```
## Use Cases

Receive old messages into a newer consumer. Consumer would handle missing new properties. (Relies on messages being backward compatible)

Receive new messages into an older consumer. Consumer would ignore new properties. (Relies on messages being forward compatible)

New message model might require new repo

Message model is modified to add additional data (v1.1.0)

Producer project updates message reference to V1.1.0

## Contraints

Binary protocol: any changes will cause incompatibilities.

Currently a bug. Messages need to be processed by any consumer (unless it is a real breaking change)

Customer needs to be able to extend messages

Message producer only sends message format associated with the model version referenced in it's project settings.

## Google Protobuf / Grpc proto

Capabilities:

References:

https://developers.google.com/protocol-buffers/docs/proto3#updating

## Repos Source Version handling

How do we handle versioning of messages

Tagging of the repo is allowed, but it since the message models are not submodules, the reference is binary and specific to the message.

The tag is applied to the repo, but a tag could indicate that it applies to a specific message model

Maybe we can continue using multiple repos for messages

```
This is a work in progress
```

Message repos and model repos version configuration

Message repos and model repos shall have similar version configuration for consistency across different domains. This approach simplifies configurator
requirements. Moreover, generated binary artifacts are tagged with similar version information.

Version is configured by means of:

```
Model configuration file
Project <Version> element
```
Model configuration file

Model configuration file <repo_name>.modelconfig.json has identical configuration for version:

```
Model configuration for version
```
### {

```
"name": "MaterialManagement",
"description": "TODO: Provide a description of the model.",
"version": "2.0.5",
"platformVersion": "2.1.7",
....
}
```
Model repos has an additional 'binaryDepenendecies' section within model configuration file to list the binary references towards message repos.

Project <Version> element

Project <Version> element is automatically managed by dotnet build. Dotnet build uses Version element to control the default values of all the version
numbers embedded in the build output (https://andrewlock.net/version-vs-versionsuffix-vs-packageversion-what-do-they-all-mean/).

That includes:

```
PackageVersion used to generate nuget packages (this is useful only for message repos)
AssemblyVersion and AssemblyFileVersion used to tag our binaries
```
Both message repos and model repos define a <MessageVersion> element to define the version. <Version> element refers to it in the following way:

```
<Version>$(ModelVersion)</Version>
```
Both message repos and model repos define the <ModelVersion> element in dependencies.props file. Hence, the definition of the model version is
consistent among different repos kinds.

```
dependencies.props file sample
```
```
<Project xmlns="http://schemas.microsoft.com/developer/msbuild/2003">
<PropertyGroup>
<ModelVersion>2.0.0</ModelVersion>
<PlatformVersion>2.1.7</PlatformVersion>
<ObservabilityVersion>2.0.0</ObservabilityVersion>
</PropertyGroup>
<ItemGroup>
....
</ItemGroup>
</Project>
```
For message repos the structure is flat, therefore the <Version> element is placed directly in csproj file.

For model repos: TBD

Nuget Package Version Handling for messages

In this paragraph we are going to address the issue of Nuget Package versioning of messages modules

```
TNT Message
O.M Message
M.M Message
```

### 1.

```
a.
```
```
2.
```
```
a.
```
```
3.
```
```
a.
```
### 4.

```
a.
5.
```
```
a.
```
in particular how to handle their reference from consumer modules

```
TNT
O.M
M.M
Factory
```
and when those Nuget Packages should be published (and being visible by the consumer modules).

Current status

```
The NugetPackage is published on the artifact registry ModMOM_Platform (siemens.com) only if the pipeline is run by the master/main branch
A consumer module uses the exact version of a published nuget package
```
```
<ItemGroup>
<PackageReference Include="M1_MaterialManagementMessages" Version="2.0.0-CI-20220328-145321" />
<PackageReference Include="M1_OrderManagementMessages" Version="2.0.0-CI-20220329-094552" />
</ItemGroup>
```
Prons: a consumer module uses only a stable nuget package of a module

Cons: if the developer forgets to update the module which consume the message, the stable version of the message is not actually used.

Cons: during the development phase it is hard to perform a test of a new package of the message (for example we wanted to deploy a consumer module
that has a reference to the nuget package we are just developing)

Requirements and development needs

The current status should be improved.

Requirements from the Feature Teams

Below the list of needs collected from the development teams:

```
Prevent breaking changes for the modules which consume the message nuget packages (TNT Message, O.M Message, M.M Message): changes
of the message module during the development phase, should not introduce regression for the stable consumer module
The guideline will provide a meaningful way to hook the version required without depending on the newer version available into the
nuget feed.
Stable versions of nuget packages should be automatically used by the consumer modules if the major version is not changed: no changes of
consumer cs project should be necessary in this case
This is not a valid requirement: it conflicts with point number 1, the guideline will provide a specific constraint to declare exactly which
stable version a module needs to be referenced.
It should be straightforward the test of a nuget package during the development phase (deploying on openshift/chimera scenario), even if the
major version is going to be changed.
The guideline should provide a way to acquire a newer development version, without affecting the module stable release, to test
beforehand the switch to another version. After that the developer will declare on the message repo the new version as stable (which is
not yet referenced by any modules) and the modules are then migrated to use the new stable version (like the platform use).
Prevent the proliferation of nuget packages published on the artifact registry ModMOM_Platform (siemens.com)
This is out of scope, but stable versions are clearly defined and so a cleanup policy could be implemented by TFS Infrastructure team.
When a new Platform version is available, message repos are not force to be upgraded (for metadata framework and interfaces references), even
though the publisher/consumer modules are upgraded to the newer platform version.
This means that the messages are using an older definition of platform types, which are compatible (at runtime) with the new platform
version. Upgrade would be mandatory only when the platform declare a compatibility break into the Release Notes.
```
Useful links and hints from the Feature Teams

```
To avoid package proliferation (point4)
```
https://docs.microsoft.com/en-us/azure/devops/pipelines/policies/retention?view=azure-devops&tabs=yaml

https://cloudblogs.microsoft.com/industry-blog/en-gb/technetuk/2019/06/18/perfecting-continuous-delivery-of-nuget-packages-for-azure-artifacts/

```
separate the packet stable versions from the preliminary ones
specify the next version of the nuget package on running the pipeline as described in the picture below (and as already done for the modules)
```

Message package dependencies

The goal of this paragraph is to describe how package reference are currently defined in the Message modules

Current status

Below an example of how package reference are defined for M1_TrackAndTraceMessages/M1_TrackAndTraceMessages.csproj :

```
<PackageReference Include="Camstar.Metadata.Aspects.Fody" Version="2.0.0-*"/>
<PackageReference Include="Siemens.MOM.MetaModel.Framework" Version="[2.0.0-*,3)"/>
<PackageReference Include="Siemens.MOM.MetaModel.Interfaces" Version="[2.0.0-*,3)"/>
```
Development on a new message repo version

Proposed solution:

Steps:

Branch A: Message Repo
Version 1.2.4 -> CHANGELOG.MD<->Dependencies.props; modelconfig.json; + *.cs
Build -> nuget messages.1.2.4-A.nupkg (pre-release from branch A) -> Nuget FEED (used only to test, see later from module Branch A)


Branch B: Message Repo
Version 1.2.4 -> Dependencies.props; modelconfig.json; ...cs
Build -> nuget messages.1.2.4-B.nupkg (pre-release from branch B) -> Nuget FEED (used only to test, see later from module Branch A)

Branch A: From Module X
Messages 1.2.4-A -> Dependencies.props;
Messages 1.2.4 -> Modelconfig.json: binary_dependencies
Build -> ModuleX.image-A
Release -> Deploy ModuleX.image-A on a test target

Branch B: From Module X
Messages 1.2.4-B -> Dependencies.props;
Messages 1.2.4 -> Modelconfig.json: binary_dependencies
Build -> ModuleX.image-B
Release -> Deploy ModuleX.image-B on a test target

...

stable releases:

Branch A Message Repo: Merge to Main (stable) ** driven by the modelsettings.json and validate against the changelog.md -> (is this ok? right now it is
like this: https://tfs05mom.industrysoftware.automation.siemens.com/MOM/ModularMOM/_git/M1_Common?path=%2FautomationPipeline.yml)
Merge PR Build to main -> nuget messages.1.2.4.nupkg (stable from main) -> Nuget FEED ; Pipeline verifies consistency of the version merged (version >
the current latest stable version) and performs the stable tag.

Branch A: From Module X :
Messages 1.2.4 -> Dependencies.props;
Messages 1.2.4 -> Modelconfig.json: binary_dependencies
Merge PR to main -> ModuleX.image-1.0.0-main-1 (pre-release)
A: From Module X :
Manual Build: (flag stable) -> ModuleX.image-1.0.x (Messages 1.2.4)

Message Repo B: Merge to Main (stable) (main is already 1.2.4)
Merge PR Build to main (validation):
checkpoints: check that model config version is less or equal than the current -> PR FAIL
From PR FAIL-> User must Increment the modelconfig.json,dependecy.props, and changelog.md accordingly, example: 1.2.5
Build again:
-> nuget messages.1.2.5.nupkg (another stable from main) -> Nuget FEED

B: From Module X:
Messages 1.2.5 -> Dependencies.props;
Messages 1.2.5 -> Modelconfig.json: binary_dependencies
Merge PR to main -> ModuleX.image-1.0.x-main-1 (pre-release)
Build a new stable after the tests.
Manual Build: (flag stable) -> ModuleX.image-1.0.x (Messages 1.2.5)

-------
Mandatory:
when the version of Message Model is changed in the main, Module repo MUST be aligned accordingly (implementing the messages). Master branch
means "stable".
It is mandatory to test the changes before making a "stable" version of the message model, and so testing messages changes, from the modules in dev
branches, before starting the activity to merge the final message definition to main.
Why:
If a new stable package of the messages is published, and another dev team is also working on changes on the same models, they will get the new
version of the messages from another team, without having the implementation on the module side. This may happen but if the window is small (just a
matter of creating the pull request with the new stable module) it is not a blocker. If instead the dev team abandons the changes on the module, this is a
permanent break.


# Message resiliency - Detailed Design

## Base Flow with Failures

## Message State Machine


Detailed Flow

Client Side


Server Side


PersistentMessage

The persistent message record should have the following structure

```
ColumnName Type Mandatory Note
```
```
Id (Implicit as CO) string yes
```
```
State string\Enum yes
```
```
MessageBody Blob yes
```
```
Lock GUID no Define the Id of the lock service, a field for provide who has lock for the message
```
```
LastActivityTime DatetimeOffset yes
```
```
OriginalMessageID string yes It's different from the Id value
```
```
OriginalMessageType string yes Needed for serialization\deserialization
```

# Module name resolution

When you set the IMessage.Destionation.Add("ModuleName") and produce a message, these main steps are performed by the Platform.

## Steps

## Network resolution override in Appsettings.json

You can override of the default behavior in Network section under ModularMOM:

```
Network section
```
### {

```
"ModularMOM": {
"Network": {
"Services": {
"Module1": "http://localhost:5002",
"Module2": "http://localhost:5012"
}
}
}
}
```
You could put this file content into appsettings.Development.json and select this override with the environment variable ASPNETCORE_ENVIRONMENT
='Development'

Notes:

```
There is no need to hard-code entries for Services under the Network section of the main appsettings.json file by using environment variables
for each one, because the network name "Module1" or "Module2" will be resolved by the kubernetes DNS, looking for the specific service
resource installed by the deployment. Conversely, if you need to use different settings because you need to redirect the name resolution to
other endpoints, you should provide a proper override file appsettings.<your_environment>.json and set a local ASPNETCORE_ENVIRONMEN
T variable to '<your_environment>' before starting the process or pod.
```

```
If you need to start more than one instance in debug mode (on your Visual Studio VDI) you must use different ports for each instance, for both
GRPC and the RestAPI endpoints:
```
Example PS1:
Instance 1:
$env:MODMOM_GRPC_PORT=5002; $env:ASPNETCORE_URLS='http://localhost:5000'; $env:ASPNETCORE_ENVIRONMENT='Debug';.
\Siemens.MOM.Platform.Api.exe
Instance 2:
$env:MODMOM_GRPC_PORT=5012; $env:ASPNETCORE_URLS='http://localhost:6000'; $env:ASPNETCORE_ENVIRONMENT='Debug';
.\Siemens.MOM.Platform.Api.exe

```
and provide the appsettings.<>.json file with the services maps pointing to each grpc port, example: 'Module2': 'http://localhost:5012'
```
```
A early KDD was to set the grpc port fixed (5002) with protocol http for the normal deployment. You can change it anyhow on your machine to dev
/test the system and when you have multiple instances running on the same machine, otherwise they will not start.
The aforementioned KDD was due to the way the aspnet core stack behaves when you are sharing the port for both GRPC and API:
If you have https enabled on the ASPNETCORE_URLS for the main port (let's say: 5000) the client should support the TLS negotiation
'ALP' mechanism on that port to select HTTP1 or HTTP2 protocol version (the latter used by GRPC protocol), however this configuration
will not work if the port is both shared and bound to the plain HTTP without TLS (the chosen default). You cannot have both HTTP1 and
HTTP2 middlewares on the same port in plain http (aspnet core does not detect this info from the headers).
The API Gateway shall perform TLS termination, so the external requests are in https and then in http inside the same K8s namespace.
The HTTP2 protocol for RestAPI is not needed (it does not give any additional benefits), however we should try to enable it for the UI
NGNIX Module to speed up the page loading (to investigate how, regressions). Most browser supports the http2 already.
```

# Pub/Sub Communication

```
ModelNATS Mappings
Naming Conventions
Use cases
Changes
Open Points
Diagram
NATS Configuration
Stream configuration
Consumer configuration
Deploying Nats Jetstream on Kubernetes
Security Concept
Authorization process
```
## ModelNATS Mappings

What we have in the model framework:

```
Message Type (M)
MessageProducer(Typeof(M))
MessageConsumer(Topic1,Topic2), IMessageConsumer<M> (Topic1 | Topic2)
```
Message type M may define a set of destinations which will be interpreted as follow:

```
MessageType (Command, Event) Event
Destination[] : Topic[]
```
### NATS:

```
Object Mapping Description
```
```
Stream <environment>_events appsettings environment id (default:
"modmom")
```
```
Subject Publisher: See Subject Naming Convention, Consumer: MessageConsumer Attribute Subject:
see Subject Naming Convention
```
```
Consumer One consumer for each topic and application (ie, one consumer for each subscription
configuration): see Durable Consumer Naming Convention
```
```
DeliveryGroup
(Queue)
```
```
ApplicationName Competition on the same queue for all
the module instances
```
Stream = CreateStream(Environment)

Consumer = CreateConsumer(Stream)

PushSubscription.WithConsumer(Consumer).WithSubject(Subject).WithCallback(OnReceiveMessage).WithDeliveryGroup
(ConsumerTypeFullName<MessageTypeFullName>)

PushSubscription.WithConsumer(Consumer).WithSubject(Subject).WithCallback(OnReceiveMessage).WithDeliveryGroup
(ConsumerTypeFullName<MessageTypeFullName>)

## Naming Conventions

Syntax Constraints:

TBD

Subject Naming Convention:

Option 1: "<environment>.events.<topic>"

Option 2: "<environment>.events.<topic>.<message type>.<version>"


```
1.
```
```
2.
```
Option 1 is selected. Handling of the backward/forward compatibility should be handled by the Platform receiving the message.

TBD: define publish and subscribe differences and the protocol (when to ack/reject if the message is not handled by the receiver).

Durable Consumer Naming Convention:

"<envirionment>_events_<appname>_<topic>"

Use cases

Multiple consumers on a given message subscribed with different topics.

Example:

Message: CreatePo

Producer: OrderManagement

Consumers (Topic: "CentralShopfloor"):

```
TrackAndTrace (2)
Material (2)
```
Consumers (Topic per Line):

```
Custom TrackAndTrace (Line 1)
Custom Material (Line 1)
```
Message aps

Producer: OrderManagement

No consumer defined in a scenario.

One consumer defined in a scenario.

These are the basic communication patterns we need to verify:

Test Cases:

```
# Description Expectation Notes
```
```
1 Send a message without any consumers running
(but previously created) to the temporary
unavailable set of consumers.
```
```
Multi/Single instance:The message is stored in
NATS. When the consumers restart, the message is
consumed by one instance for each module
subscribed.
```
```
2 Send a message with the consumers running. One
consumer of one instance crashes processing the
message (no ack).
```
```
Multi instance: The message is delivered again
to the BL of another Consumer instance (by
competition/balancing).
Single instance: The message is delivered
again to the BL of the same Consumer
instance, after the process has been restarted.
```
```
3 Send a message to a not-existent Consumer
(never deployed, never subscribed)
```
```
The message is discarded on publish by NATS
/Platform (no retry needed, no one is interested).
```
```
Even if the message is temporary stored in the
stream, the message should be discarded
thereafter.
```
```
4 Send a message to multiple running Consumers in
fan out. Two consumer modules are interested in
the same topic on the same message type.
```
```
The message is consumed by the two modules, in
one instance for each module.
```
```
5 Send multiple messages to multiple running
Consumers in competition. Multiple Consumer
instances subscribed on the same topic and
message type.
```
```
The messages are consumed one by one instance,
in a balanced way, among the consumers pool of the
same module.
```
```
6 Send multiple messages types with the same topic.
Verify the consumers receive and execute the
message of the correct type.
```
```
No messages are delivered to the wrong consumer
type and no errors are returned to the NATS broker.
```
```
In case the receiver logic gets messages which is
not interested by, the message is rejected to give
the possibility to be received by another consumer.
```
Changes


```
Add appsettings and implementation environment under ModularMOM the Environment id "ModularMOM:Environment" =
$$MODMOM_ENVIRONMENT$$
Add appsettings and implemenation in IntegrationConfiguration settings for NATS "ModularMOM:PubSubMode"="NATS"; "ModularMOM:
Messaging:NATS:..."
Change Connection handling to NATS (stay connected, retry if not) otherwise the platform is crashing at startup.
Change GetMessageConsumingServicesFor(msgEntity) to return all the consumer for a specific topic (from the message arrived)
Version information in the IMessage (support the model version).
Expose on Nats Msg all the relevant information regarding the IMessage into the headers (type, version,...)
Add the type of the message in IPersistentMessage "OriginalMessageKind" Server Recovery has to skip Kind=Event if the recovery is
implemented from the NATS service.
Add the version of the message sent or received on the IPersistentMessage (diagnostic)
```
Open Points

```
NatsCore vs JetStream different api (JetStream supports both in memory and persisted messages, depending on the server configuration) but
they are functionally different APIs. Can we just support JetStream? Answer: Yes
Message Consumers on multiple IMessageConsumer<T> messages: is this supported for pub/sub? Answer: on Hold. The business logic could
always define a single consumer class per message type.
```
Diagram


NATS Configuration

Stream configuration

https://docs.nats.io/nats-concepts/jetstream/streams

https://docs.nats.io/using-nats/developer/develop_jetstream/model_deep_dive

```
Name MOM Value Default
NATS
Value
```
```
Notes on MOM Value
```

```
Name 'environment_eve
nts'
```
```
Unique stream for each environment.
```
```
Subjects 'environment.
events.>'
```
```
One single subject that filters all the events for that environment.
```
```
StorageT
ype
```
```
File File Stream is persistent.
```
```
Retentio
nPolicy
```
```
Interest Limit Messages are kept as long as there are Consumers on the stream (matching the message's subject if they are filtered
consumers) for which the message has not yet been ACKed.
```
```
DiscardP
olicy
```
```
New Old When stream limits are reached, new messages are refused (no data loss).
```
```
MaxMsg
Size
```
```
10 MB
Configurable via
appsettings
MaxMessageSize
```
```
1 MB
```
```
shown as
unlimited
though
```
```
MaxAge? seems
unlimited
```
```
Age for oldest message that will be kept. Keep it very high to reduce discarded messages.
```
```
MaxBytes? seems
unlimited
```
```
Maximum bytes occupied by the stream. Keep it very high to reduce discarded messages.
```
```
MaxMes
sages
```
```
? seems
unlimited
```
```
Maximum number of messages in the stream. Keep it very high to reduce discarded messages.
```
```
MaxMes
sagesPe
rSubject
```
```
? Keep same as MaxMessages
```
```
MaxCons
umers
```
```
? seems
unlimited
```
```
Duplicate
Window
```
```
? 2 minutes JetStream support idempotent message writes by ignoring duplicate messages as indicated by the Nats-Msg-Id header.
Duplicate Window is the duration to hold message ids for duplicate checking purposes. When a message is older than this
duration, duplicate check is not performed anymore.
```
Consumer configuration

https://docs.nats.io/nats-concepts/jetstream/consumers

Defining the DeliverySubject or the FilterSubject causes the consumer to be a 'push' consumer and enables async consumption of messages.

```
Name MOM Value Default NATS
Value
```
```
Notes on MOM Value
```
```
Durable
Name
```
```
See Durable Consumer Naming
Convention
```
```
Not set (ie, the
consumer is
ephemeral)
```
```
The name of the Consumer, which the server will track, allowing resuming consumption where left
off.
Defining this name makes the consumer persistent.
```
```
Delivery
Subject
```
```
NA Autogenerated
identifier for inbox
(example: '_INBOX.
6QLd3oM5xbDi6-
Knkpi9pR')
```
```
FilterSub
ject
```
```
Same as related subscription No filter applied See Subject Naming Convention.
```
```
DeliverP
olicy
```
```
All All Delivers all messages that are available. Other options will cause data loss.
```
```
DeliverG
roup
```
```
Application Name Queue group name to enable competition among different instances of the same application.
```
```
AckPolicy Explicit Explicit Each individual message must be acknowledged.
```
```
AckWait 45 seconds
Configurable via appsettings
MaxConsumeTime
```
```
30 seconds Ack Wait is the time in milliseconds that the server will wait for an ack for any individual message on
ce it has been delivered to a consumer. If an ack is not received in time, the message will be
redelivered.
```
```
MaxAck
Pending
```
```
Calculated depending on
number of virtual processors
```
```
1000 It sets the maximum number of messages without an acknowledgement that can be outstanding,
once this limit is reached message delivery will be suspended. It cannot be used with AckNone ack
policy. This maximum number of pending acks applies for all of the consumer's subscriber
processes. A value of -1 means there can be any number of pending acks.
```
```
MaxDeli
ver
```
```
Calculated on the basis of
appsetting configurations for
recovery
(RecoveryMaxMessageLifetime
and RecoveryCycleTime)
```
```
unlimited? The maximum number of times a specific message will be delivered. Applies to any message that
is re-sent due to ack policy.
Recovery is handled directly by NATS server. Messages as resent when AckWait time expires or a
recoverable error occurred on some consumers.
```
```
ReplayP
olicy
```
```
Instant Instant The messages will be pushed to the client as fast as possible while adhering to the Ack Policy, Max
Ack Pending and the client's ability to consume those messages.
```

Deploying Nats Jetstream on Kubernetes

https://docs.nats.io/running-a-nats-service/nats-kubernetes

```
helm repo add nats https://nats-io.github.io/k8s/helm/charts/
helm install nats nats/nats --set nats.jetstream.enabled=true --set nats.jetstream.fileStorage.size=1Gi
```
In case of status Pending on Persistent Volume Claim in microk8s, please follow this Configuration of Microk8s/Ubuntu
VDI#PersistentVolumesonLocalhostpath.

Expose the server tcp port with port-forwarding:

```
kubectl port-forward --address=0.0.0.0 nats-0 4222:4222
kubectl port-forward --address=0.0.0.0 nats-0 8222:8222
```
use nats CLI to check the status from the host to the container:

```
nats server check
OK Connection OK:connected to nats://127.0.0.1:4222 in 3.916813ms OK:rtt time 492.571μs OK:round trip took
0.000638s | connect_time=0.0039s;0.5000;1.0000 rtt=0.0005s;0.5000;1.0000 request_time=0.0006s;0.5000;1.0000
```
Rest Monitor API at [http://localhost:8222](http://localhost:8222)

Simple pub/sub test:

```
term1: nats sub ">"
term2: nats pub test --count 10 "Message {{Count}} {{ID}}: {{ Random 10 100 }}"
```
Security Concept

The NATS server provides various ways of authenticating clients:

```
Token Authentication ( definition of a predefined token: "secret", used during connect string like nats://secret@<host>:4222 )
User/password (definition of predefined user/pass pair in the configuration, password can be bcrypted using the nats tool "nats server passwd")
TLS Certificate (TLS Client authentication supported: the server can verify a client certificate using a CA certificate with the option "tls {verify}" set
to true). Mapping of a user id from certificate is also supported .Due to the nature of the TLS upgrade mechanism NATS uses, using a TLS
Terminating Reverse Proxy with NATS is not supported. However, there are possible workarounds.
NKeys with Challenge (key system based on https://ed25519.cr.yp.to/, custom for NATS connection with a challenge verification, nonce based,
with a signature. nk tool can be used to generate the user key pairs, client will have to implement with the NATS API the secret handling).
JWT Authentication/Authorization (authentication schema to improve user account and authorization). Note: JWT must be signed with ED25519
keys, NATS adopts the convention that all Issuer and Subject fields in a JWT claim must be a public NKey. So it cannot be used with external
OIDC Identity provider. It is just a glorified NKeys authentication to correlate user ids with an identity.
```
Considerations:

Even if adopting the TLS client authentication would be better in a final well-designed scenario (with Certificate Authorities capable of providing single
signed certificate for each application), for the first step, and in order to simplified the deployment process in absence of an optional infrastructure, we can
rely on a similar asymmetric approach, like the one NKeys provides, with the goal of having a better self-contained solution out-of-the-box.

Possible direct or indirect external application access should be taken into account. It would be beneficial to enable multiple ways to configure the NATS
cluster in order to let the customers to choose which is the best one for their requirements.

To summarize:

```
Self contained deployment: NKeys authentication of any application modules in a specific namespace. Namespace separation through single
/multiple broker service deployed for each namespace:
The applications running a namespace cannot communicate on another namespace (this is true in general and enforced anyhow by
Kubernetes routing rules, but they are not mandatory).
Keys are generated and configured by the deployment process of NATS in each namespace (to be implemented).
Modules acquire the namespace secret key through Kubernetes configuration (SecretKeyRef) and use it to open the connection to the
NATS broker (to be implemented).
Generic Enablement: User/pass authentication in case of 3rd party connections.
```
As long as the NATS deployment is simple to integrate within a scenario for each tenant, this would be an adequate mapping to isolate and to control
accesses:


```
A user of a namespace (let say, development) whose has no access to another namespace (let say, production), cannot interact with applications
deployed on that external namespace. The secrets are kept and stored securely within the Kubernetes namespace and the access are regulated
though Kubernetes RBAC access rules. The "namespace key" is shared across all the applications deployed in a given namespace, it represents
the key to validate the access to a local namespace NATS broker.
Having a separate key for each application is possible but it does not add anything more in term of security, so a coarse grained (per namespace)
approach is better: having access to a namespace is more than enough to see each secret key and the management of this fined grained key
handling is clearly more difficult and without any added benefit.
```
Future developments and assumptions:

NATS has a very scalable architecture. It will be also possible to have a single (high performance) NATS cluster per tenant (on prem, it could be also a
physical one). In this case the above considerations still hold: the key for each environment has to be registered to provide authentication for each
namespace (by a manual procedure or by onboarding). It is also possible to create leaf nodes to extend the cluster beyond network boundaries and have a
local broker per namespace. This depends on the requirements of network deployment and the level of customizations choosen.

Key Rotation:

Key rotation is also a simple feature to add, automating the generation of a new key pair, registering the new public key on NATS and updating the secret
shared across the apps.

Authorization process

NKeys roles are:

```
Operators
Accounts
Users
```
Roles are hierarchical and form a chain of trust. Operators issue Accounts which in turn issue Users. Servers trust specific Operators. If an account is
issued by an operator that is trusted, account users are trusted.

From an authorization point of view, the account provides information on messaging subjects that are imported from other accounts (including any ancillary
related authorization) as well as messaging subjects exported to other accounts. Accounts can also bear limits, such as the maximum number of
connections they may have. A user JWT can express restrictions on the messaging subjects to which it can publish or subscribe.


Check Leaf nodes intercommunication for 3rd party remotes.


# Observability Layer

## Instrumentation - Observability Layer Map

This the internal Instrumentation class organization and the flow used to obtain an activity

More information on Weaver can be found here: Observability Weaver

## Use Cases

## Observability Runtime Configuration changes denial process Use Case

The chose approach of changes the Runtime configuration of Observability layer follow this document (Section 4)


We need to be able to provide a way to disable the log level propagation for ModMOM app behind our control in case the 3° requires it.


### 1.

### 2.

# Instrumenting Applications

Instrumenting an application indicates the creation of Activity primitives in specific code locations to monitor the application's performance and capture
information about application behavior at runtime to be used for diagnosis.

This operation can be performed only in specific parts of the code (libraries, frameworks, methods inside the libraries) that must be identified, keeping in
mind that code coverage and tracing scenarios in production must be monitored.

## Workflow

```
Identify the parts of the code to be instrumented
Set the configuration level of the instrumentation
```

# Identifying the Code Parts to be Instrumented

To identify the code parts you have to add attributes in a platform library or a Model library in order to produce the traces to be analyzed.

This operation can be performed by using attributes (available at class level and method level) to model Observability or manually by using the Observabilit
y wrapper.

## Setting Observability to a Class

```
Type Attribute Inherited attributes Description
```
```
No attribute No attributes in hierarchy No methods are Observable
```
```
[Observable] No attributes / [Observable] attribute / [NotObservable] attribute in hierarchy All methods are Observable
```
```
No attribute Inherits directly or indirectly from type marked as [Observable] All methods are Observable
```
```
[NotObservable] No attributes / [Observable] attribute / [NotObservable] attribute in hierarchy No methods are Observable
```
```
No attribute Inherits directly or indirectly from type marked as [NotObservable] No methods are Observable
```
```
No attribute Both attributes in hierarchy, [Observable] is the most immediate All methods are Observable
```
```
No attribute Both attributes in hierarchy, [NotObservable] is the most immediate No methods are Observable
```
This is a graphical example of Observability configuration in a class hierarchy:

```
Class observability code example
```
```
[Observable]
public class Factory
{
// This class is Observable
}
```
```
[NotObservable]
```

### 1.

### 2.

```
public class SpecialFactory : Factory
{
// This class is not Observable
}
```
```
public class SuperSpecialFactory : SpecialFactory
{
// This class is not Observable because it has SpecialFactory as immediate base class
}
```
```
public class AnotherFactory : Factory
{
// This class is Observable because it has Factory in its own hierarchy chain
}
```
Setting Observability to a Method that belongs to an Observable class

```
Attribute Description
```
```
No attributes on method Method is Observable and trace level set to Informational (default value)
```
```
[DoNotTrace] Method is not Observable
```
```
[Trace(ActivityLevel.Debug)] Method is Observable and trace level set to the one specified. Allowed values: Informational, Debug
```
```
Method observability code example
```
```
[Observable]
public class Factory
{
// This class is Observable
```
```
public void DefaultMethod()
{
// This method is Observable, as it belongs to an Observable class. It has default
ActivityLevel.
}
```
```
[DoNotTrace]
public void HiddenMethod()
{
// This method is not Observable
}
```
```
[Trace(ActivityLevel.Debug)]
public void DebugMethod()
{
// This method is Observable, as it belongs to an Observable class. It has a configured
ActivityLevel.
}
}
```
Instrumenting code platform using the Observability wrapper

```
Add references to Siemens.MOM.Platform.Observability.
```
```
Instrumenting MetadataEngine
```
```
To Instrument MetadataEngine classes DO NOT use aspect oriented attributes, but call the Observability wrapper instead.
```
```
This is due to the fact that we have two different observability weavers, one referenced by metadata models and the other referenced by
platform. These weavers share the same attributes. So, if you reference platform observability weaver to instrument MetadataEngine you will
end up in dependency conflicts on the metadata models.
```

### 2.

### 3.

```
On each class to be instrumented define a field of type ActivityWrapper and instantiate its value in the constructor or during the initialization by
using the Factory implemented in the class ObservabilityManager.
Example:
```
```
private readonly ActivityWrapper _activitySource;
```
### ...

```
_activitySource = ObservabilityManager.GetActivityWrapper(this);
```
```
Do either of the following:
Pass the object to GetActivityWrapper to define it as information source of the activity name.
If the object instance is not available, use the GetActivityWrapper overload and replace 'Type' with 'Object'.
Example:
```
```
_activitySource = ObservabilityManager.GetActivityWrapper<classType>();
```
Additional Operations

```
To instrument all methods, place an instruction at the beginning of the method to call one of the overloads of StartActivity in the ActivityWrapper
class as follows:
```
```
using var activity = _activitySource.StartActivityInformation();
```
```
To instrument a part of a method, use the statement below:
```
```
//outside span
```
```
using(var activity = _activitySource.StartActivityDebug())
```
### {

```
//instrumented
```
### }

The following code is an example of the available operations:

```
public class MyClass
{
private IGovernanceGateway _governanceGateway;
protected readonly ActivityWrapper _activitySource;
```
```
public MyClass (IGovernanceGateway governanceGateway)
{
_governanceGateway = governanceGateway;
_activitySource = ObservabilityManager.GetActivityWrapper(this);
}
```
```
public void MyMethod()
{
using var activity = _activitySource.StartActivityInformation();
```
```
Notes
```
```
All overloads of StartActivity return an Activity as defined in https://docs.microsoft.com/en-us/dotnet/api/system.diagnostics?view=net-5.0
All overloads of StartActivity can support the same argument as the Microsoft method https://docs.microsoft.com/en-us/dotnet/api/system.
diagnostics.activitysource.startactivity?view=net-5.0 in addition to the level indication.
If it is required a reference different from the one used creating the activity source wrapper, pass an object to startActivity.
```

// all body instrumented
}

public void Method2()
{
//outside span

using(var activity = _activitySource.StartActivityDebug())
{
//instrumented
}

//outside span
}
}

public static MyStaticClass
{
//Initialization of source wiyh type
private static ActivityWrapper _activitySource = ObservabilityManager.GetActivityWrapper<MyStaticClass>();

public static void MyStaticMethod()
{
using var activity = _activitySource.StartActivity(TraceActivityLevel.Informational, ActivityKind.
Internal);

// all body instrumented
}
}

public class DerivedClass : MyClass
{
public void MySpecilizedMethod()
{
//using base class wrapper (same assembly) but activity take name from this
using var activity = _activitySource.StartActivity(this, TraceActivityLevel.Debug);

// all body instrumented
}
}


# Configuring the Application for the Deployment

The following procedure allows you to set the available configuration levels for the instrumentation configurability:

```
disable/enable instrumentation of the entire library/framework
filter instrumentation of some methods inside the library/framework
```
## Procedure

Depending on the operations to perform, modify the Observability path in the appsettings.json file as described in the example below:

### {

```
"ModularMOM": {
```
```
"Observability": {
"Instrumentation": {
"DefaultLevel": "Informational",
"Providers": [
"Siemens.MOM.Platform.Application",
"Siemens.MOM.MetaModel.Framework",
"Siemens.MOM.Platform.Information"
],
"Siemens.MOM.Platform.Application": {
"Level": "Debug"
}
},
"ModelInstrumentation": {
"Providers": [
"Camstar.Core.SampleModel"
]
},
"3rdPartyInstrumentation": {
"Providers": [
"AspNetCore",
"HttpClient",
"SqlClient",
"GrpcClient"
],
"SqlClient": {
"SetDbStatementForText": true,
"SetDbStatementForStoredProcedure": false,
"EnableConnectionLevelAttributes": false,
"RecordException": false
},
"GrpcClient": {
"SuppressDownstreamInstrumentation": false
},
"AspNetCore": {
"EnableGrpcAspNetCoreSupport": true,
"RecordException": false
},
"HttpClient": {
"SetHttpFlavor": false
}
},
//"Filters": {
// "Activity": {
// "Kind": "Internal"
// }
//},
"UseExporter": "jaeger",
"Jaeger": {
"Host": "localhost",
"Port": 6831
},
"Zipkin": {
```

```
"Endpoint": "http://localhost:9422/api/v2/spans"
},
"otlp": {
"Endpoint": "http://localhost:49165"
},
"Resources": {
"Service": "MOM Module"
},
"UseSampler": "AlwaysOn",
"TraceIdRatioBased": {
"rate": 0.5
}
}
}
}
```
Available Operations

```
Setting the default instrumentation level
```
```
Modify the path of ModularMOM:Observability:Instrumentation:DefaultLevel, allowed values: "Informational", "Debug".
```
```
If not present, the default value is "Informational"
```
```
Enabling platform providers
```
```
Modify the path of ModularMOM:Observability:Instrumentation:Providers, can contain the following array values: "Siemens.MOM.Platform.
Application","Siemens.MOM.MetaModel.Framework","Siemens.MOM.Platform.Information".
```
```
If not present, all available sources are enabled.
```
```
Configuring provider levels
```
```
Modify the path of ModularMOM:Observability:Instrumentation:ProviderName:Level, allowed values: "Informational", “Debug”.
```
```
Configuring providers for Data model
```
```
Modify the path of ModularMOM:Observability:ModelInstrumentation:Providers, can contain the array value "Model assembly name".
If not present, all related sources are enabled
```
```
Configuring levels for Data model providers
```
```
Modify the path of ModularMOM:Observability:ModelInstrumentation:ProviderName:Level, allowed values: "Informational", “Debug”.
```
```
Configuring third-party supported libraries
```
```
Modify the path of ModularMOM:Observability:3rdPartyInstrumentation:providers, can contain the following array values: "AspNetCore",
"HttpClient", "SqlClient", "GrpcClient".
```
```
If not present all supported libraries are enabled.
```
```
Note that you can also set the details of third-party libraries as follows:
```
```
ModularMOM:Observability:3rdPartyInstrumentation "SqlClient": {
"SetDbStatementForText": true,
"SetDbStatementForStoredProcedure": false,
"EnableConnectionLevelAttributes": false,
"RecordException": false
},
```
```
ModularMOM:Observability:3rdPartyInstrumentation "GrpcClient": {
"SuppressDownstreamInstrumentation": false
},
```
```
ModularMOM:Observability:3rdPartyInstrumentation "AspNetCore": {
"EnableGrpcAspNetCoreSupport": true,
"RecordException": false
},
```
```
ModularMOM:Observability:3rdPartyInstrumentation "HttpClient": {
"SetHttpFlavor": false
}
```

Configuring the Resources Service name

Modify the path of ModularMOM:Observability:Resources:Service, allowed value is any non empty string representing the service name.

If not present, the default value is "MOM Module"

Specifying the samplers

You can specify one of the following samplers:

```
ModularMOM:Observability:Sample:TraceRatioBased, default values = -1, 0
ModularMOM:Observability:Sample:UseSampler, default values = AlwaysOn, AlwaysOff, TraceIdRatioBased
```

### 1.

### 2.

### 3.

# OpenTelemetry

## Distributed Tracing

Debugging distributed applications such as microservices which consist of several communicating services is extremely hard. When something goes
wrong, inspecting log files or stack trace of individual services is not sufficient to pinpoint the root cause. To understand causality, correlation between logs
and operations of different services is required. Distributed tracing provides this correlation by tracking the journey of a single request as it is handled by
various services of an application. Using distributed tracing, the failure occurred in one service can be linked to the original request and all operations in
between. In distributed tracing, the trace context which contains in simplest form the unique id of the current request as well as the unique id of the parent
request, is captured and propagated from one service to another service.

In the following sections we discuss distributed tracing in .NET applications.

## OpenTelemetry

OpenTelemetry is a Cloud Native Computing Foundation (CNCF) project which offers a single set of APIs and libraries that standardize collecting and
transferring telemetry data. Telemetry data is collected at runtime and provides observability to system and users behavior. It is built on three pillars of traci
ng, logging, and metrics. The key terms in OpenTelemetry are:

```
Traces: track journey of a single request as it is handled by different services of an application.
Span: each unit of work or single operation within a trace.
```
```
A trace is a tree of spans containing a single root span which encapsulates the end-to-end latency for the entire request.
```
```
Span context: A span contains a span context, which is a set of globally unique identifiers that represent the unique request that each
span is a part of.
Context propagation: When the service makes a remote call to another service, the current span context is serialized and forwarded to
the next service by injecting the span context into the headers or message envelope.
Metrics: A metric is a measurement about a service, captured at runtime. OpenTelemetry defines three metric instruments: counter, measure,
and observer.
Logs: a timestamped text record either structured or unstructured. (they may also be attached to spans)
```
OpenTelemetry provides a single open standard, supports a variety of languages and backends (cross-platform), and represents a vendor-agnostic path to
capturing and transferring telemetry data to backends.

## OpenTelemetry .NET Workflow

This is a typical workflow for capturing, transferring, and exploring Telemetry data :

```
Instrumentation: to capture or generate telemetry data. We refer to each generated data item as an event or diagnostic event.
Collecting: data generated by instrumentation is usually collected using a listener. The collected data by listener then can be transferred or
exported to a backend. (in OpenTelemetry this step is performed by an exporter)
Visualization and reporting: data stored in a backend can be explored and analyzed by visualization and reporting.
```
Instrumentation

In .NET applications, the recommended way of instrumenting is by using .NET Activity API (https://opentelemetry.io/docs/net/getting-started/ ). The Activity
and associated classes are shipped as part of System.Diagnostics.DiagnosticSource nuget package. Version 5.0.0 of this package contains
improvements to Activity class which makes it more closely aligned with OpenTelemetry API specification. This means, users can instrument their
applications/libraries to emit OpenTelemetry compatible traces by using just the .NET Runtime. Users need to just take dependency on System.
Diagnostics.DiagnosticSource package version 5.0.0 or above to instrument their code. In https://github.com/open-telemetry/opentelemetry-dotnet/issues
/947, there exists a comparison between primitives of OpenTelemetry tracing API and .NET Activity API. Guidance for instrumenting using .NET Activity
API can be found in:

https://github.com/open-telemetry/opentelemetry-dotnet/blob/master/src/OpenTelemetry.Api/README.md#introduction-to-opentelemetry-net-tracing-api

In latest version of .NET Activity API, W3C standard is used by default for the format of unique identifiers inside the trace context. (Using this standard,
identifiers can be used cross-languages)

The following libraries are already instrumented ( https://github.com/open-telemetry/opentelemetry-dotnet ):

```
NET
NET Core
gRPC client
HTTP clients
Redis client
```
```
It is recommended to use OpenTelemetry for capturing and collecting telemetry data:
```
```
https://jimmybogard.com/building-end-to-end-diagnostics-opentelemetry-integration/
https://medium.com/opentelemetry/opentelemetry-beyond-getting-started-5ac43cd0fe26
https://blog.newrelic.com/product-news/what-is-opentelemetry/
```

```
SQL client
NBusService
```
As it can be seen from the links given above, it is easy to enable these instrumentations at application startup and to apply configurations.

Collecting

In .NET Activity API, the class ActivityListener can be used to listen to or collect diagnostic events. However, the common method (observed during
research) is using OpenTelemetry for listening to events and exporting them to a backend:

```
https://devblogs.microsoft.com/aspnet/improvements-in-net-core-3-0-for-troubleshooting-and-monitoring-distributed-apps/
https://www.youtube.com/watch?v=kfyCHtqk-Ts
https://www.youtube.com/watch?v=cN6G86dUNhU
```
```
using Microsoft.Extensions.DependencyInjection;
using OpenTelemetry.Trace;
```
```
public void ConfigureServices(IServiceCollection services)
{
services.AddOpenTelemetryTracing(
(builder) => builder
.AddAspNetCoreInstrumentation()
.AddJaegerExporter()
);
}
```
For the backend, the exporter libraries from OpenTelemetry are listed below:

```
Console
In-memory
Jaeger
OTLP (OpenTelemetry Protocol)
Prometheus
Zipkin
```
Summary


As discussed, usage of .NET Activity API for instrumenting libraries/applications and OpenTelemetry for collecting and exporting traces to a backend are
recommended. The aspects to be considered in case of using these libraries for distributed tracing in .NET are listed below:

```
Implementation effort:
Frequently used libraries such as ASP.NET core, HttpClient and SQL client are already instrumented. Therefore, to collect diagnostic
events from these libraries, just their instrumentation needs to be enabled at startup.
Instrumenting a new library or application using .NET Activity API requires relatively low amount of coding: https://github.com/open-
telemetry/opentelemetry-dotnet/blob/master/src/OpenTelemetry.Api/README.md#introduction-to-opentelemetry-net-tracing-api
To process, collect and export traces, OpenTelemetry SDK can be used with a few of lines of configuration: https://opentelemetry.io/docs
/net/getting-started/
Required dependencies:
instrumentation: Diagnostics.DiagnosticSource package version 5.0.0 or above (.NET Activity API)
collecting: OpenTelemetry packages.
Documentation
Quick start guides and documentation from OpenTelemetry: https://opentelemetry.io/docs/
Sample codes: https://github.com/open-telemetry/opentelemetry-dotnet/tree/master/examples
Active community
Maturity: 1.0.1 for OpenTelemetry .NET Tracing
The focus of OpenTelemetry has been on distributed tracing so far. Version 5.0.0 of Diagnostics.DiagnosticSource was released with.
NET 5 release.
OpenTelemery .NET Metric is in Alpha release status, but it is planned that with .NET 6, it will become GA: https://github.com/open-
telemetry/opentelemetry-dotnet/issues/1501
Types of telemetry data:
Logging in .NET is Activity aware. With ActivityTrackingOptions, it is possible to annotate logs with trace ids and span ids.
Activity has start and end time. (end-to-end latency of the entire request is available)
Configurability:
Enabling instrumentation (configurable at startup)
Adding user defined properties to Activity via tags, links, events, ...
Sampling which is useful to reduce the cost of tracing and reducing noise. (needs more investigation)
Defining a custom Sampler to filter out synthetic traffic, such as calls to a “/health” endpoint.
Configurable backend for receiving and storing traces.
```
```
Maintenance: not costly due to following a single open standard, for example:
Without changing existing instrumentation, different backends or implementations can be plugged in.
Integration with other services is easier as the system evolves.
Runtime overhead (see Performance Evaluation) :
When disabled, it seems practically it has no overhead.
When enabled, the overhead is under control for example by sampling or controlling the amount of data which is generated per
diagnostic event.
It seems whenever possible API is asynchronous and does not block the application.
```

# Observability Architecture

## Overall Design

## Current Setup


Local Setup

Code Design: Tracing



# Reference Materials

Quick Reference on API:

Open API Spec
adidas API Guidelines: Guidelines for the API design and development at adidas
Microsoft REST API Guidelines: Microsoft's internal company-wide REST API design guidelines


# Distributed Tracing


# Distributed Tracing Configuration

## Compile Time Configuration

```
Allows to have different instrumentation for debug and release builds
Not available for .NET Activity API
```
## Runtime Configuration

```
All or nothing: Boolean configuration
When creating OpenTelemetry tracer providers, you can enable instrumentation for different sources (e.g. libraries) via the AddSource e
xtension method.
Different instrumentation levels like debug, information, error, ...
Not available for .NET Activity API (a wrapper has been implemented. see section Instrumenting code platform using the
Observability wrapper in Identifying the Code Parts to be Instrumented )
Filtering
OpenTelemetry tracer provider can be configured by defining a custom Processor to filter some of the Activities at runtime. (filtering is
applied when activity is stopped)
Processors can be added to a tracer provider after it is built.
```
```
Sampling: OpenTelemetry tracer provider can be configured to sample traces at runtime instead of listening to all traces. (a custom sampler can
be defined in addition to built-in samplers of OpenTelemetry) see Sampling.
Changing configuration on the fly, for example:
Enabling the instrumentation of a library at runtime without restarting the application (Note that enabling the instrumentation of a library is
only possible at the time of creation of an OpenTelemetry tracer provider and before it is built.)
Changing the level of tracing for different parts of the code.
```
```
Note
```
```
When using custom Processor, instead of using extension method to register the exporter, they must be registered manually.
```

# Sampling

The volume of telemetry data can increase exponentially with the number of deployed services. Naively tracing every request/transaction can cause both
application overhead as well as prohibitive cost in data centralization and storage. A strategic approach to observability data ingestion is required. Samplin
g which is the decision to capture or discard a specific trace is used to address the volume of trace data generated. In fact, sampling is widespread in
observability systems to lower the cost of producing, collecting, and analyzing data in systems.

## Types of Sampling

Head-based sampling

In head-based sampling (also referred to as “upfront” sampling), a sampling decision will be applied to a single trace when that trace is initiated. In this
type of sampling, the tracer creating the very first span in the trace will decide whether all other spans should be stored, and this decision is then
propagated down the chain via the regular context propagation mechanism. Due to its simplicity, it is the most common type of sampling. It reduces the
network traffic between the tracer and the collector. Since traces are randomly chosen upfront, "valuable" traces may be discarded.

The sampling type provided by OpenTelemetry SDK which can be configured at startup is head-based sampling. Using samplers, a decision is made
before creation of an activity based on information available before the activity is created such as TraceId. Currently, we have the following sampling
strategies:

```
constant: for accepting or dropping all traces
AlwaysOnSampler
AlwaysOffSampler
probabilistic: for sampling the traces based on a given probability
TraceIdRatioBasedSampler(rate)
```
In the probabilistic strategy, TraceIdRatioBasedSampler(rate), the requirement is that sampling decision is made solely based on TraceId. Therefore, all
spans belong to the same trace are either sampled or not sampled. (the sampling algorithm should be deterministic). Currently, since the sampling
algorithm is not specified, it is recommended to use this algorithm only for root activities by using Parentbased sampler (to avoid inconsistent results for the
same input). Parentbased sampler allows to make sampling decisions based on the sampling state of the parent activity. For example, for the root
activities we can use TraceIdRatioBasedSampler(rate) sampler and then all child activities will be sampled or not based on whether their root activity is
sampled or not.

Custom samplers can be implemented: demo. Note that implemented sampler should give unbiased samples which do not affect the data fidelity. (See the
link for more info) For example, rate limiting is an interesting sampling strategy which seems currently not to be available in built-in head-based samplers of
OpenTelemetry. (it is available in tail-based sampler of collector)

In our code, it is possible to configure these strategies for deployment.

Tail-based sampling

In tail-based sampling, sampling is done after traces have fully completed. Therefore, it typically requires, all spans (activities) for the same trace to be sent
to the same collector. The collector then usually waits a pre-defined amount of time, under the assumption that by then, all spans for the given trace were
received by the collector. Once a trace is deemed complete by the collector, a decision is made based on the trace’s characteristics such as specific tags
like error tags or trace duration, etc. In this approach, 100% of traces are observed and analyzed.

OpenTelemetry collector supports tail-based sampling. The tail sampling processor samples traces based on a set of defined policies, for example, policy
based on string attributes or number attributes. Currently, this processor only works with a single instance of the collector (see the issue). Moreover, the
sampling policies support, currently, simple scenarios (see the issue) and also (another issue). Note that there is a plan for deprecating tail-based sampling
and replacing it with several processors (see the issue).

OpenTelemetry collector also support Probabilistic sampling processor, but this should be of type head-based sampling where a sampling percentage is
configured.

Ideally we should be able to change the sampling strategy based on the current application behavior. This type of sampling is referred to as Adaptive
sampling and more info. One simple way doing that is by using configuration files.

Conclusion

In summary, we should consider the following points:

```
Sampling in distributed tracing solutions is necessary.
For sampling strategies, the important aspects are:
Generating "unbiased" samples (especially when we are developing our own statistical samplers)
Not to lose interesting traces like traces with errors or unusual latencies (this can be achieved with tail-based sampling where 100% of
traces are observed)
Sampling rate which determines the number of samples. The challenge is the choosing of the proper value for the sampling rate which
results in representative samples. Too-small samples can't be representative.
Each module can have its own sampling rate. Moreover, for each endpoint a different sampling rate can be configured.
Static sampling strategies vs. dynamic or adaptive sampling strategies. Ideally sampling strategies should be changed (manually or
automatically) based on the current program behavior at runtime.
As we have seen, in OpenTelemetry we can have sampling inside the application as well as outside the application in collector. Moreover, if we
use Jaeger as the backend, Jaeger collector can be configured with static sampling strategies through strategies.json. (see also the link)
Currently, in OpenTelemetry sampling strategies like TraceIdRatioBasedSampler(rate) or tail-based sampler are not mature.
```

# Performance Evaluation

## OpenTelemetry Benchmarking Reports

## Benchmarks.Trace.OpenTelemetrySdkBenchmarksActivity

BenchmarkDotNet=v0.12.1, OS=Windows 10.0.18362.1256 (1903/May2019Update/19H1) Intel Xeon Gold 6248 CPU 2.50GHz, 2
CPU, 4 logical and 4 physical cores .NET Core SDK=5.0.101 [Host] : .NET Core 3.1.10 (CoreCLR 4.700.20.51601,
CoreFX 4.700.20.51901), X64 RyuJIT DefaultJob : .NET Core 3.1.10 (CoreCLR 4.700.20.51601, CoreFX 4.700.20.51901),
X64 RyuJIT

```
Method Mean Error StdDev Gen 0 Gen 1 Gen 2 Allocated
```
```
CreateActivity_NoopProcessor 1,382.9 ns 27.43 ns 41.05 ns 0.0458 - - 680 B
CreateActivity_WithParentContext_NoopProcessor 196.1 ns 3.95 ns 10.89 ns 0.0033 - - 48 B
CreateActivity_WithParentId_NoopProcessor 430.7 ns 8.05 ns 14.73 ns 0.0134 - - 192 B
CreateActivity_WithAttributes_NoopProcessor 1,579.3 ns 30.91 ns 42.31 ns 0.0610 - - 880 B
CreateActivity_WithAttributesAndCustomProp_NoopProcessor1,744.1 ns 33.91 ns 55.72 ns 0.0706 - - 1024 B
CreateActiviti_WithKind_NoopProcessor 1,365.8 ns 27.08 ns 43.73 ns 0.0458 - - 680 B
```
## Benchmarks.Trace.TraceBenchmarks

BenchmarkDotNet=v0.12.1, OS=Windows 10.0.18362.1256 (1903/May2019Update/19H1) Intel Xeon Gold 6248 CPU 2.50GHz, 2
CPU, 4 logical and 4 physical cores .NET Core SDK=5.0.101 [Host] : .NET Core 3.1.10 (CoreCLR 4.700.20.51601,
CoreFX 4.700.20.51901), X64 RyuJIT DefaultJob : .NET Core 3.1.10 (CoreCLR 4.700.20.51601, CoreFX 4.700.20.51901),
X64 RyuJIT

```
Method Mean Error StdDev Gen 0 Gen 1 Gen 2 Allocated
```
```
NoListener 40.29 ns 0.836 ns 1.529 ns - - - -
PropagationDataListner 1,269.01 ns 24.909 ns 40.927 ns 0.0458 - - 680 B
AllDataListner 1,255.79 ns 24.515 ns 35.159 ns 0.0458 - - 680 B
AllDataAndRecordedListner1,249.13 ns 24.632 ns 35.327 ns 0.0458 - - 680 B
OneProcessor 1,316.80 ns 26.356 ns 51.405 ns 0.0458 - - 680 B
TwoProcessors 1,329.72 ns 26.520 ns 46.447 ns 0.0458 - - 680 B
ThreeProcessors 1,348.35 ns 26.612 ns 56.712 ns 0.0458 - - 680 B
```
## Benchmarks.Instrumentation.UninstrumentedAspNetCoreBenchmark

BenchmarkDotNet=v0.12.1, OS=Windows 10.0.18362.1256 (1903/May2019Update/19H1) Intel Xeon Gold 6248 CPU 2.50GHz, 2
CPU, 4 logical and 4 physical cores .NET Core SDK=5.0.101 [Host] : .NET Core 3.1.10 (CoreCLR 4.700.20.51601,
CoreFX 4.700.20.51901), X64 RyuJIT

Job=InProcess Toolchain=InProcessEmitToolchain

```
Method Mean Error StdDev Gen 0 Gen 1 Gen 2 Allocated
```
```
SimpleAspNetCoreGetPage443.7 s 8.78 s 21.70 s - - - 4.61 KB
```
## Benchmarks.Instrumentation.InstrumentedAspNetCoreBenchmark

BenchmarkDotNet=v0.12.1, OS=Windows 10.0.18362.1256 (1903/May2019Update/19H1) Intel Xeon Gold 6248 CPU 2.50GHz, 2
CPU, 4 logical and 4 physical cores .NET Core SDK=5.0.101 [Host] : .NET Core 3.1.10 (CoreCLR 4.700.20.51601,
CoreFX 4.700.20.51901), X64 RyuJIT

Job=InProcess Toolchain=InProcessEmitToolchain

```
Method Mean Error StdDev Gen 0 Gen 1 Gen 2 Allocated
```
```
InstrumentedAspNetCoreGetPage3.802 ms 0.9442 ms 2.694 ms - - - 8.08 KB
```
```
Summary
```
```
Runtime increase: 8.57 times
```

Benchmarks.Instrumentation.UninstrumentedHttpClientBenchmark

BenchmarkDotNet=v0.12.1, OS=Windows 10.0.18362.1256 (1903/May2019Update/19H1) Intel Xeon Gold 6248 CPU 2.50GHz, 2
CPU, 4 logical and 4 physical cores .NET Core SDK=5.0.101 [Host] : .NET Core 3.1.10 (CoreCLR 4.700.20.51601,
CoreFX 4.700.20.51901), X64 RyuJIT DefaultJob : .NET Core 3.1.10 (CoreCLR 4.700.20.51601, CoreFX 4.700.20.51901),
X64 RyuJIT

```
Method Mean Error StdDev Gen 0 Gen 1 Gen 2 Allocated
```
```
SimpleHttpClient 423.8 s 8.38 s 21.94 s - - - 6.08 KB
```
Benchmarks.Instrumentation.InstrumentedHttpClientBenchmark

BenchmarkDotNet=v0.12.1, OS=Windows 10.0.18362.1256 (1903/May2019Update/19H1) Intel Xeon Gold 6248 CPU 2.50GHz, 2
CPU, 4 logical and 4 physical cores .NET Core SDK=5.0.101 [Host] : .NET Core 3.1.10 (CoreCLR 4.700.20.51601,
CoreFX 4.700.20.51901), X64 RyuJIT DefaultJob : .NET Core 3.1.10 (CoreCLR 4.700.20.51601, CoreFX 4.700.20.51901),
X64 RyuJIT

```
Method Mean Error StdDev Gen 0 Gen 1 Gen 2 Allocated
```
```
InstrumentedHttpClient 460.1 s 9.11 s 22.35 s - - - 8.18 KB
InstrumentedHttpClientWithParentActivity463.8 s 9.00 s 14.28 s - - - 8.96 KB
```
Benchmarking the code using BenchmarkDotNet package

As a sample, the runtime increase of CreateService method of MetadataEngine was measured when it is instrumented.

```
Summary
```
```
Runtime increase: 8.56%
```
```
Summary
```
```
Runtime increase: 0.83%
```

# Code Design

## Instrumentation of Application Code with AOP Attributes

Setting Observability for a class

```
Attribute Description
```
```
No attributes in hierarchy No methods are Observable
```
```
[Observable] All methods are Observable
```
```
Inherits from Observable type All methods are Observable
```
```
Inherits from Observable type and [NotObservable] No methods are Observable
```
Configuring Observability in a Method belonging to an Observable class

```
Attribute Description
```
```
No attributes on method Method is Observable and trace level set to Informational (default value)
```
```
[DoNotTrace] Method is not Observable
```
```
[Trace(ActivityLevel.Debug)] Method is Observable and trace level set to the one specified. Allowed values: Informational, Debug
```
## Application Configuration for Deployment

The code below is an example of Observability fragment for appsettings.json file:

### {

```
"ModularMOM": {
```
```
"Observability": {
"Instrumentation": {
"DefaultLevel": "Information",
"Providers": [
"Siemens.MOM.Platform.Application",
"Siemens.MOM.MetaModel.Framework",
"Siemens.MOM.Platform.Information"
],
"Siemens.MOM.Platform.Application": {
"Level": "Debug"
}
},
"ModelInstrumentation": {
"Providers": [
"Camstar.Core.SampleModel"
]
},
"3rdPartyInstrumentation": {
"Providers": [
"AspNetCore",
"HttpClient",
"SqlClient",
"GrpcClient"
],
"SqlClient": {
"SetDbStatementForText": true,
"SetDbStatementForStoredProcedure": false,
"EnableConnectionLevelAttributes": false,
"RecordException": false
},
"GrpcClient": {
"SuppressDownstreamInstrumentation": false
```

### },

```
"AspNetCore": {
"EnableGrpcAspNetCoreSupport": true,
"RecordException": false
},
"HttpClient": {
"SetHttpFlavor": false
}
},
//"Filters": {
// "Activity": {
// "Kind": "Internal"
// }
//},
"UseExporter": "jaeger",
"Jaeger": {
"Host": "localhost",
"Port": 6831
},
"Zipkin": {
"Endpoint": "http://localhost:9422/api/v2/spans"
},
"otlp": {
"Endpoint": "http://localhost:49165"
},
"Resources": {
"Service": "MOM Module"
},
"UseSampler": "AlwaysOn",
"TraceIdRatioBased": {
"rate": 0.5
}
}
}
}
```
Instrumentation of Platform Code Using Observability Wrapper

C# code

```
public class MyClass
{
private IGovernanceGateway _governanceGateway;
protected readonly ActivityWrapper _activitySource;
```
```
public MyClass (IGovernanceGateway governanceGateway)
{
_governanceGateway = governanceGateway;
_activitySource = ObservabilityManager.GetActivityWrapper(this);
}
```
```
public void MyMethod()
{
using var activity = _activitySource.StartActivityInformation();
```
```
// all body instrumented
}
```
```
public void Method2()
{
//outside span
```
```
using(var activity = _activitySource.StartActivityDebug())
{
//instrumented
```

### }

//outside span
}
}

public static MyStaticClass
{
//Initialization of source wiyh type
private static ActivityWrapper _activitySource = ObservabilityManager.GetActivityWrapper<MyStaticClass>();

public static void MyStaticMethod()
{
using var activity = _activitySource.StartActivity(TraceActivityLevel.Informational, ActivityKind.
Internal);

// all body instrumented
}
}

public class DerivedClass : MyClass
{
public void MySpecilizedMethod()
{
//using base class wrapper (same assembly) but activity take name from this
using var activity = _activitySource.StartActivity(this, TraceActivityLevel.Debug);

// all body instrumented
}
}


# Observability Weaver

```
Introduction
Exposed Attributes
Attributes available at class level
Attributes available at method level
Dependency towards Observability library
Observability library contract
Considerations for weaver release
Execute effective weaver unit tests on CI/CD pipeline
Injected code for instrumentation
Versioning of Observability weavers nuget packages
```
## Introduction

Observability weaver manipulates libraries user code adding proper MSIL (Microsoft Intermediate Language) in order to implement Observability strategy
in automatic fashion.
For generic details on how a weaver solution should be structured please follow this link: https://github.com/Fody/Home/blob/master/pages/addin-
development.md

Observability weaver projects are properly configured to generate a fody weaver package that can be easily referenced by user application projects.
You can find details on weaver packaging here: https://github.com/Fody/Home/blob/master/pages/addin-packaging.md

Two different weavers are available to instrument different target libraries:

```
Siemens.MOM.Metadata.Observability.Aspects for model libraries
Siemens.MOM.Platform.Observability.Aspects for platform libraries
```
These weavers are available in our feed as nuget packages. To use them you need to reference our Observability library (Siemens.MOM.Platform.
Observability) as well.
For model libraries you should already find the needed references in your repo (from common submodule props), instead for platform libraries you need to
add these references to your project.

To enable the weaving process in an application, simply add the weaver name in FodyWeavers.xml file.

```
Weaver configuration for model libraries in FodyWeavers.xml file
```
```
<Weavers xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="FodyWeavers.xsd">
<Siemens.MOM.Metadata.Observability.Aspects />
</Weavers>
```
```
Weaver configuration for platform libraries in FodyWeavers.xml file
```
```
<Weavers xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="FodyWeavers.xsd">
<Siemens.MOM.Platform.Observability.Aspects />
</Weavers>
```
## Exposed Attributes

## Attributes available at class level

The following attributes are available at class level:

```
Observable
NotObservable
```
Criteria to determine if a class is observable:

```
Attribute Hierarchy Behavior
```
```
No attributes No attributes in hierarchy Not Observable
```
```
[Observable] No attributes in hierarchy Observable
```
```
[NotObservable] No attributes in hierarchy Not Observable
```
```
No attributes Inherits from Observable class Observable
```

```
[NotObservable] Inherits from Observable class Not Observable
```
```
No attributes Inherits from NotObservable class Not Observable
```
```
[Observable] Inherits from NotObservable class Observable
```
Attributes available at method level

The following attributes are available at method level:

```
DoNotTrace
Trace
```
Criteria to determine if a method belonging to an observable class is observable:

```
Attribute Behavior
```
```
No attributes on method Observable
```
```
[DoNotTrace] Not Observable
```
Instrumentation configuration available for an observable method:

```
Configuration Attribute Description Default value
(model libraries)
```
```
Default value
(platform libraries)
```
```
Trace Level [Trace(ActivityLevel.Debug)] Trace level configuration. Allowed values:
Informational, Debug
```
```
Informational Debug
```
```
Activity Kind [Trace(ActivityLevel.Debug, ActivityKind
= ActivityKind.Server)]
```
```
Describes the relationship between the Activity, its
parents and children in a Trace.
Allowed values: System.Diagnostics.ActivityKind
enum values
```
```
Internal Internal
```
Details on ActivityKind configuration in OpenTelemetry can be found here: https://github.com/open-telemetry/opentelemetry-specification/blob/main
/specification/trace/api.md#spankind

Dependency towards Observability library

Observability weaver has dependency towards the Observability library that provide the API (method) required to inject Observability.

Observability library assembly name is hardcoded and is expected to be equal to 'Siemens.MOM.Platform.Observability'.

If observability assembly is not found among available references, Observablity API is searched within the inspected assembly.

Available references are evaluated exploiting @(ReferencePath) MSBuild list property.
Thus, both assembly references and package references are taken into account.

Observability library contract

Observability weaver expected that Observability library contains a class named ActivityWrapper that provides a methods with the following signatures:

```
Contract for model libraries
```
```
public static Activity StartActivity(IBaseObject sourceObj, TraceConfig traceConf, [CallerMemberName] string
methodName = "")
```
```
Contract for platform libraries
```
```
public static Activity StartPlatformActivity(IBaseObject sourceObj, TraceConfig traceConf, [CallerMemberName]
string methodName = "")
```
```
The dependency towards Observability library is hardcoded in order to avoid code injection from an untrusted library.
```

TraceConfig class provides all available configuration.

```
Configuration property Type Allowed values
```
```
Level string 'Informational', 'Debug'
```
```
ActivityKind System.Diagnostics.ActivityKind From System.Diagnostics.ActivityKind enum
```
Considerations for weaver release

When platform services execute a weaved assembly, it shall handle the same cautions required by the execution of user code, as explained here: User
code execution risk & mitigation.

These cautions are required because the injected code might not fit well with specific code constructs and cause some sections of the weaved assembly
MSIL or metadata to be invalid.
In case an invalid MSIL is executed at runtime, then an InvalidProgramException will be thrown.

Execute effective weaver unit tests on CI/CD pipeline

Develop effective sample model and unit tests that execute the released weavers upon it to verify that the resulting MSIL is valid.
The sample model must cover all relevant code constructs that are impacted by the weaving process.
It is required a high code coverage for this sample model to assure that invalid MSIL will not be found at runtime.

To facilitate the development of weavers, unit tests should use 'PeVerify.exe' tool during class initialization to enforce type consistency checks on MSIL
during assembly loading stage.
This verification will be executed only on development machine, because 'PeVerify.exe' tool will not be available on CI/CD build machines.
This tool is available only on Windows machines where Visual Studio or at least Windows SDK is installed.

Injected code for instrumentation

```
Injected code for Observable method without attributes (metadata weaver)
```
```
MomTraceConf traceConf = new MomTraceConf
{
Level = "Informational",
ActivityKind = ActivityKind.Internal
};
using (this.StartActivity(traceConf, "MethodWithoutAttribute"))
{
// Original method code
}
```
```
Injected code for Observable method with Trace attribute
```
```
MomTraceConf traceConf = new MomTraceConf
{
Level = "Debug",
ActivityKind = ActivityKind.Server
};
using (this.StartActivity(traceConf, "MethodWithTraceAttribute"))
{
// Original method code
}
```
Versioning of Observability weavers nuget packages

As long as semantic versioning for observability weavers is not available, their nuget packages use the standard prerelease number versioning scheme.
When a major development is performed (for instance, technological upgrade), major version of the nuget package is increased.

In order to reference the same version for observability nuget packages within MetadataRuntime repo, we use the common 'platform.target.props' file. In
this file the desired version for observability weavers is defined as follows:


```
<ObservabilityVersion>2.0.0-*</ObservabilityVersion>
```
In order to reference the correct version for observability weavers within a MetadataRuntime project, the related csproj contains the following:

```
<PackageReference Include="Siemens.MOM.Platform.Observability.Aspects.Fody"
Version="$(ObservabilityVersion)" />
```
Note that within MetadataRuntime all projects already import directly or indirectly 'platform.target.props' file.

In order to reference the same version for observability nuget packages within a module repo is it enough to have M1_Common as submodule.
In fact, M1_Common provides 'common.props' file, where all required references are already implemented.


# Metrics


# Introduction to Metrics

## Definition

Metric is a measurement about the execution of an application at runtime.

The goal of metrics is monitoring and alerting by observing:

```
Application performance: response rate, failure rate, CPU/Memory/Network usage
Application availability
Application usage
```
## Metric concepts (OpenTelemetry)

The following image displays the concepts and semantics of Observability metrics:

## Sample Metrics for .NET Applications

```
System.Runtime Microsoft.
AspNetCore
```
```
gRPC metrics SQL Client Metrics Cache
Metrics
```
```
App specific/custom
metrics
```
```
CPU Usage
Exception Count
Allocation Rate (memory)
Working Set (memory)
Bytes allocated by small/large
object heap
GC metrics
Monitor Lock Contention Count
Number of Assemblies Loaded
JIT compilation metrics
Threadpool metrics
```
```
Failed
Requests
Request Rate
Duration of
Requests
Request Size
```
```
Current Requests
(active)
Total Calls
Current Calls
Calls Failed
Messages Sent
Messages
Received
Message Size
```
```
Total Requests
Request duration
Query type
Success
Connection related
metrics
Connection pool related
metrics
Total Requests
Request duration
Query type
Success
Connection related
metrics
Connection pool related
metrics
```
```
Total
Entries
Cache
Hits
Cache
Misses
```
```
Import PO Requests
ProcessB2MML
Body Size
```
## Current Status of OpenTelemetry Metrics

```
OTel .NET Metrics will be built on top of Metric API exposed by .NET 6
```

```
EventCounters are .NET Core APIs used for lightweight, cross-platform, and near real-time performance metric collection. (.NET 3 and later
versions)
OTel Metrics specification is currently experimental.
OTel Metrics API is currently obsolete and recommended not to be used for production use.
Preview releases will be available by June 2021.
```
References

```
https://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/metrics/api.md
https://lightstep.com/blog/opentelemetry-101-what-are-metrics/
https://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/overview.md#metrics
https://prometheus.io/docs/practices/instrumentation/
https://prometheus.io/docs/practices/histograms/
https://docs.microsoft.com/en-us/dotnet/core/diagnostics/available-counters
https://docs.microsoft.com/en-us/dotnet/framework/data/adonet/performance-counters
https://github.com/prometheus-net/prometheus-net
https://github.com/open-telemetry/opentelemetry-dotnet/issues/1370
https://github.com/open-telemetry/opentelemetry-dotnet/tree/main/examples
https://github.com/open-telemetry/opentelemetry-dotnet/issues/1501
https://docs.microsoft.com/en-us/dotnet/core/diagnostics/event-counters
```

# Structured Logging

One important aspect of logging is the log content. It is almost impossible to have a "unique" structure (in terms of fields inside a log entry) for the entire
application. Some log entries may need more fields than others, therefore it is wasting of space for those log entries that do not need all the extra fields.
However, broadly speaking each log entry contains two major parts "context" and "message". Although contextual properties are repeated across all or
some log entries, log messages are specific to each log entry. Enough information should be added to log messages in order to make them useful for
diagnostics, analytics and monitoring purposes. However, the performance impact and storage cost should be considered when providing details inside log
messages. (avoiding too much details)

Another important aspect of logging log levels. Here, are the common log levels which specify the log severity:

```
Trace: Anything and everything you might want to know about a running block of code. At this level, sensitive application data can also be logged.
This level should never be enabled in a production environment.
Debug: Internal system events that aren't necessarily observable from the outside, but useful for determining how something happened. Logs at
this level are used for interactive investigation during development. These logs should primarily contain information useful for debugging and have
no long-term value.
Information: Information events describe things happening in the system that correspond to its responsibilities and functions. Generally these are
the observable actions the system can perform. Logs at this level track the general flow of the application. These logs should have long-term
value.
Warning: When service is degraded, endangered, or may be behaving outside of its expected parameters, Warning level events are used. Logs at
this level highlight an abnormal or unexpected event in the application flow, but do not otherwise cause the application execution to stop.
Error: When functionality is unavailable or expectations broken, an Error event is used. Logs at this level highlight when the current flow of
execution is stopped due to a failure. These should indicate a failure in the current activity, not an application-wide failure.
Critical: The most critical level, Fatal events demand immediate attention. Logs at this level describe an unrecoverable application or system
crash, or a catastrophic failure that requires immediate attention.
```
For exceptions not only the exception message but also the stack trace and all other useful information inside the exception should be logged. The correct
log level should be used to distinguish critical, erroneous and informational exceptions. In general, it is important to use the appropriate log level to convey
the type of information logged. It is recommended to enable only high-severity log levels at production, for example, Warning and above or Error and
above. For more guidelines regarding log levels see https://docs.microsoft.com/en-us/aspnet/core/fundamentals/logging/?view=aspnetcore-5.0.

## Overall Design


# Enabling Structured Logging

Modular MOM logs provide information about the operation performed. The structure of the log varies depending on the application required to distinguish
the type of information logged.

A log is composed of entry fields that contain a context, a message, contextual properties (repeated across log entries), specific log messages for each log
entry and the necessary fields to better describe the application activity.

## Log Levels

Depending on the information provided, the common log levels which specify the log severity are described below:

```
Verbose: Sensitive application data and any information regarding a running block of code.
Debug: Internal system events used for interactive investigation during development. These logs contain information useful for debugging and
have no long-term value.
Information: Describe observable actions the system can perform that correspond to its responsibilities and functions. Logs at this level track the
general flow of the application and have long-term value.
Warning: highlights an abnormal or unexpected event in the application flow when a service is degraded, endangered, or may be behaving
outside of its expected parameters.
Logs at this level highlight an abnormal or unexpected event in the application flow, but do not otherwise cause the application execution to stop.
Error: Indicates a failure in the current activity, highlight when the current flow of execution is stopped due to a failure and a functionality is
unavailable or expectations broken.
Fatal: The most critical level, describes an unrecoverable application or system crash, or a catastrophic failure that requires immediate attention.
```
## Contextual Log Properties

Contextual properties are repeated across all or some log entries and are divided in the following groups:

```
Static global context: available at startup for every structured log entry
Dynamic global context: per request and available at startup for every structured log entry
Local context: related to a specific application layer and available only for logs within a specific application layer scope.
```
For more information about global context properties, see Contextual properties for structured logging.

## Log Providers

The considered provider output/sink is Console with Json format, in order to provide structured logging data to a logging collector, such as Fluentbit.
Output format has been configured to well integrate with Fluentbit logging collector (ie, emitting one log entry per line).

```
Exception Level
```
```
Exceptions include the exception message, the stacktrace and all other useful information inside the exception. The correct log level is used to
distinguish critical, erroneous and informational exceptions.
```

# Contextual properties for structured logging

## Global context properties

Global context properties are available for every log entry.

Some of these properties are available only for log entries correlated with a given Http request, others are also available for bootstrap logs entries during
startup phase.

```
Property Description Available Example in log entry (TBD)
```
```
Timestamp Date and time when the log entry has been written Always
```
```
Level The log level Always
```
```
Message Template Template of the message specifying message arguments as properties Always
```
```
Message Parsed message containing resolved values for the arguments Always
```
```
Message State Set of properties used as arguments in the message template Always
```
```
SourceContext Code source of the log entry, represented as the full class name Always
```
```
ActionId Identifier of the controller action Http Request
```
```
ActionName Controller action full name Http Request
```
```
RequestId Identifier of the request Http Request
```
```
RequestPath Request path Http Request
```
```
ConnectionId Identifier of the connection Http Request
```
```
ThreadId Identifier of the thread issuing the log entry Always
```

# Spike: comparison of available features in .NET

# JsonConsole and Serilog for structured logging

```
Introduction
Effort to configure contextual log properties
Classification of contextual log properties
Comparison between .NET JsonConsole and Serilog
Sample log statements
Code samples for implementations required with .Net JsonConsole
Asp Net Core Web Api configuration to use .Net JsonConsole
Asp Net Core custom middleware to configure logging global contextual properties
Code samples for implementations required with Serilog
Third-party dependencies
Asp Net Core Web Api configuration to use Serilog
Implementation of custom enricher
Code samples for implementations required for application layers
Effort to log message state and custom object properties
Message Template
Effort to log custom object properties with .Net JsonConsole
Effort to log custom object properties with Serilog
How to optimize performances to render message templates with LoggerMessage.Define
```
## Introduction

Both .NET JsonConsole and Serilog logging providers have been evaluated for Asp Net Core Web Api use case.

The considered provider output/sink is Console with Json format, in order to provide structured logging data to a logging collector, such as Fluentbit.
Output format has been configured to well integrate with Fluentbit logging collector (ie, emitting one log entry per line).

## Effort to configure contextual log properties

## Classification of contextual log properties

Contextual log properties can be divided in the following groups:

```
Static global context, available at startup
Dynamic global context per request
Local context relative to a specific application layer
```
Global context properties are available for every structured log entry, while local context properties are available only for logs within a specific application
layer scope.

In the analysis, the following global context properties have been evaluated:

```
Property Context Description / Notes
```
```
Module Name Static Available from appsettings configuration file
```
```
Distributed Trace ID Dynamic Link to the Trace Activity containing the log
```
```
Machine Name Static
```
```
Client IP Dynamic Available from request HttpContext
```
```
Source Context / Category Static
```
```
Related Event ID Static Association of certain logs with defined events
```
```
Timestamp Static
```
As an example of local context relative to a specific application layer, you can consider the TransactionId for Information layer.

The preferred approach for application layers is to avoid third party-dependencies, but just use ILogger interface to add local context properties.

## Comparison between .NET JsonConsole and Serilog

Some properties are available out of the box for both providers:

```
Source Context (in .NET JSonConsole it is named Category and can be overridden)
```

```
Related Event ID
Timestamp
```
```
Log
context
type
```
```
.Net JsonConsole Serilog
```
```
Static global
context
```
```
Requires developing Asp Net core custom middleware Mostly Available via “additional” enricher packages
```
```
Otherwise requires “little” effort to develop a custom enricher (for
instance for our custom data such as Module Name)
```
```
To retrieve data from appsetting configuration it is suggested to use
two-stage initialization
```
```
Dynamic
global
context
```
```
Available out of the box for some like Distributed Trace ID or
Http request path via appsettings configuration
(enabling scopes)
```
```
For others requires developing Asp Net Core custom
middleware
```
```
Mostly Available via “additional” enricher packages
```
```
Otherwise requires “little” effort to develop a custom enricher (for
instance for Distributed Trace ID)
```
```
Local
context per
library
```
```
Requires developing code using ILogger.BeginScope Available a third-party dependency for libraries
```
```
Otherwise requires developing code using ILogger.BeginScope
```
Sample log statements

Both .NET JsonConsole and Serilog logging providers have been evaluated in similar conditions:

```
with contexual properties of all types
with log statements containing parameterized data (state)
```
Here you can see a sample of the obtained log statements:

```
Sample log statement generated with .NET JSON Console
```
```
{"Timestamp":"16:57:46 ","EventId":2,"LogLevel":"Information","Category":"JsonConsoleWebApi.Controllers.
WeatherForecastController","Message":"03/31/2021 18:57:46 is Mild","State":{"Message":"03/31/2021 18:57:46 is
Mild","Date":"03/31/2021 18:57:46","Summary":"Mild","{OriginalFormat}":"{Date} is {Summary}"},"Scopes":
[{"Message":"SpanId:cbfc69b5aa5d154f, TraceId:8fe6acfc9433e043843957fef121d7f2, ParentId:f730976343904d41","
SpanId":"cbfc69b5aa5d154f","TraceId":"8fe6acfc9433e043843957fef121d7f2","ParentId":"f730976343904d41"},
{"Message":"ConnectionId:0HM7IQEMR7SVO","ConnectionId":"0HM7IQEMR7SVO"},{"Message":"RequestPath:
/WeatherForecast RequestId:0HM7IQEMR7SVO:0000000D","RequestId":"0HM7IQEMR7SVO:0000000D","RequestPath":"
/WeatherForecast"},{"Message":"ModuleName: Dispatch, MachineName: VM-VDIP48-006, CorrelationId: 0HM7IQEMR7SVO:
0000000D, UserName: \u003CUnknown\u003E","ModuleName":"Dispatch","MachineName":"VM-VDIP48-006","CorrelationId":"
0HM7IQEMR7SVO:0000000D","UserName":"\u003CUnknown\u003E"},{"Message":"JsonConsoleWebApi.Controllers.
WeatherForecastController.Get (JsonConsoleWebApi)","ActionId":"3f213ea7-028a-4dd2-9534-d2d1188a7cec","
ActionName":"JsonConsoleWebApi.Controllers.WeatherForecastController.Get (JsonConsoleWebApi)"},{"Message":"
AppGuid: 752ba20b-a7ae-4ec6-ab2d-be31e13b7f4a","AppGuid":"752ba20b-a7ae-4ec6-ab2d-be31e13b7f4a"}]}
```
```
Sample log statement generated with Serilog
```
```
{"@t":"2021-04-01T17:04:30.7682049Z","@mt":"{Date} is {Summary}","Date":"2021-04-04T19:04:30.7681920+02:00","
Summary":"Bracing","EventId":{"Id":2,"Name":"Forecast management"},"SourceContext":"JsonConsoleWebApi.
Controllers.WeatherForecastController","AppGuid":"5a6f41fa-ffa4-439e-8403-e0a49975b7a6","ActionId":"815e502e-
f2a1-45ba-902f-cb4d473bbc6e","ActionName":"JsonConsoleWebApi.Controllers.WeatherForecastController.Get
(SerilogWebApi)","RequestId":"0HM7L60OMG78V:00000005","RequestPath":"/WeatherForecast","ConnectionId":"
0HM7L60OMG78V","ModuleName":"Dispatch","MachineName":"VM-VDIP48-006","TraceId":"
0cb5dffbe7b03f4e9eb9ae318745eacc","SpanId":"113ecbd1e0a3fa48","ParentId":"0000000000000000","ThreadId":5}
```
Code samples for implementations required with .Net JsonConsole


Asp Net Core Web Api configuration to use .Net JsonConsole

.Net JsonConsole provider can be entirely configured via appsetting, as long as Asp Net Core Web Application pipeline automatically includes it (together
with other built-in providers).

Including scopes, several global contextual properties are available to provide details on the HttpRequest.
Also details on instrumented Activities are available out of the box.

```
appsettings.json section to configure logging using .Net JsonConsole provider
```
```
"Logging": {
"LogLevel": {
"Default": "Information",
"Microsoft": "Warning",
"Microsoft.Hosting.Lifetime": "Information"
},
"Console": {
"LogLevel": {
"Default": "Information",
"Microsoft": "Warning",
"Microsoft.Hosting.Lifetime": "Information"
},
"FormatterName": "json",
"FormatterOptions": {
"SingleLine": true,
"IncludeScopes": true,
"TimestampFormat": "yyyy-MM-dd HH:mm:ss.fff",
"UseUtcTimestamp": true,
"JsonWriterOptions": {
"Indented": false
}
}
}
}
```
Otherwise, .Net JsonConsole provider can be configured programmatically at application start-up, in order to use just one provider.

```
CreateHostBuilder implementation to configure .Net JsonConsole provider
```
```
public static IWebHostBuilder CreateHostBuilder(string[] args)
{
return Host.CreateDefaultBuilder(args)
.ConfigureLogging(loggingBuilder =>
{
loggingBuilder.ClearProviders();
loggingBuilder.AddJsonConsole(options =>
{
options.IncludeScopes = true;
options.TimestampFormat = "yyyy-MM-dd HH:mm:ss.fff";
options.UseUtcTimestamp = true;
options.JsonWriterOptions = new JsonWriterOptions
{
Indented = false,
SkipValidation = false
};
});
})
.UseStartup<Startup>();
}
```
Asp Net Core custom middleware to configure logging global contextual properties

The overall picture of Asp Net Core middleware pipeline is described here: https://docs.microsoft.com/en-us/aspnet/core/fundamentals/middleware/?
view=aspnetcore-5.0


Implementation of custom Asp Net Core middleware to configure global log context properties

/// <summary>
/// ASP Net Core Middleware that provides structured logging context
/// </summary>
public class GlobalLogContextMiddleware
{
private readonly RequestDelegate _next;
private readonly ILogger _logger;
private readonly IConfiguration _configuration;

public GlobalLogContextMiddleware(RequestDelegate next, ILoggerFactory loggerFactory, IConfiguration
configuration)
{
if (next == null)
{
throw new ArgumentNullException(nameof(next));
}

if (loggerFactory == null)
{
throw new ArgumentNullException(nameof(loggerFactory));
}

if (configuration == null)
{
throw new ArgumentNullException(nameof(configuration));
}

_next = next;
_logger = loggerFactory.CreateLogger<GlobalLogContextMiddleware>();
_configuration = configuration;
}

/// <summary>
/// Entry point
/// </summary>
/// <param name="context"></param>
/// <returns></returns>
public Task Invoke(HttpContext context)
{
var moduleName = _configuration.GetSection("ModularMOM:ApplicationName").Get<string>();
var machineName = Environment.MachineName;
var clientIp = $"{context.Connection.RemoteIpAddress}:{context.Connection.RemotePort}";
using var scope = _logger.BeginScope(new GlobalLoggingScope(moduleName, machineName, clientIp));

return _next(context);
}
}


```
ApplicationBuilder extension to provide custom middleware to the pipeline
```
```
public static IApplicationBuilder UseGlobalLogContext(this IApplicationBuilder app)
{
if (app == null)
{
throw new ArgumentNullException(nameof(app));
}
```
```
return app.UseMiddleware<GlobalLogContextMiddleware>();
}
```
Code samples for implementations required with Serilog

Third-party dependencies

Useful nuget packages available from Serilog github project (https://github.com/serilog/serilog) include the following:

```
Serilog
Serilog.AspNetCore
Serilog.Enrichers.Thread
Serilog.Formatting.Compact
Serilog.Sinks.Console
```
Asp Net Core Web Api configuration to use Serilog

Serilog provider can be configured programmatically at application start-up.

Also out of the box and custom enrichers can be configured at this level to manage global context properties.

In order to properly retrieve global context properties also from appsettings configuration file, it is suggested to adopt two-stage initialization.

Using FromLogContext enricher, several global contextual properties are available to provide details on the HttpRequest.

To properly output structured logging preserving the message template, use CompactJsonFormatter. In this way, message rendering is postponed to a
later stage.
CompactJsonFormatter outputs a condensed JSON format, removing redundant data. For instance, log level is omitted when it equals 'Informational'.

```
Asp Net Core Web Api Program implementation to configure Serilog with two-stage initialization
```
```
public class Program
{
public static void Main(string[] args)
{
Log.Logger = new LoggerConfiguration()
.MinimumLevel.Override("Microsoft", LogEventLevel.Information)
.Enrich.FromLogContext()
.WriteTo.Console(new CompactJsonFormatter(), LogEventLevel.Information)
.CreateBootstrapLogger();
```
```
try
{
CreateHostBuilder(args).Build().Run();
}
catch (Exception ex)
{
Log.Fatal(ex, "Host terminated unexpectedly");
}
finally
{
Log.CloseAndFlush();
}
}
```
```
public static IHostBuilder CreateHostBuilder(string[] args) =>
Host.CreateDefaultBuilder(args)
.UseSerilog((context, services, configuration) => configuration
.ReadFrom.Configuration(context.Configuration)
```

```
.ReadFrom.Services(services)
.Enrich.FromLogContext()
.Enrich.WithThreadId()
.WriteTo.Console(new CompactJsonFormatter(), LogEventLevel.Information))
.ConfigureWebHostDefaults(builder => builder.UseStartup<Startup>());
}
```
Implementation of custom enricher

In order to integrate Log messages with Tracing information with Serilog the faster solution is to write a custom enricher like the following:

```
Serilog Enricher for Trace Activity integration
```
```
public class LogEnricher : ILogEventEnricher
{
public void Enrich(LogEvent logEvent, ILogEventPropertyFactory propertyFactory)
{
var act = Activity.Current;
```
```
if (act == null)
{
return;
}
```
```
logEvent.AddPropertyIfAbsent(propertyFactory.CreateProperty(
"TraceId", act.Context.TraceId));
logEvent.AddPropertyIfAbsent(propertyFactory.CreateProperty(
"SpanId", act.Context.SpanId));
logEvent.AddPropertyIfAbsent(propertyFactory.CreateProperty(
"ParentId", act.ParentSpanId));
}
}
```
Then in Log Configuration the enricher can be used

```
public static IHostBuilder CreateHostBuilder(string[] args) =>
Host.CreateDefaultBuilder(args)
.UseSerilog((context, services, configuration) => configuration
.ReadFrom.Configuration(context.Configuration)
.ReadFrom.Services(services)
.Enrich.With(new LogEnricher())
.Enrich.FromLogContext()
.Enrich.WithThreadId()
.WriteTo.Console(new RenderedCompactJsonFormatter(), LogEventLevel.Information))
.ConfigureWebHostDefaults(builder => builder.UseStartup<Startup>());
```
A similar approach can be used to implement an enricher that retrieves data from the appsettings configuration.

```
public class LogEnricherFromConf : ILogEventEnricher
{
private readonly IConfiguration _configuration;
```
```
public LogEnricherFromConf(IConfiguration configuration)
{
_configuration = configuration;
}
```
```
public void Enrich(LogEvent logEvent, ILogEventPropertyFactory propertyFactory)
{
var moduleName = _configuration.GetSection("ModularMOM:ApplicationName").Get<string>();
logEvent.AddPropertyIfAbsent(propertyFactory.CreateProperty(
"ModuleName", moduleName));
```
```
logEvent.AddPropertyIfAbsent(propertyFactory.CreateProperty(
```

```
"MachineName", Environment.MachineName));
}
}
```
In Log Configuration you simply feed the enricher with the configuration:

```
public static IHostBuilder CreateHostBuilder(string[] args) =>
Host.CreateDefaultBuilder(args)
.UseSerilog((context, services, configuration) => configuration
.ReadFrom.Configuration(context.Configuration)
.ReadFrom.Services(services)
.Enrich.With(new LogEnricherFromConf(context.Configuration))
.Enrich.FromLogContext()
.Enrich.WithThreadId()
.WriteTo.Console(new CompactJsonFormatter(), LogEventLevel.Information))
//.WriteTo.Console(new RenderedCompactJsonFormatter(), LogEventLevel.Information))
.ConfigureWebHostDefaults(builder => builder.UseStartup<Startup>());
```
Code samples for implementations required for application layers

The preferred approach for application layers is to avoid third party-dependencies, but just use ILogger interface to add local context properties.

In detail, they will use ILogger.BeginScope method, passing local context properties by means of custom state object.

```
Sample of local context state object
```
```
public class AppLoggingScope : IReadOnlyCollection<KeyValuePair<string, object>>
{
public AppLoggingScope(Guid transactionId)
{
TransactionId = transactionId;
}
```
```
public Guid TransactionId { get; }
```
```
public IEnumerator<KeyValuePair<string, object>> GetEnumerator()
{
yield return new KeyValuePair<string, object>("TransactionId", TransactionId);
}
```
```
IEnumerator IEnumerable.GetEnumerator()
{
return GetEnumerator();
}
```
```
public int Count => 1;
```
```
public override string ToString()
{
var values = new List<KeyValuePair<string, object>>();
using var enumerator = GetEnumerator();
while (enumerator.MoveNext())
{
values.Add(enumerator.Current);
}
```
```
return string.Join(", ", values.Select(p => $"{p.Key}: {p.Value}"));
}
}
```
```
Sample to create local scope context specific for the application layer
```

```
var appLoggingState = new AppLoggingScope(transactionId);
using var localScope = _logger.BeginScope(appLoggingState);
```
Effort to log message state and custom object properties

Message Template

Frequently log statements need to be parameterized with some data (its state).

To properly specify parameterized data, the suggestion is to avoid string interpolation, but use message templates instead.

```
Sample of log with string interpolation
```
```
logger.LogWarning($"The order {orderId} could not be found.");
```
```
Sample of log with message template
```
```
logger.LogWarning("The order {OrderId} could not be found.", orderId);
```
Advantages of message templates in structured logging:

```
Flexibility. Output log structure contains explicitly such named parameters as context properties. Also the message template becomes a context
property. This enables to perform queries on log statements on such additional properties. For instance, I can search for all logs with OrderId
equal to '12345' or with message template equal to 'The order {OrderId} could not be found.'.
Performance. The rendering of the parameterized message can be optimized or even postponed and performed only when the log statement is
actually queried.
```
Named parameters provide data of various types:

```
primitive types
object types
collection types
```
It is important that log statements contain a meaningful representation of all these data types.

Effort to log custom object properties with .Net JsonConsole

.Net JsonConsole provider relies on specific ToString() implementation to represent parameterized data.

Most C# collection types has a meaningful ToString() implementation.
Also some C# object constructs, such as anonymous types, have a well designed ToString() implementation.
Anyway, for custom object types you must provide your own ToString() implementation override.

```
Sample .NET JSON Console log statement including list of anonymous types
```
```
{"Timestamp":"16:57:49 ","EventId":1,"LogLevel":"Information","Category":"JsonConsoleWebApi.Controllers.
WeatherForecastController","Message":"Created forecast dtos: { Date = 30/03/2021 18:57:46, Temperature = 2 }, {
Date = 31/03/2021 18:57:46, Temperature = -20 }, { Date = 01/04/2021 18:57:49, Temperature = -13 }, { Date = 02
/04/2021 18:57:49, Temperature = 46 }, { Date = 03/04/2021 18:57:49, Temperature = -14 }, { Date = 04/04/2021
18:57:49, Temperature = -20 }, { Date = 05/04/2021 18:57:49, Temperature = 9 }, { Date = 06/04/2021 18:57:49,
Temperature = -7 }, { Date = 07/04/2021 18:57:49, Temperature = -12 }, { Date = 08/04/2021 18:57:49,
Temperature = -15 }","State":{"Message":"Created forecast dtos: { Date = 30/03/2021 18:57:46, Temperature = 2
}, { Date = 31/03/2021 18:57:46, Temperature = -20 }, { Date = 01/04/2021 18:57:49, Temperature = -13 }, { Date
= 02/04/2021 18:57:49, Temperature = 46 }, { Date = 03/04/2021 18:57:49, Temperature = -14 }, { Date = 04/04
/2021 18:57:49, Temperature = -20 }, { Date = 05/04/2021 18:57:49, Temperature = 9 }, { Date = 06/04/2021 18:57:
49, Temperature = -7 }, { Date = 07/04/2021 18:57:49, Temperature = -12 }, { Date = 08/04/2021 18:57:49,
Temperature = -15 }","forecastDtos":"{ Date = 30/03/2021 18:57:46, Temperature = 2 }, { Date = 31/03/2021 18:57:
46, Temperature = -20 }, { Date = 01/04/2021 18:57:49, Temperature = -13 }, { Date = 02/04/2021 18:57:49,
Temperature = 46 }, { Date = 03/04/2021 18:57:49, Temperature = -14 }, { Date = 04/04/2021 18:57:49,
Temperature = -20 }, { Date = 05/04/2021 18:57:49, Temperature = 9 }, { Date = 06/04/2021 18:57:49, Temperature
= -7 }, { Date = 07/04/2021 18:57:49, Temperature = -12 }, { Date = 08/04/2021 18:57:49, Temperature = -15 }","
{OriginalFormat}":"Created forecast dtos: {forecastDtos}"},"Scopes":[{"Message":"SpanId:f730976343904d41,
TraceId:8fe6acfc9433e043843957fef121d7f2, ParentId:8fc8b8a3b6764844","SpanId":"f730976343904d41","TraceId":"
8fe6acfc9433e043843957fef121d7f2","ParentId":"8fc8b8a3b6764844"},{"Message":"ConnectionId:0HM7IQEMR7SVO","
```

```
ConnectionId":"0HM7IQEMR7SVO"},{"Message":"RequestPath:/WeatherForecast RequestId:0HM7IQEMR7SVO:0000000D","
RequestId":"0HM7IQEMR7SVO:0000000D","RequestPath":"/WeatherForecast"},{"Message":"ModuleName: Dispatch,
MachineName: VM-VDIP48-006, CorrelationId: 0HM7IQEMR7SVO:0000000D, UserName: \u003CUnknown\u003E","ModuleName":"
Dispatch","MachineName":"VM-VDIP48-006","CorrelationId":"0HM7IQEMR7SVO:0000000D","UserName":"
\u003CUnknown\u003E"},{"Message":"JsonConsoleWebApi.Controllers.WeatherForecastController.Get
(JsonConsoleWebApi)","ActionId":"3f213ea7-028a-4dd2-9534-d2d1188a7cec","ActionName":"JsonConsoleWebApi.
Controllers.WeatherForecastController.Get (JsonConsoleWebApi)"}]}
```
```
Sample .NET JSON Console log statement including list of custom objects with default ToString() implementation
```
```
{"Timestamp":"16:57:49 ","EventId":1,"LogLevel":"Information","Category":"JsonConsoleWebApi.Controllers.
WeatherForecastController","Message":"Created forecasts: JsonConsoleWebApplication.WeatherForecast,
JsonConsoleWebApplication.WeatherForecast, JsonConsoleWebApplication.WeatherForecast, JsonConsoleWebApplication.
WeatherForecast, JsonConsoleWebApplication.WeatherForecast, JsonConsoleWebApplication.WeatherForecast,
JsonConsoleWebApplication.WeatherForecast, JsonConsoleWebApplication.WeatherForecast, JsonConsoleWebApplication.
WeatherForecast, JsonConsoleWebApplication.WeatherForecast","State":{"Message":"Created forecasts:
JsonConsoleWebApplication.WeatherForecast, JsonConsoleWebApplication.WeatherForecast, JsonConsoleWebApplication.
WeatherForecast, JsonConsoleWebApplication.WeatherForecast, JsonConsoleWebApplication.WeatherForecast,
JsonConsoleWebApplication.WeatherForecast, JsonConsoleWebApplication.WeatherForecast, JsonConsoleWebApplication.
WeatherForecast, JsonConsoleWebApplication.WeatherForecast, JsonConsoleWebApplication.WeatherForecast","
forecasts":"JsonConsoleWebApplication.WeatherForecast, JsonConsoleWebApplication.WeatherForecast,
JsonConsoleWebApplication.WeatherForecast, JsonConsoleWebApplication.WeatherForecast, JsonConsoleWebApplication.
WeatherForecast, JsonConsoleWebApplication.WeatherForecast, JsonConsoleWebApplication.WeatherForecast,
JsonConsoleWebApplication.WeatherForecast, JsonConsoleWebApplication.WeatherForecast, JsonConsoleWebApplication.
WeatherForecast","{OriginalFormat}":"Created forecasts: {forecasts}"},"Scopes":[{"Message":"SpanId:
f730976343904d41, TraceId:8fe6acfc9433e043843957fef121d7f2, ParentId:8fc8b8a3b6764844","SpanId":"
f730976343904d41","TraceId":"8fe6acfc9433e043843957fef121d7f2","ParentId":"8fc8b8a3b6764844"},{"Message":"
ConnectionId:0HM7IQEMR7SVO","ConnectionId":"0HM7IQEMR7SVO"},{"Message":"RequestPath:/WeatherForecast RequestId:
0HM7IQEMR7SVO:0000000D","RequestId":"0HM7IQEMR7SVO:0000000D","RequestPath":"/WeatherForecast"},{"Message":"
ModuleName: Dispatch, MachineName: VM-VDIP48-006, CorrelationId: 0HM7IQEMR7SVO:0000000D, UserName:
\u003CUnknown\u003E","ModuleName":"Dispatch","MachineName":"VM-VDIP48-006","CorrelationId":"0HM7IQEMR7SVO:
0000000D","UserName":"\u003CUnknown\u003E"},{"Message":"JsonConsoleWebApi.Controllers.WeatherForecastController.
Get (JsonConsoleWebApi)","ActionId":"3f213ea7-028a-4dd2-9534-d2d1188a7cec","ActionName":"JsonConsoleWebApi.
Controllers.WeatherForecastController.Get (JsonConsoleWebApi)"}]}
```
Effort to log custom object properties with Serilog

Serilog has a built-in DSL to specify message templates. With this DSL, you can use the operator '@' to serialize the subsequent named parameter.

```
Code sample: how to instruct serilog to serialize a custom object
```
```
_logger.LogInformation(ApplicationLogEvents.GetForecasts, "Created forecasts: {@forecasts}", forecasts);
```
```
Sample Serilog log statement including serialized lists of custom objects
```
```
{"@t":"2021-04-01T17:04:30.8191676Z","@mt":"Created forecasts: {@forecasts}","forecasts":[{"Date":"2021-04-
02T19:04:30.7599158+02:00","TemperatureC":5,"TemperatureF":40,"Summary":"Warm","$type":"WeatherForecast"},
{"Date":"2021-04-03T19:04:30.7678748+02:00","TemperatureC":38,"TemperatureF":100,"Summary":"Bracing","$type":"
WeatherForecast"},{"Date":"2021-04-04T19:04:30.7681920+02:00","TemperatureC":-16,"TemperatureF":4,"Summary":"
Bracing","$type":"WeatherForecast"},{"Date":"2021-04-05T19:04:30.7826178+02:00","TemperatureC":11,"
TemperatureF":51,"Summary":"Mild","$type":"WeatherForecast"},{"Date":"2021-04-06T19:04:30.7856819+02:00","
TemperatureC":11,"TemperatureF":51,"Summary":"Freezing","$type":"WeatherForecast"},{"Date":"2021-04-07T19:04:
30.7871547+02:00","TemperatureC":50,"TemperatureF":121,"Summary":"Bracing","$type":"WeatherForecast"},{"Date":"
2021-04-08T19:04:30.7884925+02:00","TemperatureC":3,"TemperatureF":37,"Summary":"Freezing","$type":"
WeatherForecast"},{"Date":"2021-04-09T19:04:30.7900225+02:00","TemperatureC":30,"TemperatureF":85,"Summary":"
Freezing","$type":"WeatherForecast"},{"Date":"2021-04-10T19:04:30.7914868+02:00","TemperatureC":-7,"
TemperatureF":20,"Summary":"Scorching","$type":"WeatherForecast"},{"Date":"2021-04-11T19:04:30.8035341+02:00","
TemperatureC":39,"TemperatureF":102,"Summary":"Bracing","$type":"WeatherForecast"}],"EventId":{"Id":1,"Name":"
GetForecasts action"},"SourceContext":"JsonConsoleWebApi.Controllers.WeatherForecastController","ActionId":"
815e502e-f2a1-45ba-902f-cb4d473bbc6e","ActionName":"JsonConsoleWebApi.Controllers.WeatherForecastController.Get
(SerilogWebApi)","RequestId":"0HM7L60OMG78V:00000005","RequestPath":"/WeatherForecast","ConnectionId":"
0HM7L60OMG78V","ModuleName":"Dispatch","MachineName":"VM-VDIP48-006","TraceId":"
0cb5dffbe7b03f4e9eb9ae318745eacc","SpanId":"113ecbd1e0a3fa48","ParentId":"0000000000000000","ThreadId":5}
```

How to optimize performances to render message templates with LoggerMessage.Define

LoggerMessage.Define method provides an optimization for structured message templates rendering.
Logger extension methods must parse the message template whenever a log message is rendered. LoggerMessage must parse a template only once, at
the time when the message is defined.

```
Definition of sample log message template
```
```
public static void ForecastDataLog(this ILogger logger, DateTime date, string summary)
=> ForecastData(logger, date, summary, null);
```
```
private static readonly Action<ILogger, DateTime, string, Exception> ForecastData =
LoggerMessage.Define<DateTime, string>(LogLevel.Information, ApplicationLogEvents.ManageForecast,
"{Date} is {Summary}");
```
```
Usage of defined log message template
```
```
_logger.ForecastDataLog(forecast.Date, forecast.Summary);
```
```
This section is mostly related to .NET Json Console logging provider.
```
```
With Serilog, message templates rendering can be postponed by using CompactJsonFormatter.
```

# Spike: comparison of performance between .NET

# JsonConsole and Serilog for structured logging

## Main result

Tested with BenchmarkDotNet=v0.12.1

BenchmarkDotNet=v0.12.1, OS=Windows 10.0.18363.1440 (1909/November2018Update/19H2)
Intel Xeon Gold 6248 CPU 2.50GHz, 2 CPU, 4 logical and 4 physical cores
.NET Core SDK=5.0.201
[Host] : .NET Core 5.0.4 (CoreCLR 5.0.421.11614, CoreFX 5.0.421.11614), X64 RyuJIT
DefaultJob : .NET Core 5.0.4 (CoreCLR 5.0.421.11614, CoreFX 5.0.421.11614), X64 RyuJIT

```
Method NCiclesUse ObjectsUse Template Use Serilog Mean
```
```
ms
```
```
Error
```
```
ms
```
```
StdDev
```
```
ms
```
```
Log per test Mean time for message
```
```
ms
DoLog 10 False False False 113.2 3.48 10.15 102 1.110
DoLog 10 False False True 102.1 3.12 9.21 102 1.001
DoLog 10 False True False 105.4 3.19 9.41 102 1.033
DoLog 10 False True True 106.7 3.46 10.16 102 1.046
DoLog 10 True False False 112.0 3.10 9.13 102 1.098
DoLog 10 True False True 104.8 3.13 9.22 102 1.027
DoLog 10 True True False 108.0 3.10 9.08 102 1.059
DoLog 10 True True True 102.7 3.35 9.87 102 1.007
DoLog 100 False False False 1,054.922.19 65.42 1002 1.053
DoLog 100 False False True 1,011.520.21 55.32 1002 1.009
DoLog 100 False True False 1,070.821.42 54.90 1002 1.069
DoLog 100 False True True 1,030.520.55 35.99 1002 1.028
DoLog 100 True False False 1,060.820.65 52.93 1002 1.059
DoLog 100 True False True 1,001.322.50 66.34 1002 0.999
DoLog 100 True True False 1,073.121.22 55.90 1002 1.071
DoLog 100 True True True 1,009.523.12 67.80 1002 1.007
```
Tests are executed in a number of thread and number of iterations automatically set by BenchmarkDotNet.

Output format is Json for both Serilog and JsonConsole and context it’s a minimal one.

Each test has one Start and one Stop Log message and repeat NCicles times a block of 10 Log messages

Conclusion:

Each single log message written synchronously in Json to console take about 1 ms. Little advantage from using Serilog and Message template.

## Thread, Load, Template evaluation

To evaluate more in deep also the performance in a concurrency environment a set of Tests has been developed.

Test written in C# and executed using multithread. Table below refer to test executed from 5 thread in parallel each test called 200 times total iteration = 5
x 200 = 1000

```
Name Total duration ms iterations Average duration ms Log per test Mean time for message ms
```
```
OneJson 15.629,659 1000 1,563 12 1,302
```
```
OneSeri 13.595,167 1000 13,595 12 1,133
```
```
TenJson 126.819,021 1000 126,819 102 1,243
```
```
TenSeri 86.517,548 1000 86,518 102 0,848
```
```
TenJsonTemplate 119.932,586 1000 119,933 102 1,176
```

```
TenSeriTemplate 91.686,549 1000 91,687 102 0,899
```
```
ThousandJson 12.182.061,367 1000 12.182,061 10002 1,218
```
```
ThousandSeri 8.305.409,946 1000 8.305,410 10002 0,830
```
```
ThousandJsonTemplate 11.970.078,654 1000 11.970,079 10002 1,197
```
```
ThousandSeriTemplate 8.276.195,572 1000 8.276,196 10002 0,827
```
Table below refer to test executed from10 thread in parallel each test called 200 times total iteration = 10 x 200 = 2000

```
Name Total duration ms iterations Average duration ms Log per test Mean time for message ms
```
```
OneJson 41,933.134 2000 20.967 12 1.747
```
```
OneSeri 43,799.774 2000 21.900 12 1.825
```
```
TenJson 423,663.899 2000 211.832 102 2.077
```
```
TenSeri 329,678.008 2000 164.839 102 1.616
```
```
TenJsonTemplate 436,712.975 2000 218.356 102 2.141
```
```
TenSeriTemplate 339,060.797 2000 169.530 102 1.662
```
```
ThousandJson 42,494,249.289 2000 21,247.125 10002 2.124
```
```
ThousandSeri 31,674,830.124 2000 15,837.415 10002 1.583
```
```
ThousandJsonTemplate 43,494,051.959 2000 21,747.026 10002 2.174
```
```
ThousandSeriTemplate 31,409,308.217 2000 15,704.654 10002 1.570
```
Test description:

Behavior of test it's the same for all test but each test some parameters that are represented in the name of test:

Each test has one Start and one Stop Log message and repeat N times a block of 10 Log messages.

Name of test has 2 fixed parts and an optional one.

First part is the literal representation of N the number of time the block of message is repeated inside test.

Second part is "Json" for test using as provider .Net JsonConsole or "Seri" for test using SeriLog provider.

Third part describe some details that can change in message content. May consist in:

Template if 6 of the 10 messages are written using Message template

Object if 2 of 10 messages contains object that are serialized differently between the 2 providers do to use of ToString() in .Net JsonConsole


# Evaluate integration of fluentbit environment and EKS stack

## The goal

The goal of this evaluation activity is to understand if integration of Fluent Bit with Elasticsearch stack can be an alternative to OpenTelemetry Logs in
case, when needed, the log feature in OpenTelemetry is still not mature enough.

## The problem

The project momosprj13 located in the OpenShift environment contains a deployment named EKS with Fluent Bit, ElasticSearch and Kibana configured
and running.

```
Fluent Bit is configured to receive TCP inputs in Json format and sent them to Elasticsearch with the index logstash*
Kibana is configured to see the log of the index logstash*.
The same environment contains other tools like Prometheus.
```
Sending a log message in json format to Fluent Bit from a shell in the same Pod like the following:

```
echo { "name":"value", "att1":123 } | nc 127.0.0.1 5170
```
the received Kibana response is:

```
@timestamp:Apr 27, 2021 @ 10:32:20.615 log:{ name:value, att1:123 } log.keyword:{ name:value, att1:123 } _id:
FVV1EnkBaS7s2Wr_NCtg _index:logstash-2021.04.27 _score: - _type:_doc
```
but the expected one looks like this:

```
@timestamp:Apr 27, 2021 @ 10:32:20.615 name:value, att1:123 ....
```
All attributes of our json file (in this case only 2 attributes for simplicity) become part of a unique field named log then it is not possible to access to all
attributes of our log with original timestamp, context information messages and so on.

## Suggested Solution from the Developer Community

The following solutions (using parser with some configurations) seem to deal with our scenario:

```
https://stackoverflow.com/questions/56841754/fluent-bit-splitting-json-log-into-structured-fields-in-elasticsearch
http://www.inanzzz.com/index.php/post/rel5/using-fluent-bit-to-forward-docker-php-fpm-and-nginx-logs-to-elasticsearch
```
## Solution applied to OpenShift Scenario

The initial Config Map in the scenario was:

```
fluent-bit.conf: |
[SERVICE]
Flush 1
Log_Level debug
Daemon off
Parsers_File parsers.conf
HTTP_Server On
HTTP_Listen 0.0.0.0
HTTP_Port 2020
```
```
@INCLUDE input-tcp.conf
@INCLUDE input-http.conf
@INCLUDE input-statsd.conf
@INCLUDE output-elasticsearch.conf
input-http.conf: |
[INPUT]
name http
```
```
Information of our structured log are no more present.
```

host 0.0.0.0
port 8888
input-statsd.conf: |
[INPUT]
Name statsd
Listen 0.0.0.0
Port 8125
input-tcp.conf: |
[INPUT]
Name tcp
Listen 0.0.0.0
Port 5170
Chunk_Size 32
Buffer_Size 64
Format none
output-elasticsearch.conf: |
[OUTPUT]
Name es
Match *
Host 127.0.0.1
Port 9200
Logstash_Format On
Index fluent_bit
Replace_Dots On
Retry_Limit False
parsers.conf: |
[PARSER]
Name apache
Format regex
Regex ^(?<host>[^ ]*) [^ ]* (?<user>[^ ]*) \[(?<time>[^\]]*)\] "(?<method>\S+)(?: +(?<path>[^\"]*?)
(?: +\S*)?)?" (?<code>[^ ]*) (?<size>[^ ]*)(?: "(?<referer>[^\"]*)" "(?<agent>[^\"]*)")?$
Time_Key time
Time_Format %d/%b/%Y:%H:%M:%S %z

### [PARSER]

Name apache2
Format regex
Regex ^(?<host>[^ ]*) [^ ]* (?<user>[^ ]*) \[(?<time>[^\]]*)\] "(?<method>\S+)(?: +(?<path>[^ ]*)
+\S*)?" (?<code>[^ ]*) (?<size>[^ ]*)(?: "(?<referer>[^\"]*)" "(?<agent>[^\"]*)")?$
Time_Key time
Time_Format %d/%b/%Y:%H:%M:%S %z

### [PARSER]

Name apache_error
Format regex
Regex ^\[[^ ]* (?<time>[^\]]*)\] \[(?<level>[^\]]*)\](?: \[pid (?<pid>[^\]]*)\])?( \[client (?
<client>[^\]]*)\])? (?<message>.*)$

### [PARSER]

Name nginx
Format regex
Regex ^(?<remote>[^ ]*) (?<host>[^ ]*) (?<user>[^ ]*) \[(?<time>[^\]]*)\] "(?<method>\S+)(?: +(?<path>
[^\"]*?)(?: +\S*)?)?" (?<code>[^ ]*) (?<size>[^ ]*)(?: "(?<referer>[^\"]*)" "(?<agent>[^\"]*)")?$
Time_Key time
Time_Format %d/%b/%Y:%H:%M:%S %z

### [PARSER]

Name json
Format json
Time_Key time
Time_Format %d/%b/%Y:%H:%M:%S %z

### [PARSER]

Name docker
Format json
Time_Key time
Time_Format %Y-%m-%dT%H:%M:%S.%L
Time_Keep On

### [PARSER]

# [http://rubular.com/r/tjUt3Awgg4](http://rubular.com/r/tjUt3Awgg4)


```
Name cri
Format regex
Regex ^(?<time>[^ ]+) (?<stream>stdout|stderr) (?<logtag>[^ ]*) (?<message>.*)$
Time_Key time
Time_Format %Y-%m-%dT%H:%M:%S.%L%z
```
### [PARSER]

```
Name syslog
Format regex
Regex ^\<(?<pri>[0-9]+)\>(?<time>[^ ]* {1,2}[^ ]* [^ ]*) (?<host>[^ ]*) (?<ident>[a-zA-Z0-9_\/\.
\-]*)(?:\[(?<pid>[0-9]+)\])?(?:[^\:]*\:)? *(?<message>.*)$
Time_Key time
Time_Format %b %d %H:%M:%S
```
After applying the suggestions above in our OpenShift scenario, the ConfigMap for Fluent Bit is the following:

```
fluent-bit.conf: |
[SERVICE]
Flush 1
Log_Level info
Daemon off
Parsers_File parsers.conf
HTTP_Server On
HTTP_Listen 0.0.0.0
HTTP_Port 2020
```
```
@INCLUDE parsers.conf
```
```
@INCLUDE input-tcp.conf
@INCLUDE input-http.conf
@INCLUDE input-statsd.conf
@INCLUDE output-elasticsearch.conf
```
```
input-http.conf: |
[INPUT]
name http
host 0.0.0.0
port 8888
input-statsd.conf: |
[INPUT]
Name statsd
Listen 0.0.0.0
Port 8125
input-tcp.conf: |
[INPUT]
Name tcp
Listen 0.0.0.0
Port 5170
Chunk_Size 32
Buffer_Size 64
Format json
```
### [FILTER]

```
Name parser
Parser json
Match *
Key_Name log
Reserve_Data On
Preserve_Key On
output-elasticsearch.conf: |
[OUTPUT]
Name es
Match *
Host 127.0.0.1
Port 9200
Type logs
Include_Tag_Key On
Tag_Key tag
```
```
parsers.conf: |
```

### [PARSER]

```
Name json
Format json
Time_Key time
Time_Format %Y-%m-%dT%H:%M:%S.%L
Time_Keep On
# Command | Decoder | Field | Optional Action
# =============|==================|=================
Decode_Field_As escaped_utf8 log do_next
Decode_Field_As json log
```
In any case the solution does not work as expected.

Despite trying some different configurations including the ones using Regular Expressions Parser, nothing changes.

Remarks

```
The installation of Fluent Bit and the full stack of Elasticsearch (the target of overall test) must be performed at first at cluster level so that the real
Fluentbit integration with the stndout and Kubernetes can be leveraged
A local installation of Fluent Bit in a Kubernetes on a VDI machine has been also experimented but the configuration is too much complex
compared with the advantage of having more methods to send logs to Fluent Bit.
```
Conclusion

Given all what exposed above, the use of Serilog direct sending logs to Elascticsearch seems a good temporary solution.

The Fluent Bit option is temporarily discarded, if in the future we will resume this option we will continue this analysis and investigate the causes of the
malfunctions: incorrect configuration, software version issues, existence of third-party tools that might interfere with Fluent Bit, etc.


# Provide an example of logging usage inside a specific layer

# with a layer specific context

As a part of User Story 15042: "[STRUCTURED LOG] Provide an example of logging usage inside a specific layer with a layer specific context" some
example logs have been added to information gateway

The original Goal was to insert the property "TransactionId" that identify transactions on the SQL engine to each log of the gateway so that log messages
can be related to others information of the same transation.

Due to some issue described below the goal it's not fulfilled but in any case the example shows

```
how to create a scope in module
how to add properties to scope
how to close scope
```
## The example code

## How to create a scope

In this example we define 2 different scopes inside information gateway

The first with the Section ID and a second with Transaction ID

For each scope a field/property of type IDisposable is need to handle the scope itself, mainly to manage the proper closure of the scope.

```
Fields definition
```
```
private ILogger<InformationGateway> _logger;
private IDisposable _sectionLogScope = null;
private IDisposable _transactionLogScope = null;
```
Opening Section ID scope
The code below shows how to open a scope and save a reference for later close it. The syntax used in the BeginScope method allow to easily add both a
description of scope and a property named "sessionId" that will be applied to log messages until scope is closed
The syntax chosen can be summarized as BeginScope("Description {Property1Name} , {Property2Name}", Porety1Value, Property2Value)
The message "Started new information session" is the first of the new Scope and with the new property.

```
// begin the logging scope with session Id
_sectionLogScope = _logger?.BeginScope("Session {sessionId}", _session.GetSessionImplementation().
SessionId);
```
```
//Log a message to signal the correct start of a new session
_logger.LogInformation("Started new information session");
```
Opening Transaction ID scope

In a similar way inside the method that Begin a transaction we get the transaction id (always "unknown" see issue below) then create a scope and store
reference

This Scope still have a description and a Property but if needed can have more properties.

The message "Begin Transaction" is the first of the new Scope and with the new property.

```
_dbTransaction = _sessionprovider.BeginTransaction(_session);
```
```
//Try to begin a new Logging scope containing the transaction Id to correlate all log of the same
transaction
var transactionId = GetTransactionID(_dbTransaction);
_transactionLogScope = _logger.BeginScope("Transaction {TransactionID}", transactionId);
```
```
//Log message that signal that the transaction is Started
_logger.LogInformation("Begin Transaction");
```

How properties work

Now we can see the result regarding the 2 messages in the previous example, looking at the Scope property of structured log and specific properties of
each message.

The description of the scope is not a new message but in inserted in the Scope property that is an array of descriptions of all open Scopes.

The "start session" message has the new property sessionId and a new Scope

The "begin transaction" message has also the new TransactionID property and the nested Scopes

How to close scope

The scopes are closed by disposing the reference.

Closing Transaction ID scope for example in case of rollback

```
transaction.Rollback();
transaction.Dispose();
_transactionLogScope?.Dispose();
_transactionLogScope = null;
```
Closing Section ID scope

```
_logger.LogInformation("Closing current session");
_sessionprovider.DisposeSession(_session);
_sectionLogScope?.Dispose();
_sectionLogScope = null;
```
The Issue with Transaction ID:

Up to now due some problem retrieving the correct transaction id the string "unknown" is written in place

The method GetTransactionID (actually returning "unknown") is intended to get the correct transaction ID and return it as a string. It contains (actually
commented) some different try of getting the transaction Id.


```
private string GetTransactionID(ITransaction dbTransaction)
{
if (dbTransaction == null)
{
return "no transaction";
}
```
```
var retstr = string.Empty;
```
```
//try to use System.Transaction but the ID used in that Transaction it's different from the one of
the DB engine
using (var ts = new System.Transactions.TransactionScope())
{
DbCommand command = new SqlCommand();
command.Connection = _session.Connection;
```
```
dbTransaction.Enlist(command);
```
```
var distid =Transaction.Current.TransactionInformation.DistributedIdentifier;
var locid =Transaction.Current.TransactionInformation.LocalIdentifier;
```
```
//Independently from System.Transaction this retrieve the SQL transaction ID
//The problem is that it depends on the type of server
//It's valid for SQL Server but not for Oracle, PostgreSQL, MySQL ....
command.CommandText = "SELECT CURRENT_TRANSACTION_ID();";
var ret = command.ExecuteScalar();
retstr = $"{ret}";
}
```
```
return retstr;
}
```
NHibernate doesn't give access to the transaction ID.

A try has been done of using System Transaction (see using clause at line 11 and information retrieved at line 18 and 19) but the identifier returned in this
case are the ones that identify transaction inside Microsoft framework and not in the DB engine.
Another try has been implemented to query transaction id from the engine as soon the transaction is created.
The implementation is the one at the line 24 and following and works fine also without using clause at line 11.
The limitation of this implementation it's that it work only for SQL Server. The use of other DB engine, like Oracle, PostgreSQL or MySQL, need to call a
different query/procedure.
May be that an implementation of such queries can be done in the ORM layer or in a layer as close as possible to it.

see https://vladmihalcea.com/current-database-transaction-id/ for other details.

Note:
Log messages added to information layer are just a first piece of information the content and the level of messages must be improved.


# Example of logging usage inside a metadata model

```
Example of Logger usage
How to add logging contextual properties local to a metadata model
Sample output
```
## Example of Logger usage

In order to add logging inside all metadata model classes (both services and models), simply exploit the extension methods available for Logger property.

```
Information log sample
```
```
Logger.LogInformation("Created ProductionOrderHeader '{ProductionOrderHeaderName}'", ProductionOrderHeader.
Name);
```
Remember to add logs for error/warning paths.

```
Error log sample
```
```
if (!hashset.Add(name))
{
Logger.LogError("Operation Name '{B2MML_ProductionOrderOperationName}' occurred more than once.", name);
// Return error logic
}
```
Use LogDebug to add logs useful for debugging the application.

```
Debug log sample
```
```
Logger.LogDebug($"Validating ImportProductionOrderService...");
```
Use LogTrace to add detailed logs. This is the most verbose logging level.

```
Trace log sample: here detailed information on the received input is logged
```
```
Logger.LogTrace("Import B2MML document: {B2MMLDocument}", B2MMLDocument);
```
## How to add logging contextual properties local to a metadata model

Within metadata model business logic, you can add local logging contextual properties.

As an example, consider ProcessB2MMLProductionSchedule shopfloor service.
This service may be issued to import from B2MML multiple ProductionRequest. While importing data for each ProductionRequest, it is useful to add
contextual information regarding the current ProductionRequest.

To accomplish it, you can simply leverage on Logger.BeginScope API and pass to it a message that contains the required contextual properties.

```
foreach (var productionRequest in dataArea.ProductionRequest)
{
using var logScope = Logger.BeginScope("Import B2MML for Production Request
'{B2MML_ProductionRequestId}'",
productionRequest.ID.Value);
// Here add logic to import the current ProductionRequest
} // At the end of the scope, local contextual information will no more be present in logs
```
```
If a log message contains sensitive data, then Trace Level must be used to log it.
```

Sample output

```
Sample log output using a custom console formatter for metamodel unit tests
```
```
2021-06-14 13:55:19.945 ### Debug ### {OriginalFormat}=Validating ImportProductionOrderService...
###
2021-06-14 13:55:19.946 ### Trace ### B2MMLDocument=<?xml version="1.0" encoding="utf-8"?
><ProcessProductionSchedule releaseID="0.0.2" xmlns="http://www.mesa.org/xml/B2MML-V0600" xmlns:xsi="http://www.
w3.org/2001/XMLSchema-instance" xsi:schemaLocation ="http://www.mesa.org/xml/B2MML-V0600 B2MML-V0600-
ProductionSchedule.xsd" xmlns:Extended="http://www.mesa.org/xml/B2MML-V0600-AllExtensions"
><ApplicationArea><Sender><LogicalID>ModularMOM</LogicalID></Sender><Receiver><LogicalID>ModularMOM</LogicalID><
/Receiver><CreationDateTime>2020-06-21T13:20:00.000-05:00</CreationDateTime><
/ApplicationArea><DataArea><Process></Process><ProductionSchedule><ProductionRequest><ID>Z2-PO-Demo-0911<
/ID><StartTime>2020-09-10T03:20:00.000-05:00</StartTime><EndTime>2020-09-11T13:20:00.000-05:00<
/EndTime><Priority>5</Priority><SegmentRequirement><ID>0010<
/ID><MaterialRequirement><MaterialDefinitionID>Steel</MaterialDefinitionID><MaterialUse>Consumable<
/MaterialUse><Quantity><QuantityString>500</QuantityString><UnitOfMeasure>Unit</UnitOfMeasure></Quantity><
/MaterialRequirement><MaterialRequirement><MaterialDefinitionID>Mat-1<
/MaterialDefinitionID><MaterialUse>Consumable</MaterialUse><Quantity><QuantityString>100<
/QuantityString><UnitOfMeasure>Unit</UnitOfMeasure></Quantity></MaterialRequirement><Extended:Type>Assembling<
/Extended:Type><Extended:WorkCenterName>WC-Demo911</Extended:WorkCenterName><Extended:Milestone>milestone-1<
/Extended:Milestone></SegmentRequirement><SegmentRequirement><ID>0020<
/ID><MaterialRequirement><MaterialDefinitionID>Copper</MaterialDefinitionID><MaterialUse>Consumable<
/MaterialUse><Quantity><QuantityString>600</QuantityString><UnitOfMeasure>Unit</UnitOfMeasure></Quantity><
/MaterialRequirement><MaterialRequirement><MaterialDefinitionID>Mat-1<
/MaterialDefinitionID><MaterialUse>Consumable</MaterialUse><Quantity><QuantityString>200<
/QuantityString><UnitOfMeasure>Unit</UnitOfMeasure></Quantity></MaterialRequirement><Extended:Type>Painting<
/Extended:Type><Extended:WorkCenterName>WC-Demo912</Extended:WorkCenterName><Extended:Milestone>milestone-2<
/Extended:Milestone></SegmentRequirement><Extended:Type>Manufacturing</Extended:Type><Extended:
AutomaticLaunch>false</Extended:AutomaticLaunch><Extended:LocationName>Delhi</Extended:LocationName><Extended:
Material><Extended:ID>P003</Extended:ID><Extended:Revision>P2020</Extended:Revision></Extended:
Material><Extended:Quantity><Extended:QuantityString>100</Extended:QuantityString><Extended:UnitOfMeasure>Each<
/Extended:UnitOfMeasure></Extended:Quantity></ProductionRequest></ProductionSchedule></DataArea><
/ProcessProductionSchedule> ### {OriginalFormat}=Import B2MML document: {B2MMLDocument}
###
2021-06-14 13:55:19.947 ### Information ### ProductionOrderHeaderName=Z2-PO-Demo-0911 ### {OriginalFormat}
=Created ProductionOrderHeader '{ProductionOrderHeaderName}'
### B2MML_ProductionRequestId=Z2-PO-Demo-0911
```

# Logging Runtime Configuration - finding about wrapping

# Serilog with a custom logger

## Target

The goal of the Feature 14321: "Observability Runtime Configuration - Backend" was to introduce a way to have different level of log and trace in different
execution context with the capability to change it at run-time.

For example in cyclic operations like health-check a lower amount of information is needed with respect to the default configured, instead in the context of
execution of a particular RestAPI a large amount of information can be necessary to diagnose a problem.

## Idea and Implementation

The idea is to put logic to filter messages on level and category inside a part of our code so that can be extended with the logic that take in account also
the context (RestAPI or Cyclic).

The solution, that had limited impact on the existing code and interaction with AspNetCore and SeriLog, was to create a custom logger implementing
ILogger interface that instantiates and wraps an ILogger that is the one of SeriLog.

For doing this a LoggerProvider (implementing ILoggerProvider) has been implemented so that wraps and calls SerilogLoggerProvider and creates
instances of the new logger class. Extension methods to configure the new LoggerProvider at StartUp has been implemented too.

SeriLog keeps its static configuration with Enricher and Sinks but set the minimum Log Level to Verbose so that do not perform any filter.

For the configuration of a specific context the the OpenTelemetry.Baggage has been used

Also on AspNetCore side Middleware didn''t change and the only change was to be sure to not configure any filter for the new logger class.

All the filtering logic was placed in the new Log Wrapper in this way was possible to increase or decrease LogLevel threshold.

To do this in a clean why and to avoid confusion the classical AspNetCore Logging section and the SeriLog section of appsettings file has been removed.
Information in AspNetCore Logging section, concerning filtering by level and category, has been moved in a section ModularMOM::Logging

Cyclic context are defined in the code itself inside for example health-check threads.
RestAPI can had baggage to header like for example
baggage:MinimumLogLevel=Trace

MinimumLogLevel is fixed

Trace can be any valid AspNetCore Loglevel

configuring log level also trace level is changed accordingly

## Definition of level with category (class or namespace) granularity

In Rest API is possible to define log level with a category granularity like usually done with AspNetCore

some thing like:

baggage:MinimumLogLevel=Debug,Siemens.MOM.Platform.Information_MinimumLogLevel=Trace,Microsoft_MinimumLogLevel=Warning

## Limit

Changing log Level for a class or namespace only affect log not trace in current implementation

## Note

Changing log Level for a class or namespace may change the implementation in the moment we'll decide to support organization in group of Log and trace
provider.

## Test

All this implementation, also the one that may change in future, is covered by some Unit tests and a meaningful tests in ItegrationTest solution, in project
Siemens.MOM.Platform.ApiIntegrationTests in the class LoggerTest.


# How to use observability data?

Observability data including logs, traces and metrics are produced and collected by instrumenting the application. The collected data can be used for
various purposes such as troubleshooting and debugging, monitoring and alerting, usage pattern analysis, etc. In fact, observability data can provide
valuable insight about the application and user behavior which can be used to understand the root cause of issues and to improve the system. However, in
order to achieve these goals, there are two main steps:

Instrumentation

How we "instrument" the application to generate observability data has a crucial role in the effectiveness of observability stack for troubleshooting,
monitoring, user behavior analysis, and so on. "Relevant", "appropriate" and "valuable" observability data including logs, traces and metrics should be
produced and collected. Generating a lot of uninteresting log entries, trace spans or metric values results in a large amount of noise in the observability
data which not only affects the performance but also makes the analysis and maintenance of data difficult. Instrumenting an application properly is not, for
sure, trivial but can be considered as an skill that can be learned and improved over time by gaining more experience. Initially and at the beginning, it is
better not to instrument the code generously and arbitrarily but cautiously and conservatively and to improve it over time. In other words, instrumenting an
application should be considered as on on-going process which is being adjusted and improved continuously.

When instrumenting an application, it is important to consider the role and purpose of each pillar of observability, namely logging, tracing and metrics. A
log entry is the record of an event that happened in the system. Logs provide a complete and accurate record of events including, for example, the system
state when the event occurred. A trace is the record of a series of causally related event which are part of the same request flow. The events of a trace do
not necessarily occur inside a single application (distributed trace). A trace, in fact, tracks the journey of a single request as it is handled by different
modules, services or apps. The events inside a trace are referred to as "spans" (or activities in .NET). A span (activity) corresponds to a single unit of work
or operation. Therefore, when instrumenting an app for tracing, we need to decide about which units of work or operations are critical or important to track
in each layer of apps while handling a request. Some sample factors in this decision can be tracking performance or memory utilization of some specific
operations, or tracking some usage pattern.

A Trace is a tree of spans. The root of the tree shows the end-to-end runtime for the entire request. Each span also shows the amount of time spent by the
operation the span represents. For example, the following figure shows the trace for the request Create Factory in sample model. As you can see, the
trace is a tree which tracks the method call in different layers of the application. The root span of the trace corresponds to the entire POST request and
took 427.63 ms. The trace shows that around 324.1ms of 427.63ms (around 75%) was spend in Metadata engine for dynamically creating and initializing
objects like Factory and its sub-entities like ProductionStatus or Location. 82.43ms of 427.63ms (around 19%) was spent in API layer to populate
previously created business objects with values of DTO sent by the request. Finally, 12.11ms of 427.63ms (around 2.8%) was taken by information layer to
submit the corresponding write models in db. Actually, tracing can provide a performance-based profiling as we have seen in this example. Here, spans
correspond to method calls but spans can also be created for parts of the body of the method call.

In addition to runtime, each span has tags providing meta data or contextual information. For example, the following figure shows that we added a tag to
CreateInstance methods to show the type of the object which has been created:


As we have seen, traces can provide an overview of how a request has been handled in the system. Since spans record method calls or operations
happened in the system, when adding logs to the application we should be careful not to generate redundant log entries which they have been also
covered by spans. In other words, log entries and spans should complement each other but not repeat each other. For example, in the following piece of
code, the log entries seem to be redundant since we have a span for the method call which can be enriched with tags regarding object type and id:

The power of observability is that we can associate log entries with spans or metrics with spans (via trace and span Ids). Therefore, we should generate
them in a way to complement and enrich each other.

In conclusion, we use tracing to provide a high-level view and an overall flow while handling a request and we use logging and metrics to provide more
details to this overall flow. Therefore, logs should not be used to record the flow. For example, we may not want to track operations for getting an entity
from the cache, therefore, we do not add tracing to those operations. However, we may be interested to record cache hits, therefore, we add a log entry
when an entity is served from memory cache or Redis. Similarly, we may not want to track operations for adding an entity to cache but we may be
interested to record when an entity is added to the cache. Therefore, we log events for adding an entity to cache.

Considerations when instrumenting

Previously, we discussed that observability data should complement but not repeat each other. For example, in case we use tracing to track some method
call, we should not use also logging to track the same method call.

Here, we discuss some additional considerations when adding log events to the application.

Log level: A log levels tells how important a log message is. Log levels should be used properly in order to reflect the importance or severity of log
messages. Alerts can be configured based on log levels (for example for Error or Critical levels) and also log messages can be filtered based on log level
(refer to Structure Logging for explanation of different log levels). By configuring log level of the application, we can limit the amount of log messages which
are generated by the application. Note that we have the order for the log levels is as follows:

```
Critical Error Warning Information Debug Trace
```
When configuring the log level for the application, all the log events with that level or levels to the left will be generated. For example, for the level
"Information" all logs with level "Information", "Warning", "Error" and "Critical" are generated. As move to the right in the sequence of log levels, more log
messages are generated. In summary, we adding a log event we need to decide about the level and it is not appropriate to generate the same log events
with several log levels like in the following code:


Currently in our code base, log messages with level "Debug" are mostly parametrized queries:

Log messages with level "Information", currently in our code base seem to be for tracking method calls. As we discussed for method calls which are also
tracked by tracing, these log entries are redundant.


Moreover, it seems we have similar log entries in our code base which should be avoided:

One important part of a log event is the log message itself. The log messages should be readable not only for developers of the code but also for other
developers and especially end users for level "Information" (and levels to its left in log level sequence). For example, the following log messages are not
readable:

Instead of "Set Cache Async for entity of type "Location", it may be better to say something similar to "Added to cache an entity of type "Location". Such
message makes it easy to filter and count number of adding to cache.

In summary we need to consider the following items when instrumenting an application:

```
Log level or the purpose of the log event. For example, if the level is "Information", we need to think how informational it is. The following log
message does not seem to carry any interesting information:
```
```
Log message: log messages should be meaningful and understandable. Avoid cryptic log messages the require deep understanding of the
program internals. Always think about the audience of the logs: end users, other developers, system administrators.
Log context and metadata: parameters or other contexts should be added to logs to make it understandable. However, sensitive information
should not be logged.
```
Analysis

After producing and collecting observability data, using and analyzing effectively the collected data is important. This is our current development setup for
observability:

```
Storing of traces and logs in Elasticsearch and metrics in Prometheus
Traces are collected from the apps by OTel collector which then exports traces to Jaeger collector. The backend of Jaeger collector is
Elasticsearch. (we may consider to export traces directly from OTel collector to Elasticsearch and remove Jaeger from the stack. It
seems that we need APM component of Elastic for that which needs a license.)
Logs are written to Elasticsearch by the application.
Using of Jaeger UI to visualize traces as Gantt charts or graphs. (as well as explore and search traces)
Using of Kibana to
explore and search logs
create diagrams, charts or graphs from logs for monitoring or alerting purposes.
```

```
Using of Grafana to visualize metrics via graphs
```
Jaeger UI

In Jaeger UI, we can explore the traces of different services or apps. It is possible to apply some filters like operation name, tag values or duration. It is
also possible to view individual traces as Gantt charts (trace timeline) and inspect all the meta data and contextual information of individual spans like span
Id, trace Id, tags, etc.

Kibana

We use Kibana to explore and analyze logs which stored in Elasticsearch. The exploration of logs from different indices can be done from the "Discover"
menu item. As the figure in the following shows, different indices as the source of data can be chosen in "2", fields which are shown in the list of logs can
chosen in "3" and finally search criteria can be chosen in "1":


Additional information about KQL can be found in https://www.elastic.co/guide/en/kibana/7.12/kuery-query.html. As we can see in the previous figure, one
useful filtering of logs is based on trace Id. The trace Ids can be found easily in Jaeger UI (also in jaeger-span index in Kibana).

Kibana visualization can also be used to create diagrams, graphs and charts for analysis and monitoring purposes. In the following, we can see some
basic charts like count of log messages for various API calls in different modules, count of exceptions in different modules, count of logs per trace Id in a
module, count of log message per level in a module:

Grafana

Metrics are collected by Prometheus from MOM modules. Currently, Prometheus libraries SystemMetrics and AspNetCore are used to generate system
related metrics such as memory usage, CPU usage, ... as well as ASP Net core metrics. The raw metrics can be viewed via metrics endpoint of the
module (e.g. [http://samplemodel-modmom-18.apps.openshift03.swqa.tst/metrics),](http://samplemodel-modmom-18.apps.openshift03.swqa.tst/metrics),) Prometheus link (e.g. [http://prometheus-modmom-18.apps.openshift03.](http://prometheus-modmom-18.apps.openshift03.)
swqa.tst/graph) and explore menu item in Grafana as in the following figure. Using explore, we can view all the raw metrics gathered by Prometheus and
visualized them as a graph.


However, in order to easily monitor values of metrics, dashboards with different graphs can be created. Prometheus query language (PromQL) should be
used to aggregate and process raw metrics. As an example, Modular MOM 2.x dashboard is created with some sample graphs for memory and CPU
utilization using raw metrics related to node and process.

Two free dashboards for Prometheus net metrics (Prometheus-net, ASP.NET Core - controller summary) are also imported which can be used for
monitoring, for example, GC collection count.

In addition to metrics from different MOM modules like sample model, metrics from other services such as Jaeger and OTel collector are gathered in
Prometheus and can be used for monitoring and troubleshooting:


The Kubernetes cluster needs also to be monitored by gathering metrics, for example, related to Pods, containers, etc.

In summary, in order to use data gathered for metrics effectively, first we need to understand the raw metrics themselves and then to understand how to
process them using PromQL in order to produce meaningful values for monitoring.


# Runtime Layer

This is a macro group to contain all the internal architecture documentation related to the Runtime layer. This group contains:

```
The runtime behaviors.
The Metadata Engine interactions.
```

# API Interaction

This section describes the different APIs available to access Platform Services.


# REST API

```
The REST API accepts objects passed in the content-body. ASPNETCore transforms the body into objects required by the endpoint controllers.
```
```
General flow
__inputData
Data field
Object reference
Object
Clearing Values
Updating lists
__requestData configuration
__opitions for the response
Simple request without __options the response
__requestData and __responseData
Native Fields
Enum Field
Endpoint : GetAll
Endpoint : Get
Enum values returned in SelectionValues based on Query mode:
Object References and Lists
Subentities, embedded objects and object lists
Subclasses
Pagination
__requestData
Selection Values
__queryRequest (Not implemented yet)
__exceptions (Not implemented yet)
Additional Processing
API Controller Signatures
```
## General flow

```
For most services, the controller uses the __inputData from the body of the http request to set values in the service. It then executes the service or
performs one of the exposed methods. Performing a method will not cause any changed values to be persisted. Frequently, performing a method is
used to retrieve data to populate or configure the UI. E.g. getting SelectionValues, ESig or Data Collection configurations...
```
```
Once the service completes processing, the endpoint controller will use the __requestData from the body to determine what values need to be used to
populate the __responseData. After the controller is complete, ASPNETCore serializes the generated responseData to the body of the http response.
```
```
The Additional Processing section describes some more advanced use cases.
```

__inputData

The __inputData format is generated based on fields in the service, and values that can be set are based on the field metadata.

Data field

Non-list data fields can be set with appropriate data:

```
Non-list data fields
```
```
"Name": "GigaFactory",
"StatusEnum": "down",
"StartupDate": "2000-01-26T14:15:16.001Z-08:00",
"hoursPerDay": 42,
```
Object reference

Object reference fields are set using RefInfo types:

```
Object References
```
```
"Enterprise": {
"__name": "Tesla"
},
// Set a specific revision of a revisioned object
"BOM": {
"__name": "Tesla",
"__rev": "A",
"__type": "BOMSubclass"
},
// Setting a revisioned object to use RevOfRcd
```

```
// is achieved by not passing __rev.
"Workflow": {
"__name": "Tesla"
}
```
Object

Object fields are simply collections of fields, so an owned subentity looks like a regular object:

```
Address object
```
```
"Address": {
"City": "San Jose",
"State": "CA",
"Country": "USA"
}
```
as does a list of objects:

```
List of objects
```
```
"Locations": [
{
"Name": "Nevada",
"Thruput": 15000,
"Operation": {
"__name": "Op1"
},
"Units": "perDay"
},
{
"Name": "Berlin",
"Thruput": 12000,
"Operation": {
"__name": "Op3"
},
"Units": "perDay"
}
]
```
Clearing Values

Updating fields in an object (usually while maintaining a modeling entity) need a little more information. As it can be hard to determine if a field hasn't
been specified or is intended to be cleared, an explicit instruction is used to inform the controller. The instruction can be used in nested objects as well.

```
Clearing fields
```
```
"__fieldsToClear": [ "hoursPerDay" ],
"Address": {
"__fieldsToClear": [ "City", "Country" ]
}
```
Updating lists

Updating a data field or a reference field is straight forward, as the data uses the same format as when the field was set during creation. Updating lists
requires a little more information also so the action that should be taken for each element is passed with the data.

These examples are acting on lists that have existing data. To clear a list in its entirety, use the "__fieldsToClear" notation above. Combining that with
the examples below, the whole list can be replaced.


A list of strings:

```
Modification of a list of owned objects
```
```
"keywords": [
// delete an item
{
"__key": "of chars",
"__listItemAction": "Remove"
},
// append a couple items
{
"__value": "of"
},
{
"__value": "characters",
"__listItemAction": "Append" // optional
},
// replace an item
{
"__key": "arrays",
"__listItemAction": "Update",
"__value": "lists"
}
]
```
A list of references:

```
Updating a list of object references
```
```
"Operations": [
// Append an item
{
"__value": { "__name": "SubOp 03", "__type": "OperationSubtype" }
},
// Remove an item
{
"__key": { "__name": "Operation 01" },
"__listItemAction": "Remove"
},
// Update/Replace an item
{
"__key": { "__name": "Operation 02" },
"__value": { "__name": "Operation 04" },
"__listItemAction": "Replace"
}
]
```
And a list of owned objects (sub-entities)

```
Updating a list of owned objects (sub-entities)
```
```
"Locations": [
// Update an item
{
"__key": {
"__index": 1,
"__name": "Berlin",
"__parent": {
"__name": "GigaFactory"
}
},
"__listItemAction": "Update",
"__fieldsToClear": [
```

```
"Thruput",
"Units"
]
},
// append a new item
{
"Name": "Milwakee",
"Thruput": 1050,
"Units": "perDay",
"Operation": {
"__value": { "__name": "Operation 08" }
}
},
// replace an item
{
"__key": {
"__name": "Austin",
"__parent": {
"__name": "GigaFactory"
}
},
"__type": "LocationSub",
"__listItemAction": "Replace",
"Name": "Houston",
"Thruput": 1250,
"Units": "perHour",
"Operation": {
"__value": { "__name": "Operation 03" }
}
},
// Delete an item
{
"__key": {
"__name": "Shanghai",
"__parent": {
"__name": "GigaFactory"
}
},
"__listItemAction": "Remove"
}
]
```
__requestData configuration

__opitions for the response

The __options attribute configures what data is returned in the __responseData. The options for a request are specified by the "__options" property of
the __requestData object. The options can be overridden for each object requested where a property can be specified to override the inherited values.

Note: The values in this code block are how the system behaves if the "__options" property is not specified.

```
__options for requesting data
```
```
"__requestData": {
"__options": {
// In subobjects, false can be explicit to override a value of true
// that was specified earlier in the hierarchy
```
```
// Return values of data fields, refInfo of object reference fields.
// For Object fields (e.g. subentities), returns selected fields or
// all serializable fields if specific fields aren't selected.
// __value will be omitted for null objects
"__value": true,
```
```
// In the case of object references, the default behavior is to
// return the alternate key information. The options for this
// configuration are:
// __altKey - This is the default. It returns the alternate
```

```
// key information of the object.
// __id - This returns the instanceId of the object
// __all - This returns the alternate key information and the
// instanceId of the object.
"__refInfo": "__altKey",
```
```
// Return label information for the field
"__label": false,
```
```
// Return the type of the field. TODO: Need a list of return values,
// and also if a list field has any impact on the type.
"__type": false,
```
```
// Return selectionValues for the field (i.e. picklists)
"__selectionValues": null,
```
```
// Specify paging control for list fields
"__listPagination": null
}
}
```
To request a different level of detail for an object than what was specified in the "__options", specify the overrides in the field request. This override
will apply for any subfields. Note that not all of the configuration options need to be specified here. A single property can be specified.

```
Override __options for a sub object
```
```
"__requestData": {
"__options": { "__value": true, "__refInfo": "__altKey", "__label": false, "__type": false,
"__selectionValues": null, "__listPagination": null },
```
```
// overrides the __value setting from the __inputData for any fields
// serialized within the ServiceDetails1 context, including any
// nested objects
"ServiceDetails1": {
"__options": {
"__refInfo: "__all" }
}
},
```
```
// uses settings from the __inputData
"Description": {}
}
```
Simple request without __options the response

Example of how the system responds if __options is not specified.

```
Simple request
```
### {

```
"__requestData": {
"EmptyField": { },
"NamedRef": { },
"RevisionedRef": { },
"Native": { },
"Subentity": { },
"NamedSubentity": { },
"SubentityRef": { },
"NamedSubentityRef": { },
"ListNamedRef": { },
"ListRevisionedRef": { },
"ListNative": { },
"ListSubentity": { },
"ListNamedSubentity": { },
"ListSubentityRef": { },
```
```
Simple response
```
### {

```
"__responseData": {
// If a field is null, it is
output as an empty object. This will
// show that the field has been
processed and not skipped.
"EmptyField": {}
"NamedRef": {
"__value": {
"__name":
"Resource1"
}
},
"RevisionedRef": {
"__value": {
"__name":
```

"ListNamedSubentityRef": { }
}
}

```
"ProductName",
"__rev":
"RevName",
"__useROR": true
}
},
"Native": { "__value": 42 },
"Subentity": {
"__value": {
"Native1": {
"__value": 78 },
"NamedRef1": {
"__value":
{
```
```
"__name": "Resource1"
}
}
}
},
"NamedSubentity": {
"__value": {
"Name": {
"__value": "NameOfSubentity" },
"Native2": {
"__value": 78 },
"NamedRef2": {
"__value":
{
```
```
"__name": "EnterpriseName"
}
}
}
},
"SubentityRef": {
"__value": {
"__id":
"0ddsa5ase66ds",
"__parent": {
"__name":
"Factory1"
}
}
},
"NamedSubentityRef": {
"__value": {
"__name":
"Location1"
"__parent": {
"__name":
"Factory1"
}
}
},
"ListNamedRef": { "__value": [
{ "__name": "Machine1" }
{ "__name": "Machine6" }
{ "__name": "Machine8" }
]},
"ListRevisionedRef": { "__value": [
{ "__name": "Standard",
"__rev": "A1", "__useROR": true }
{ "__name": "Standard",
"__rev": "A2", "__useROR": false }
{ "__name": "Check",
"__rev": "665", "__useROR": true }
]},
"ListNative": { "__value": [58,
32, 44] },
"ListSubentity": {
```

```
"__value": [
{
"TxnId": {
"__value": "000410800000001c"},
"TxnDate":
{ "__value": "2021-11-30T18:45:06.015+01:00" },
```
```
"TrackedObject": { "__value": { "__name":
"Container5", "__type": "Container" } }
},
{
"TxnId": {
"__value": "000410800000001d"},
"TxnDate":
{ "__value": "2021-12-31T23:44:00.026Z" },
```
```
"TrackedObject": { "__value": { "__name":
"Container5-1", "__type": "UnitContainer" } }
}]
},
"ListNamedSubentity": {
"__value": [
{
"Name": {
"__value": "San Jose" },
"Local": {
"__value": true },
"Parent":
{ "__value": { __name: "GigaFactory" } },
```
```
"Timezone": { "__value": "UTC-8" }
},
{
"Name": {
"__value": "Austin" },
"Local": {
"__value": false },
"Parent":
{ "__value": { __name: "GigaFactory" } },
```
```
"Timezone": { "__value": "UTC-6" }
},
]
},
"ListSubentityRef": {
"__value": [
{ "__id":
"asd5565gasd65" },
{ "__id":
"awr56ytrwrtio" },
]
},
"ListNamedSubentityRef": {
"__value": [
{ "__name": "San
Jose", "__parent": {"__name": "GigaFactory"} },
{ "__name":
"Austin", "__parent": {"__name": "GigaFactory"} }
]
}
}
}
```
__requestData and __responseData

The purpose of the __requestData is to configure the request so only data that is needed is returned. In addition to the value of a field, additional
information can be requested. The __requestData and __responseData formats are generated based on fields in the service.


Native Fields

To request a field, simply add an object. To request a different level of detail for a field than what was specified in the "__options", specify the
overrides in the field request:

```
Requesting data fields
```
```
// Requesting a field, overriding the default
return data
"Name": {
"__label": true
},
"Qty": {
"__type": true
},
"LastActivityDate": {
"__label": true,
"__type": true
},
"TxnComments": { },
```
```
"ResourceStatusEnum": { }
```
```
Responses for data requests
```
```
"Name": {
"__value": "JohnJacob",
"__label": "User name"
},
"Qty": {
"__value": 586,
"__type": "Qty"
},
"LastActivityDate": {
"__value": "2021-01-21T15:45:00.002Z-08:
00",
"__label": "Last Activity Date",
"__type": "dateTimeOffset"
},
"TxnComments": {
"__value": [
"I love to comment on my
transactions",
"Some people don't want to,
though",
]
"__type": string
},
"ResourceStatusEnum": {
"__value": "Down"
}
```
Enum Field

Endpoint : GetAll

When an entity returns a value from an enum field, the default response format is :

```
Enum Field Response
```
```
"Status":{
"__Value":{
"__Value": 0 // numeric value
"__Name" : "InProcess" // enum Name
"__LocalizedName" : "In Process - En"
}
}
//LocalizedName : If enumName key is configured in resx file then localized value will be returned
```
There are three different attributes that may be returned, and they can all be set in the request. To customize the return format, provide
__EnumOptions in the request:

```
Enum Request Option
```
```
// Reqesting EnumOptions at GlobalLevel
{
"__RequestData": {
"__Options": {
"__EnumOptions": {
"__Value": {
"__Value":
false,
"__Name":
true,
```
```
Enum Response
```
```
"Status":{
"__Value":{
"__Name" : "InProcess"
"__LocalizedName" : "In
Process - En"
}
}
```

```
"__LocalizedName": true
}
}
}
}
}
```
EnumOptions can also be supplied at the entity field level, with alternative settings, overriding the global configuration. This can be applied at the field
level as well.

```
Enum Request Option
```
```
// EnumOptions provided at field in entity
{
"__RequestData": {
"__Options": {
"__EnumOptions": {
"__Value": {
"__Value":
true,
"__Name":
true,
```
```
"__LocalizedName": true
}
}
},
"Status": {
"__EnumOptions": {
"__Value": {
"__Value":
false,
"__Name":
true,
```
```
"__LocalizedName": true
}
}
}
}
}
```
```
Enum Response
```
```
"Status":{
"__Value":{
"__Name" : "Ready"
"__LocalizedName" : "Ready
```
- En"
}
}

```
Enum Request Option
```
```
// EnumOptions provided at field in Subentity
{
"__RequestData": {
"__Options": {
"__EnumOptions": {
"__Value": {
"__Value":
true,
"__Name":
true,
```
```
"__LocalizedName": true
}
}
},
"Subentity": {
"Status": {
"__EnumOptions": {
"__Value":
{
```
```
"__Value": false,
```
```
Enum Response
```
```
"Status":{
"__Value":{
"__LocalizedName" : "Ready
```
- En"
}
}


```
"__Name": false,
```
```
"__LocalizedName": true
} } } } } }
```
If no enum options are provided at the field level, the options from the parent level will be applied, recursively traversing up till the root of the Request
and enum options from the GlobalOptions will be applied. If no enum options are specified any where, default response will be returned.

Endpoint : Get

Since we don't have a request payload for Get endpoint, all attributes of enum will be returned by default.

```
Enum Response
```
```
"Status":{
"__Value":{
"__Value": 0
"__Name" : "InProcess"
"__LocalizedName" : "In Process - En"
}
}
```
Enum values returned in SelectionValues based on Query mode:

To configure enum options for SQL query, we need to update the *.momsql.json file, where we provide the configuration for enum return attributes.

```
Enum Filed Configruation in Inquiry Service
```
```
"ColumnHeaders": {
"0": {
"Name": "$PersistentEntity.ByteEnum",
"Alias": "Label_Default",
"Label": "EnumFormat:Default"
},
"1": {
"Name": "$PersistentEntity.ByteEnum",
"Label": "EnumFormat:-Name",
"Alias": "Label_Name",
"EnumFormat": "Name"
},
"2": {
"Name": "$PersistentEntity.ByteEnum",
"Label": "EnumFormat:DisplayName",
"Alias": "Label_DisplayName",
"EnumFormat": "DisplayName"
},
"3": {
"Name": "$PersistentEntity.ByteEnum",
"Label": "EnumFormat:Value",
"Alias": "Label_Value",
"EnumFormat": "Value"
}
}
```
Example Enum definition:

```
Enum
```

```
public enum EntityStateEnum : byte
{
[EnumDisplayName("DN-Created")]
[EnumDescription("Desc for Created")]
Created = 0,
```
```
[EnumDisplayName("DN-Active")]
[EnumDescription("Desc for Active")]
Active,
```
```
[EnumDisplayName("DN-Closed")]
[EnumDescription("Desc for Closed")]
Closed
}
```
Query configuration and the corresponding Response format:

```
Enum Field Request
```
### "3": {

```
"Name": "$Entity.Status",
"Alias": "Status",
"EnumFormat": "Value"
},
"4": {
"Name": "$Entity.Status",
"Alias": "Status_Localize",
"EnumFormat": "Name"
},
"5": {
"Name": "$Entity.Status",
"Alias": "Status_Localize",
"EnumFormat": "DisplayName"
}
```
```
Enum Response
```
```
"Status\" : { \"__Value\" : \"1\"}
```
```
"Status_Localize\" : { \"__Value\" : \"Active\"}
```
```
"Status_Localize\" : { \"__Value\" : \"DN-Active\"}
//OR"Status_Localize\" : { \"__Value\" : \"Active-
DE\"}
```
```
**returns localized display name, if localization available,
otherwise it returns key (resx)
```
Object References and Lists

Objects can be referenced individually and in lists. By default, only the value is returned, but any desired subfields can be requested.

```
Accessing fields from an object reference or reference list
```
```
"Product": {
"__options": {
"__refInfo": "__all"
}
"Workflow": {}
},
"Resources": {
"Entries": {
"__options": {
"__refInfo": "__all"
}
}
}
```
```
Responses for data requests
```
```
"Product": {
"__value": {
"__id": "88sdzko~aiuds"
"__name": "Pepperoni",
"__rev": "1",
"__isROR": true
"Workflow": {
"__value": {
"__id": "88sdzko~aiuds"
"__name": "Standard",
"__rev": "A1",
"__isROR": true
}
}
},
"Resources": {
"Entries": { "__value": [
{ "__id": "acd398sz_w992aoqu",
"__name": "Machine1" }
{ "__id": "acd398sz_w992a3so",
"__name": "Machine6" }
{ "__id": "acd398sz_w992ow6u",
"__name": "Machine8" }
]}
},
```

Subentities, embedded objects and object lists

These should not be confused with Object References. In this example, The list of steps will be returned with all of the fields, the FieldInLocation field
will be returned for each Location and the entire ProductCostInfo object will be returned.

```
Accessing fields from an embedded object or object list
```
```
"currentStatus": {
"__options": {
"__type": true,
"__label": true
},
"subentityRef" : {}
},
```
```
// Subentities will be returned with all subfields
"SubentityList": { },
```
```
// All fields returned
"NamedSubentityList1": {
"__options": {
"__refInfo": "__all"
}
},
// All fields returned
"NamedSubentityList2": {
"__options": {
"__refInfo": "__all"
}
},
```
```
// Only "Spec" will be returned
"NamedSubentityList2": {
"Spec": {}
},
```
```
// All fields will be retuned for Subentity and
only value for the SubentityRef
"Product": {
"Subentity": {
"__options": {
"__refInfo": "__altKey"
}
},
"SubentityRef": {}
}
```
```
Responses for data requests
```
```
"currentStatus": {
"__value": {
"subentityRef": {
"__value": {
"__parent": {
"__name":
"Subflow1",
"__rev":
"1a",
"__isROR":
false
},
"__label":
"Subentity reference",
"__type":
"aSubentity"
}
}
},
"__label": "Current Status",
"__type": "CurrentStatus"
},
```
```
"SubentityList": {
"__value": [
{
"seField1": { "__value":
15 },
"seField2": { "__value":
"2021-11-30T18:45:06.015+01:00"}
},
{
"seField1": { "__value":
15 },
"seField2": { "__value":
"2021-11-30T18:45:06.015+01:00" }
}
]
},
"NamedSubentityList1": {
"__value": [
{
"Name": "Inspection",
"Spec": { "__id":
"acd398sz_w992aoqu", "__name": "RO1", "__rev":
"B6" },
"ResourceGroup": {"__value": {
"__id": "acd398sz_w992aoqu", "__name": "RG1" } }
"Parent": { "__id":
"acd398sz_w992aoqu" },
},
{
"Name": "Wash",
"Spec": { "__id":
"acd398sz_w992aoqu", "__name": "RO1", "__rev":
"B6" },
"ResourceGroup": {"__value": {
"__id": "acd398sz_w992aoqu", "__name": "RG1" } }
"Parent": { "__id":
"acd398sz_w992aoqu" },
}
]},
```

```
"NamedSubentityList2": {
"__value": [
{ "Spec": { "__id":
"acd398sz_w992aoqu", "__name": "Inspect", "__rev":
"S1", "__isROR": true } },
{ "Spec": { "__id":
"acd398sz_w992aoqu", "__name": "Clean", "__rev":
"C1", "__isROR": false } }
]
},
"Product": {
"Subentity": {
"__value": {
"BestVendor": { "__value":
{ "__name": "Acme" } },
"CheapestVendor": {
"__value": { "__name": "Amazon" } }
}
},
"SubentityRef": {
"__value": {
"__id": "acd398sz_w992aoqu"
}
}
}
```
```
"Locations": { "__value": [
{
"Name": { "__value": "San Jose" },
"Local": { "__value": true },
"Parent": { "__value": { __name:
"GigaFactory" } },
"Timezone": { "__value": "UTC-8" }
},
{
"Name": { "__value": "Austin" },
"Local": { "__value": false, },
"Parent": { "__value": { __name:
"GigaFactory" },
"Timezone": { "__value": "UTC-6" }
},
]}
```
Subclasses

As stated above, the requestData format is generated based on fields in the service. For an object field, if access is needed to a subclass of the field
type, the "__tag" property is used.

```
Accessing fields from subclass of field type
```
```
"TrackedObject": {
"__options": {
"__type": true
},
"__tag": "ContainerDTO" // support for
polymorphic request data
"fieldFromSubclass": {}
}
```
The __tag is here to enable selection of a field from
a subclass of the field type, and is feature is a
work in progress.

```
Responses for data requests
```
```
"TrackedObject": {
"__value": {
"__id": "acd398sz_w992aoqu"
"fieldFromSubclass: {
"__value": "bob",
"__type": "String"
}
},
"__type": "TrackedObject"
}
```
Pagination

For a list, pagination can be specified.


For a list of objects, any desired fields can be specified. If none are specified, fields are serialized according to their type.

For a list of references, any desired fields can be specified. If none are specified, only the reference information is serialized.

```
Specification of pagination options for list fields
```
```
"HistoryDetails": {
"__options": {
"__listPagination": {
"__startingAt": 10,
"__get": 5
}
}
"TxnId": {},
"TxnDate": {},
"TrackedObject": {
"__options": {
"__type": true
},
"__tag": "ContainerDTO",
"ContainerComments": {}
}
}
```
```
Responses for data requests
```
```
"HistoryDetails": {
"__startingAt": 2,
"__returnedCount": 11,
"__value": [
{
"TxnId": { "__value":
"000410800000001c"},
"TxnDate": { "__value":
"2021-11-30T18:45:06.015+01:00" },
"TrackedObject": {
"__value": { "__name": "Container5", "__type":
"Container" } }
},
{
"TxnId": { "__value":
"000410800000001d"},
"TxnDate": { "__value":
"2021-12-31T23:44:00.026Z" },
"TrackedObject": {
"__value": { "__name": "Container5-1", "__type":
"UnitContainer" } }
}
]
}
```
__requestData

Two edge cases need to be considered

```
Empty __requestData block
```
```
// Case 1:
__requestData: {}
```
```
// Some options:
// 1. Serialize the service as an object.
Individual fields can have their
// serialization attributes
configured to acheive the desired level
// of detail.
// 2. Serialization will be based on the type of
service:
// Maintenance will serialize the
modeling object
// Shopfloor will serialize the
completion message
// GetAll will serialize the
__altKey for each item
```
```
No __requestData block
```
```
// Case 2:
// no __requestData attribute has been specified.
```
```
// In this case, the httpStatus code is returned
(e.g. 200), but no __responseData is generated.
```
Selection Values

Requesting selection values for a field returns a recordset with column headers listed once, and the data retuned in a compact format.

```
Specification of pagination options for list fields
```
```
"__requestData": {
"TrackedObject": {
"__selectionValues": {
```
```
RecordSet Response
```
```
"__responseData": {
"TrackedObject": {
"__selectionValues": {
```

```
"__pagination": {
"__startingAt": 30,
"__get": 15
}
}
}
}
```
```
// When SelectionValues are configured in Query
mode and the query has parameters, then the
parameter values
// can be passed in the query parameters below as
explicit values...
"__requestData": {
"TrackedObject": {
"__selectionValues": {
"__queryParameters": [
{
"__name":
"WorkCenterName",
"__value": "WC1701"
},
{
"__name":
"OperationName",
"__value": "OP18-%"
},
{
"__name": "Qty",
"__value": 10.0
},
```
### }

### }

### }

```
// TODO: Add an example for passing in a value for
Object type parameter (yet to be implemented...)
```
```
"__startingAt": 30,
"__returnedCount": 15,
"__headers": [
{
"__name":
"InstanceId",
"__type":
"Container",
"__label":
"Container Id"
},
{
"__name":
"Container",
"__type":
"Name",
"__label":
"Container Name"
},
{
"__name":
"Product",
"__type":
"Product",
"__label":
"Product",
},
{
"__name":
"DueDate",
"__type":
"Date",
"__label":
"Due Date",
},
{
"__name":
"Qty",
"__type":
"Qty",
"__label":
"Quantity",
}
],
"__recordsetData": [
[
"000410800000001c", "DP04", { "__name": "Product",
"__rev": "A", "__isROR": true }, "9999-12-31T00:00:
00.000Z", 55.3 ],
[
"000410800000001d", "DP05", { "__name": "Product",
"__rev": "E", "__isROR": false }, "2021-12-31T00:
00:00.000Z", 65.4 ]
: : ] } } }
```
__queryRequest (Not implemented yet)

Queries and selectionValues share many attributes, including parameters and column headers.

User Query Input

To run a query, specify the query name, parameters,
recordsetConfiguratoin and pagination.

```
User Query Response
```
```
For recordsets, column headers are listed once, and the data is retuned
in a compact format.
```

```
Specification of pagination options for list fields
```
```
"__queryRequest": {
// Pre-defined query. Could morph to
RevisionedObject
"__userQueryDef": {
"__name": "myWorkItems"
},
// parameters for query
"__queryParameters": [
{
"__name": "assignee",
"__value": {
"__name": "Ramesh"
}
},
{
"__name": "startDate",
"__value": "2021-03-01T23:
00:00.000+07:00"
},
{
"__name": "sprint",
"__value": "Sprint19"
},
{
"__name": "product",
"__value": {
"__name":
"Pepperoni"
}
}
],
"__responseConfig": {
"__pagination": {
"__startingAt": 30,
"__get": 15
}
}
}
```
```
RecordSet Response
```
```
"__responseData": {
"__recordset": {
"__startingAt": 30,
"__returnedCount": 15,
"__headers": [
{
"__name": "InstanceId",
"__type": "Container",
"__label": "Container Id"
},
{
"__name": "Container",
"__type": "Name",
"__label": "Container
Name"
},
{
"__name": "Product",
"__type": "Product",
"__label": "Product",
},
{
"__name": "DueDate",
"__type": "Date",
"__label": "Due Date",
},
{
"__name": "Qty",
"__type": "Qty",
"__label": "Quantity",
}
],
"__items": [
[ "000410800000001c", "DP04", {
"__name": "Product", "__rev": "A", "__isROR": true
}, "9999-12-31T00:00:00.000Z", 55.3 ],
[ "000410800000001d", "DP05", {
"__name": "Product", "__rev": "E", "__isROR":
false }, "2021-12-31T00:00:00.000Z", 65.4 ]
:
:
]
}
}
```
__exceptions (Not implemented yet)

Any problems encountered during the processing of the request will be returned in an __exception item.

```
Exceptions
```
```
// Usually, processing will cease when an exception is encountered,
// but in case there are multiple exceptions, a collection is desired.
"__exceptions": [
{
// Message is the fully resolved and localized error message text
"__message": "'Grater1' not found in ResourceGroup 'Spreaders'",
```
```
// Code is the error code known by the system
"__code": 15689,
```
```
// Id is the individual id of this specific instance. Probably
// should follow format of instanceId.
"__id": 1565478,
```
```
// The request id is generated by the API gateway and follows the
```

```
// request throughout the processing flow. It can be used to
// merge information from disparate parts of the system.
"__requestId": "sscdop4270c-5",
```
```
// ExceptionParameters contain information specifc to the error
// that may or may not be included in the resolved message text,
// but can be used to ease analysis of the cause.
"__exceptionParameters": [
{ "key": "userName", "value": "Dave" },
{ "key": "txnDate", "value" "2021-11-05T15:30:00Z" },
{ "key": "resource", "value": "Grater1" },
{ "key": "resourceGroup", "value": "Spreaders" },
{ "key": "operation", "value": "DoughSpread" },
]
}
]
```
Additional Processing

Many services will utilize the inputData, requestData and responseData obects. Some services will require a different combination. For example, a
batch import might have a list of inputs and no request. This will be dictated by the service and implemented by the controller.

API Controller Signatures

These signatures need to be brought into line with the actual implementation

```
Endpoint Controller API Signatures
```
```
class ModelingActionController<Employee, EmployeeRequest, EmployeeResponse, NamedRefDTO>{
```
```
// Get Employee by Key
// URL: <app_url>/Employee/
// Notes:
// 1. Specifying the employee using NamedRefDTO will result in a single element
// 2. Configuration of the resulting item is done using the EmployeeRequest object.
// Body: EmployeeRequest to facilitate field selection and pagination
[HttpGet]
ActionResult<EmployeeResponse> GetEntity([FromBody] GetRequest<EmployeeRequest, NamedRefDTO>)
[HttpGet]
ActionResult<ProductResponse> GetEntity([FromBody] GetRequest<ProductRequest, RevisionedRefDTO>)
```
```
// Get Employee by Id
// URL: <app_url>/Employee/id/{id}
[HttpGet]
ActionResult<TResponse> GetEntityById([FromRoute] string id)
```
```
// Get Employee by Name
// URL: <app_url>/Employee/{name}
[HttpGet]
ActionResult<TResponse> GetEntityByName([FromRoute] string name)
```
```
// Get All Entities
// URL: <app_url>/Employee/GetAll
[HttpGet]
ActionResult<List<ReferenceResponse>> GetAllEntities()
```
```
// Get All Entities
// URL: <app_url>/Employee/GetAll
[HttpPost]
ActionResult<List<EmployeeResponse>> GetAllEntities([FromBody]
GetAllModellingRequest<EmployeeRequest> getAllmodellingRequest)
```
```
// Create
// URL: <app_url>/Employee/
// Body: CRUDService<Employee, EmployeeRequest>
// Notes:
// TODO: Multiple service types based on CRUD vs SF
```

[HttpPost]
ActionResult<EmployeeResponse> Sync([FromBody] CreateModellingRequest<Employee, EmployeeRequest>)
ActionResult<ProductResponse> Sync([FromBody] CreateModellingRequest<Product, ProductRequest>)

// Partial update
// URL: <app_url>/Employee/
// Body: CRUDService<NamedRef, Employee, EmployeeRequest>
// Notes:
// 1. NamedRefDTO specifies the object to change, which must be found
[HttpPatch]
ActionResult<EmployeeResponse> Update([FromBody] UpdateModellingRequest<Employee, EmployeeRequest,
NamedRefDTO>)
[HttpPatch]
ActionResult<ProductResponse> Update([FromBody] UpdateModellingRequest<Product, ProductRequest,
RevisionedRefDTO>)

// Delete object
// URL: <app_url>/Employee/
// Body: NamedRefDTO
// Notes:
// 1. NamedRefDTO specifies the object to delete, which must be found
// 2. If namedref is null, name in query specifies the object to delete, which must be found
[HttpDelete]
ActionResult<EmployeeResponse> Delete([FromBody] NamedRefDTO)
[HttpDelete]
ActionResult<ProductResponse> Delete([FromBody] RevisionedRefDTO)

// Batch export
// URL: <app_url>/__export/type
// Body: A configuration of objects to export.
// Notes:(Not implemented yet)
[HttpPost]
IActionResult<ModelingExportResponse> Export([FromBody] EmployeeExport)
[HttpPost]
IActionResult<ModelingExportResponse> Export([FromBody] ProductExport)
// Batch export
// URL: <app_url>/__export/
// Body: A configuration of objects to export.
// Notes: This allows export of multiple object types. Export configuration isn't defined
// yet as it depends heavily on undefined product requirements.
[HttpPost]
IActionResult<ModelingExportResponse> Export([FromBody] ExportConfiguration)

// Batch import
// URL: <app_url>/__import/
// Body: A collection of ModelingRequests of any number of types with the request ignored.
// Notes:(Not implemented yet)
[HttpPost]
IActionResult<ModelingImportResponse> Import([FromBody] ModelingImport<List<ModelingRequest<NamedRef,
Employee>>)
[HttpPost]
IActionResult<ModelingImportResponse> Import([FromBody]
ModelingImport<List<RevisionedModelingRequest<RevisionedRef, Product>>)
}

class RevisionActionController<Employee, EmployeeRequest, EmployeeResponse>{
// Revise
// URL: <app_url>/Product/Revise
// Body: CRUDService<Employee, EmployeeRequest>
// Notes:
[HttpPost]
ActionResult<ProductResponse> Revise([FromBody] RevisionModellingRequest<Product, ProductRequest>)

// Get by Name and Revision
// URL: <app_url>/Product/name/{name}/rev/{rev}
// Notes:
[HttpGet]
ActionResult<ProductResponse> GetEntityByNameAndRev([FromRoute] string name, [FromRoute] string rev)

// Get Revision
// URL: <app_url>/Product/GetRevision


// Notes:
[HttpGet]
ActionResult<ProductResponse> GetRevision([FromQuery] string name, [FromQuery] string rev)
}

class InquiryAction {
// Execute an inquiry service
// URL: <app_url>/ContainerHistoryInquiry/
// Body: ContainerHistoryInquiry to specify input (ContainerHistoryInquiryRequest to facilitate
selectionValues input and requestData
[HttpGet]
IActionResult<ContainerHistoryInquiryResponse> Get([FromBody] InquiryRequest<ContainerHistoryInquiry,
ContainerHistoryInquiryRequest>)
}

class QueryAction {
// Execute a user defined query
// URL: <app_url>/runQuery
// Body: query<query, queryRequest>
[HttpGet]
IActionResult<queryResponst> Get([FromBody] QueryRequest<query, queryRequest>)
}

class ShopfloorAction {
// Get SelectionValues
// URL: <app_url>/MoveStd/Get
// Body: ShopfloorRequest to facilitate selectionValues input and requestData
[HttpPost]
ActionResult<MoveStdResponse> Get([FromBody] ShopfloorRequest<MoveStd, MoveStdRequest>)

// Get SelectionValues
// URL: <app_url>/MoveStd/Perform/methodName
// Body: ShopfloorRequest to perform a method without a commit.
// Note: The methods allowed will be based on metadata attributes. If the list is an enumeration, we
// will need to research how to "translate" the string from the route to that enumeration value.
[HttpPost]
ActionResult<MoveStdResponse> Perform([FromRoute]string methodName, [FromBody] ShopfloorRequest<MoveStd,
MoveStdRequest>)

// Normal Transaction
// URL: <app_url>/MoveStd/Execute
// Body: ShopfloorRequest to supply input and requestData configuration
// Notes:
[HttpPost]
ActionResult<MoveStdResponse> Execute([FromBody] ShopfloorRequest<MoveStd, MoveStdRequest>)
}


# Request/Response Examples

```
This page contains examples of a number of requests and their associated responses
```
## Initial UI Prep (for Shopfloor Services)

```
POST: [approot]/Move/RequestDataUsingInput
```
```
Request initial page info
```
### {

```
"__config": {
"__traceLevel": 3
},
"__inputData": { }, // is optional or can
be empty
"__requestData": {
// Get dispatch list and selection values
"__options": {
"__label": true,
"__value": true, // to get default
values of fields, if any
},
// Dispatch list is retrieved via
selection values
"TrackedObject": {
"__selectionValues": {
"__pagination": {
"__startingAt": 1,
"__get": 15
}
}
},
"Machine": { },
"Comments": { }
}
}
```
```
Receive initial page info
```
```
// retrieve current values and selectionValues
// for fields based on specified container.
{
"__responseData": {
"TrackedObject": {
"__label": "Container",
"__selectionValues": {
"__startingAt": 1,
"__returnedCount":
2,
"__headers": [
{
```
```
"__name": "InstanceId",
```
```
"__type": "Container"
},
```
```
"__label": "Container Id",
```
```
"__columnIndex": 0
},
{
```
```
"__name": "Container",
```
```
"__type": "String",
```
```
"__label": "Container Name",
```
```
"__columnIndex": 1
},
{
```
```
"__name": "Product",
```
```
"__type": "Product",
```
```
"__label": "Product",
```
```
"__columnIndex": 2
},
{
```
```
"__name": "DueDate",
```
```
"__type": "dateTimeOffset",
```
```
"__label": "Due Date",
```
```
"__columnIndex": 3
},
{
```
```
"__name": "Qty",
```
```
"__type": "Decimal",
```
```
"__label": "Quantity",
```

```
"__columnIndex": 4
}
],
"__items": [
[
"000410800000001c", "DP04", { "__name": "Product",
"__useROR": true }, "9999-12- 31T00:00:
00.000Z", 55.3 ],
[
"000410800000001d", "DP05", { "__name": "Product",
"__rev": "E" }, "2021-12-31T00:00:00.000Z", 65.4 ]
]
},
"Machine": {
"__label": "Machine"
},
"Comments": {
"__label": "Txn Comments"
}
}
}
```
Update UI after some input

POST: [approot]/Move/RequestDataUsingInput

```
Request values based on TrackedObject
```
```
// retrieve current values and selectionValues
// for fields based on specified container.
{
"__config": {
"__traceLevel": 3
},
"__inputData": {
"TrackedObject": {
"__name": "2021.03.03-Batch1"
}
},
"__requestData": {
"__options": {
"__value": true,
"__type": true,
"__selectionValues": { }
},
"Machine": {},
"Qty": {},
"Path": {}
}
}
```
```
Receive values based on TrackedObject
```
### {

```
"__responseData": {
"Machine": {
"__value": {
"__name":
"DoughSpreader1"
},
"__type":
"Resource",
"__selectionValues": {
"__startingAt":
1,
"__returnedCount":
2,
"__headers": [
{
```
```
"__name": "InstanceId",
```
```
"__type":
"Resource",
```
```
"__label": "Resource Id",
```
```
"__columnIndex": 0
},
{
```
```
"__name": "Name",
```
```
"__type": "String",
```
```
"__label": "Resource",
```
```
"__columnIndex": 1
}
],
"__items": [
```

### [

```
"000410800000001c", "DoughSpreader1" ],
[
"000410800000001d", "DoughSpreader2" ]
]
}
},
"Qty": {
"__value": 586,
"__type": "decimal",
"__selectionValues": {}
},
"Path": {
"__value": {},
"__type": "Path",
"__selectionValues": {
"__startingAt": 1,
"__returnedCount":
2,
"__headers": [
{
```
```
"__name": "InstanceId",
```
```
"__type":
"Path",
```
```
"__label": "Path Id",
```
```
"__columnIndex": 0
},
{
```
```
"__name": "Name",
```
```
"__type": "String",
```
```
"__label": "Path Name",
```
```
"__columnIndex": 1
},
{
```
```
"__name": "DefaultPath",
```
```
"__type": "Boolean",
```
```
"__label": "Default Path",
```
```
"__columnIndex": 2
}
],
"__items": [
[
"000410800000001c", "ToSauce", true ],
[
"000410800000001d", "ToCheese", false ]
]
}
}
}
}
```
Service submission

POST: [approot]/Move/


```
Submit entered values and execute
```
```
// Send collected data to execute service
{
"__config": {
"__traceLevel": 3
},
"__inputData": {
"TrackedObject": { "__name": "2021.03.03-
Batch1" },
"Machine": { "__name": "Dough Spreader" },
"Qty": 28,
"Path": {
"__parent": {
"__parent": {
"__name": "Pizza Workflow"
},
"__name": "Spread Dough Step"
},
"__name": "toSauce"
},
"Comments": "Dough was a little sticky.
Check HVAC for humidity settings."
},
"__requestData": {
"__options": {
"__value": true
},
"CompletionMsg": {},
"TxnDate": {}
}
}
```
Initial UI Prep (for Modeling Services - to

request data from an instance)

POST: [approot]/Factory/RequestDataUsingKey

Request initial page info

### {

```
"entityKey": {
```
```
"__name": "Factory-1"
```
```
},
"__requestData": {
// Get default values and selection values
"__options": {
"__label": true,
"__value": true, // to get default
values of fields, if any
},
// Choice list is retrieved via selection
values
"Employees": {
"__selectionValues": {
"__pagination": {
"__startingAt": 1,
"__get": 15
}
}
},
"State": {
```
```
"__selectionValues": {}
```
```
Receive transaction completion data
```
### {

```
"__responseData": {
"CompletionMsg": {
"__value": "Container
2021.03.03-Batch1 moved to Sauce on 21.01.2021 15:
45:00"
}
"TxnDate": {
"__value": "2021-01-21T15:
45:00.002Z-08:00"
}
}
}
```

### }

### }

### }

Initial UI Prep (for Modeling Services - to

request data without an instance - new

object page)

POST: [approot]/Factory/RequestDataUsingKey

Request initial page info

### {

```
// entityKey section is optional
```
```
"__requestData": {
```
```
// Get default values and selection values
"__options": {
"__label": true,
"__value": true, // to get default
values of fields, if any
},
// Choice list is retrieved via selection
values
"Employees": {
"__selectionValues": {
"__pagination": {
"__startingAt": 1,
"__get": 15
}
}
},
"State": {
```
```
"__selectionValues": {}
```
```
}
}
}
```

# Rest API Examples - Endpoints

## Create Factory

```
Post : http://localhost:5000/api/Factory
```
```
Create Factory
```
### {

```
"Input": {
"Name": "Siemens",
"DefaultQty": 20,
"MyProp": "Foo",
"FavEmployeeName": "John",
"Locations": [
{
"value": {
"Name": "Berlin"
},
"ListItemAction": "Add"
}
],
"PersistentListEmployees": [
{
"value": {
"__Name": "Smith",
"__TypeOfRefDto": "NamedRefDto"
},
"ListItemAction": "Add"
}
]
},
"__RequestData": {
"__Options": {
"__Label": true,
"__Value": true,
"__Type": true
},
"Name":{},
"DefaultQty": {},
"MainLocation": {
"status": {}
}
}
}
```
## Update Factory

```
Patch : http://localhost:5000/api/Factory
```
```
Update Factory
```
### {

```
"Input": {
"Name": "Siemens",
"DefaultQty": 10,
"Locations": [
{
"value": {
"Name": "Loc-2",
"Status": "20"
},
"ListItemAction": "Add"
}
],
```

```
"Qty": 300
},
"__RequestData": {
"__Options": {
"__Label": true,
"__Value": true,
"__Type": true
},
"Name":{},
"DefaultQty": {},
"MyProp": {},
"MainLocation":{}
},
"entityKey": {
"__Name": "Siemens",
"__TypeOfRefDto": "NamedRefDto"
}
}
```
Delete Factory

Delete : [http://localhost:5000/api/Factory](http://localhost:5000/api/Factory)

```
Delete Factory
```
### {

```
"__Name": "Siemens"
}
```
Create Employee

Post : [http://localhost:5000/api/Employee](http://localhost:5000/api/Employee)

```
Create Employee
```
### {

```
"Input": {
"Department": "manufacturing",
"Description": "employee",
"Email": "Sample@Test.com",
"Factory": {
"__Name": "Siemens",
"__TypeOfRefDto": "NamedRefDto"
},
"Name": "Smith"
},
"__RequestData": {
"__Options": {
"__Label": true,
"__Value": true,
"__Type": true
},
"Name":{},
"Email": {},
"Department":{}
}
}
```
Create Employee with polymorphic Inputs (add special or superspecial employee)


Post : [http://localhost:5000/api/Employee](http://localhost:5000/api/Employee)

```
Create Employee with polymorphic inputs
```
### {

```
"Input": {
"Department": "manufacturing",
"Description": "employee",
"Email": "Sample@Test.com",
"SpecialField": "specialemp",
"typeofsubclass": "SpecialEmployee",
"Name": "Smith"
},
"__RequestData": {
"__Options": {
"__Label": true,
"__Value": true,
"__Type": true
},
"Name":{},
"Email": {},
"Department":{},
"SpecialField":{}
}
}
```
Create Factory with polymorphic Inputs (add locations)

Post : [http://localhost:5000/api/Factory](http://localhost:5000/api/Factory)

```
Create Factory with polymorphic inputs
```
### {

```
"input": {
"Name": "Siemens",
"DefaultQty": 10,
"Locations": [
{
"value": {
"Name": "Loc-1",
"SpecialField":"specialloc",
"typeofsubclass":"SpecializedLocation"
},
"ListItemAction": "Add"
}
]
},
"__RequestData": {
"__Options": {
"__Label": true,
"__Value": true,
"__Type": true
},
"Name":{},
"Locations":{}
}
}
```
FieldsToClear

It will help to clear existing value from the database

Patch : [http://localhost:5000/api/Factory](http://localhost:5000/api/Factory)


```
FieldsToClear
```
### {

```
"Input": {
"Name": "Siemens",
"DefaultQty": 10,
"FieldsToClear": [
"FavEmployeeName"
],
"Locations": [
{
"key": {
"Name": "Loc-1"
},
"value": {
"FieldsToClear": [
"Status"
]
},
"ListItemAction": "Update"
}
]
},
"__RequestData": {
"__Options": {
"__Value": true
},
"DefaultQty": {},
"FavEmployeeName": {}
},
"entityKey": {
"__Name": "Siemens",
"__TypeOfRefDto": "NamedRefDto"
}
}
```
Update And Replace ListItemAction

This is helpful to update sub entities values.

Update - It will help to update particular values from database without modifying other values

Replace - It will help to replace all values from database and should update to the defaults values

Patch : [http://localhost:5000/api/Factory](http://localhost:5000/api/Factory)

```
Update And Replace ListItemAction
```
### {

```
"Input": {
"Name": "Siemens",
"DefaultQty": 10,
"Locations": [
{
"key": {
"Name": "Loc-1"
},
"value": {
"Status": "50"
},
"ListItemAction": "Update"
},
{
"key": {
"Name": "Loc-2"
},
```

```
"value": {
"Name": "Loc-2"
},
"ListItemAction": "Replace"
}
]
},
"entityKey": {
"__Name": "Siemens",
"__TypeOfRefDto": "NamedRefDto"
},
"__RequestData": {
"__Options": {
"__Value": true
},
"DefaultQty": {},
"Locations": {}
}
}
```
GetById

Post: [http://localhost:5000/api/Factory/Get](http://localhost:5000/api/Factory/Get)

```
GetById
```
### {

```
"entityKey": {
"__Id": "0DD9ho_Fx050dmM2CW"
},
"__RequestData": {
"__Options": {
"__Label": true,
"__Value": true,
"__Type": true
},
"Name":{},
"DefaultQty":{},
"ProductionStatus":{}
}
}
```
GetByName

Post: [http://localhost:5000/api/Factory/Get](http://localhost:5000/api/Factory/Get)

```
GetByName
```
### {

```
"entityKey": {
"__Name": "Siemens"
},
"__RequestData": {
"__Options": {
"__Label": true,
"__Value": true,
"__Type": true
},
"Name":{},
"DefaultQty":{},
"ProductionStatus":{},
"Locations":{}
```

### }

### }

GetAll

Post: [http://localhost:5000/api/Factory/GetAll](http://localhost:5000/api/Factory/GetAll)

```
GetAll
```
### {

```
"__RequestData": {
"__Options": {
"__Label": false,
"__Value": true,
"__Type": true,
"__PaginationList":{
"__get":10,
"__startingAt":0
}
},
"__sortfields": [
{
"__FieldName": "Name",
"__Direction": "descending"
}
],
"Name":{},
"DefaultQty":{},
"ProductionStatus":{},
"Locations":{}
"Qty":{}
}
}
```
Get SelectionValues

In below example, we are specified selectionvalues in multiple ways.

For Status : It will applied from global __options

For PersistentListEmployees : Defined at it's own level itself

For State: Defined it's own __options

Post: [http://localhost:5000/api/Factory/Get](http://localhost:5000/api/Factory/Get)

```
Get SelectionValues
```
### {

```
"entityKey": {
"__Name": "Siemens",
"__TypeOfRefDto": "NamedRefDto"
},
"__RequestData": {
"__Options": {
"__selectionValues": {
"__pagination": {
"__startingAt": 0,
"__get": 10
}
}
},
"Status": {},
```

```
"PersistentListEmployees": {
"__selectionValues": {
"__pagination": {
"__startingAt": 0,
"__get": 20
}
}
},
"State": {
"__Options": {
"__selectionValues": {
"__pagination": {
"__startingAt": 0,
"__get": 5
} } } } } }
```
Paginations On List Field

Post: [http://localhost:5000/api/Factory/Get](http://localhost:5000/api/Factory/Get)

```
Paginations_On_List_Field
```
### {

```
"entityKey": {
"__Name": "Siemens",
"__TypeOfRefDto": "NamedRefDto"
},
"__RequestData": {
"__Options": {
"__Label": false,
"__Value": true,
"__Type": false
},
"locations": {
"__PaginationList": {
"__get": 10,
"__startingAt": 0
},
"Name": {},
"Status": {}
}
}
}
```
System Defaults With Empty RequestData

Maintenance will serialize the modeling object : It will serialize all props for object and serialize only
RefInfo for ObjectRef

Shopfloor will serialize the completion message

Post: [http://localhost:5000/api/Factory/Get](http://localhost:5000/api/Factory/Get)

```
System_Defaults_With_Empty_RequestData
```
### {

```
"entityKey": {
"__Name": "Siemens",
```

```
"__TypeOfRefDto": "NamedRefDto"
},
"__RequestData": {}
}
```
System Defaults Without RequestData

In this case, the httpStatus code is returned (e.g. 200), but no __responseData is generated.

Post: [http://localhost:5000/api/Factory/Get](http://localhost:5000/api/Factory/Get)

```
System_Defaults_Without_RequestData
```
### {

```
"entityKey": {
"__Name": "Siemens",
"__TypeOfRefDto": "NamedRefDto"
}
}
```

# ShopFloor RestAPI Example

## ShopFloor Execute Example

/api/ShopfloorService/Execute

### {

```
"input": {
"intValue": 90,
"name": "t0-17"
},
"__RequestData": {
"__Defaults": {
"__Value": true,
"__Label": true,
"__Type": true
},
"Name": {
"__Value": true
}
}
}
```
## Shop Floor Perform Example

/api/ShopfloorService/Execute/Perform

### {

```
"input": {
"intValue": 90,
"name": "t0-8"
},
"__RequestData": {
"__Defaults": {
"__Value": true,
"__Label": true,
"__Type": true
},
"Name": {
"__Value": true
}
}
}
```
## ShopFloor Request Data Example

/api/ShopfloorService/RequestdataUsingInput

### {

```
// input section is optional and can be empty if the request is for default values or static selection
values
// if the request is for context specific information then 'input' section needs to be filled with the
relevant input data
"input": {
"intValue": 90,
"name": "t0-17"
},
"__RequestData": {
"__Defaults": {
"__Value": true,
"__Label": true,
"__Type": true
},
```

"Name": {
"__Value": true
},
"TrackedObject": {
"__selectionValues": {
"__pagination": {
"__startingAt": 1,
"__get": 15
}
}
}
}
}


# Class Based Options and Flag

## Options Overview

```
The existing platform implementation of enums is not extensible, i.e. a model cannot add additional values to an existing enum. To remedy this, a new
type of object is created (BaseOptions) which provides the same functionality as an enum, but uses partial classes instead of native enums to enable
the module extensibility required.
```
```
This change will required some changes to modules, so the existing objects and methods are deprecated in favor of the new field type, and the
original support will be removed in an upcoming sprint.
```
## Options Definition

```
Deprecated enums
```
```
Enum definition
```
```
/// <summary>
/// Actions that are available to the modeling
services.
/// </summary>
public enum ModelingActionEnum
{
/// <summary>
/// Create a new modeling object
/// </summary>
[EnumDisplayName("Create")]
[EnumDescription("Object was created")]
Create,
```
```
/// <summary>
/// Update an existing modeling object
/// </summary>
[EnumDisplayName("Update")]
[EnumDescription("Object was updated")]
Update,
```
```
/// <summary>
/// Delete an existing modeling object
/// </summary>
[EnumDisplayName("Delete")]
[EnumDescription("Object was deleted.")]
Delete
}
```
```
Extendable options
```
```
Options definition
```
```
/// <summary>
/// Actions that are available to the modeling
services.
/// </summary>
public partial class ModelingActions :
BaseOptions<ModelingActions, byte>
{
public ModelingActions(string
name, byte value)
: base(name, value)
{ }
```
```
/// <summary>
/// Create a new modeling object
/// </summary>
[EnumDisplayName("Create")]
[EnumDescription("Object was created")]
public static readonly
ModelingActions Create = new ModelingActions(nameof
(Create), 0);
```
```
/// <summary>
/// Update an existing modeling object
/// </summary>
[EnumDisplayName("Update")]
[EnumDescription("Object was updated")]
public static readonly
ModelingActions Update = new ModelingActions(nameof
(Update), 1);
```
```
/// <summary>
/// Delete an existing modeling object
/// </summary>
[EnumDisplayName("Delete")]
[EnumDescription("Object was deleted.")]
public static readonly
ModelingActions Delete = new ModelingActions(nameof
(Delete), 2);
}
```
```
Extending Options
```
```
public partial class ModelingActions
{
[EnumDisplayName("Sync")]
[EnumDescription("Object was synchronized.")]
public static readonly ModelingActions Sync =
new ModelingActions(nameof(Sync), 10);
}
```

Options Declaration

Deprecated enums

```
Enum Declaration
```
```
protected override void _RegisterFieldTypes()
{
base._RegisterFieldTypes();
RegisterField(FieldInitializer.
EnumField<ModelingAuditTrail, ModelingActionEnum>.
Get("ModelingAction"));
}
```
```
public ModelingActionEnum ModelingAction { get =>
ModelingActionField.Value; set =>
ModelingActionField.Value = value; }
[PersistAs(PersistAsAttribute.EnumAs.Name)]
public EnumField<ModelingActionEnum>
ModelingActionField = null;
```
```
Extendable options
```
```
Options declaration
```
```
protected override void _RegisterFieldTypes()
{
base._RegisterFieldTypes();
RegisterField(FieldInitializer.
OptionsField<ModelingAuditTrail, ModelingActions,
byte>.Get("ModelingAction"));
}
```
```
public ModelingActions ModelingAction {
get => ModelingActionField.
GetOptionsValue();
set => ModelingActionField.SetOptionsValue
(value);
}
[PersistAs(PersistAsAttribute.EnumAs.Name)]
public OptionsField<ModelingActions, byte>
ModelingActionField = null;
```
Options Usage

Deprecated enums

```
Enum Usage
```
```
public virtual TModelObj Create(string name = "")
{
objectAction = ModelingActionEnum.Create;
}
```
```
Extendable options
```
```
Options Usage
```
```
public virtual TModelObj Create(string name = "")
{
objectAction = ModelingActions.Update;
objectAction = (dynamic)1;
}
```
Flags

In addition to replacement of the standard enum functionality, support is added for flags which can be used as masks with support for bitwise
operations.

Flags Definition

```
Options flags definition
```
```
public partial class UsageOptionFlags : FlagOptions<UsageOptionFlags>
{
UsageOptionFlags(string name, byte value) : base(name, value) { }
```
```
public static readonly UsageOptionFlags None = new UsageOptionFlags(nameof(None), 0);
public static readonly UsageOptionFlags Named = new UsageOptionFlags(nameof(Model), 1);
public static readonly UsageOptionFlags Revisioned = new UsageOptionFlags(nameof(Revisioned), 2);
public static readonly UsageOptionFlags Subentity = new UsageOptionFlags(nameof(Subentity), 4);
public static readonly UsageOptionFlags Cacheable = new UsageOptionFlags(nameof(Cacheable), 8);
}
```
```
Flag Extension
```

```
public partial class UsageOptionFlags
{
public static readonly UsageOptionFlags Traceable = new UsageOptionFlags(nameof(Traceable), 2048);
}
```
Flags Declaration

```
Flags Declaration
```
```
protected override void _RegisterFieldTypes()
{
base._RegisterFieldTypes();
RegisterField(FieldInitializer.FlagOptionsField<ModelingAuditTrail, UsageFlags>.Get("UsageFlags"));
}
```
```
public UsageOptionFlags UsageFlags {
get => UsageFlagsField.GetOptionsValue();
set => UsageFlagsField.SetOptionsValue(value);
}
public FlagOptionsField<UsageOptionFlags> UsageFlagsField = null;
```
Flags Usage

```
Flags Usage
```
```
UsageFlags = UsageOptionFlags.None;
UsageFlags = (UsageOptionFlags)(UsageOptionFlags.Named & UsageOptionFlags.Subentity);
UsageFlags |= UsageOptionFlags.Cacheable;
```
```
if (!UsageFlags.HasFlag(UsageOptionFlags.Cacheable))
UsageFlags.SetFlag(UsageOptionFlags.Cacheable);
```

# Exception Framework

## Overview

The design intent of the Exception Framework is to provide a uniform facility for producing localizable runtime exception messages for errors raised by the
Platform and Business modules.

This is achieved with two Exception Types that were (re) introduced in the 1.0.5 stable release of the Platform.

```
MOMException - This is the exception type thrown by the Platform and Framework itself.
MOMUserException - This is the exception thrown from Module Code by module developers.
```
Each exception class has an attendant 'Resources' file which stores the literal mappings explained later in the document.

When exceptions are thrown, an error key is provided and the middleware resolves this key to the language specific literal.

The diagram below shows how exceptions are processed by the framework.



### 1.

### 2.

### 1.

```
a.
2.
a.
3.
a.
```
Exception Signatures

Each exception type offers two signatures.

```
<exception type>(string key, int errorcode, params object[] paramlst)
```
```
<exception type>(Exception innerException, string key, int errorcode, params object[] paramlst)
```
The parameters of the first signature are:

```
A string 'key' that corresponds to the key in the SharedResources.resx (or ModelShareResources.resx)
See SharedResources and ModelSharedResources sections below
An error code that corresponds to the area of code the fault originates
See ErrorCodes section below
A variable array of argument values
These are the values substituted at runtime into the error message.
```
The second signature is preceded with an innerException argument. This is relevant when an exception is thrown by a dependency or underlying library,
and you want to retain the original exception stack.

Exception Usage

The MOMException and MOMUserException shall be used instead of C# language system and framework exceptions.

When these exceptions are thrown, they're caught by the ExceptionHandler Middleware, where the messages are localized and returned as a common
JSON structure for runtime generated errors.

There is no enforcement mechanism that prohibits the use of system exceptions however SonarQube rules may be added in the future to prevent their
use.

Additionally, bugs will be raised when system exceptions are found in the code bases.

Here's an example of a MOMException thrown from the Platform code with three arguments

```
throw new MOMException("AssemblyNotLoadedinClass", 0020601, nameof(metadataEngine), MethodBase.
GetCurrentMethod().Name, this.GetType().Name);
```
Here is the corresponding entry in the SharedResources file.

The same approach is used with MOMUserException in business modules.

Token Substitution

In the example above, consider this message:

Error loading {Model} Assemblies during execution of {MethodName} in {ClassName}

This message contains three tokens: {Model} {MethodName} and {ClassName}

At runtime the tokens are substituted for the values of nameof(metadataEngine) , MethodBase.GetCurrentMethod().Name and this.GetType().Name r
espectively.

```
Anything enclosed in braces { } is considered a token
```

```
A message many contain any number of tokens.
Tokens are only for string substitution purposes. They do not execute or evaluate. Value must be resolved when passing arguments to the
exception constructor.
```
Therefore, the message below is technically equivalent to the one shown above:

Error loading {0} Assemblies during execution of {1} in {2}

The token substitution function works similar to String.Format with one important distinction. Arguments passed with the exception are expected to be
ordered in the same manner (left to right) as tokens appear (left to right) in the message.

This means the token's ordinal position, NOT ITS NAME, determines how it's substituted. Consequently, the following string produces the same exact
output as the two other examples.

Error loading {2} Assemblies during execution of {1} in {0}

Token names are meant to be human-readable symbols of the values substituted in the string. Make them meaningful and unambiguous.

Note:

If more parameters are passed to the exception than exist as tokens in the message, they're ignored.

If fewer parameters are passed to the exception than exist as tokens in the message, remain tokens are replaced with empty stings.

Exception Response Structure

The resulting API response that's produced when either MOMException or MOMUserException is raised is in the following format.

{
"ErrorCode" : "0012345",
"Status" : 500,
"Date" : "2021-11-30T15:00:00+00:00",
"Api" : "/api/Factory",
"Method" : "POST",
"Message" : "A failure occurred creating factory with name 'f1'"
}

Where:

```
ErrorCode - is a string representation of the numeric value that semantically identifies the origin of the error.
Status - is a numeric value that semantically identifies the origin of the error (discussed later in this document).
Date - the time the exception occurred.
Api - The endpoint that was called in the form /api/<endpoint>.
Method - The HTTP verb used in the call.
Message - is the culture specific literal text that's returned with the error.
```
Error Keys

Each exception must have an associated string 'ErrorKey'.

Naming Specification

```
The ErrorKey can be any valid string, however:
It must be composed of US ASCII characters and numbers, no spaces, special characters or punctuation shall be used.
The ErrorKey must start with a letter
The ErrorKey must be in CamelCase format
The ErrorKey should be self-explanatory and indicate what the error is. If there is a missing message for a key, the key itself is returned and so it
should be intuitive.
```
Examples:

ThisIsAnExampleOfAGoodKey

this_is_a_bad_example

1another_bad_key!

this is another bad example


Error Codes

Each exception includes a specific error code that provides semantic information about the origin of the error. The error code is meant to help in support
situations and provides a hint about potential causes.

The management of the error codes is somewhat informal, and the link below is considered the single source of truth for all codes. When adding a new
code, it must be added to this spreadsheet.

https://splm-my.sharepoint.com/personal/vqcd6k_splm_siemens_com/Documents/MOMExceptionDataDictionaryKeysErrorcodes.xlsx?web=1

Each module shall store its codes on one sheet with the name of the module, for example M1_MaterialManagement

Error codes are not globally unique however the must be unique across two realms.

```
Across the Platform Codebase
Across all business modules combined
This is achieved using a module prefix
```
Numbering Specification

```
Numbers are 7 digits long and may contain leading zeros.
The first two digits indicates if it's a platform or model error:
00 = Platform
<Non-Zero First Digit><Any Digit> = Model, for example, 10 = M1_Common and 99 = M1_POC
The next digit is used to signify the containing module:
Platform
1 = MetaModel Framework
2 = Platform API
3 = Platform Application Layer
4 = Platform Information Layer
5 = Platform Integration Layer
6 = Platform Configuration Layer
7 = Platform ID Generator
8 = Platform DTO and Write Model Base
9 = Platform General Errors
M1_Common
1 = M1_Common Project
M1_POC
1 = M1_POC Project
The next two digits signify the grouping:
For the platform, there are ~20 grouping codes that indicate the type of error.
For the modules these two numbers can be used in any capacity, or if everything is grouped together, use 01.
The next two digits signify the actual error
This provides 100 possible error codes per group.
```
Shared Resources

Below is a portion of the SharedResources.resx file.

Model Shared Resources

Below is a portion of the ModelSharedResources.resx for the POC model


### 1.

### 2.

### 3.

### 4.

### 5.

### 6.

### 7.

### 1.

### 2.

### 3.

```
a.
```
Process of Creating a new Exception

```
Determine if this is a Module or Platform related Error.
Identify that an error code and message doesn't already exist for the error you must raise.
Find a unique error as per the numbering specification.
Find a unique key as per the key specification.
Enter a row in the appropriate sheet in the spreadsheet specified in the Error Codes section.
Enter the message text in the appropriate resources file, include the error code in the Comment field.
Implement the thrown exception and pass the corresponding parameters.
```
Setting a Custom HTTP Status Code

```
By default, all exceptions are set to Http Status 500 however the status code for a particular exception may be overridden.
This is achieved by calling the SetStatus(int) method on MOMExceptionBase
The logic currently allows statuses >= 400 and <= 599 and the code must be a valid http status otherwise the default status is used.
If status codes less than 400 are needed, changes to the platform are required.
```
Below shows an example of how to raise an error with HTTP status code 503

```
throw new MOMException("CommError", 0051901, ex.Message).SetStatus((int)HttpStatusCode.ServiceUnavailable); //
503
```
Usage of HTTP Status Codes

I am adding section as a scratchpad for discussing this topic

The following links provide a description and intended uses of HTTP Status codes:

HTTP status code overview - Internet Information Services | Microsoft Docs

List of HTTP status codes - Wikipedia

Based on these descriptions, the following responses should be used:

```
HTTP
Status
```
```
Cause Response
```
```
Content
```
```
Client
```
```
Processing
```
```
Notes
```
```
401, 401.
x
```
```
Auth failure None/Generated While the usual usage is for Authentication, we could add
Authorization as well.
```
```
404, 404.
x
```
```
Unknown
API
```
```
None/Generated Basically a bug
```
```
5xx, 5xx.
x
```
```
Unexpecte
d Server
Exception
```
```
None/Generated This does not cover errors handled within the application.
```
```
200 Normal
response
from API
Request
```
```
Response is based on the "RequestData". In the case
of an error that was caught and handled by the
application, The content is the Exception serialized.
```
```
Since we're using mostly using the GET and POST verbs, using the
action specific status codes don't really make sense.
```
```
409 "Conflict". A JSON representation of the Exception. This could be used as a response status where an application
handled exception occurs. We could add 409.1 as a platform error
and 409.2 as an BL error. I don't know if we can set these
"extended" error codes, though.
```
Notes:

```
Usage of the IIS specific response codes seem to provide a benefit in that the client could provide additional behavior/details to the user.
Response content for some conditions could be generated by the API Gateway. For example, if an "application down" condition is encountered, a
retry page could be generated.
```

# Gateway Interaction - Pipeline call-chain

## First Interaction draft with object lifecycles

This diagram describes the interaction between gateways and inner services, giving a focus on they lifecycle. Here the microsoft notation is used:

```
Singleton: same instance across the whole execution
Transient: a new instance every time an instantiation of the service is asked
Request-Scope: The same instance is used across a scope, in our case, a request handling
```
In green we defined the services initialized at startup time and defined as singleton. In yellow the ones that are created during a Metamodel Service
execution.

For each Gateway a small box is placed below them identifying which major functionalities they provide.

The flow stars from an incoming request, a web API Call invoking a Service. We foresee the presence of middleware (right now there are not, but most
probably we will need for the security layer). After the middleware\s execution, the controller will be triggered. Once the controller is created it start our
pipeline. It's our pipeline duty to create the Information and Integration gateway.

```
This page is a working progress.
If you have some doubts or are review it, please post a comment to
Accorsi, Carlo
Bardini, Matteo
```
```
Thivaharraja, Antony
```
```
Observability, being a factory, is marked as "Scope" and not "Request-Scope" because it's not directly handled as a gateway by the
IServiceProvider (the IoC Container)
```

# Healthchecks - K8 Healthiness probes

## Introduction

The idea behind this document is to define the an organization of the code for the Healthiness probes,

## Global Code organization

## Configuration

Publisher Cycle can be configured as following:

```
ModularMOM.Healthiness.ReadinessCycleTimeout
ModularMOM.Healthiness.LivenessCycleTimeout
```
## Current Health checks

Service Execution

This Health checks is meant to validate if a service can be executed. Implicitly, this checks too if the database is reachable since it will perform a
creation\update.

This Health check leverages a system CO and a system service to perform the needed activities.


# Localization Support for Metadata Runtime components

# and Platform

## String to be localized inside platform code

All string to be localized inside platform are placed in a resource files named "SharedResources.resx" in the default en-US culture. The file is present
inside the project Siemens.MOM.Platform.Core together with the reference public empty class in the file SharedResources.cs

Strings that are to be localized in Platfom are mainly error messages used in exception management.

## String to be localized inside Metadata

In the MetaData Model the strings to be translated are error string and labels of fields and objects.

Each module must have a ModelSharedResources.resx file that collect all strings in the default en-US culture and the associated ModelSharedResources.
cs file.

Labels of fields and objects in ModelSharedResources.resx must have as key the value of the label in the json file (see example below) and an entry for
each label must be placed manually in the file.

```
ModelSharedResources - one entry for Employee Department label
```
```
<data name = "Employee_Department" xml:space="preserve">
<value>Employee Department</value>
</data>
```
```
emploeey.json with field Employee Department label
```
### {

```
"Description": "Employee object represents users.",
"Label": "CSICDOName_Employee",
"Cacheable": true,
"Persistence": {
"TableName": "Employee",
"Cacheable": true
},
"Fields": {
"Department": {
"Description": "Department of the employee",
"Label": "Employee_Department",
```
ModelSharedResources.cs. must contain a public empty class named ModelSharedResources in a namespace that has the name of the module in the
Module config file (see below)

```
Module configuration File for OrderManagement
```
### {

```
"name": "OrderManagement",
"description": "TODO: Provide a description of the model.",
"version": "1.0.0",
"platformVersion": "1.0.0",
"gitInfo": {
"repoName": "M1_OrderManagement",
"url": ""
},
"modelDependencies": [
{
"name": "Common",
"version": "1.0.0"
}
```

### ]

### }

```
ModelSharedResources.cs - namespace for OrderManagement
```
```
namespace OrderManagement
{
public class ModelSharedResources
{
}
}
```
Generated Model and module dependencies

All resources effectively used are placed in the generated model during the generation process so that the final result is a unique .resx file to be translated
and distributed.

This file contains the resources also of modules on which the final module depend.

This file is named ModelSharedResources.resx, contains all strings in the default en-US, and is placed in the project Namespace.GeneratedModel.
Common together with the ModelSharedResources.cs file that contains a public empty class named ModelSharedResource.

If a Module that depends on an other one define a new value for a label already present in its dependency the new value overrides the previous also in
resources.

Labels of fields and objects are placed also in a CheckModelSharedResources resx file.
This file is not used for localization but collect all labels so that is possible to check if all labels are placed in at least a resx file.

Still open point in localization

Activity that can be done out of the normal development flow are still to be defined:

```
Translation to culture different from the default en-US
compilation of .resx file into satellite assemblies
deployment of satellite assemblies
```
All of this are out of scope of what developed until the Spint12

Some batch command has been versioned to build test resources and to give a starting point for future developments in using resgen.exe al.exe (linker)
ect ....


# Spike: Localization Support for Metadata Runtime

# components

Other details on localization

```
Appling Localization to Platform controllers
Generation of Localization Resx File in GeneratedModel starting from the Metadata Input Model
```
## The POC

On the WS_Spikes Repo a POC has been added

The content of POC is a solution LocalizedTest with 2 Projects:

A Console project "ConsoleLocalized" to test, with minimal dependency, the proposed pattern essential features

Web API project "LocalizedAPI" to verify use of pattern in controller, logs and taking in account the detection of culture.

## Storage of translations: .resx file and satellite assembly

OOB implementation from Microsoft of IStringLocalizer only manage satellite assembly and embedded resources

It's possible to write a custom implementation of the interface that can deal with resx files or both resx files and satellite assembly.

Implementation of the OOB class is ResourceManagerStringLocalizer and can be found at the following link

https://github.com/dotnet/aspnetcore/blob/main/src/Localization/Localization/src/ResourceManagerStringLocalizer.cs

implementing reading from resx need to have a similar project using ResXResourceReader Class in place of ResourceManager Class and eventualy logic
to use both.

other option: Json files

Other implementations of IStringSerializer can use Json files or SQL to store localized strings.

An example of Json can be found here

https://github.com/aodpi/Anvyl.JsonLocalizer

or a more complex one that extend the basic interface with cache and pluralization

https://github.com/AlexTeixeira/Askmethat-Aspnet-JsonLocalizer

## missing culture

When trying to get a string not defined for the current culture the default value (the one used as string name) is used.

Recommended approach is to start writing program without resources using the default language as key for localized strings.

In this way all resources can be added later.

it' also possible define name as short name for each string, provide an embedded resource for a culture typically "en-US" and define this culture as default
culture.

## Parameter and format of DateTime and Float

Suppose to have the following code ready to be localized but not yet actually using the IStringLocalizer pattern.

```
Original code not localized
```
```
namespace LocalizedAPI.Controllers
{
```

```
[Route("api/[controller]")]
[ApiController]
public class MessagesController : ControllerBase
{
private ILogger<MessagesController> _logger;
private IStringLocalizer<SharedResource> _localizer;
```
```
public MessagesController(IStringLocalizer<SharedResource> sharedLocalizer, ILogger<MessagesController>
logger)
{
_localizer = sharedLocalizer;
_logger = logger;
}
```
```
[HttpGet]
public string GetMessage()
{
var state = "open";
```
```
_logger.LogError("Today {date: dddd dd MMM} the value is {value}", DateTime.Now,
12345.678);
```
```
var retval = string.Format("The door is {0}",state);
// or
var retval = $"The door is {state}";
```
```
return retval;
}
}
}
```
Actually using the IStringLocalizer the code of Get Message become

```
modified code
```
```
[HttpGet]
public string GetMessage()
{
var state = "open";
```
```
_logger.LogError(_localizer["Today {date: dddd dd MMM} the value is {value}"], DateTime.Now, 12345.678);
_logger.LogError(_localizer["Today {0: dddd dd MMM} the value is {1}", DateTime.Now, 12345.678]);
```
```
var locstate = _localizer[state];
var retval = _localizer["The door is {0}", locstate ];
```
```
return retval;
}
```
Now suppose that the current culture is "fr-FR" and the resources for translation are available the output will bee

```
Output
```
```
First Log Message
Aujourd'hui Tuesday 04 May la valeur est 12345.678
Second Log Message
Aujourd'hui mardi 04 mai la valeur est 12345,678
```
```
Response of controller
La porte est ouverte
```
Note:

```
the format string is always the one of string format with arguments identified by number
in log using _localazier[message] and passing argument to the logger method preserve semantic or log structure but arguments are formatted in
a thread of logging system not affected by the correct culture. In this case args can have message template names.
in log using _localizer[message, args, ...] resolve the above problem but log structure is lost.
```

```
in a plain text if an arguments can/must be translated _localizer must be used twice and proper translation should be placed in resources.
```
Generation of satellite assemblies and sharing translation

The pattern basically is intended to supply a set of localized string for each class that means for example for each controller.

But It 'possible to have a unique class in the process to share strings among all classes of the same assembly. The example typically proposed for this
approach is a empty class named SharedResources that will be associated to a resx file named SharedResouces.cultrue.resx.

If needed It's also possible to share the same strings among a group of different but related assemblies with some attention to naming convention. If a
class named SharedResouces is present in each assembly a unique file SharedResources.cultrue.resx can be managed and from it all satellite assembly
can be generated.

.With Resgen.exe a resource file can be generated for each target assembly

resgen SharedResources.culture.resx Assembly1.SharedResource.culture.resources
resgen SharedResources.culture.resx Assembly2.SharedResource.culture.resources

And each resource file can be linked to the proper satellite assembly with assembly linker command al.exe

al -target:lib -embed:Assembly1.SharedResources.culture.resources -culture:culture -out:culture\Assembly1.resources.dll
al -target:lib -embed:Assembly2.SharedResources.culture.resources -culture:culture -out:culture\Assembly2.resources.dll

This instruction are also example of how manage translation outside Visual Studio and having the capability to develop and distribute them in a separate
time respect o product sources and executables.

Sample of this also in the POC.

Detection of Culture

ConsoleLocalized in the POC programmatically set both "Culture" and "UICulture" so is not involved in culture detection.

LocalizedAPI in the POC has been tested calling API from Postman and setting client culture or in QueryString or in Accept-Language

Explanation of culture detection at: https://docs.microsoft.com/en-us/aspnet/core/fundamentals/localization?view=aspnetcore-5.0#localization-middleware-2

The culture detection algorithm used was the default one.

To have the detection correctly working it's necessary to set, at the startup of the server, the list of supported cultures. This imply one of the following:

```
The list of supported cultures is well known at compile time and hard coded in the service binary
This system is not flexible and for sure doesn't feet requirement of real use case of distribution. We do not know at the moment we write software
the full list of cultures that will be requested in the future.
The supported cultures are configured with all the cultures supported by asp.net core (>250 cultures and >500 dialect)
This system configures a big list of cultures and cover most common case, but not the case of a custom dialect related to a specific plant.
The culture are dynamically detected at start-up from the ones effectively presents on the disk
An example of this algorithm is in the POC (may be not the best one) and can cover almost any case. Adding a new culture require the restart of
the service that it's required in any case of change.
```

# Appling Localization to Platform controllers

## Description of work

In the MetadataRuntime Repo has been created a branch named user/gv_localizer.

On that Branch the work done for spike User Story 13571: "Spike: Localization Support for Metadata Runtime components" has been integrated in the
Platform solution to translate some error messages.

The chosen error messages are in the class CommandApplication used by the ModelingActionController.

Some point of the implementation that can be followed in the wall project is detailed below.

## Shared resources

A class SharedResources has been added to the Siemens.MOM.Platform.Core project that is referenced in a lot of other projects so that it can contain
string for a large part of the platform (from controllers to gateways).

Outside the solution a set of resx files named SharedResources.culture.resx has been created together with some script to compile them.

This test the capability to share resources across different assemblies. More simple is to have one resources file per assembly sharing among class of
each assembly in this case the resx file will be named AssemblyName.SharedResources.culture.resx.

## IStringLocalizer propagation

Startup of Siemens.MOM.Platform.Api has been modified to configure localization and to add it to dependencies injection.

Controllers receive the correct instance of IStringLocalized and must propagate it to other classes like already done for ILogger instances.

In projects where it's not possible or relevant the use of the dependency injection and relative propagation from controllers a factory can be used to
configure and instantiate the string localizer. An example is present in WS_Spikes Repo in the project "ConsoleLocalized". Examples of this type of
project can be AOP weavers or generators that up to now seems they do not need localization.

## Change in existing call

here same example of code change to in invocation of localizer

```
// original call
throw new KeyNotFoundException($"Entity with key '{nameKey}' not found");
```
```
// call with localizer a default string directly used as key
// note no interpolation string used and parameter inside []
throw new KeyNotFoundException(localizer["Entity with key '{0}' not found", nameKey]);
```
```
// using resources embedded in an other assembly and shared in the workspace Siemens.MOM.Platform.Localization
using Siemens.MOM.Platform;
throw new KeyNotFoundException(localizer[Localization.SharedResources.Entity_with_key___0___not_found,
nameKey]);
```
## Detection of Culture improvement

The algorithm for culture detection has been to check also the all supplied file are well-formed.

## Open Point to choose Localization strategy

## granularity

One point is the granularity of resource files. It's better to have a big file with all resource strings used in platform or one file for each assembly?

Having investigated the possibility of sharing string among assemblies a proposal to start can be summarized in 3 points:


1) Put localized strings in Siemens.MOM.Platform.Core and use them for the group of project that can easily share them without add new adhoc
dependency. A candidate group of projects is Siemens.MOM.Platform.Api.csproj, Siemens.MOM.Platform.Application.csproj and all platform gateway.

Advantages

```
for this group will be easy management of translations files: adding localization for a new gateway only requires to add new strings
some simple string used in more than one assembly can be managed once
deploy of one file per culture.
```
Attention

```
resources file may become to big and difficult to manage.
```
2) Assembly not having the dependency from Siemens.MOM.Platform.Core or having a different build order (or may any other reason) should have a their
own resource files.

3) In any case a different choice can be easily taken for each assembly in the moment that we take care of localizing its strings.

default langue

how to manage resource strings of the default langue

Using en-US ad default possible choice are;

```
default en-US strings directly used as key of the localizer without resx file
Pros easy to develop
Cons difficult to translate because typically translation are produced starting from the original en-US one
default en-US strings directly used as key of the localizer but also defined within resources (emended or not is not too much relevant)
Pros easy to develop
easy to translate
Cons string must be kept aligned in code and in resources and this is error prone
default en-US strings in a Resource embedded and each string has a short name used as key of localizer
Pros easy to translate
not error prone
Cons need a little bit more work
```
In the last case it's necessary to take care of the namespace given to the class that share the embedded resources. Follow the example in The POC.


# Generation of Localization Resx File in GeneratedModel

# starting from the Metadata Input Model

Analyzing the User Story 19752: "Spike - Generate Localization Resx File in GeneratedModel starting from the Metadata Input Model" some code ha been
produced in the gv_modelLocalizer branch:

https://tfs05mom.industrysoftware.automation.siemens.com/MOM/ModularMOM/_git/MetadataRuntime?version=GBgv_modelLocalizer

Code is not intended to be merged to master.

!!!An up to date version of work related to spike is in branch

https://tfs05mom.industrysoftware.automation.siemens.com/MOM/ModularMOM/_git/MetadataRuntime?path=%
2F&version=GBgv_modeldeplocalizer&_a=contents

This is the version partially merged to master

At the end on Spint12 all development have been merged to master.

As possible translation example the value returned in the __Label from a GET API has been used and the value is coming from the Label in the
file NamedObject.json

"Name": {
"Label": "Object Name",

Translation is applied while elaborating response to API.

This approach can work when the target is to translate values related to Objects and Fields of the Metadata Input Model

The export has been implemented with Template and Providers similar to the ones already in GeneratorModel.

String resources coming from the Model (Lalbel of fields and Objects) has been organized in one file named CheckModelSharedResources.resx placed
in the generated project Siemens.MOM.SampleModel.Common or more in general In the project named "Namespace.GeneratedModel.Common".

String resources coming form ModelSharedResources of the module and all its dependencies has been organized in one file named ModelSharedResou
rces.resx relate to an empty class ModelSharedResources.cs both placed in the generated project Siemens.MOM.SampleModel.Common or more in
general In the project named "Namespace.GeneratedModel.Common".

Note that the use case may be not realistic and code not totally clean from different tentative of implementation.

Only when will be defined what translate and when translate all the answers concerning the implementation can be definitive.

Note on exception message

No particular try has been done about exceptions but looking to actual code some considerations can done :

From the Object Metadata it's possible to get related methods and exceptions via reflection but only the definition of exceptions can be reached not the
messages itself.

Seem better in this case to use a different approach and store exception messages in a way they can be reached together with other metadata ( a
dedicated json file , some dedicated entity in the model ??).

## Resources embedded in models

All the generation of resource in resx file not directly coming from Labels is guided from the ModuleName.modelconfig.json


Here an example of a modelconfig of module named Dispatch_Customer and depending from Common and Dispatch

```
ModuleName.modelconfig.json
```
### {

```
"name": "Dispatch_Customer",
"description": "Dispatch_Customer Model to showcase theProduction Order Dispatch_Customer capabilities",
"version": "1.0.0",
"platformVersion": "1.0.0",
"gitInfo": {
"repoName": "M1_Dispatch_Customer",
"url": ""
},
"modelDependencies": [
{
"name": "Common",
"version": "1.0.0"
},
{
"name": "Dispatch",
"version": "1.0.0"
}
],
"binaryDependencies": [
{
"name": "DispatchMessages",
"version": "1.0.0"
},
{
"name": "Siemens.MOM.MessageModel",
"version": "1.0.0"
}
]
}
```
String resources embedded in module are also merged in the same unique Resx file together with Labels under the following condition:

```
The resource file and the related C# class must be named "ModelSharedResources"
the class must be in a namespace equal to the module named in modelconfig file.
```
```
Model shared resources
```
```
//namespace ModuleName
namespace Dispatch_Customer
{
public class ModelSharedResources
{
}
}
```
Note:

to manage different names of classes, resources and namespace at least one new argument must added to the Siemens.MOM.Model.Generator.exe.

Model composition

String resources embedded in module the are in composition with the current module are merged in the same unique Resx file together with Labels under
the following condition:

```
The resource file and the related C# class must be named "ModelSharedResources"
```

```
the module must be configured in modelDependencies in the the ModuleName.modelconfig.json file
the class must be in a namespace with name equal to module name as in the ModuleName.modelconfig.json file.
```
```
Model shared resources
```
```
//namespace ModuleName
namespace Common
{
public class ModelSharedResources
{
}
}
```
Note:

to manage different namespace more information must be added to the ModuleName.modelconfig.json file

Model binary dependencies

String resources embedded in module the are binary dependency of the current module are merged in the same unique Resx file together with Labels
under the following condition:

```
The resource file and the related C# class must be named "ModelSharedResources"
the module must be configured in binaryDependenciesin the the ModuleName.modelconfig.json file
the class must be in a namespace with the default name equal to module name in the short form as in the ModuleName.modelconfig.json file
the assembly containing the embedded resource must be named ModuleName.dll
```
Note:

to manage different namespace and assembly name more information must be added to the ModuleName.modelconfig.json file


# Unique Id Generation

```
Overview
Implementation
Concept
Migration
Acceptance
Rationale
Layout
Sample Code
```
## Overview

Every object in the system must be uniquely identified by an id that won’t be duplicated in any other instance of Modular MOM, and used to access it in
memory and identify it in the data store if it is persisted. This also holds true for every object definition (CDO). This should not be confused with application
related key values like “Name”. Similarly, each CO definition must also have a unique Id. The COTypeId will be generated by the metadata designer tool
when the object is created.

There are several other requirements that must also be met by the technique used to generate these Ids:

```
When an InstanceId is examined, the table that holds the row that it identifies must be able to be determined. This means that the Id must contain
the COTypeId. The need for this arises from configurations that enable multiple types of objects in the same list. When the list is processed, all
that is available is the object Id.
Inexhaustible supply of ids.
Generation of Ids must not bottleneck transaction throughput.
Records should be returned in the order they were added, if not explicitly ordered.
The Ids should consume as little space as possible.
The generation should not rely on an out of process server that could be a cause of failure.
```
The selected technique is to generate “smart” Ids that are composed of the COTypeId, a timestamp and a block of random data.

## Implementation

Concept

Using bit level processing enables the generation of Ids using the least amount of processing time. Encoding the data comprising the Ids will conserve the
space used to store the Id. Basing the Id on timestamp will ensure that we won’t run out of Ids and will enable lexical sorting to return the rows in the order
they were added. Generation being done in process will perform better than a database-based approach like sequence.

The platform servers will use the generator for the InstanceIds when objects are created. The Ids for object definitions will be generated by the metadata
designer tool. A standalone tool can also be created to generate Ids as needed for a migration tool.

Migration

While migration of existing data is not a requirement or constraint, there is likely data available in the existing systems (like TxnDate or LastChangeDate) th
at would allow a migration of existing record and object identifiers.

Acceptance

We will validate the format of the id with regard to the order of the data in the id before the platform is released, even in alpha form to any customer. This is
to ensure that the best possible performance can be achieved.

Rationale

Using the current system time and a random number will provide an id that is so unlikely as to be next to impossible and can be further constrained by a
seed to the random number.
The size of the fields will prevent any chance of running out of ids before the MOM platform is no longer is use.
Basing the ids on the system time also provides a default sorting mechanism that will generally return the records in the order that they were created.

Layout

The COTypeIds will be generated based on the number of seconds since a known starting point in time (hereafter known as the epoch) and will be Base64
encoded. InstanceIds will be generated based on the number of milliseconds since the epoch and will include the COTypeId.

COTypeId (8 Bytes)

```
Bytes Value Range Notes
```
```
1-5 Seconds since epoch began 1M (0 - 1,073,741,823) Base64 encoded
```
```
6-8 Random number 0 - 262,143 Base64 encoded
```

InstanceId (18 Bytes)

```
Bytes Value Range Notes
```
```
1-7 Milliseconds since epoch began 4.4T (0 - 4,398,046,511,103) Base64 encoded
```
```
8-10 Random number 0 - 262,143 Base64 encoded
```
```
11-18 COTypeId See Table 1
```
Values for comparison

```
Per Seconds Milliseconds Notes
```
```
Day 86,400 86,400,000 86M ms
```
```
Year 31,557,600 31,557,600,000 31B ms
```
```
50 Years 1,577,880,000 1,577,880,000,000 1T ms
```
```
Century 3,155,760,000 3,155,760,000,000 3T ms
```
Sample Code

Sample generation code can be found in the MetadataRuntime repo at Platform/Tests/IdGenerator (IdGenerator - Repos (siemens.com))


### 1.

### 2.

### 3.

# IdGenerator Library

## Description

IdGenerator library exposes different api to generate COTypeId and InstanceId. Below are the details of apis:

```
GenerateCOTypeId
This api method generates COTypeId for configurable object.
It takes no input parameter and returns 7 byte random base64 encoded string.
GenerateInstanceId
This api method generates random instanceId and it include COTypeId within.
Takes COTypeId as input and returns 18 byte random base64 encoded string.
This api assumes COTypeId provided in input is valid one.
GetCOTypeId
This api retrieves COTypeId associated with an InstanceId
Takes 18 byte InstanceId as input and returns associated COTypeId.
```
```
API Method Input Type Return Type Description
```
```
GenerateCOTypeId NA String (Length-7) Generates COTypeId for configurable object.
```
```
GenerateInstanceId String (COTypeId) String (Length-18) Generates random instanceId for a given COTypeId
```
```
GetCOTypeId String (InstanceId) String (Length-7) Retrieves COTypeId associated with an InstanceId
```

# Validation of COTypeId with metadata aspect weaver

```
Overview
Weaver configuration
Validation check errors troubleshooting
Weaver execution flow
Validation checks on referenced libraries
```
## Overview

Every object definition (CO Type) in a module must be uniquely identified by means of dedicated attribute COTypeId. This constraint is verified at build
stage using metadata aspect weaver validation checks.

Validation checks verify that each module model class is configured with COTypeId attribute and the value of such attribute is unique within the module
and any directly referenced model assemblies (for instance, Messages assembly).

Module model classes are identified as all classes inheriting from BaseObject class. Nested classes are not valid model classes.

## Weaver configuration

Metadata aspect weaver can be configured to manage validation checks failure with build errors or build warnings, by means of 'TreatValidationErrorsAsW
arnings' flag.

```
Sample weaver configuration for TreatValidationErrorsAsWarnings
```
```
<Camstar.Metadata.Aspects TreatValidationErrorsAsWarnings="true" />
```
When no weaver configuration is provided, validation checks failure generates build errors.

```
Sample without configuration
```
```
<Camstar.Metadata.Aspects />
```
## Validation check errors troubleshooting

Validation error messages contains a unique error code and are collected in an embedded resource file.

```
Error
code
```
```
Sample error message Action
```
```
MM001 Fody/Camstar.Metadata.Aspects: MM001: CO Type 'Camstar.Core.
SampleModel.Factory/NestedClass' is nested.
```
```
Rework your model, because model classes cannot be nested.
```
```
MM002 Fody/Camstar.Metadata.Aspects: MM002: COTypeId attribute is missing
for CO Type 'Camstar.Core.SampleModel.Factory'.
```
```
Decorate the model class with COTypeId attribute.
```
```
MM003 Fody/Camstar.Metadata.Aspects: MM003: CO Types 'Camstar.Core.
SampleModel.Factory' and 'Camstar.Core.SampleModel.Location' have
the same COTypeId '0dqfDfu'.
```
```
COTypeId value must be unique within the module and any
directly referenced library. Generate a new COTypeId for one of
the involved model classes.
```
## Weaver execution flow

Metadata aspect weaver must first collect all validation errors and then will proceed with the weaving only for those types that have no validation errors. If 'T
reatValidationErrorsAsWarnings' flag is set to true, then the weaving is performed for all types.

## Validation checks on referenced libraries

In order to speed up performances of validation checks on referenced libraries, the following assumptions have been applied:

```
Manage only direct references that are declared in the assembly manifest (this is leveraged by Mono.Cecil library).
Do not evaluate references having well known system namespaces (such as Microsoft, System, Siemens.MOM.MetaModel, Siemens.MOM.
Platform)
```


# Security Layer

```
Security concepts
Use cases
Deployment options
Authentication (AuthN)
OIDC ID Token definition
UI Authentication Flow
Security considerations
Coarse Grained Authorization (AuthZ)
External AuthService Implementation Details
How the token is validated
Authorization Code grant with PKCE
Token Formats
Open Issues
```
Things to clarify:

```
Integration between moving parts with use cases (From UI to the backend calls)
User identity information on the Rest API Layer (tokens consumption)
mTLS termination support on the ingress layer
Less priority: Keep in consideration also any service-based authentication, for the case of non-interactive calls started from any data
integration software (can we use SPIFFE/SPIRE mtls protocol to do that, on the ingress front-end, and being compatible/transparent on
the Rest API?)
Service to service user identity propagation through gRPC messaging
Identity aware Traces and Logs
```
## Security concepts

Customers need to authenticate their user base in a smooth way, integrating products they are using in the same security ecosystem. ModularMOM 2.x
has to take this into consideration in its design, providing a simple way to interface with different Identity Providers in the most standard and open way. For
this reason, the main target to address for the support in the Security Layer should be a well-known and widely used standard: OpenID Connect has all of
these capabilities and so it has been selected for the task.

The OpenID Connect protocol (OIDC from short) is a simple identity layer on top of the OAuth 2.0 protocol. It enables clients to verify the identity of the
End-User based on the authentication performed by an Authorization Server, as well as to obtain basic profile information about the End-User in an
interoperable and standard manner.

The specification defines authentication functionality built on top of the OAuth 2 Bearer token (RFC 6750) and the use of Claims (pieces of information
asserted about an end-user).

In order to achieve this, OIDC is interfaced with a Relying Party, that is a OAuth 2.0 Client application requiring End-User Authentication and Claims from
an OpenID Provider.

The OIDC protocol, in abstract, follows these main steps:


### 1.

### 2.

### 3.

### 4.

### 5.

```
e
```
```
The RP (Client) sends a request to the OpenID provider
The OP authenticates the End-User and obtains authorization.
The OP responds with an ID Token and usually an Access Token (AuthN response)
The RP can send a request with the Acess Token to the UserInfo Endpoint.
The UserInfo Endpoint returns Claims about the End-User.
```
On a more general level, we need to integrate the UI site and App with an Ingress API Gateway (such as Ambassador) that has to handle all the
unauthorized requests redirecting them to an OIDC provider such as Dex.

Dex can be configured to handle different Identity Providers as back-ends (Dex connectors) so that an OIDC ID Token is provided always after the
aforementioned exchange with Dex.

In case Dex has been configured with an OIDC Provider as the connector, Dex act as a "proxy" in the chain (to be more precise, it will handle the Callback
for the remote OIDC provider and then it will call the Relying Party Callback endpoint registered on the auth request)

... TODO: Simple diagram explaining the relationship from the UI App (End-User interaction) to the back-end API calls....

### DRAFT:


The main clarification is needed to understand whether this flow works in all situations or, if there is some UI requirement (handling of the redirections and
endpoints) to complete the request from the User Agent (Web Browser) to the Identity Provider:

```
The requests are first invoked on the API gateway to load the UI site (web pages and scripts), those pages may be unauthorized at the beginning,
triggering the Authentication flow. This would redirect the User Agent to load the page of the Identity Provider instead. This is different to have an
"anonymous" access to the site resources and then handle the authentication protocol from javascript scripts on-loaded page, in the latter case
the UI should be aware on different things to handle the AuthN handshake properly.
The mechanism employed to redirect and progress the flow is depending on the type of OAuth 2.0 Grant used: there are different types,
"Authorization Code Grant" is one of them, "Implicit Grant" should not be used from a SPA application. Another one is the Client Credentials
grant type, typically used from non interactive processes such as automation tools.
```
Use cases

<waiting for specific customer requirements and business cases>

For now the addressed ones are generic support for both local Users support (implemented by the Identity Provider + backend services) and integration
with Active Directory User base through specific connectors. Integrated Windows Authentication (IWA), that is the support to log in without prompting
credentials, is a nice to have but not required for Dec MVP release (also, the support is not ready available from the OSS running on Linux ecosystem).


Deployment options

On the frontend:

```
Emissary Ingress (OSS Ambassador API Gateway) with authentication support enabled with a separate Auth service used to handle
authentication on the backend through an OIDC compliant OAuth2 provider.
OIDC-aware SPA + Emissary Edge Stack (commercial) + validate tokens on incoming requests to backend services with the JWT filter
A combination of the two?
```
On the backend:

```
Emissary + Dex + 3rd party Identity Provider (supported by Dex connectors)
Emissary + Dex + KeyCloak (KC used to manage local user db + LDAP Integration with Active Directory).
Emissary + KeyCloak (KeyCloak is an OIDC compliant provider)
```
Authentication (AuthN)

The primary extension that OpenID Connect makes to OAuth 2.0 to enable End-Users to be Authenticated is the ID Token data structure. The ID Token is
a security token that contains Claims about the Authentication of an End-User by an Authorization Server when using a Client, and potentially other
requested Claims.

OIDC ID Token definition

The ID Token is represented as a JSON Web Token (JWT)

The following Claims are used within the ID Token for all OAuth 2.0 flows used by OpenID Connect:

```
Claim Descriptiton
```
```
iss REQUIRED. Issuer Identifier for the Issuer of the response. The iss value is a case sensitive URL using the https scheme that contains
scheme, host, and optionally, port number and path components and no query or fragment components.
```
```
sub REQUIRED. Subject Identifier. A locally unique and never reassigned identifier within the Issuer for the End-User, which is intended to be
consumed by the Client, e.g., 24400320 or AItOawmwtWwcT0k51BayewNvutrJUqsvl6qs7A4. It MUST NOT exceed 255 ASCII characters in
length. The sub value is a case sensitive string.
```
```
aud REQUIRED. Audience(s) that this ID Token is intended for. It MUST contain the OAuth 2.0 client_id of the Relying Party as an audience
value. It MAY also contain identifiers for other audiences. In the general case, the aud value is an array of case sensitive strings. In the
common special case when there is one audience, the aud value MAY be a single case sensitive string.
```
```
exp REQUIRED. Expiration time on or after which the ID Token MUST NOT be accepted for processing. The processing of this parameter
requires that the current date/time MUST be before the expiration date/time listed in the value. Implementers MAY provide for some small
leeway, usually no more than a few minutes, to account for clock skew. Its value is a JSON number representing the number of seconds from
1970-01-01T0:0:0Z as measured in UTC until the date/time. See RFC 3339 [RFC3339] for details regarding date/times in general and UTC in
particular.
```
```
iat REQUIRED. Time at which the JWT was issued. Its value is a JSON number representing the number of seconds from 1970-01-01T0:0:0Z
as measured in UTC until the date/time.auth_timeTime when the End-User authentication occurred. Its value is a JSON number representing
the number of seconds from 1970-01-01T0:0:0Z as measured in UTC until the date/time. When a max_age request is made or when
auth_time is requested as an Essential Claim, then this Claim is REQUIRED; otherwise, its inclusion is OPTIONAL. (The auth_time Claim
semantically corresponds to the OpenID 2.0 PAPE [OpenID.PAPE] auth_time response parameter.)
```
```
nonce String value used to associate a Client session with an ID Token, and to mitigate replay attacks. The value is passed through unmodified
from the Authentication Request to the ID Token. If present in the ID Token, Clients MUST verify that the nonce Claim Value is equal to the
value of the nonce parameter sent in the Authentication Request. If present in the Authentication Request, Authorization Servers MUST
include a nonce Claim in the ID Token with the Claim Value being the nonce value sent in the Authentication Request. Authorization Servers
SHOULD perform no other processing on nonce values used. The nonce value is a case sensitive string.
```
```
acr OPTIONAL. Authentication Context Class Reference. String specifying an Authentication Context Class Reference value that identifies the
Authentication Context Class that the authentication performed satisfied. The value "0" indicates the End-User authentication did not meet
the requirements of ISO/IEC 29115 [ISO29115] level 1. Authentication using a long-lived browser cookie, for instance, is one example where
the use of "level 0" is appropriate. Authentications with level 0 SHOULD NOT be used to authorize access to any resource of any monetary
value. (This corresponds to the OpenID 2.0 PAPE [OpenID.PAPE] nist_auth_level 0.) An absolute URI or an RFC 6711 [RFC6711] registered
name SHOULD be used as the acr value; registered names MUST NOT be used with a different meaning than that which is registered.
Parties using this claim will need to agree upon the meanings of the values used, which may be context-specific. The acr value is a case
sensitive string.
```
```
amr OPTIONAL. Authentication Methods References. JSON array of strings that are identifiers for authentication methods used in the
authentication. For instance, values might indicate that both password and OTP authentication methods were used. The definition of
particular values to be used in the amr Claim is beyond the scope of this specification. Parties using this claim will need to agree upon the
meanings of the values used, which may be context-specific. The amr value is an array of case sensitive strings.
```
```
azp OPTIONAL. Authorized party - the party to which the ID Token was issued. If present, it MUST contain the OAuth 2.0 Client ID of this party.
```

```
This Claim is only needed when the ID Token has a single audience value and that audience is different than the authorized party. It MAY be
included even when the authorized party is the same as the sole audience. The azp value is a case sensitive string containing a StringOrURI
value.
```
ID Tokens MAY contain other Claims. Any Claims used that are not understood MUST be ignored (See Sections 3.1.3.6, 3.3.2.11, 5.1, and 7.4 for
additional Claimes defined by the OpenID Connect Core RFC6749)

ID Tokens MUST be signed using JWS [JWS] and optionally both signed and then encrypted using JWS [JWS] and JWE [JWE] respectively, thereby
providing authentication, integrity, non-repudiation, and optionally, confidentiality, per Section 16.14.

If the ID Token is encrypted, it MUST be signed then encrypted, with the result being a Nested JWT, as defined in [JWT]. ID Tokens MUST NOT use none
as the alg value unless the Response Type used returns no ID Token from the Authorization Endpoint (such as when using the Authorization Code Flow)
and the Client explicitly requested the use of none at Registration time.

Example:

### {

```
"iss": "https://server.example.com",
"sub": "24400320",
"aud": "s6BhdRkqt3",
"nonce": "n-0S6_WzA2Mj",
"exp": 1311281970,
"iat": 1311280970,
"auth_time": 1311280969,
"acr": "urn:mace:incommon:iap:silver"
}
```
UI Authentication Flow

Here is depicted the main (simplified) UI interactions from the resource owner (the user to be identified) to the backend service calls (resource server).

This flow is for Authentication Code grant with PKCE.

Note that this picture is simplified focusing on the UI side of the flow instead of the backend interactions among infrastructure services such as the API
Gateway, the Auth Service, the OIDC provider and the configured Identity Provider,.


Notes:

```
Remote URI calls are passing through the Ingress+API Gateway routing to reach the remote http services (either the UI module site or the OIDC
Provider or, finally, the backend ModMOM service)
The UI App, integrated with the SWF Framework, performs the check whether the current SPA session is authenticated or not. This is
implemented in the checkIfSessionIsAuthenticated() hook. This hook will check if a valid token is already stored into the SPA App context
(probably persisted into the session storage of the browser).
The UI Module Site URI endpoint must not have any validation enforced (loading the assets of the Single Page Application), otherwise this will
trigger a validation from the API Gateway that can start the authentication by itself from the infrastructure. In this case the UI App will not see any
tokens and the aforementioned diagrams will not be valid.
The Tokens are stored from the App logic implementing the SWF Session Service Authenticate() hook. To do this there are various abstractions,
one is the application context, so that the first checkIfSessionIsAuthenticated() can check whether to start the authentication or not in the first
place.
After the Authentication Flow is completed, the UI APP will actively use its own tokens, returned by the OIDC client library, during each service
backend call using the Authorization Bearer standard.
In this flow there are no session cookie involved or used, this is because there is no App backend to delegate the authentication logic (ui business
logic is running inside the browser like any SPA).
In case the intermediate API Gateway is configured to relay the authentication to an AuthService, please see External AuthService
Implementation Details
```
Security considerations

Refresh tokens are handle by the SPA itself, through an automatic update before the current tokens expire.

Because of security reasons, the refresh token should be provided with a short expiration time: the OIDC provider should be configured to return refresh
tokens with a configurable amount in order to set to the proper required time span (let's say for example 10 minutes instead of 1 day by default). Also, in a
SPA the Refresh Token Rotation should be configured on the provider side in order to invalidate any attempt to re-use the same refresh token more than
once.

During the time span where the Refresh token is valid, the ID and Access tokens are renewed with the current Refresh token, performing a request to the
OIDC provider (on the /Token endpoint). The new set of tokens are then returned back to the relaying party (the SPA) and the old refresh token is
invalidated for good (Refresh Token Rotation).


Note: the Refresh tokens expiration time is not asked by the relaying party in a parametric way, but it is handled by the configuration of the server itself, so
we must be sure that for both Dex or KeyCloak this parameter is configurable.

For example:

ID + Access Tokens expiration : 5 minutes

Refresh Token expiration: 10 minutes

A timer is set to refresh the tokens pro-actively each 4 minutes (before the access tokens expires).

Default Keyclock values for token expirations:

```
Token Default Configuration Keys Overridden by Notes
```
```
Refresh
Tokens
```
```
30 min Realm Settings/Tokens/
```
```
The Minimum between:
```
```
MIN (SSO Session Idle,
SSO Session Max)
```
```
and
```
```
MIN (Client Session Idle,
Client Session Max)
```
```
Clients/*/Advanced Settings/
```
```
The Minimum between:
```
```
MIN (Client Session Idle,Client Session Max)
```
```
and the Realm Settings
```
```
Note: You cannot extend the token expiration of
a specific client beyond the realm settings ones
```
```
https://keycloak.discourse.group/t/difference-between-client-
session-and-sso-session-in-timeout-settings/4190
```
```
Refresh tokens are not returned in client credentials flow (by
default, no session is associated to client cred.)
```
```
Access
Tokens
```
```
5 min Realm Settings/Tokens/ A
ccess Token Lifespan
```
```
/Clients/*/Advanced Settings/Access Token
Lifespan
```
```
Note: You can extend the token expiration of a
specific client beyond the Realm Settings one
```
```
ID
Tokens
```
```
5 min The same as the Access
Tokens
```
```
Realm Settings/Tokens/ A
ccess Token Lifespan
```
```
The same as the access token
```
```
/Clients/*/Advanced Settings/Access Token
Lifespan
```
```
Hardcoded in the code (not configurable by a specific key)
```
```
https://github.com/keycloak/keycloak/blob/8.0.0/services/src
/main/java/org/keycloak/protocol/oidc/TokenManager.java#L765
```
```
ID
Token
Rotation
```
```
False
```
```
0
```
```
Realm Settings/Tokens/R
evoke Refresh Token
```
```
Realm Settings/Tokens/R
efresh Token Max
Reuse
```
```
To Enable: True
```
```
Reuse: 0 (use it only once)
```
Mitigations:

1) In case the Refresh token has been compromised by any front channel attack, the short lived time mitigates the time frame in which they can use it.

2) Refresh Token Rotation: the fact that the refresh token is renewed automatically by the app gives a further protection, because, in general, it will
reduces the validity of the refresh tokens on the fly (to the time span of the refresh cycle). This is because the authorization service, when it detects that an
already-used refresh token is provided by a client, will invalidates all the refresh tokens issued previously (by designed specs), that's true even for the curre
nt valid one. This is to force the users to log in again. That is why the Refresh Token Rotation feature is important to enable in the OIDC provider used.

3) Authentication Code grant flow + PKCE prevents the exchange of the token values on the front channel (the browser adress bar URL) as a mechanism
to return those tokens. This avoids any "history"-based attacks on the browser side. They are returned without any secrets exposed in the front channel.

4) The TLS protection of the communications between the browser and the backend authentication infrastructure.

5) Standard browser protection features to segregate the runtime.

Coarse Grained Authorization (AuthZ)

To perform the authorization on the flow we can use an external authorization service such as https://github.com/arrikto/oidc-authservice/blob/master
/README.md configuring the API Gateway (Emissary Ingress) with a configuration of Envoy Filters https://www.getambassador.io/docs/edge-stack/latest
/topics/using/filters/external/

Note: the Emissary Ingress filters has to be validated because the online help refers to the Edge Stack (the commercial product) but the corresponding
section in Emissary is missing from the documentation.

The Authorization service provide a session for each logged in user, storing the information of the token associating it to a session cookie:


This flow is of type "code" or "code grant" which has to start from the oidc client library on the browser.

The Authorization service acts as a proxy in front of the real OIDC Provider that completes the code flow, taking care of getting the ID Token and storing it
in a persisted user session.

This session is then reconciled during the redirected requests (from the browser) setting and using a session cookie to fill in the original request
configurable headers from the Authorization Service, such as Authorization: bearer, containing the JWT ID Token to be used on the final request to the
target service.

When further requests are performed from the browser, using the same session cookie, the authorization service will check for the validity of the JWT
Token and the session.

The logout is similarly performed to invalidate the user session stored inside the Authorization Service and from the OIDC provider: https://github.com
/arrikto/oidc-authservice/blob/master/docs/logout.md

External AuthService Implementation Details

The API gateway forwards each incoming requests on the protected upstream backend endpoints to the external AuthService component. This is done on
the root endpoint of the AuthService, i.e: https://autservice:8000/

When the original http request arrives to the AuthService in question, the first step, marked as "Not logged in, begin OIDC flow" in the diagram, is
performed like this:

```
Go though the configured list of authenticators in this order: Session, IDToken, Kubernetes
Invoke AuthenticateRequest() for any of these.
AuthenticateRequest on Session :
Tries to find the corresponding token from the persistent session store "oauth2tokens", by using the Authorization header token or by
using the Browser session cookie as the SessionID key), return false in case this fails (no session in our case).
Retrieve the user info (Name) from the session value, checking the validity of the token retrieved from the session. If the access token
has expired: revokes the session (revoking the tokens to the remote oidc provider, revokeOIDCSession)
Retrieve and map the Groups from the session value (if any)
Return an authenticator object with the Name:session.Values[userSessionUserID], Groups: groups found
AuthenticateRequest on ID token:
Check the Authorization Bearer header and validate the JWT token found, given the proper configuration to validate the certificate ,
return false in case this fails
Get the bearer token from the incoming request
If any, get an OIDC verifier initialized with the ClientID=config.ClientID, otherwise fail with false
Verify the bearer to get the token object, in case of failure: return false
Searching for USERID_CLAIM value (default:"email" or envconfig:"USERID_CLAIM"`) into the IDToken, return false in case this fails
Maps any groups claim
Return an authenticator object with the Name:USERID_CLAIM, Groups: groups found (no session is initialized in this case, just the
object)
```

```
In case a valid user cannot be found at this stage, start the OIDC flow to authenticate (redirect to the oidc provider configured, authorize
endpoint) (*)
In case a valid user has been found at this stage, authorize the request with the configured list of authorizers: currently only one, based on the
groups found previously and the configuration settings (GroupsAllowlist with default:"*"`), return false in case this fails (invalid authz)
If the request is not allowed:
revokes the session (revoking also the tokens to the remote oidc provider, revokeOIDCSession)
Return status Forbidden to the request
If the request is allowed, insert the userInfoToHeaders to the upstream backend service call ("Logged in, Add User ID Headers" in the diagram)
```
*) Note: this should not happen if the token is properly validated by the AuthService. This will manifest as a second login for the user, it is not clear whether
this will trigger, after the second login, for each rest call to the backend or not (The UI is using the same client tokens that would trigger the failure again; it
depends on the cookie handling in the rest api call site). The ClientID to be verified is likely the culprit here (or the public keys configuration): this has to be
configured via the env variable CLIENT_ID equal to the ClientID used by the UI, i.e. "ModularMOM" or something predefined. We cannot handle multiple
ClientID coming from the UI because this setting is only one single value (normally used by this service to authenticate itself to the oidc provider in the
session authenticator).

How the token is validated

The ID token authenticator make use of the "github.com/coreos/go-oidc" library.

The provider is initialized to point to the jwks url coming from the oidc well-known discovery URL (example auth/realms/modmom/.well-known/openid-
configuration).

Jwks_uri is something like /auth/realms/modmom/protocol/openid-connect/certs containing the public keys to verify the signature for a given oidc provider
(1).

NewRemoteKeySet returns a KeySet that can validate JSON web tokens by using HTTP GETs to fetch JSON web token sets hosted at a remote URL.
This is automatically used by NewProvider using the URLs returned by OpenID Connect discovery, but is exposed for providers that don't support
discovery or to prevent round trips to the discovery URL.

The returned KeySet is a long lived verifier that caches keys based on any keys change. Reuse a common remote key set instead of creating new ones as
needed. The algorithm support key rotations (2) performing a new request to the oidc provider when the keys are not found in the cache. (3)

The Verify method (4) , called by the ID token Authenticator (5) checks:

```
The Issuer matching the one configured
The expiration of the jwt token.
The signature using the keys explained above.
```
1: https://github.com/coreos/go-oidc/blob/08563f61dbb316f8ef85b784d01da503f2480690/oidc/oidc.go#L124

2: https://openid.net/specs/openid-connect-core-1_0.html#RotateSigKeys

3: https://github.com/coreos/go-oidc/blob/15b94d97d90c9455981dbdd90a021d34254cfdaf/oidc/jwks.go#L105

4: https://github.com/coreos/go-oidc/blob/15b94d97d90c9455981dbdd90a021d34254cfdaf/oidc/verify.go#L194

5:https://github.com/arrikto/oidc-authservice/blob/df1f385406a517ec31bea3cf34d5181e0ada25b5/authenticator_idtoken.go#L34

Authorization Code grant with PKCE

PKCE (Proof Key for Code Exchange) is a protocol defined in RFC 7636 that extends the OAuth2 protocol. It allows applications to use the most reliable
OAuth 2.0 flows in public or untrusted clients - the Authorization Code flow. In order to efficiently use a dynamically generated password, it achieves this by
doing some setup work before the flow and some verification at the end of the flow.

Even if it is specified as a lower level OAuth2 security extension, the flow works also for OIDC Authorization Code flow (OIDC is just a further extension
with a scope "openid").

The problem: anybody can visit the link returned during the OAuth2 code flow to post request to the same app backend (with unsolicited requests to the
redirect_uri endpoint).

The temporary solution (first filter) was to use a "state" parameter that is checked on the server side app to correlate the original request with the final one,
so if it does not match the request is rejected.

PKCE add a layer of security to the state parameter to protect from code injection, code swapping and CSRF (cross site request forgery).

Example of a standard Authorization Code grant flow (without PKCE) using a back-end app (yelp.com) with the Google Authorization service providing
an access token, with authorization to access "contacts" in the scope:


As we can see the Client Secret, needed to identify the app (yelp.com) to the Oauth2 Authorization Service on Google, is stored and provided by the app
back-end during the Authorize request on point 3 and after the code is returned on the Token endpoint on point 8.

If this is done in a SPA running on the browser so that the client_secret is needed. Not only, even if a state parameter is provided by the app on point 3
(not showed in the picture) the same state is seen on the front-channel during the redirections and used back on point 8. This can be steal and used in a
malicious way.

Example of the Authorization Code Flow With PKCE and a SPA:

The app will generate a random string "code_verifier" (v in the picture) and a hash of it: "code_challange" ($ in the picture). Then it will use the hash
(code_challenge) to authorize the request on point 3.

The Authorization service will return to the app the "code" to be used to exchange it with tokens. The app will start a http request on the Token endpoint
only with the client_id, the code_verified and the code itself.

The Authorization service will verify the code_verifier (coming from the back channel on point 8, v) with the hash seen on the front channel on point 3 and,
if those validations are ok (calculating back the hash from v), it will authorize the app without handling the client_secret.

Notes:

Refresh tokens are still returned on the Token endpoint with a expiration, these tokens must be stored by the app to ask for a new set of tokens afterwards.

Token Formats

Logging a User Account with OIDC Code Flow:

Access Token Example

```
Access Token
```
### {

```
"exp": 1637343724,
"iat": 1637343424,
"auth_time": 1637343423,
"jti": "9672f7bc-3343-4697-abdf-884e6950c020",
"iss": "http://keycloak-http.default.svc.cluster.local/auth/realms/master",
"aud": "account",
"sub": "2a5fb587-d399-4d9a-8cae-6fe412f3f282",
"typ": "Bearer",
"azp": "modmom-ui",
"session_state": "649c8846-bf74-4f4b-8fbd-3a4ca4883d0f",
"acr": "1",
"realm_access": {
"roles": [
"default-roles-master",
"offline_access",
"uma_authorization"
]
},
"resource_access": {
"account": {
"roles": [
"manage-account",
"manage-account-links",
"view-profile"
]
}
},
"scope": "openid email profile",
"email_verified": false,
"preferred_username": "momadmin"
}
```

ID Token Example

```
ID Token
```
### {

```
"exp": 1637343724,
"iat": 1637343424,
"auth_time": 1637343423,
"jti": "c588e8c7-1dd8-4b40-bbe2-8b2f33311bb5",
"iss": "http://keycloak-http.default.svc.cluster.local/auth/realms/master",
"aud": "modmom-ui",
"sub": "2a5fb587-d399-4d9a-8cae-6fe412f3f282",
"typ": "ID",
"azp": "modmom-ui",
"session_state": "649c8846-bf74-4f4b-8fbd-3a4ca4883d0f",
"at_hash": "DJvPJHsEZhhLLLA7u_zvDw",
"acr": "1",
"email_verified": false,
"preferred_username": "momadmin"
}
```
Refresh Token Example

```
Refresh Token
```
### {

```
"exp": 1637344024,
"iat": 1637343424,
"jti": "04f6a924-2ea4-4c77-ae1e-8ccb1e06d198",
"iss": "http://keycloak-http.default.svc.cluster.local/auth/realms/master",
"aud": "http://keycloak-http.default.svc.cluster.local/auth/realms/master",
"sub": "2a5fb587-d399-4d9a-8cae-6fe412f3f282",
"typ": "Refresh",
"azp": "modmom-ui",
"session_state": "649c8846-bf74-4f4b-8fbd-3a4ca4883d0f",
"scope": "openid email profile"
}
```
Open Issues

```
# Title Description Possible Solutions
```
```
1 The
culture
of the
user
seems
to
depend
on how
the IdP
is
impleme
nted
```
```
Even if we find a way to include it into the ID Token, using custom scopes
during the initial handshake, the format is IdP dependent (which claim field
/urn is used to represent the culture) so the returned token may contain
information on a different field than expected by the Platform code,
depending on the customer configuration and IdP used. For example: some
IdP may not provide this information altogether (Active Directory may be
one of these) or, at least, may require to edit the users properties by the IT
department in order to include some "custom" property with a mapping to an
urn scope in the token. This would be very hard to ask as a requirement to
the final customer and still will not resolve the problem in a generic way for
very different OIDC providers already used on the customer site.
```
```
Solution: For the 1 point the culture is used to translate/localize the
errors in the backend. The culture in the token may be avoided.
However, the culture could be sent by the browser in the standard http
headers (Accept-Language). The UserCtx information in the Platform
can be augmented with information retrieved from the ID Token and
some additional HTTP header like the culture. Aspnet.core has a built in
support given various options to deal with the incoming culture: https://d
otnetcoretutorials.com/2017/06/22/request-culture-asp-net-core/ This
would also work in the pass-through with gRPC messages (even if
cross the service boundary) because gRPC are just HTTP2 frames with
headers/trailers.
```
```
This seems more standard and supported more widely than a custom
claim requirement on the UserInfo of a generic OIDC provider endpoint
asking for custom scopes.
```
```
The solution should be validated from the UX prospective but in
general, this is how the web works, so it should not be a real problem.
```
```
2 Dex
does
```
```
In order to complete the logout the OIDC provider must be capable to
"delete" the session associated to that user, however, because Dex is a
```
```
For now there is very few things that we can do about it, the only
options are:
```

```
not
support
OIDC
Session
Manage
ment in
the
OIDC
specs
so the
logout
endpoint
is not
impleme
nted
```
```
relay towards others IdPs (that may or not support OIDC as well) that part of
the specs was left out of the current implementation. Extend Dex with an in-house development of the full OIDC specs
for session management (this would be too risky for different
reasons : from clearing to security assessments, maintainability
and the effort required)
By-pass Dex only for the logout and ask the remote IdP to perform
the logout directly. This presents other problems such as: the fact
that we don't know which is the remote IdP/connector configured
in Dex (the point of Dex is to provide the abstraction) and possibly,
we don't know other parameters needed to complete the logout
from the UI / Api gateway.
Drop Dex for another software that fully support OIDC specs. For
now we are just saying to use Keycloak without Dex only when the
customer does not have any other IdP.
```
```
09/15/2021 meeting: For now we can state this limitation clearly (no
logout support) with the PMs, MVP is not requiring it for now. We just
wait for the timeout of the session on the IdP side.
```
3 Authoriz
ation
Service
vs UI
driven
OIDC
handsha
ke
impleme
ntation.

```
The standard authorization code flow (type: code grant) is designed to work
with an app back-end (the client business logic code of the UI), this is why
the return value from the handshake is a 'code' to be used to retrieve the
tokens later from another endpoint (in two steps). This is the separation of
the front channel vs the back channel.
```
```
Putting an Authorization Service between the API Gateway and the final
service is a way to protect the flow to track the used and store its session in
the back channel (where it is supposed to be) leaving the front channel only
for redirections, with no secrets to store. On the other hand we are
implementing pro-actively a pure oidc flow in the client SPA, running on the
browser in js. The SPA so is in charge to handle directly the secrets to
complete the flow, such as the client_secret (to ask for authorize that
particular app) and the tokens (Access Token, ID Token and the Refresh
Token) returned, they should be handled on the app back-end (in theory).
It's not clear right know for a pure SPA application, what flow is better and
how to avoid long lived tokens, such as the refresh tokens.
```
```
Open question
```
```
PKCE (Pixie) may be used to harden the code flow against some
attacks (but it must be supported by the oidc provider). This flow does
not make use of the client_secret because will use a code_verifier and
a code_challange from the app logic, later verified on the provider itself.
However, if this code is implemented in the SPA, it will still get all the
tokens. Refresh tokens are particularly sensitive because they are long
lived and a secure store is needed.
```
```
In contrast, the Authorization Service seems a good candidate to
abstract the authentication process. It will provide a session
management storing the tokens on the backend (so not in the frontend)
correlating them with a session cookie. But, in that case, the flow is in
charge of this service and not to the UI (we won't have any tokens
available on the UI side). All the requests will go though the API
Gateway that it will bootstrap the authentication flow and inject the
proper token on the called service (authorization bearer). In this case it
would be a problem for CLI tools to use the Rest APIs given that the
flow is automatically started for the interactive ui login.
```
```
Trubini, Piergiorgio : Task to explain the final flow and to spot any
conflicting functionalities and/or limitations that may arise having both at
the same time (UI and Auth Service). Also check to understand if the
refresh token usage on the Auth Service provide the function (whether
the tokens are refreshed or not periodically). Also, configuration of the
API gateway to have the routing to the OIDC provider as a pass-
through and other mixed cases where the SPA is calling a backend
service (with or without the token) How the SPA is detecting the missing
login and start the flow? Is the API gateway ever triggered to start the
login in that case? If so, how the tokens are handle and by whom.
```
```
Answers: The Auth Service does not implement the
grant_type=refresh_token on the OIDC provider Token/ endpoint. In
case of specific cases it will just invalidate the all session performing a
revocation of the tokens. This would result in a new login session (with
credentials input) to be made on the next call.
```
```
However, this session flow is not used in case we provide the ID Token
already in the request, in this case the oidc client is the SPA and it will
be in charge to renew the tokens with the refresh token. The
AuthService will just verify / validate the JWT Token provided in the
request (session-less check using the IDToken authenticator).
```
4 Coarse
grained
authoriz
ation

```
Even if we completed the authentication flow delegating to an external
authorization service, we need to verify not only the validity of the tokens
but also the scopes requested either by means of the access token or the
ID token through a simple (grouprole) mapping.
```
```
The scopes however are custom only for specific full stack implementations
(where you define a scope such as: supervisor or operator and implement
an authorization service capable to provide those). After the backend
services are capable to handle the scoped access tokens to permit/deny the
request based on the reached endpoint. Whereas, when you use a 3rd
party oidc provider, is up to the Identity Provider to support those scoped
tokens. Or to provide at least a list of "groups" which the user belongs to, so
that a mapping can be performed by configuration. A filter may be
implemented on the api gateway depending on the current endpoint
requested when the user is on the role "supervisor" or "operator" but this
seems far fetched...
```
```
Open question: we need to complete the flow design to understand
better how the API gateway is supposed to distinguish for different
users on different roles.
```
```
Answer based on the known flow with API Gateway+AuthService: the
Authservice is capable to do the authz based on the groups claim found
in the ID token (not specific claim based on permissions), this is
provided during the http request in the authorization header: this groups
claims are checked against a list of allowed groups applied for each
endpoint given the proper configuration of this service. Wildcards are
supported.
```
```
However, the urn groups claim is an optional feature for any OIDC
Provider, depending on the Identity Provider used so much so we don't
want to base the authorization on that specific mapping (Ramesh).
```
5 User
Base
needed
for the
Fine
grained
Authoriz

```
In order to map a User identity to roles, we can implement a "User
Management" module with all the required model and mappings between
Users, Roles, User Groups. However, this module need to be able to import
(or browse on behalf) the users currently configured/enabled on the IdP.
```
```
OIDC is not helping here, this is a custom implementation for each IdP.
```
```
Open
```
```
Keacloak may be used so that we can call specific KC api but in this
case it will become a requirement. A more general approach may be to
have a separate module written for a specific IdP (extended
/implemented by the integrators) to provide the minimum capabilities to
browse the users on the remote IdP (implementing a required
```

ation
model

```
Keycloak have its own rest API interface to browse the imported/configured
users (https://www.keycloak.org/docs-api/5.0/rest-api/index.
html#_users_resource). This is for admin users.
```
```
interface). Another approach may be to use a data integrator service
with a custom "connector" which call a specific endpoint of User
Management module to import users in bundle, but the interface must
be careful designed in both cases. A synchronization (polling) is still be
needed to keep the module up to dated with the actual access list (if a
user is deleted, it should be automatically invalidated or removed also in
the UM).
```

# Authenticated Api Request via Postman

## Open Shift Projects:

Openshift Internal Project Links

## Postman requests - OAuth 2.0 Authorization

In order to allow authenticated calls via postman, the following settings would be required:

```
Launch postman app and select Authorization tab as shown below and then open the configuration option:
```
```
In Configuration option provide the details as follows (for Open Shift 10 environment):
```
```
a) Grant Type= Client Credentials
```
```
b) Access Toke Url= http://keycloak-modmom-10.apps.openshift03.swqa.tst/auth/realms/modmom/protocol/openid-connect/token
```
```
c) ClientId=modmom
```
```
d) Client Secret=
```
```
e) Scope=client_cred
```
```
Once the configurations have applied, the settings should look as described in the following picture:
```
```
Click GetNewAccessToken to generate a new token and use that token before making new request from postman.
```
In order to get the access token in an automatic way , we can handle the token request in a Pre-request script as we do in the following example:


```
request keycloak token
```
```
console.log('Start Auth script');
```
```
var token_last_req = new Date (pm.environment.get('AUTH_TOKEN_LAST_REQ_TIME'));
if(!token_last_req)
{
token_last_req = 0;
}
```
```
const current_time = new Date();
var expire_in = pm.environment.get('AUTH_TOKEN_EXPIRATION_TIME');
diffSeconds = Math.floor((current_time - token_last_req)/1000);
```
```
console.log('current time: ' + current_time );
console.log('last request time: ' + token_last_req );
console.log('TokenDuration:' + expire_in + ' seconds');
console.log('Passed time:' + diffSeconds + ' seconds');
console.log('Remaining time:' + (expire_in - diffSeconds) + ' seconds');
```
```
if(diffSeconds > expire_in)
{
console.log('token expired');
```
```
const tokenUrl = pm.environment.get("TKN-MS-URL");
const clientId = pm.environment.get("AUTH_CLIENT_ID");
const clientSecret = pm.environment.get("AUTH_CLIENT_SECRET");
const scope = pm.environment.get("AUTH_SCOPE");
const grantType = pm.environment.get("AUTH_GRANT_TYPE");
```
```
const tokenRequest = {
method: 'POST',
url: tokenUrl,
header: { 'content-type': 'application/x-www-form-urlencoded' },
body: {
mode:'urlencoded',
urlencoded: [
{ key: 'grant_type', value: grantType },
{ key: 'client_id', value: clientId },
{ key: 'client_secret', value: clientSecret },
{ key: 'scope', value: scope }
]
}
};
```
```
pm.sendRequest(tokenRequest, (err, response) => {
const jsonResponse = response.json();
const newAccessToken = jsonResponse.access_token;
const expiresin = jsonResponse.expires_in
console.log('token:' + newAccessToken);
console.log('expiresin:' + expiresin);
pm.environment.set("AUTH_TOKEN", newAccessToken);
pm.environment.set("AUTH_TOKEN_EXPIRATION_TIME", expiresin);
pm.environment.set("AUTH_TOKEN_LAST_REQ_TIME", new Date().toISOString());
```
```
console.log('request result: ', err);
//console.log('request: ', pm.request);
//console.log('response: ', response);
}
);
}
```
```
console.log('End Auth script');
```
The variables for the authentication request (TKN-MS-URL, AUTH_CLIENT_ID, AUTH_CLIENT_SECRET, AUTH_SCOPE, AUTH_GRANT_TYPE,
AUTH_TOKEN_LAST_REQ_TIME) and its response (AUTH_TOKEN, AUTH_TOKEN_EXPIRATION_TIME) are stored at environment level because we
have a different authentication service for each OS scenario, so you can easily switch between the OS scenario just selecting the corresponding
environment in postman.

In order to use the token generated via script, you just need to configure the Authorization tab by selecting the "Bearer Token" Type and set the Token


### 1.

### 2.

value with the variable of the script:

Remarks:

```
The procedures can be also done at the folder\collection level in order to be shared by the requests (in the request the authorization must bet set
as "Inherit auth from parent")
Individual OS scenario will have its own Client Secret and access token url which can be retrieved from KeyCloak admin console for the
respective OS scenario.
```
```
Eg: http://keycloak-modmom-13.apps.openshift03.swqa.tst/auth/admin/master/console/
```

# Authorization Model

Authorization is a mechanism to check if the current authenticated user have access to a securable entity exposed by an API.

## Architecture

```
Metadata Engine
The Metadata Framework will define interfaces for User, Role, Role Permission and Group
These interfaces will be implemented by the Models in User Management App
Authorization Middleware
Extracts User Id from the Bearer token
Invokes Check Authorization on the Authorization Library with User Id, URL and Http Verb as arguments
If Check Authorization return true then invokes the next stage in the pipeline else return 403
Authorization Library
This is populated on the Startup by introspecting the metadata model
This is populated as by querying for this information using a gRPC sync call to the User Management App. This is done on demand for
each Request
Caches the API registry.
Caches the permission map. This map stores the User Id as the key and the permission as value
Parses the URL route and extracts information such as Resource, Path, Model type so that it can mapped to a specific action such as
‘Create’, ‘Update’ etc.
Implements logic to calculate permission
Publish API Message
This async message is used to Publish the list of APIs, to the User Management App, on the Startup of a Modular MOM App.
Update Permission Message
This async message is used to synchronize the Permission Cache in the respective Mod MOM App
Query Permission
All Mod MOM Apps use a sync call to query for permission associated with a User and a Resource
User App
Defines Models such as User, Role, Permission, Group, API Registry etc.
Caches the a de-normalized version of permission information in memory
```
## UML Diagrams


### 1.

### 2.

### 3.

### 4.

### 1.

### 2.

### 3.

### 4.

### 5.

Deployment Diagram

Dealing with multiple instances poses some challenge in the intercommunication between the generic modules and the User Management.

Analyzing these 2 basic cases, we can find some limitations to be addressed for the more general case, where we could have multiple module instances
and multiple UM instances.

Case 1:

```
Modular MOM App Multiple Instances
UM Standalone
```
Publish API behavior:

```
On UM: Cleanup + Register new model from Modular MOM App
Deployment of a new model: Permission already associated (logical) with the previous model will be preserved but potentially unused
AppSecurable table is cleaned up and refilled
Permission Cache is derived by the actual Permissions and will not change.
At least one transaction will succeed.
```
Update Message behavior:

On Modular MOM App:

```
The message is handled by only one instance (for example Pod1)
The Permission Map is updated only on that instance.
In some of the instances the Permission Map for the incoming user have some outdated permissions values, but no gRPC call is performed to the
UM (the user is already in the local map)
The RestAPI may fail for Unauthorized code randomly (N-1/N)
The RestAPI may succeed (N-1/N) even if the permissions were removed from a user
```
Workarounds and Possible solutions:

```
Modular MOM APP should restart (with all the Pods) in order to start clean. The gRPC sync request will be sent to UM to retrieve the current
permission map (one user at the time).
A distributed cache would be needed to overcome the round-robin route to a single instance
```

### 1.

### 2.

### 3.

### 4.

### 1.

### 2.

### 3.

```
The Sync gRPC service used to Query Permission (by key:user,url) could be extended with an additional method QueryAll (key:appname) to
retrieve the entire permission map at startup. This channel will be used as a stream of responses with the subsequent messages carrying just the
update to the local permission map. The multicast to all the App pod instances is performed by the UM logic sending the payload to the opened
channels for each app destination.
```
Note:

```
The Kubernetes DNS will resolve the Modular MOM App name with a single IP (in Openshift is the external one)
```
Case 2:

```
Modular MOM App Standalone
UM Multiple Instances
```
Publish API behavior:

```
On UM: Cleanup + Register new model from Modular MOM App (on the DB just one is not a problem, at least once pattern)
Deployment of a new model: Permission already associated (logical) with the previous model will be preserved but potentially unused
AppSecurable table is cleaned up and refilled
Permission Cache is derived by the actual Permissions and will not change.
At least one transaction will succeed.
```
Query Sync API behavior after a change in the UM model by means of a rest API:

```
On UM:
The handling instance of the RestAPI (for example Pod1) will update the DB and the local Permission Cache
The other instances will not update their own Permission Cache
On Modular MOM Apps:
On each RestAPI called, the Query sync API will reach just one instance of UM with an asynchronized permission cache (N-1/N)
The RestAPI may fail for Unauthorized code randomly (N-1/N)
The RestAPI may succeed (N-1/N) even if the permissions were removed from a user
```
Workarounds and Possible solutions:

```
User Management should restart (with all the Pods) in order to start clean, reading the cache at startup from the DB.
Permission Cache is removed and handled completely in the DB
Permission Cache is handled in memory and updated from the DB with a time to leave expiration reading from the DB (this mitigates the 3 point
without solving it).
Intercommunication across all the UM Instances to notify the update of the in-memory only Permission Cache.
```

### 1.

### 2.

```
gRPC cannot be used to perform the intercommunication across pods of the same module.
```
Update Message behavior:

On Modular MOM App:

```
The message is handled by only one instance
The Permission Map is updated only on that instance.
```
Authorization Flow

An incoming authenticated Request will be verified for Authorization before being allowed to access the requested resource


Update Permission

Updating Models (Roles and Groups) in the User Management App would result in a flow, shown below.

Publish API

All the Apps that need to secure their resource should publish and register them in the User Management App


Permission Interfaces

The Metadata Model defines the contracts for the Models used for authorization as interfaces.


Permission Model

The User Management App implements the interfaces defined in the Metadata Model


Permission Messages

The messages related to Authorization are defined by platform


Publish API interfaces


Publish API retrieves AppName from 'ModularMOM:ApplicationName' key in appsettings.json configuration file, since AppName represents the deployed
application name.
By default this configuration is provided by 'MODMOM_MODULE' environment variable. If no configuration is provided, Publish API message will not be
delivered.
'MODMOM_MODULE' environment variable has to be properly set up during modules' deployment stage.


# Dex Evaluation

```
Description
Architecture
OpenID Connect flows
Scopes
response_type=code
response_type=id_token token
response_type= token
Sample integration scenario
Example app integrated with GitHub Identity Provider
Limitations
```
Things to clarify:

```
Why it is beneficial (abstraction, plugin supports for different authentication types, not only oidc based?)
How it will integrate from ingress to our application in the authentication flow (does it provide the oidc token with a standard content or it depends
on the auth plugin?)
License, maturity and clearing consideration for the end-user
Does it act as an OAuth 2.0 Authorization Service or it is just a "proxy"?
```
## Description

Dex allows for pluggable authentication against many different identity providers, while presenting a unified OpenId Connect (OIDC) interface.

There are some basic principle to carry in mind when dealing with OpenID Connect flows, the provider has to implement these main endpoints:

```
The OAuth2 Authorization endpoint (ex: example.com/v1/auth), used to start the flow with a GET request specifying the proper parameters
(response_type, scope, etc... see below), used also to refresh the access token with a POST
The OAuth2 Token endpoint (ex:example.com/v1/Token) used to get an ID Token , Access Token or both, depending on the request on the
Authorization endpoint
The OpenID UserInfo endpoint (ex: example.com/v1/UserInfo), used to request additional claims information in oidc if the connector is configured
to use it, the additional claims are returned in the ID Token.
```
In Addition there is the OpenID Discovery endpoint where an application (Relying Party) can query all those endpoints with a single one:

```
Discovery endpoint (ex: example.com/.well-known/open-id-configuration) this is a fixed path under any authentication OIDC provider domain.
```
Additional there are two kind of tokens with OpenID:

```
ID Token
Access Token
```
The ID Token is a JWT token with standardized format (see below), instead the Access Token may be anything, in fact it may be completely opaque to the
client and unique to the provider: the token you receive from Facebook will be completely different from the one you’d get from Twitter or GitHub. In other
hand, the access token must be used to talk to some specific endpoint on the provider (like the UserInfo) using it as an Athorization Bearer header in the
http request.

The ID Token is a JSON Web Token signed by the OpenID Connect server, with well known fields for user ID, name, email, etc. A typical token body
(which contains the claims) from an OpenID Connect looks like :

### {

```
"iss": "https://dex.example.com:32000",
"sub": "Cgg4MzYwMTE0NxIGZ2l0aHVi",
"aud": "example-app",
"exp": 1627478479,
"iat": 1627392079,
"at_hash": "4XjnZthGxZDghVazyDEfWg",
"email": "piergiorgio.trubini@siemens.com",
"email_verified": true,
"name": "ptrubini",
"preferred_username": "ptrubini"
}
```
Another important endpoint to keep in consideration is the callback endpoint:

```
Callback Endpoint (ex: example.com/callback)
```

### 1.

### 2.

### 3.

### 4.

### 5.

### 6.

### 7.

### 8.

Where the authentication flow completes, giving back the ID Token or the Access Token (or both, depending on the initial request to the Authorization
Service) to the calling application.

Dex provides the authentication by means of connectors which can be attached to remote Identity Providers. There providers may be not OIDC complaints
(such as simple oauth2 type like github or completely different such as Saml 2, LDAP) but also OpenID Connect providers, in the latter case, the OIDC
flows are chained together in a way in which Dex is providing the OIDC frontend to the client relying party and the secondary flow is handled by Dex itself
with its own endpoints and callbacks, translating the requests and responses with its own codes, in order to decouple the authentication flows with the
remote.

For this reason Dex is used, under the hood, in many commercial products that need to provide a plug-able authentication scheme configurable by the end-
user.

Architecture

OpenID Connect flows

RFC 6749 includes the definition of a Web API called “authorization endpoint”. The API requires response_type as a mandatory request parameter.
OpenID Connect has defined flows to issue ID tokens by extending the specification of the response_type request parameter.

In RFC 6749, the value of response_type is either code or token. OpenID Connect has added a new value, id_token, and allowed any combination
of code, token and id_token. A special value, none, has been added, too. As a result, now response_type can take any one of the following values.

```
code
token
id_token
id_token token
code id_token
code token
code id_token token
none
```
Example of the first request to the Authorization endpoint:

```
Example Authorization Request
```
```
GET https://dex.example.com:32000/auth?client_id=example-app&redirect_uri=http%3A%2F%2F127.0.0.1%3A5555%
2Fcallback&response_type=code&scope=openid+profile+email+offline_access&state=I+wish+to+wash+my+irish+wristwatch
```
in this case the response_type flow asked is "code": meaning that a code is returned to the client application (on the callback endpoint) and then this code
can be exchanged for an ID Token and an Access Token to the Token endpoint.

When the value of response_type is code, but if openid is not included in the scope request parameter, the request is just an authorization code flow
which is defined in RFC 6749. On the other hand, if openid is included in the scope request parameter, an ID token is issued from the token endpoint in
addition to an access token.

Scopes

List of additional scopes to request in token response
Default is profile and email Full list at https://dexidp.io/docs/custom-scopes-claims-clients/

scopes:

```
profile
email
groups
```
Dex also support the getUserInfo option on the OIDC connector. When this is enabled, the OpenID Connector will query the UserInfo endpoint for
additional claims.

UserInfo claims take priority over claims returned by the IDToken. This option should be used when the IDToken doesn't contain all the claims requested.

The diagram for the example authorization request above is the following:

response_type=code


The bounce back to the Token Endpoint is optional, in fact the Authorization Endpoint is capable to return the ID Token and the Access Token directly (via
the usual 3xx status redirection) to the Callback endpoint, if the response_type is specified as: response_type=id_token token

response_type=id_token token

When the value of response_type is id_token token, an ID token and an access token are issued from the authorization endpoint. This flow does
not use the token endpoint.


When an access token is issued together with an ID token from the authorization endpoint, the hash value of the access token calculated in a certain way
has to be embedded in the ID token. So, be careful when you implement this flow. “3.2.2.10 ID Token” in OpenID Connect Core 1.0 says as follows:

```
at_hash
```
```
Access Token hash value. Its value is the base64url encoding of the left-most half of the hash of the octets of the ASCII
representation of the access_token value, where the hash algorithm used is the hash algorithm used in the alg Header
Parameter of the ID Token's JOSE Header. For instance, if the alg is RS256, hash the access_token value with SHA-256, then
take the left-most 128 bits and base64url encode them. The at_hash value is a case sensitive string.
```
```
If the ID Token is issued from the Authorization Endpoint with an access_token value, which is the case for the response
_type value id_token token, this is REQUIRED; it MAY NOT be used when no Access Token is issued, which is the case for
the response_type value id_token.
```
response_type= token

When the value of response_type is token, the request is an implicit flow defined in RFC 6749. Even if openid is included in the scope request
parameter, an ID token is not issued. This flow uses the authorization endpoint but does not use the token endpoint.


Other flows are omitted for brevity but basically you can have any permutation of these basic flows, given the proper response_type.

Sample integration scenario

Example app integrated with GitHub Identity Provider

Cloning the Dex repository and following the guide here https://dexidp.io/docs/kubernetes/ I managed to get Dex working on a separate Kubernetes
namespace=dex by changing some parameters to run correctly on Microk8s cluster.

The updated dex.yaml file is this:

```
dex.yaml
```
### ---

```
apiVersion: v1
kind: Namespace
metadata:
name: dex
```
```
While OAuth 2.0 also defines the token Response Type value for the Implicit Flow (response_type=token), OpenID Connect does not use
this Response Type, since no ID Token would be returned.
```

### ---

apiVersion: apps/v1
kind: Deployment
metadata:
labels:
app: dex
name: dex
namespace: dex
spec:
replicas: 1
selector:
matchLabels:
app: dex
template:
metadata:
labels:
app: dex
spec:
serviceAccountName: dex # This is created below
containers:

- image: dexidp/dex:v2.27.0 #or quay.io/dexidp/dex:v2.26.0
name: dex
command: ["/usr/local/bin/dex", "serve", "/etc/dex/cfg/config.yaml"]

ports:

- name: https
containerPort: 5556

volumeMounts:

- name: config
mountPath: /etc/dex/cfg
- name: tls
mountPath: /etc/dex/tls

env:

- name: GITHUB_CLIENT_ID
valueFrom:
secretKeyRef:
name: github-client
key: client-id
- name: GITHUB_CLIENT_SECRET
valueFrom:
secretKeyRef:
name: github-client
key: client-secret
- name: KUBERNETES_POD_NAMESPACE
valueFrom:
fieldRef:
fieldPath: metadata.namespace
readinessProbe:
httpGet:
path: /healthz
port: 5556
scheme: HTTPS
volumes:
- name: config
configMap:
name: dex
items:
- key: config.yaml
path: config.yaml
- name: tls
secret:
secretName: dex.example.com.tls
---
kind: ConfigMap
apiVersion: v1
metadata:
name: dex
namespace: dex
data:


config.yaml: |
issuer: https://dex.example.com:32000
storage:
type: kubernetes
config:
inCluster: true
web:
https: 0.0.0.0:5556
tlsCert: /etc/dex/tls/tls.crt
tlsKey: /etc/dex/tls/tls.key
connectors:

- type: github
id: github
name: GitHub
config:
clientID: $GITHUB_CLIENT_ID
clientSecret: $GITHUB_CLIENT_SECRET
redirectURI: https://dex.example.com:32000/callback
org: kubernetes
oauth2:
skipApprovalScreen: true

staticClients:

- id: example-app
redirectURIs:
- 'http://127.0.0.1:5555/callback'
name: 'Example App'
secret: ZXhhbXBsZS1hcHAtc2VjcmV0

enablePasswordDB: true
staticPasswords:

- email: "admin@example.com"
# bcrypt hash of the string "password"
hash: "$2a$10$2b2cU8CPhOTaGrs1HRQuAueS7JTT5ZHsHSzYiFPm1leZck7Mc8T4W"
username: "admin"
userID: "08a8684b-db88-4b73-90a9-3cd1661f5466"
---
apiVersion: v1
kind: Service
metadata:
name: dex
namespace: dex
spec:
type: NodePort
ports:
- name: dex
port: 5556
protocol: TCP
targetPort: 5556
nodePort: 32000
selector:
app: dex
---
apiVersion: v1
kind: ServiceAccount
metadata:
labels:
app: dex
name: dex
namespace: dex
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
name: dex
rules:
- apiGroups: ["dex.coreos.com"] # API group created by dex
resources: ["*"]
verbs: ["*"]
- apiGroups: ["apiextensions.k8s.io"]
resources: ["customresourcedefinitions"]


```
verbs: ["create"] # To manage its own resources, dex must be able to create customresourcedefinitions
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
name: dex
roleRef:
apiGroup: rbac.authorization.k8s.io
kind: ClusterRole
name: dex
subjects:
```
- kind: ServiceAccount
name: dex # Service account assigned to the dex pod, created above
namespace: dex # The namespace dex is running in

In particular the KUBERNETES_POD_NAMESPACE environment variable was added because the latest versions of Microk8s are not compatible with
Dex without this configuration.

Also, the readinessProbe is changed to work with the scheme: HTTPS otherwise the readyness will return 400 and the 32000 nodePort is not exposed on
the host.

The secrets are created separately into the dex namespace (otherwise it will not start) with the following commands:

```
kubectl create secret tls dex.example.com.tls --cert=ssl/cert.pem --key=ssl/key.pem -n dex
kubectl -n dex create secret generic github-client --from-literal=client-id=$GITHUB_CLIENT_ID --from-
literal=client-secret=$GITHUB_CLIENT_SECRET
```
Where GITHUB_CLIENT_ID and GITHUB_CLIENT_SECRET are two environment variables containing the client_id and client_secret, generated from the
GitHub OAUTH2 application settings available on this page: https://github.com/settings/developers

Furthermore, the Microk8s API Server has been changed as suggested by the guide modifying the argument file here:


```
/var/snap/microk8s/current/args/kube-apiserver
```
```
Adding the following parameters:
```
```
# Dex integration
--oidc-issuer-url=https://dex.example.com:32000
--oidc-client-id=example-app
--oidc-ca-file=/etc/ssl/certs/openid-ca.pem
--oidc-username-claim=email
--oidc-groups-claim=groups
```
```
before restarting microk8s
```
and to get all working with the dex.example.com authority, I also changed the local name resolution by adding the "127.0.0.1 dex.example.com" entry in
the system /etc/hosts file.

Running the example-app delivered by the Dex project with this command line:

```
bin/example-app --issuer https://dex.example.com:32000 --issuer-root-ca examples/k8s/ssl/ca.pem --debug
```
Connecting the the [http://localhost:5555](http://localhost:5555) endpoint of the example-app and selecting the Github provider you will get to the initial authorization to allow the
dex app to get the specified claims in the ID token:

And, finally, after the authorization GitHub will redirect back to Dex and Dex to the application page getting the tokens from the flow:

```
ID Token:
```
```
eyJhbGciOiJSUzI1NiIsImtpZCI6ImZhMWNiNjRjNWFhZWYwOWZiOTFkMWI5NjY1MWEzZTMxMjg0NjJiZTUifQ.
eyJpc3MiOiJodHRwczovL2RleC5leGFtcGxlLmNvbTozMjAwMCIsInN1YiI6IkNnZzRNell3TVRFME54SUdaMmwwYUhWaSIsImF1ZCI6ImV4YW1w
bGUtYXBwIiwiZXhwIjoxNjI3NDg5MDQ1LCJpYXQiOjE2Mjc0MDI2NDUsImF0X2hhc2giOiJNZVlNUzZfZzdvSW1pbzBxUnBhRkZnIiwiZW1haWwi
OiJwaWVyZ2lvcmdpby50cnViaW5pQHNpZW1lbnMuY29tIiwiZW1haWxfdmVyaWZpZWQiOnRydWUsIm5hbWUiOiJwdHJ1YmluaSIsInByZWZlcnJl
ZF91c2VybmFtZSI6InB0cnViaW5pIn0.iySB5ZSDkPiYWEkXepV_iyymXqP34hr-LI_4eWXt-
```

```
lEiOdopeCvPJ9ssbPlGh4j1a6MMGcw8sdeXLQ3r_Id2rMpHIzJwg_lxmaWB5hCxdsCE9pbrvSNM1A_BOOOxljJQt1jVvFiuW-NbBzch7-
E0RBPck-xSQjTOc_TdJkYFQ5EBhgys5OMAG3K6H5b5r-H1-dFm_MB4AHviUVJ6QyR_Gi6ND0rJWcsTQfVXiIx3nXLblw66LeJ3R0gZnjtKle2d-
gYoBtAnMHcVXyTHI2hzxC-tgR5umWbw4V4YXS9tBIwXo9e8W5qh_M_OMIdlSZnnLbT2hCzteWtOlNwolWRLeA
```
```
Access Token:
```
```
eyJhbGciOiJSUzI1NiIsImtpZCI6ImZhMWNiNjRjNWFhZWYwOWZiOTFkMWI5NjY1MWEzZTMxMjg0NjJiZTUifQ.
eyJpc3MiOiJodHRwczovL2RleC5leGFtcGxlLmNvbTozMjAwMCIsInN1YiI6IkNnZzRNell3TVRFME54SUdaMmwwYUhWaSIsImF1ZCI6ImV4YW1w
bGUtYXBwIiwiZXhwIjoxNjI3NDg5MDQ1LCJpYXQiOjE2Mjc0MDI2NDUsImF0X2hhc2giOiJNdy14NE9iUkVwU2NEMG02Zk1SQ1h3IiwiZW1haWwi
OiJwaWVyZ2lvcmdpby50cnViaW5pQHNpZW1lbnMuY29tIiwiZW1haWxfdmVyaWZpZWQiOnRydWUsIm5hbWUiOiJwdHJ1YmluaSIsInByZWZlcnJl
ZF91c2VybmFtZSI6InB0cnViaW5pIn0.aOtkIcdJvp_3NYxmK9ooeD5Q6d4VYem8UzzTJkFsOrcBb3IlCmdGILGipwPiAt-
wjb5PFQAaNEmcV3HhmRvgvsqr8mcvJfLK8gOTyIZejqctf-nccyZOrGs-0fKxE0T0vKGCKEh4q0hIm6d5iUYJabogSG5VTVbXGlZInb0rwX-
R3FY8sNrLV8AKb95ZY_RvriqoKAXOH3JHE8a7eS3icdOUYofmLfkPRvTHN0F8YNyjrj3RPKGSdItvhY_k7XT5T9Fv0PRovUG8OiB_I-
J1xbynfuh-nYEFRHP1f4if9n4_kFX1boW78Ft0CmJFKOlNISNiDEimscIwqGu72lDCkA
```
```
Claims:
```
### {

```
"iss": "https://dex.example.com:32000",
"sub": "Cgg4MzYwMTE0NxIGZ2l0aHVi",
"aud": "example-app",
"exp": 1627489045,
"iat": 1627402645,
"at_hash": "MeYMS6_g7oImio0qRpaFFg",
"email": "piergiorgio.trubini@siemens.com",
"email_verified": true,
"name": "ptrubini",
"preferred_username": "ptrubini"
}
```
```
Refresh Token:
```
```
ChlqdzVzdDNrZGZjamZkeW8zemw3bWJybW9lEhluaGdvZWt2bmRraGZyY3Jwamd1cGt3dGhn
```
This basic example just enables you to get started, experimenting with the flow and deployment established.

Note that Github does not provide a way to federate its identity provider (it is just a OAuth2 authorization server): you integrate Dex as an application
registered on your account, that in turn, will authenticate itself on behalf of you.

Next topic is to use Dex to have land on a login page in order to authenticate a user base with different accounts.

Limitations

Dex implement a subset of the full OIDC specs, namely OIDC Core + Discovery.

OIDC Session Management is not implemented so it is difficult or impractical to satisfy these use cases:

```
Use it against different services without logging in each time (no session cookie)
No support on the logout flow.
```
Even if something may be done on the SPA side or on the API Gateway to avoid some drawbacks, the logout missing support is particularly difficult to
overcome: given that Dex should provide the abstraction to hide the remote IdP, the only way to do it is to bypass it for the logout, knowing which IdP
(Connector) is configured inside Dex and all the relevant parameters (provided or acquired during the login) needed to perform the logout against the IdP
on "behalf" of Dex. This is not a simple task to accomplished when this is done into the SPA logic on the browser side.

These limitations seems to be more like a design decision right now and not simple issues that need a fix, it is not clear how the software will evolve in the
near future and if and when these limitations are solved.

The implementation of the full OIDC specs on Session Management to extend it (by us) is not a practical task that can be achieved easily too, because it
will raise problems on maintainability of an Open Source project, Clearing issues and in general, support and effort to have it.

References:

https://github.com/dexidp/dex/issues/963

https://github.com/dexidp/dex/issues/32

https://github.com/dexidp/dex/issues/1697



# Emissary Ingress (Ambassador API Gateway)

```
Description
Configuration
Deployment
Authentication and IdP integration
Conclusion
```
## Description

## Configuration

## Deployment

## Authentication and IdP integration

## Conclusion


# Enable Authentication in Business Module

UI Module changes

Please refer OrderManagement UI repo for reference.

```
In package.json of module add modmom-authenticator package under dependencies:
```
```
Example of Module package.json
```
```
"dependencies": {
"@apollo/gateway": "0.6.10",
"afx-next-react": "5.0.0",
"afx-graph": "next",
"apollo-server-express": "2.6.7",
"http-proxy": "1.17.0",
"jsonwebtoken": "8.4.0",
"superagent": "4.1.0",
"mom-images": "0.69.0",
"mom-ui": "2.0.1-pre.2",
"modmom-authenticator": "0.1.3"
}
```
```
In build.json add reference to modmom-authenticator package:
```
```
Example of Module build.json
```
### {

```
"srcPaths": [
"node_modules/afx-next-react/@(src|test)",
"node_modules/modmom-authenticator/src",
"node_modules/mom-ui/src",
"src"
]
}
```
```
In Kit.json of solution
```
```
Example of Module kit.json
```
```
"kitDeps": [
"afx",
"mom-ui",
"modmom-authenticator"
],
```
```
Create a file named "actionTemplateDefs.json" inside solution
```

```
Example of actiontemplatedef.json
```
### {

```
"ModularMOMHttpRequest": {
"actionType": "JSFunctionAsync",
"method": "ModularMOMHttpCall",
"inputData": {
"url": "{{inputData.url}}",
"data": "{{inputData.data}}",
"config": "{{inputData.config}}",
"options": "{{inputData.options}}"
},
"deps": "js/modularMOMHttpInterceptor"
}
}
```
Modify the viewmodels where we are making api call as below:

Update the authenticator.json to add dependency of modmomauthenticatorservice and update commandviewmodel.json to add code for signOut
functionality.

Update the webappconfig.json with auth configuration:

```
Example of webapp-config.json
```
```
"auth": {
"enabled": true,
"oidcClient": {
"auth": {
"client_id": "$CLIENT_ID",
"authority": "$AUTHORITY_URL",
"redirect_uri": "",
"post_logout_redirect_uri": "",
"response_type": "code",
"automaticSilentRenew":false,
```

```
"loadUserInfo": true,
"monitorSession": true,
"response_mode":"query",
"scope":"openid profile email",
"revokeAccessTokenOnSignout": true
},
"cache": {
"cacheLocation": "sessionStorage",
"storeAuthStateInCookie": false
}
}
```
```
Note: Please refer OrderManagement UI repo for reference.
```
https://tfs05mom.industrysoftware.automation.siemens.com/MOM/ModularMOM/_git/UI_BusinessModules?
version=GBmain&_a=contents&path=%2FOrderManagement


# Identity Brokering & SamAuth Integration


# Keycloak Deployment


# SamAuth as IdentityProvider Configurations

## Keycloak Identity broker Configuration

An Identity Broker is an intermediary service that connects multiple service providers with different identity providers. As an intermediary service, the
identity broker is responsible for creating a trust relationship with an external identity provider in order to use its identities to access internal services
exposed by service providers.

General Configurations :

Identity providers are created for each realm, which means that users from a realm can use any of the registered identity providers when signing in to an
application.

In order to add samAuth as identity provider click the Identity Providers left menu item in Keycloak and select OpenId Connect v1.0 under a
specific realm.

Configuration parameters:

```
Parameter
Name
```
```
Description Example
```
```
Redirect URI Redirect URI to use when configuring identity provider. Url pattern is like this: https://<Keycloak
server>/auth/realms/<realm>/broker/<alias>/endpoint
```
```
http://keycloak-modmom-14.
apps.openshift03.swqa.tst/au
th/realms/modmom/broker/sa
mAuth/endpoint
```
```
Alias The alias is an unique identifier for an identity provider. It is used to reference an identity provider
internally. Some protocols such as OpenID Connect require a redirect URI or callback url in order to
communicate with an identity provider. In this case, the alias is used to build the redirect URI. Every
single identity provider must have an alias.
```
```
samAuth
```
```
Enabled Turn the provider on/off ON
```
```
Store Tokens Whether or not to store the token received from the identity provider. ON
```
```
Stored
Tokens
Readable
```
```
Whether or not users are allowed to retrieve the stored identity provider token. This also applies to the
broker client-level role read token
```
### ON

```
First Login
Flow
```
```
This is the authentication flow that will be triggered for users that log into Keycloak through this IDP
for the first time ever.
```
```
First Broker Login
```
```
Sync Mode Sync mode determines when user data will be imported import
```
```
OpenId Connect Config
```
```
Authorization
URl
```
```
Authorization Uri of SamAuth. Url differs based on environment like prod or pre-prod
```
```
pre-prod: server https://samauth.us-east-1.preprod.teamcenterwebservices.com
```
```
prod: https://samauth.us-east-1.sws.siemens.com
```
```
pre-prod: https://samauth.us-
east-1.preprod.
teamcenterwebservices.com
/auth
```
```
prod: https://samauth.us-east-1
.sws.siemens.com/auth
```
```
Token URL https://<env server>/token https://samauth.us-east-1.
preprod.
```

```
teamcenterwebservices.com
/token
```
```
Logout URL https://<env server>/session/end https://samauth.us-east-1.
preprod.
teamcenterwebservices.com
/session/end
```
```
Client
Authentication
```
```
Client authentication method "Client Secret Sent as basic
auth"
```
```
ClientId Client identifier registered in identity provider (samAuth). This would be generated per tenant by
Onboard manager in samAuth App registration process and will have to be passed to TenantOperator
```
```
Client Secret This would be generated per tenant by Onboard manager in samAuth App registration process and
will have to be passed on to TenantOperator
```
```
Client Assert
Signature
Algorithm
```
```
Signature algorithm to do JWT assertion as client authentication HS256
```
```
Default
Scopes
```
```
Scopes to be sent when asking for authorization openid profile email samauth.
ten samauth.skey idp_account
```
```
Use PKCE ON
```
Default Identity provider Configuration

It’s possible to automatically redirect to a identity provider instead of displaying the login form. To enable this go to Authentication select the Browser flow.
Then click on config for the Identity Provider Redirector authenticator. Set Default Identity Provider to the alias of the identity provider you want to
automatically redirect users to.

If the configured default identity provider is not found the login form will be displayed instead.

This authenticator is also responsible for dealing with the kc_idp_hint query parameter.

We have to create config under Browser section like below and then we have to enable that config as "Required"


Dedicated client

A dedicated client "onboard_manager", with client credential enabled and permissions to create users inside the realm, is needed:

Please, also add any Token mapper related to the scope "client_cred" to this one.

```
onboard_manager client definition
```
### {

```
"id": "modmom",
"realm": "modmom",
```
```
"users": [
{
"id": "1c33ce5e-d292-4078-a4b0-1f9d888bc66e",
"createdTimestamp": 1645632189281,
"username": "service-account-onboard_manager",
"enabled": true,
"totp": false,
"emailVerified": false,
"serviceAccountClientId": "onboard_manager",
"disableableCredentialTypes": [],
"requiredActions": [],
"clientRoles": {
"realm-management": [
"manage-identity-providers",
"manage-users"
]
},
"notBefore": 0
```

### }

### ],

"clients": [
{
"id": "2a3083de-74fa-4f74-a0e3-dab7503af3cd",
"clientId": "onboard_manager",
"surrogateAuthRequired": false,
"enabled": true,
"alwaysDisplayInConsole": false,
"clientAuthenticatorType": "client-secret",
"secret": "fc3d93a6-0725-4f95-ade6-d6c3f1298b1c",
"redirectUris": [],
"webOrigins": [],
"notBefore": 0,
"bearerOnly": false,
"consentRequired": false,
"standardFlowEnabled": false,
"implicitFlowEnabled": false,
"directAccessGrantsEnabled": false,
"serviceAccountsEnabled": true,
"publicClient": false,
"frontchannelLogout": false,
"protocol": "openid-connect",
"attributes": {
"id.token.as.detached.signature": "false",
"saml.assertion.signature": "false",
"saml.force.post.binding": "false",
"saml.multivalued.roles": "false",
"saml.encrypt": "false",
"oauth2.device.authorization.grant.enabled": "false",
"backchannel.logout.revoke.offline.tokens": "false",
"saml.server.signature": "false",
"saml.server.signature.keyinfo.ext": "false",
"use.refresh.tokens": "true",
"exclude.session.state.from.auth.response": "false",
"oidc.ciba.grant.enabled": "false",
"saml.artifact.binding": "false",
"backchannel.logout.session.required": "false",
"client_credentials.use_refresh_token": "false",
"saml_force_name_id_format": "false",
"require.pushed.authorization.requests": "false",
"saml.client.signature": "false",
"tls.client.certificate.bound.access.tokens": "false",
"saml.authnstatement": "false",
"display.on.consent.screen": "false",
"saml.onetimeuse.condition": "false"
},
"authenticationFlowBindingOverrides": {},
"fullScopeAllowed": true,
"nodeReRegistrationTimeout": -1,
"protocolMappers": [
{
"id": "e73bbe8e-a81a-4863-b355-959a8e04d58d",
"name": "Client IP Address",
"protocol": "openid-connect",
"protocolMapper": "oidc-usersessionmodel-note-mapper",
"consentRequired": false,
"config": {
"user.session.note": "clientAddress",
"id.token.claim": "true",
"access.token.claim": "true",
"claim.name": "clientAddress",
"jsonType.label": "String"
}
},
{
"id": "28af44f4-eab5-41ad-ae37-3df8546d4bdd",
"name": "Audience Mapper",
"protocol": "openid-connect",
"protocolMapper": "oidc-audience-mapper",
"consentRequired": false,


"config": {
"included.client.audience": "modmom",
"id.token.claim": "true",
"access.token.claim": "true"
}
},
{
"id": "30f7d384-d4cb-47b1-b8cd-75f2940777ba",
"name": "Client ID",
"protocol": "openid-connect",
"protocolMapper": "oidc-usersessionmodel-note-mapper",
"consentRequired": false,
"config": {
"user.session.note": "clientId",
"id.token.claim": "true",
"access.token.claim": "true",
"claim.name": "clientId",
"jsonType.label": "String"
}
},
{
"id": "e49bacf6-3ef5-4f48-a1bd-c3e6fca2ba7a",
"name": "Client Host",
"protocol": "openid-connect",
"protocolMapper": "oidc-usersessionmodel-note-mapper",
"consentRequired": false,
"config": {
"user.session.note": "clientHost",
"id.token.claim": "true",
"access.token.claim": "true",
"claim.name": "clientHost",
"jsonType.label": "String"
}
}
],
"defaultClientScopes": [
"web-origins",
"roles",
"profile",
"email"
],
"optionalClientScopes": [
"address",
"phone",
"offline_access",
"microprofile-jwt"
]
}
]
}


# Oidc-Client library evaluation

Oidc-Client is a JavaScript library intended to run in browsers. It provides protocol support for OIDC and OAuth2, as well as management functions for
user sessions and access tokens management.

Download link for library: https://www.npmjs.com/package/oidc-client

There are two main classes that is useful:

```
The UserManager class provides a higher level API for signing a user in, signing out, managing the user's claims returned from the OIDC
provider, and managing an access token returned from the OIDC/OAuth2 provider. The UserManager is the primary feature of the library.
The OidcClient class provides the raw OIDC/OAuth2 protocol support for the authorization endpoint and the end session endpoint in the
authorization server. It provides a bare-bones protocol implementation and is used by the UserManager class. Only use this class if we simply
want protocol support without the additional management features of the UserManager class.
```
## UserManager

The UserManager constructor requires a settings object as a parameter. The settings has these properties.

Required Settings

```
authority (string): The URL of the OIDC/OAuth2 provider.
client_id (string): client application's identifier as registered with the OIDC/OAuth2 provider.
redirect_uri (string): The redirect URI of your client application to receive a response from the OIDC/OAuth2 provider.
response_type (string, default: 'id_token'): The type of response desired from the OIDC/OAuth2 provider.
scope (string, default: 'openid'): The scope being requested from the OIDC/OAuth2 provider.
```
There are other optional settings are there which can be explored Oidc-Client-Js


# Modules

This section describes the structure, usage and processing of modules


# Third-Party Software Used in Platform

## Introduction

This page is meant to keep track of all our third party software already used in platform.

## List of Libraries

```
Group Name Version source code link Licence type Notes
```
```
.NET Core .NET Core 5.x https://github.com/dotnet
/core
```
```
MIT License
```
```
Aspnet
Core
```
```
Aspnet Core 5.x
(depending on
the installed
SDK)
```
```
https://github.com/dotnet
/aspnetcore
```
```
MIT License
```
```
Grpc Grpc 2.35.0 https://github.com/grpc/grpc Apache-2.0 License may be substituted by dapr sdk
```
```
Grpc Grpc.Core 2.35.0 https://github.com/grpc/grpc Apache-2.0 License may be substituted by dapr sdk
```
```
Grpc Grpc.Tools 2.35.0 https://github.com/grpc/grpc Apache-2.0 License may be substituted by dapr sdk
```
```
Grpc Grpc.AspNetCore 2.35.0 https://github.com/grpc
/grpc-dotnet
```
```
Apache-2.0 License may be substituted by dapr sdk
```
```
Grpc Grpc.Net.ClientFactory 2.32.0 https://github.com/grpc
/grpc-dotnet
```
```
Apache-2.0 License may be substituted by dapr sdk
```
```
Google.
Protobuf
```
```
Google.Protobuf 3.14.0 https://github.com
/protocolbuffers/protobuf
```
```
Google License (it
should be BSD 3)
```
```
may not be latest, need to finalize
```
```
Fody Fody (FodyHelpers &
FodyPackaging as well)
```
```
6.3.0 https://github.com/Fody
/Fody
```
```
MIT License
```
```
Microsoft.
Extensions
```
```
Microsoft.Extensions.
Configuration (& Abstraction,
FileExtensions, Binder, Json)
```
```
5.0.0 https://github.com/dotnet
/runtime
```
```
MIT License
```
```
Microsoft.
Extensions
```
```
Microsoft.Extensions.
DependencyInjection (&
Abstraction)
```
```
5.0.1 (5.0.0) https://github.com/dotnet
/runtime
```
```
MIT License
```
```
Microsoft.
Extensions
```
```
Microsoft.Extensions.Hosting.
Abstractions
```
```
5.0.0 https://github.com/dotnet
/runtime
```
```
MIT License
```
```
Microsoft.
Extensions
```
```
Microsoft.Extensions.Logging.
Abstractions
```
```
5.0.0 https://github.com/dotnet
/runtime
```
```
MIT License
```
```
NHibernate NHibernate 5.3.10 https://github.com
/nhibernate/nhibernate-core
```
```
LGPL-2.1 License may not be latest, need to finalize
```
```
NHibernate FluentNHibernate 3.1.0 https://github.com
/nhibernate/fluent-
nhibernate
```
```
James Gregory (BSD) may not be latest, need to finalize
```
```
NHibernate NHibernate.Caches.Caching.
Memory
```
```
5.7.0 https://github.com
/nhibernate/NHibernate-
Caches
```
```
LGPL-2.1 License
```
```
NHibernate NHibernate.Caches.
CoreDistributedCache
```
```
5.7.0 https://github.com
/nhibernate/NHibernate-
Caches
```
```
LGPL-2.1 License
```
```
NHibernate NHibernate.Caches.
CoreDistributedCache.Redis
```
```
5.7.0 https://github.com
/nhibernate/NHibernate-
Caches
```
```
LGPL-2.1 License
```
```
Newtonsof
t.Json
```
```
Newtonsoft.Json 12.0.3 https://github.com
/JamesNK/Newtonsoft.Json
```
```
MIT License
```
```
OpenTele
metry
```
```
OpenTelemetry (sdk) 1.0.1 https://github.com/open-
telemetry/opentelemetry-
dotnet
```
```
Apache-2.0 License
```
```
OpenTele
metry
```
```
OpenTelemetry Exporter.Console 1.0.1 https://github.com/open-
telemetry/opentelemetry-
dotnet
```
```
Apache-2.0 License
```
```
OpenTele
metry
```
```
OpenTelemetry Exporter.Jaeger 1.0.0-rc2 https://github.com/open-
telemetry/opentelemetry-
dotnet
```
```
Apache-2.0 License
```
```
OpenTele OpenTelemetry Exporter. 1.0.1 https://github.com/open- Apache-2.0 License
```

metry OpenTelemetryProtocol telemetry/opentelemetry-
dotnet

OpenTele
metry

```
OpenTelemetry Exporter.Zipkin 1.0.1 https://github.com/open-
telemetry/opentelemetry-
dotnet
```
```
Apache-2.0 License
```
OpenTele
metry

```
OpenTelemetry Extensions.
Hosting
```
```
1.0.0-rc2 https://github.com/open-
telemetry/opentelemetry-
dotnet
```
```
Apache-2.0 License
```
OpenTele
metry

```
OpenTelemetry Instrumentation.
AspNetCore
```
```
1.0.0-rc2 https://github.com/open-
telemetry/opentelemetry-
dotnet
```
```
Apache-2.0 License
```
OpenTele
metry

```
OpenTelemetry Instrumentation.
GrpcNetClient
```
```
1.0.0-rc2 https://github.com/open-
telemetry/opentelemetry-
dotnet
```
```
Apache-2.0 License
```
OpenTele
metry

```
OpenTelemetry Instrumentation.
Http
```
```
1.0.0-rc2 https://github.com/open-
telemetry/opentelemetry-
dotnet
```
```
Apache-2.0 License
```
OpenTele
metry

```
OpenTelemetry.Instrumentation.
SqlClient
```
```
1.0.0-rc2 https://github.com/open-
telemetry/opentelemetry-
dotnet
```
```
Apache-2.0 License
```
Swashbuc
kle.
AspNetCo
re

```
Swashbuckle.AspNetCore 5.5.1 https://github.com
/domaindrivendev
/Swashbuckle.AspNetCore
```
```
MIT License
```
Utf8Json Utf8Json 1.3.7 https://github.com/neuecc
/Utf8Json/

```
MIT License
```
Serilog Serilog 2.10.0 https://github.com/serilog
/serilog

```
Apache-2.0 License
```
Serilog Serilog.AspNetCore 4.1.0 https://github.com/serilog
/serilog-aspnetcore

```
Apache-2.0 License
```
Serilog Serilog.Enrichers.Thread 3.1.0 https://github.com/serilog
/serilog-enrichers-thread

```
Apache-2.0 License
```
Serilog Serilog.Formatting.Compact 1.1.0 https://github.com/serilog
/serilog-formatting-compact

```
Apache-2.0 License
```
Serilog Serilog.Formatting.Elasticsearch 8.4.1 https://github.com/serilog
/serilog-sinks-elasticsearch

```
Apache-2.0 License temporary?
```
Serilog Serilog.Sinks.Console 3.1.1 https://github.com/serilog
/serilog-sinks-console

```
Apache-2.0 License temporary?
```
Serilog Serilog.Sinks.Elasticsearch 8.4.1 https://github.com/serilog
/serilog-sinks-elasticsearch

```
Apache-2.0 License temporary?
```
Serilog Serilog.Sinks.Network 2.0.2.68 https://github.com
/pauldambra/Serilog.Sinks.
Network

```
Apache-2.0 License temporary?
```
prometheusprometheus-net.AspNetCore 4.1.1 https://github.com
/prometheus/prometheus

```
Apache-2.0 License temporary?
```
prometheusprometheus-net.SystemMetrics 1.0.1 https://github.com
/prometheus/prometheus

```
Apache-2.0 License temporary?
```
Serilog Serilog.Sinks.Seq 5.0.1 https://github.com/serilog
/serilog-sinks-seq

```
Apache-2.0 License temporary?
```
Polly Polly 7.2.2 https://github.com/App-
vNext/Polly

```
BSD
```
SQL
Server
Client

```
Microsoft.Data.SqlClient 4.1.0 https://github.com/dotnet
/SqlClient
```
```
MIT License Migrated from System.Data.SqlClient
```
PosgresS
QL
Provider

```
NpgSql 5.0.11 https://github.com/npgsql
/npgsql
```
```
PostgreSQL License
```
```
https://github.com
/npgsql/npgsql/blob
/main/LICENSE
```
```
This targets net5.0, the 6.x version of this
driver targets net6.0 which has increased
DateTimeOffset support.
```
SQL Lite System.Data.SQLite.Core 1.0.115.5 https://system.data.sqlite.
org/index.html/doc/trunk
/www/index.wiki

```
https://www.sqlite.org
/copyright.html
```
```
not needed in product
```
System.
Linq.
Dynamic.
Core

```
System.Linq.Dynamic.Core 1.2.9 https://github.com
/zzzprojects/System.Linq.
Dynamic.Core
```
```
Apache-2.0 License
```
UnitsNet UnitsNet? 4.102.0 https://github.com MIT License


```
/angularsen/UnitsNet
```
3° Party Side-car / Services (Containers)

```
Group Name Version source code link License
Type
```
```
OpenTelemetry OpenTelemetry Collector (To be Evaluated if to be shipped
with product)
```
```
beta https://github.com/open-telemetry
/opentelemetry-collector
```
```
Apache-2.0
License
```
```
DAPR (To be
Evaluated)
```
```
https://github.com/dapr/dapr MIT License
```

# Application Catalog

Each application will consist of a single model. Its version will be the same as that of its model. The configuration of the all of the applications (the Catalog)
will be stored in a central git repo.

## Configuration Definition

```
Application Configuration Format
```
### {

```
"env": {
"appName": {
"description": "A brief description of the Application",
"version": "Same as that of the model",
"model": "Application model name"
}
}
}
```
## Example configuration

```
Application Configuration Example
```
### {

```
"prod": {
"OrderManagement": {
"model": "CustomerOrderModel",
"version": "0.5.1",
"description": "Overrides for Order Management"
```

### },

"MaterialModeling": {
"model": "MaterialModel",
"version": "0.5.8",
"description": "Importing material definitions"
},
"OrderExecution": {
"description": "Processing of the orders",
"version": "0.3.12",
"model": "WorkOrderModel"
},
"ResourceManagement": {
"description": "Processing of the orders",
"version": "0.3.12",
"model": "ResourceManagement"
}
},
"test": {
"OrderManagement": {
"model": "CustomerOrderModel",
"version": "0.5.1",
"description": "Overrides for Order Management"
},
"MaterialModeling": {
"model": "CustomerMaterialModel",
"version": "0.2.3",
"description": "Importing material definitions"
},
"OrderExecution": {
"description": "Processing of the orders",
"version": "0.3.12",
"model": "WorkOrderModel"
},
"ResourceManagement": {
"description": "Processing of the orders",
"version": "0.3.12",
"model": "ResourceManagement"
}
},
"dev": {
"OrderManagement": {
"model": "CustomerOrderModel",
"version": "0.5.1",
"description": "Overrides for Order Management"
},
"MaterialModeling": {
"model": "CustomerMaterialModel",
"version": "0.2.3",
"description": "Importing material definitions"
},
"OrderExecution": {
"description": "Processing of the orders",
"version": "0.1.2",
"model": "CustomerWorkOrderModel"
},
"ResourceManagement": {
"description": "Processing of the orders",
"version": "0.3.12",
"model": "CustomerResourceManagement"
},
"ESigs": {
"description": "Collecting electronic approval signatures.",
"version": "0.3.12",
"model": "ESigsModel"
}
}
}



# Business Model (Metadata) Development Guidelines

## General Guidelines:

Notes:

```
The Models will be distributed to Customers in the form of source code
Model code should be customizable & extensible by the customers
```
Model code should:

```
be conformant with the Metamodel
align to the common patterns & templates where applicable
be properly commented with and limit access to specific hookup/exit points for customizations or extensions by customers or industry solutions
The standard templates will help here but any custom logic patterns should also consider these aspects
define logic in Configurable Objects as much as possible
Some reusable/generic logic can go into standard dotnet classes without compromising on the customizability & extensibility
not use the _User() methods for implementing OOTB Business Logic, as these methods are for customer level extensions.
```
IP protected logic/model could be defined in a non-Metamodel-based assembly and used as a binary reference

## Nuget Package Versioning

Binary references using nuget packages should reference a specific version. These dependencies for the platform code are found in the common.props file
in M1_Common. References to message model nuget packages should be made in the dependency.props file in the project directory.

<PackageReference Include="Siemens.MOM.MetaModel.Interfaces" Version="$(PlatformVersion)" />

For more information on Platform & Model versioning information, refer to page: Platform & Model Versioning

## Object Type Id:

Each object type in the model is identified by a COTypeId. This id must be unique and must be generated using the IdGenerator tool. See Creating a New
Named Object for a description on how to create a new named object.

## Engineering Models vs. Tracked Objects:

While NamedModelingObjects and TrackedObjects are both identified by name, the big difference is the intended usage.

```
Modeling objects
Don't change frequently
The changes are not triggered by shopfloor events
The audit trail is simply a list of who changed what and when
Represent physical or logical objects that describe how things are processed.
Generally are not deleted or archived as they are used for reporting
Tracked objects
Can handle significant volumes
Conceptually represent material (WIP) being processed
Have a much more limited lifetime than modeling objects. Generally, the retention period is determined by the reporting
and analysis requirements.
The manufacturing audit trail records the processing of the WIP in addition to data points needed to report and analyze
how the processing has been done in a way that apps like SPC can improve the processing in the future.
May generate a report for the customer as proof of quality of the finished good.
Audit Trail could facilitate Device History Record (eDHR)
Will be archived as needed to maintain a performant system.
```
## Engineering Models: (UOM, Product, Workflow, Factory, etc.)

Engineering model data is used to describe how material is processed/produced in the factory.

```
Should be Named or Revisioned Objects.
Should leverage the OOTB corresponding Modeling Service
May provide a customized version of a Modeling Service if customizations are needed that cannot be implemented by
configuring the existing service.
Should leverage the patterns from Common/Platform such as Modeling Audit Trail etc. instead of re-inventing
Within Business Logic, instances should always be created using their respective modeling service to take advantage of platform features
like audit-trail, training records or any model specific validations
```
## Trackables: (TrackedObject, WorkOrder, etc.)


The TrackedObject category represents material/WIP that flow through the factory as part of day-to-day manufacturing processing being conducted by
shopfloor operators and machines

```
All such Objects related to day-to-day business operations should be derived from TrackedObject
Services creating & manipulating them should be derived from Shopfloor Service
Implementations should align with the Shopfloor Service template
All such objects should create a corresponding HistorySummary subclass
Maps should be extended between Service, TrackedObject & HistoryHeader/Summary
Please see the M1_POC model for reference & sample code
```

# Developing a Metadata Module(V1)

A Metadata Module, as described in the Modularity Approach, is an isolated and independently deployable unit that can be composed with other loosely
couple modules in order to create a customized Product. A metadata model is used to configure the business objects and processes used by a Modular
MOM application. A metadata model will have dependencies on many other models that are maintained separately and stored in their own git repos.

```
Notes on directory structure
Module Directory Layout
Creating a new module
Prepping an existing module for modification
Adding a new submodule to an existing module
Building the module
Updating test code
Updating a submodule to a new commit point
Removing a submodule from an existing module
Troubleshooting
```
## Notes on directory structure

Because the metadata is persisted as C# files and must be compiled into a single binary, a module and all of its dependencies must all be accessible to
the project, both during development (for intellisense to work properly) and during the build. However, nested dependencies will cause compilation issues if
they are all included. To address this, each module will specify a flat list of all of the dependencies required. Example:

Example of nested dependencies and the obvious duplicates:

```
MESApp
Common
Dispatch
Common
ESigs
Common
TrackAndTrace
Common
DataCollection
Common
ESigs
Common
ESigs
Common
DataCollection
Common
```
Example of the module dependencies without the duplication:

```
MESApp
Common
Dispatch
TrackAndTrace
ESigs
DataCollection
```
In order to create a consistent Module directory structure, Modular MOM uses the submodules feature of git, so each model must observe a consistent
directory structure.

The complete repos for each dependency will be cloned into a directory under the Code folder, excluding any deeper dependencies.

Each module will have an associated .Tests repo managed as a dependency to the module.

## Module Directory Layout

To fulfill this requirement, Modular MOM is utilizing the submodules feature of git. To more easily manage these dependency submodules, each model will
conform to a consistent directory structure. The structure without any dependencies added will have the following structure:

MESApp
\Code
\Model
\Services
\Tracking
dependencies.props
nuget.config
.csproj


### 1.

```
a.
b.
2.
3.
a.
4.
a.
b.
5.
a.
b.
i.
1.
2.
```
```
3.
ii.
6.
```
### 1.

### 2.

### 3.

### 4.

### 5.

### 6.

\Tests
MESApp.sln
README.md
.gitmodules
Copy-From-MESApp.ps1

The complete repos for each dependency will be cloned into a directory under the Code folder:

MESApp
\Code
\M1_Common
\M1_DataCollection
\M1_Dispatch
\M1_ESigs
\M1_TrackAndTrace

One effect of this change is that each module will have an associated .Tests repo, and this repo is treated as a dependency to the module.

MESApp.Tests
\IntegrationTests\MyApp.IntegrationTests
\Unit.Tests

MESApp
\Tests
\MyApp.Tests

Creating a new module

A new module is created by modifying a copy of the M1_Template module. Follow the steps below to create the MESApp module:

```
Create the "remote" repos. This requires the assistance of someone who has the permission to add repos. These repos should be empty (i.e.
without history).
Module: M1_MESApp
Module tests: M1_MESApp.Tests
Open a Powershell command prompt window
Navigate to the directory containing the repos
CD \Repos
If you have not yet, get or update the Template repos
git clone https://tfs05mom.industrysoftware.automation.siemens.com/MOM/ModularMOM/_git/M1_Template M1_Template
git clone https://tfs05mom.industrysoftware.automation.siemens.com/MOM/ModularMOM/_git/M1_Template.Tests M1_Template.Tests
Copy the template as a starting point for the new model:
CD M1_Template
Execute the ./Copy-From-Template.ps1 script
Parameters:
newName - This is the name of the repo you're trying to create. In this example, that's MESApp
newURL - This is the URL of the remote git server where the model will be created. It defaults to the same repo as the
M1_Template. If you're using a separate git server, e.g. https://gitlab.com/username/
-replaceRepo - This switch will cause any data in the remote repo to be replaced.
Example: ./Copy-From-Template.ps1 MESApp -replaceRepo
Using your IDE, search for TODO and manually make the changes described such as the module description.
```
Prepping an existing module for modification

```
Open a Powershell command prompt window
Go to the directory that contains the repos
CD \Repos
Get the module code:
git clone https://tfs05mom.industrysoftware.automation.siemens.com/MOM/ModularMOM/_git/M1_MESApp M1_MESApp
Enter the module directory:
CD M1_MESApp
Pull the code for existing dependencies:
git submodule update --init --depth=1
```
```
DO NOT clone the new git repo to your local machine. The /Copy-From-Template.ps1 MESApp will create the repo and push it to the remote
repo.
```
```
Prerequisites
```
```
In order to complete the following steps, you need to install git with a version >= 2.3 (mandatory for the script Copy-From-Template.ps1). If
possible, it's highly recommended to install the latest version.
```

### 6.

### 1.

### 1.

### 2.

### 1.

### 2.

### 1.

### 2.

### 3.

```
a.
i.
ii.
b.
i.
ii.
```
### 1.

```
a.
2.
a.
3.
a.
4.
5.
```
### 1.

### 2.

```
a.
3.
a.
```
```
Put the Tests submodule onto the main branch
cd Tests/M1_MESApp.Tests
git checkout main
```
Adding a new submodule to an existing module

```
Add the new module e.g. ESigs:
prior to v2.0.0
git submodule add https://tfs05mom.industrysoftware.automation.siemens.com/MOM/ModularMOM/_git/M1_ESigs Code
/M1_ESigs
v2.0.0+
Add-Submodule.ps1 M1_ESigs
```
Building the module

```
Open the module solution.
Build the module project.
```
Notes for troubleshooting:

```
Building submodules is not needed and will cause problems in compiling the containing module.
If you are receiving an error indicating that something cannot be found in Common.Tests, chances are good that the "--depth=1" was missing
from the "git submodule update" command
```
Updating test code

```
Open the module solution
Make the changes to the test project
Commit and push the changes from the test submodule
From the Tests/MESApp.Tests directory:
Commit and push changes
This will update the reference in the module to the new commit point.
From the M1_MESApp directory:
Commit and push changes
This will save the updated commit point for the next time it's cloned
```
Updating a submodule to a new commit point

Once changes have been made to a submodule repo, The other repos that depend on the submodule must be updated to reference the new commit point.

```
Navigate to the submodule directory
cd /repos/M1_MESApp/Code/ESigs
Get potential commit points
git fetch
Update the commit point
git checkout \[main | branchName]
cd ../..
Commit and push the changes to the M1_MESApp module
```
Removing a submodule from an existing module

```
Remove the code:
git submodule deinit Code/M1_ESigs
Remove the reference
git rm Code/M1_ESigs
Cleanup git
Remove-Item -LiteralPath ".git/modules/Code/M1_ESigs" -Force -Recurse
```
Troubleshooting

Symptom:

remote: TF401035: The object '0bfc9ad18ee19be277937ff25a8f70ea0acab2d2' does not exist.
fatal: protocol error: bad pack header

Cause:

A submodule was updated in place (e.g. changes made to M1_POC\Code\M1_Common instead of \M1_Common) and committed but not pushed. and the
submodule reference in the parent module was committed and pushed. This causes the remote repo to reference a commit identifier from the local repo
that hasn't been pushed to the remote repo.

Fix:


Go to the submodule and checkout main.


# Developing a Metadata Module(V2+)

A Metadata Module, as described in the Modularity Approach, is an isolated and independently deployable unit that can be composed with other loosely
couple modules in order to create a customized Product. A metadata model is used to configure the business objects and processes used by a Modular
MOM application. A metadata model may have dependencies on other models that are maintained separately and stored in their own git repos.

Prior to version 2, the tests for a module were maintained in a separate repo. This pattern has been changed, and tests now reside in the module's main
repo.

For customers, the out of the box models, including M1_Common and M1_Template will be mirrored to their git server.

```
Notes on directory structure
Module Directory Layout
Creating a new module
Prepping an existing module for modification
Adding a new submodule to an existing module
Building the module
Updating a submodule to a new commit point
Removing a submodule from an existing module
Troubleshooting
```
## Notes on directory structure

Because the metadata is persisted as C# files and must be compiled into a single binary, a module and all of its dependencies must all be accessible to
the project, both during development (for intellisense to work properly) and during the build. However, nested dependencies will cause compilation issues if
they are all included. To address this, each module will specify a flat list of all of the dependencies required. Example:

Example of nested dependencies and the obvious duplicates:

```
MESApp
Common
Dispatch
Common
ESigs
Common
TrackAndTrace
Common
DataCollection
Common
ESigs
Common
ESigs
Common
DataCollection
Common
```
Example of the module dependencies without the duplication:

```
MESApp
Common
Dispatch
TrackAndTrace
ESigs
DataCollection
```
In order to create a consistent Module directory structure, Modular MOM uses the submodules feature of git, so each model must observe a consistent
directory structure.

The complete repos for each dependency will be cloned into a directory under the Code folder, excluding any deeper dependencies.

## Module Directory Layout

To fulfill this requirement, Modular MOM is utilizing the submodules feature of git. To more easily manage these dependency submodules, each model will
conform to a consistent directory structure. The structure without any dependencies added will have the following structure:

MESApp
\Code
\Model
\Services
\Tracking
dependencies.props
nuget.config
.csproj


### 1.

```
a.
b.
2.
3.
a.
4.
a.
5.
a.
b.
i.
1.
2.
ii.
6.
```
### 1.

### 2.

### 3.

### 4.

### 5.

### 1.

### 2.

### 1.

### 2.

\Tests
MESApp.sln
README.md
.gitmodules
Copy-From-MESApp.ps1

The complete repos for each dependency will be cloned into a directory under the Code folder:

MESApp
\Code
\M1_Common
\M1_DataCollection
\M1_Dispatch
\M1_ESigs
\M1_TrackAndTrace

Creating a new module

A new module is created by modifying a copy of the M1_Template module. Follow the steps below to create the MESApp module:

```
Create the "remote" repos. This requires the assistance of someone who has the permission to add repos. These repos should be empty (i.e.
without history).
Module: M1_MESApp
Module tests: M1_MESApp.Tests
Open a Powershell command prompt window
Navigate to the directory containing the repos
CD \Repos
If you have not yet, get or update the Template repos
git clone https://tfs05mom.industrysoftware.automation.siemens.com/MOM/ModularMOM/_git/M1_Template M1_Template
Copy the template as a starting point for the new model:
CD M1_Template
Execute the ./Copy-From-Template.ps1 script
Parameters:
newName - This is the name of the repo you're trying to create. In this example, that's MESApp
-replaceRepo - This switch will cause any data in the remote repo to be replaced.
Example: ./Copy-From-Template.ps1 MESApp -replaceRepo
Using your IDE, search for TODO and manually make the changes described such as the module description.
```
Prepping an existing module for modification

```
Open a Powershell command prompt window
Go to the directory that contains the repos
CD \Repos
Get the module code:
git clone https://tfs05mom.industrysoftware.automation.siemens.com/MOM/ModularMOM/_git/M1_MESApp M1_MESApp
Enter the module directory:
CD M1_MESApp
Pull the code for existing dependencies:
git submodule update --init --depth=1
```
Adding a new submodule to an existing module

```
Add the new module e.g. ESigs:
Add-Submodule.ps1 M1_ESigs
After adding the submodule, the dependencies of the submodule need to be copied to the containing model.
```
Building the module

```
Open the module solution.
Build the module project.
```
```
DO NOT clone the new git repo to your local machine. The /Copy-From-Template.ps1 MESApp will create the repo and push it to the remote
repo.
```
```
Prerequisites
```
```
In order to complete the following steps, you need to install git with a version >= 2.3 (mandatory for the script Copy-From-Template.ps1). If
possible, it's highly recommended to install the latest version.
```

### 1.

### 2.

### 1.

```
a.
2.
a.
3.
a.
4.
5.
```
### 1.

### 2.

```
a.
3.
a.
4.
a.
```
Notes for troubleshooting:

```
Building submodules is not needed and will cause problems in compiling the containing module.
If you are receiving an error indicating that something cannot be found in Common.Tests, chances are good that the "--depth=1" was missing
from the "git submodule update" command
```
Updating a submodule to a new commit point

Once changes have been made to a submodule repo, The other repos that depend on the submodule must be updated to reference the new commit point.

```
Navigate to the submodule directory
cd /repos/M1_MESApp/Code/ESigs
Get potential commit points
git fetch
Update the commit point
git checkout \[main | branchName]
cd ../..
Commit and push the changes to the M1_MESApp module
```
Removing a submodule from an existing module

```
Remove the code:
git submodule deinit Code/M1_ESigs
Remove the reference
git rm Code/M1_ESigs
Cleanup git
Remove-Item -LiteralPath ".git/modules/Code/M1_ESigs" -Force -Recurse
Remome the appropriate line from the target module's csproj file.
<Compile Remove="M1_ESigs\Tests\**" />
```
Troubleshooting

Symptom:

remote: TF401035: The object '0bfc9ad18ee19be277937ff25a8f70ea0acab2d2' does not exist.
fatal: protocol error: bad pack header

Cause:

A submodule was updated in place (e.g. changes made to M1_POC\Code\M1_Common instead of \M1_Common) and committed but not pushed. and the
submodule reference in the parent module was committed and pushed. This causes the remote repo to reference a commit identifier from the local repo
that hasn't been pushed to the remote repo.

Fix:

Go to the submodule and checkout main.


# User code execution risk & mitigation

## Big picture for user code execution risk

Platform API services, through the Metadata Engine execute business functionalities as defined in the metadata.

Business functionalities are provided as follows:

```
Out of the box
Customized user code
```
Customized user code cannot be trusted to be secure.
For instance, some execution paths of user code might throw exceptions and cause a service failure.
A malicious part could exploit such unintentional failures to disrupt the availability of the system.

Metadata Engine must be capable to safely handle the execution of customized user code.

## Proposed mitigation

Platform code that invokes user code should be carefully protected to manage unintentional failures. The normal behavior should be to avoid the service to
go down when an unexpected failure occurs.

A specific abstraction layer should be implemented on top of user code hooks to assure execution safety of such code.


# Creating a new version of M1_Common

## Introduction

The M1_Common module is used as a submodule to many other Modular MOM modules. It provides base Model and Service implementations, Audit trail
functionality and schema definitions.

## Revising the version of M1_Common

The Common module is referenced by other modules by its version in their Config/<module>.modelconfig.json file

The version defined in the model config corresponds to a git tag in tfs. For example if the common version is 2.1.0, this corresponds to the git tag v2.0.1

The common module also has a platform dependency. So typically when the platform is revised and the new features are required by the Common module
then a new revision of the common module is produced, referencing the new platform version.

## Workflow

Make changes on a feature branch, then submit a Pull Request (PR) with your changes and the required version changes described below.

## Version Changes Required

In addition to any code changes you make, the following files must include required version changes for the pipeline to succeed.

```
common.props, if Platform Version is changing
<model>.modelconfig.json
CHANGELOG.md
```
## Changes Required to common.props

Here is an example of the changes to the common.props, updating from 2.1.3 to 2.1.4

## Changes Required to <model>.modelconfig.json

Here is an example of the changes to the <model>.modelconfig.json, updating from 2.1.3 to 2.1.4


Changes Required to CHANGELOG.md

Every new change to the M1_Common module, requires a CHANGELOG entry in the file to record the history of changes.

Changes must be made to the TOP of the file, new entries appear above existing entries.

The minimal version entry includes a header line in this exact format.

#<space>X.Y.Z<space>-<space>Short description of change

Where X.Y.Z is the major.minor.patch value.

Optionally add additional lines of history immediately under the header beginning with a hyphen -

Ensure there's a blank line between the new version section and the previous one.

Reasons the pipeline will fail

When the version information is not property prepared on your feature branch, the Pull Request will fail.

```
When you submit a PR and the common.props contains an updated version but the modelconfig.json does not have a corresponding version, the
following pipeline failure will result.
The remedy is to make sure the version defined in the common.props and <model>.modelconfig.json are the same and are incremented from the
value defined in the main branch.
```

When you change the version but do not have a corresponding entry in the CHANGELOG.md, the pipeline fails with the following error.
To correct this make sure versions are consistent and the CHANGELOG.md has an entry corresponding to the new version.


Pipeline Logic

The pipeline uses this logic to perform the version update / merge


Pipeline Success

When the version information is properly configured, the PR succeeds as depicted in the diagram above, changes are merged to the main branch and a
tag is added to the repo in the form v<version> for example: v2.1.4

The image below shows the set of updates required to successfully increment the version of the M1_Common repository.


And the corresponding version tag is added with the CHANGELOG description


# Technological Upgrade


# Upgrade to .NET 6

## Framework version

Upgrade to .NET 6.0.3

https://dotnet.microsoft.com/en-us/download/dotnet/6.0

## Nuget Version

NuGet 6.1 Release Notes | Microsoft Docs

6.1.0

## Install VS 2022

https://visualstudio.microsoft.com/vs/

mandatory features:

```
cross platform dotnet core
aspnet core
```
## Upgrade of the VDI

```
Install Visual Studio 2022 (v17.0)
```
Installing just the features necessary for MOM, the system requirements should be:

Operative System
RAM (recommended 16 GB):
HD (15-16 GB)

```
Upgrade to the recommended Framework version (read the previous paragraph)
```
## Working with NET 6 Modules

```
clean the nuget package cache
clone the repository (or pull changes and delete all binary/output temporary folders)
```
Be careful:

On upgrading from NET 5 to NET 6, the target framework has been removed from the output paths: all the projects have the following setting

```
<AppendTargetFrameworkToOutputPath>false</AppendTargetFrameworkToOutputPath>
```
## Repo projects refactoring

One of the goal of "Feature 25890: Tech Debt-dotnet 6.0 upgrade" is also to simplify the upgrade to a new version of .NET framework.

For this purpose it has been necessary the refactoring of several projects and other configuration files.

The steps below will be valid once that the US has been completed (code changes merged into the master)

## General notes for the upgrade of MOM repositories


On changing the target framework of a module (all projects in the repo) it is recommended to clean up all the temporary/binaries folders before building.

An alternative is to clone the repository and work in this way from a clean scenario.

Metadataruntime

In order to switch the target framework,

```
modify the TargetFramework value in the file .\metadataruntime\platform\platform.target.props
```
for example:

```
<TargetFramework>net6.0</TargetFramework>
```
```
modify the model.setting.json if necessary (the paths of the modules to be loaded could contain the target framework with which they have been
built)
All projects should import platform.target.props as shown in the code block below. Anyway, check that all projects are really importing the
common props file instead of defining their target framework. Fix the projects if it defines its TargetFramework instead of importing the common
props.
```
```
How to import platform.common.props
```
```
<Import Project="../platform.common.props" />
```
```
Modify the generator .txt files target framework setting the desired target framework (WriteProject.txt, DtoProject.txt etc)
```
Notes

```
Running Siemens.MOM.Platform.Api.exe from Visual Studio 2022 in debug mode, the working directory is set to the output directory. Therefore
the modelsettings.json file will be loaded from there (the previous behavior was to have the working directory set to the folder of Siemens.MOM.
Platform.Api..csproj.
The output folder path does not longer contains the Target Framework
```
Microsoft and 3 party package references upgrade

The following tables show the list of the package references that has been upgraded for the .NET 6 support.

Each table represents the repository that has been updated.

```
Metadataruntime
```
```
referenced package previous
version
```
```
new
version
```
```
projects
```
```
Microsoft.Extensions.Configuration.Json 3.1.8 * (6.0.0) Camstar.Core.SampleModel
Siemens.MOM.Model
Siemens.MOM.Metadata.Aspects.IntgrationTests
Siemens.MOM.Metamodel.Framework
Siemens.MOM.Model.Generator
Siemens.MOM.Model.Platform.Configuration
Siemens.MOM.Model.Platform.Observability
```
```
Microsoft.Extensions.Logging.Abstractions 5.0.0 * (6.0.0) DummyDep.SampleModel
Siemens.MOM.Metadata.Aspects.IntgrationTests
Siemens.MOM.Metamodel.Framework.OptionalFeature
Siemens.MOM.Metamodel.Framework.Tests
Siemens.MOM.Metamodel.Interfaces
Siemens.MOM.Model.WriteBase
Siemens.MOM.Model.Platform.Information
Siemens.MOM.Model.Platform.Integration
```
```
Microsoft.Extensions.Configuration 5.0.0 * (6.0.0) Siemens.MOM.Metadata.Aspects.IntgrationTests
```
```
Siemens.MOM.Model.Generator
Siemens.MOM.Model.WriteBase.Tests
Siemens.MOM.Model.Platform.Configuration
Siemens.MOM.Model.Platform.Observability
Siemens.MOM.Model.Platform.ApiIntegrationTests
```
```
Microsoft.Extensions.DependencyInjection. 5.0.0 * (6.0.0) Siemens.MOM.Metadata.Aspects.IntgrationTests
```

```
Abstractions Siemens.MOM.Model.Platform.Ioc
```
```
Microsoft.Extensions.Logging 5.0.0 * (6.0.0) Siemens.MOM.Metadata.Aspects.IntgrationTests
Siemens.MOM.Metamodel.Framework
Siemens.MOM.Model.Generator
Siemens.MOM.Model.Platform.Api
```
```
Microsoft.Extensions.Logging.Console 5.0.0 * (6.0.0) Siemens.MOM.Metadata.Aspects.IntgrationTests
Siemens.MOM.Metamodel.Framework
Siemens.MOM.Model.Dto.Base
Siemens.MOM.Model.Generator
```
```
Microsoft.Extensions.DependencyInjection 5.0.1 * (6.0.0) Siemens.MOM.Metamodel.Framework Siemens.MOM.Metamodel.
Framework.OptionalFeature
Siemens.MOM.Metamodel.Interfaces
Siemens.MOM.Model.Generator
Siemens.MOM.Model.MessageBase
Siemens.MOM.Model.WriteBase
Siemens.MOM.Model.Platform.Api
Siemens.MOM.Model.Platform.Application
Siemens.MOM.Model.Platform.Information
Siemens.MOM.Model.Platform.Interface
Siemens.MOM.Model.Platform.Ioc
Siemens.MOM.Model.Platform.Observability
```
```
Microsoft.Extensions.Logging.Debug 5.0.0 * (6.0.0) Siemens.MOM.Metamodel.Framework.Tests
```
```
Microsoft.Extensions.Localization.
Abstractions
```
```
5.0.8 * (6.0.2) Siemens.MOM.Metamodel.Interfaces
Siemens.MOM.Model.Dto.Base
Siemens.MOM.Model.Platform.Application
```
```
Microsoft.Extensions.Localization 5.0.8 * (6.0.2) Siemens.MOM.Model.Dto.Base
Siemens.MOM.Model.ExceptionFramework
```
```
Google.Protobuf 3.14.0 3.19.4 Siemens.MOM.Model.MessageBase
Siemens.MOM.Model.Platform.Api
Siemens.MOM.Model.Platform.Integration
Siemens.MOM.Model.Platform.ApiIntegrationTests
```
```
Microsoft.Extensions.Http 5.0.0 * (6.0.0) Siemens.MOM.Model.MessageBase
```
```
Microsoft.Extensions.Caching.Abstractions 5.0.0 * (6.0.0) Siemens.MOM.Model.WriteBase
```
```
Microsoft.Extensions.Caching.Memory 5.0.0 * (6.0.1) Siemens.MOM.Model.WriteBase
```
```
OpenTelemetry 1.1.0 1.2.0-rc2 Siemens.MOM.Model.Platform.Api
```
```
OpenTelemetry.Exporter.Console 1.1.0 1.2.0-rc2 Siemens.MOM.Model.Platform.Api
```
```
OpenTelemetry.Exporter.Jaeger 1.1.0 1.2.0-rc2 Siemens.MOM.Model.Platform.Api
```
```
OpenTelemetry.Exporter.
OpenTelemetryProtocol
```
```
1.0.1 1.2.0-rc2 Siemens.MOM.Model.Platform.Api
```
```
Siemens.MOM.Model.MessageBase (new reference)
```
```
Siemens.MOM.Model.Platform.Integration (new reference)
```
```
OpenTelemetry.Exporter.Zipkin 1.1.0 1.2.0-rc2 Siemens.MOM.Model.Platform.Api
```
```
OpenTelemetry.Extensions.Hosting 1.0.0-rc7 1.0.0-rc9 Siemens.MOM.Model.Platform.Api
```
```
OpenTelemetry.Instrumentation.
AspNetCore
```
```
1.0.0-rc7 1.0.0-rc9 Siemens.MOM.Model.Platform.Api
```
```
OpenTelemetry.Instrumentation.
GrpcNetClient
```
```
1.0.0-rc7 1.0.0-rc9 Siemens.MOM.Model.Platform.Api
```
```
OpenTelemetry.Instrumentation.Http 1.0.0-rc7 1.0.0-rc9 Siemens.MOM.Model.Platform.Api
```
```
OpenTelemetry.Instrumentation.SqlClient 1.0.0-rc7 1.0.0-rc9 Siemens.MOM.Model.Platform.Api
```
```
Microsoft.Extensions.Logging.Configuration 5.0.0 * (6.0.0) Siemens.MOM.Model.Platform.Api
Siemens.MOM.Model.Platform.Configuration
Siemens.MOM.Model.ExceptionFramework
```
```
Microsoft.Extensions.Configuration.Binder
```
```
5.0.0 * (6.0.0) Siemens.MOM.Model.Platform.Configuration
Siemens.MOM.Model.Platform.Observability
```
```
Microsoft.Extensions.Configuration.
FileExtensions
```
```
5.0.0 * (6.0.0) Siemens.MOM.Model.Platform.Configuration
Siemens.MOM.Model.Platform.Observability
```
```
Microsoft.Extensions.Hosting.Abstractions 5.0.0 * (6.0.0) Siemens.MOM.Model.Platform.Configuration
```
```
Microsoft.Extensions.Configuration.
Abstractions
```
```
5.0.0 * (6.0.0) Siemens.MOM.Model.Platform.Interface
```
*: version downloaded at runtime setting in the project the version to 6.0.*


```
M1_Common
```
```
referenced package previous
version
```
```
new
version
```
```
projects
```
```
Microsoft.Extensions.Logging.Abstractions 5.0.0 6.0.0 M1_Common
```
```
M1_OrderManagement.Tests
```
```
referenced package previous
version
```
```
new
version
```
```
projects
```
```
Microsoft.Extensions.Logging 5.0.0 * (6.0.0) M1_OrderManagement.Tests.csproj
```
```
M1_MaterialManagement.Tests
```
```
referenced package previous
version
```
```
new
version
```
```
projects
```
```
Microsoft.Extensions.Logging 5.0.0 * (6.0.0) M1_MaterialManagement.Tests.csproj
```
```
M1_Factory.Tests
```
```
referenced package previous
version
```
```
new
version
```
```
projects
```
```
Microsoft.Extensions.Logging 5.0.0 * (6.0.0) M1_Factory.Tests.csproj
```
```
M1_UserManagement.Tests
```
```
referenced package previous
version
```
```
new
version
```
```
projects
```
```
Microsoft.Extensions.Logging.Debug 5.0.0 * (6.0.0) M1_UserManagement.Tests.csproj
```
Upgrading a MOM Module to a new NET version

Prerequisite: working with the module and submodules branches for target framework

```
steps for the module repository
```
clone the module repository

initialize the submodules (git submodule update --init --depth=1)

create the branch for the upgrade of the submodule (git checkout main, git branch branchname)

check out of the branch just created

```
steps for the submodule test
```
create the branch for test submodule upgrade (git checkout main, git branch branchname)


check out of the branch just created

```
steps for the M1_Common submodule
```
checkout of the M1_Common branch dedicated to the target framework upgrade and git fetch

if no branch is visible execute the following commands:

```
git config remote.origin.fetch "+refs/heads/*:refs/remotes/origin/*"
```
```
git fetch
```
```
steps for the platform
```
clone the metadataruntime repository

check out of the upgrade branch

Read the following link for further notes on working with submodules

Git How To - RevMOM - MES Wiki (siemens.com)

Developing a Metadata Module

Upgrade to a new NET Version

```
Search for the old target framework (e.g. net5.0) in *.csproj folder and subfolders of the module and submodules and replace it with the new
target framework (e.g. net6.0) as below:
```
```
<TargetFramework>net6.0</TargetFramework>
<AppendTargetFrameworkToOutputPath>false</AppendTargetFrameworkToOutputPath>
```
```
search for Microsoft.Extensions.Logging* in *.csproj,*.props
```
and update the version of the reference so found as seen in the MetadataRuntime table seen before, example:

Microsoft.Extensions.Logging from 5.0.0 to 6.0.0

Microsoft.Extensions.Logging.Abstraction from 5.0.0 to 6.0.0

Microsoft.Extensions.Logging.Debug from 5.0.0 to 6.0.0

If there are other modules to be updated, check the MetadataRuntime table seen before for the correct new package version

Platform version, Common version, Observability version, modules version

Platform

```
Set the current version number in platform.common.props
```
```
<Version>2.1.0</Version>
```
```
Declare a major release creating also a git Tag with the same value of <version>
```
M1_Common

```
Set in in M1_Common.modelconfig (/Code/Config/M1_Common.modelconfig.json), the version for M1_Common and set the platform version to
the value set in metadataruntime repo
```
```
"version": "2.1.0",
"platformVersion": "2.1.0",
```
```
Create a git Tag with the same value of "version" (v2.1.0) for the M1_Common repo
```

```
in common.props (/common.props) update the PlatformVersion to the value set for the Platform and change also the ObservabilityVersion (the
change of the Target Framework is a breaking change)
```
```
<Project xmlns="http://schemas.microsoft.com/developer/msbuild/2003">
<PropertyGroup>
<PlatformVersion>2.1.0</PlatformVersion>
<ObservabilityVersion>2.0.0</ObservabilityVersion>
</PropertyGroup>
</Project>
```
Module Repository

```
in modulename.modelconfig (for example M1_OrderManagement.modelconfig) set platformVersion with the same value assigned to platform, and
set version in the modelDependencies of Common with the same value seen for M1_Common module
```
### {

```
"description": "TODO: Provide a description of the model.",
"version": "2.0.0",
"platformVersion": "2.1.0",
"modelDependencies": [
{
"name": "Common",
"version": "2.1.0"
}
]
}
```
The module should refer to the same git Tag value for M1_Common1 assigned in modelDependencies

Change also the "version" (for example with 2.0.0) of the our module (e.g M1_OrderManagement) because the change of target framework is considered
a breaking change

```
In dependencies.props (/Code/dependencies.props) set the PlatformVersion and the ObservabilityVersion
```
```
<Project xmlns="http://schemas.microsoft.com/developer/msbuild/2003">
<PropertyGroup>
<PlatformVersion>2.1.0</PlatformVersion>
<ObservabilityVersion>2.0.0</ObservabilityVersion>
</PropertyGroup>
</Project>
```
In order to generate the model

```
update the bat: callGenerateModel.cmd with our local machine settings
build module solution
generate the model (e.g invoking callGenerateModel.cmd)
run unit tests
run integration tests
```
MOM Messages Repo .NET Upgrade

This paragraph is about the following repos upgrade:

```
M1_OrderManagementMessages
M1_MaterialManagementMessages
M1_TrackAndTraceMessages
```
and the modules that has a reference to them

Steps for message repos upgrade

The repos involved are:

```
M1_OrderManagementMessages
M1_MaterialManagementMessages
```

### 1.

```
M1_TrackAndTraceMessages
```
```
Modify the csproj projects of the repos as below:
```
```
upgrade the NET target framework (and force the generation of an output path which does not contain the "target framework" subfolder)
```
```
<TargetFramework>net6.0</TargetFramework>
<AppendTargetFrameworkToOutputPath>false</AppendTargetFrameworkToOutputPath>
```
```
upgrade the nuget package of the references using a set of versions supported (for example [2.0.0-*,3) means, reference to the most recent
version found between a prerelease version of 2.0.0 and and 3 (3 not included)
```
```
<ItemGroup>
<PackageReference Include="Camstar.Metadata.Aspects.Fody" Version="2.0.0-*"/>
<PackageReference Include="Siemens.MOM.MetaModel.Framework" Version="[2.0.0-*,3)"/>
<PackageReference Include="Siemens.MOM.MetaModel.Interfaces" Version="[2.0.0-*,3)"/>
</ItemGroup>
```
2. Modify the BuildModulePipeline.yml as below:

```
use the proper group variable defined in the Library - Pipelines (siemens.com) ((see DevOps pipeline documentation for further info)
```
```
variables:
```
- group: "NET Version 6"

```
Install the proper version of Nuget (in the example, 6.1.0)
```
```
task: NuGetToolInstaller@1
displayName: 'Install Nuget tool'
inputs:
versionSpec: '6.1.0'
checkLatest: true
```
```
Build the Module Nuget Package setting the correct major version number (in the example the Nuget Package will be create with version 2.0.0,
where 2 is defined by majorVersion)
```
- task: DotNetCoreCLI@2
displayName: 'Build the solution'

```
packDirectory: '$(Build.ArtifactStagingDirectory)/nugets/'
nobuild: true
versioningScheme: 'byPrereleaseNumber'
majorVersion: '2'
```
The NugetPackage will be published on the artifact registry ModMOM_Platform (siemens.com) only if the pipeline is run by the master/main branch (as
described in the "condition" field below.

- task: NuGetCommand@2
displayName: 'Publish resultant Nuget package into TFS registry'
condition: and(succeeded(), eq(variables['Build.SourceBranchName'], 'master'))
inputs:
command: 'push'
packagesToPush: '$(Build.ArtifactStagingDirectory)/nugets/*.nupkg'

Upgrade the modules which have a reference to the message nuget packages

The repos involved are:

```
M1_OrderManagement
M1_MaterialManagement
M1_TrackAndTrace
M1_Factory
```
and the related tests repo


```
upgrade the nuget package of the references using the exact version, for example
```
```
<ItemGroup>
<PackageReference Include="M1_MaterialManagementMessages" Version="2.0.0-CI-20220328-145321" />
<PackageReference Include="M1_OrderManagementMessages" Version="2.0.0-CI-20220329-094552" />
</ItemGroup>
```
.NET Upgrade in the Pipeline

The .NET version can be set in the Pipeline setting properly the variable described in the link:

NET Version X

The variables in the link Library - Pipelines (siemens.com) define the Docker Image used as base image for the platform (https://hub.docker.com/_
/microsoft-dotnet-aspnet), the NET version used for the dotnet command, the sdk installed on the agent machines.

Useful commands to test the NET upgrade on a pod

```
Check net version
```
```
dotnet --info
```
```
Check modules matching the net version (in /app and /app/_models)
```
```
grep 'NETCoreApp,Version=v6' *.*
```

# Localization

This page

Points concerning back-end localization addressed in the sprint 23

```
Translation process defined
Checked that all satellite assemblies are created and deployed
```
Open points concerning back-end localization for the next sprints

```
Check that all modules included COMMON and template have the correct resource file as described in the link at the bottom of the page
Check the all en-US (default-culture) string present in control file CheckModelSharedResources.resx are placed correctly in the
ModelSharedResources.resx file for each module
Check that Enumeration strings are present in the ModelSharedResources.resx file for each module
Check that all relevant error messages are present in the ModelSharedResources.resx file for each module
Create a versioned copy of the ModelSharedResources.resx file of each module
```
After all point will be addressed this documentation page will be updated with the results

See Localization vs. Internationalization (w3.org)

"Internationalization is the design and development of a product, application or document content that enables easy localization for target audiences that
vary in culture, region, or language."

"Localization refers to the adaptation of a product, application or document content to meet the language, cultural and other requirements of a specific
target market (a locale)."

## Useful momwiki links

```
Back end guideline for localization: Backend Localization - Modular MOM - MOM Wiki (siemens.com)
Front end (SWF): Localization(L10N) support provided by Siemens Web Framework (SWF) - Modular MOM - MOM Wiki
```
Further links

```
Back end Localization Support for Metadata Runtime components and Platform and Spike: Localization Support for Metadata Runtime
components
```
## Useful external links

```
Apollo SWF doc
```
Localization(L10N) support provided by Siemens Web Framework (SWF) - Modular MOM - MOM Wiki

## Troubleshooting tips

Below a list of possible issues and related root cause

```
missing field in the ModelSharedResources.resx file produced by the generator: the label is missing from the current module or from one of its
modules (e.g. OrderManagement, M1_Common)
The satellite is present on the runtime machine (vdi, pod) but it is not loaded by runtime. A possible root cause is a wrong name of resx file used
for the translation: the culture is in fact case sensitive. For example the ModelSharedResources.resx for german (Germany) is
ModelSharedResources.de-DE.resx. If we rename this file as ModelSharedResources.de-De.resx (de-De is wrong), the related satellite is saved
in the folder de-De which is not recognized as a culture on a linux machine
```
```
Under Construction
```
```
This page is under construction: feel free to add more content and improve it
```

# Open points: Feature 32266: Ensure Localization on all

# modules

## Topic to be discussed

```
Destination folder for the final resx file (To be decided and to be documented)
```
The destination folder for the final resx file (for english):in this folder the file must be copied and versioned

In the same folder must be put the complete resx files for the translated languages (file translated from the English final resx file)

```
Executable/layer copies the final resx files to the generated module common folder: the generator is our first choice
To evaluate: Generator with a new parameter to produce just the resource files
Adding new cultures to Platform: out of scope of the feature (BR2 06/05)
```
## Out of scope of the feature: Adding new cultures to Platform

1) Support of a predefined set of cultures like “en” and “de-DE” (done by R&D mom)

We simply add the “SharedResources.culture.resx” files to the Project Siemens.MOM.Platform.Core and distribute our platform in a docker image with just
the supported cultures: possible only by R&D, procedure to be documented

2) Support of not predefined culture (translation performed by the integrator/customer)

We distribute a Platform Image with only the default cultures (e.g en, de): the customer cannot modify it.
The satellite assembly should be generated offline. It would be loaded by the customer.

We have to provide or document:

```
a way to build satellite assembly for a culture starting from a resx file. (Microsoft doc recommends to use a Windows machine with Visual Studio
installed)
a way to install satellite assemblies in the folder of binary of Platform image (in this case, how it would be integrated in Saas?)
```
We can support the customer via:

```
Documentation
A dedicated tool or script that we supply
A part of the configurator (that should modify the docker image?)
```
3) Not standard languages to be supported (required in the past in foundation for asiatic languages): out of scope for this feature?

External tool necessary, such as resgen and al dotnet linker (msbuild of dotnet core skips the not standards languages and cannot be used.)
A Windows machine with Visual Studio installed is necessary for the build.
If the satellite is manually generated, it seems to work.
The assembly (satellite) so produced could be loaded somehow by the configurator, but in that case, the binary should be validated (security issue).

A possible workaround for the customer would be to support the dialect using a resx files dedicated for a standard language


# Internal Technical Documentation

```
This section contains Internal Technical Documentation. It will be mainly in charge of the Technical Writes (currently Alvarez Villanueva, Beatriz).
Please use Internal Collaborative Content for document related to our day by day work or notes you want to share with everyone
```

# Inspection Planning Shopfloor

This page contains information regarding the new app Inspection Planning Shopfloor.

This is the IPS documentation space: https://momwiki01.industrysoftware.automation.siemens.com/display/IPSD
/Modular+Manufacturing+Quality+Documentation+Home

Contact Senft, Dagmar for more information.


# Momwiki02 Structure Draft

```
If you want to know about Go to
```
```
OCMOD people and roles Modular MOM Organization
```
```
Agile & scrum concepts Agile Scrum Practice
```
```
OCMOD basics and procedures Introduction to OCMOD
```
```
DevOps Activities CI/CD Processes and Practices
```
```
General Concepts and useful data Internal Documentation
```
```
Test procedures and concepts Test Activities
```
```
Platform procedures and concepts Platform Architecture
```
```
Modules procedures and concepts Modules Architecture
```
```
1 Organization Modular MOM
1.1 Feature Team "Mauna Kea" (Genoa - Mountain)
1.2 Feature Team "Monviso" (Genoa - Mountain)
1.3 Feature Team "Pratapgad" (Pune - Fort)
1.4 Feature Team "Shivneri" (Pune - Fort)
1.5 Feature Team "Sinhagad" (Pune - Fort)
1.6 Photogallery
1.7 Distribution Lists
1.8 TimeZone issues - find a meeting slot
2 Agile Scrum Practice
2.1 Agile Scrum Practice Modular MOM2
2.2 User Story - Definition of Ready
2.3 User Story - Definition of Done
2.4 User Story - Definition of Done (checklist)
2.5 Overall Retrospectives (Sprint 1 to Sprint 23)
2.6 Scrum Events
2.7 TFS Azure DevOps
2.8 Communication Rules
3 Introduction to Opcenter Modular Manufacturing
3.1 Acronyms Definition
3.2 Glossary
3.3 Before you start
3.4 First Steps for Startup Users
3.5 Common Guidelines
3.5.1 Best practices for Git Branching and Merging
3.5.2 Building Redis on Windows for Development
3.5.3 [Obsolete] Deploy in Openshift from Feature Branch
3.5.4 Enabling LongPath Support in Windows
3.5.5 Git Procedures
3.5.6 Interfacing with ModularMOM Adopter Startups
3.5.7 Localization(L10N) support provided by Siemens Web Framework (SWF)
3.5.8 Monitoring Traces and Logs in Development Environment
3.5.9 Procedure for Pull Request
3.5.10 Viewing Logs from OpenShift with Kibana
4 CI/CD Processes and Practices
4.1 Process And Phases
4.1.1 Modular Manufacturing 2112
4.1.2 ModularMOM 2.0 Private Registry
4.1.3 ModularMOM 2.x Build and Images Versioning
4.1.4 Authentication POC - tooling
4.2 Integration and Deployment Tasks
4.2.1 Prepare
4.2.2 Develop
4.2.3 Deploy
4.3 CI/CD Configuration
4.3.1 Build
4.3.2 Release
4.3.3 Deployment
4.4 Delivery
4.5 Monitoring
```

4.6 Development Activities
4.6.1 Create a Feature Branch
4.6.2 Run a development build
4.6.3 Deploy to Openshift environment
4.7 Integration Activities
4.7.1 Manage Pull Request
4.7.2 Check/verify the integration release
4.7.3 Deploy an integration release to a dev environment
4.8 Delivery Activities
4.8.1 Run a stable build
4.9 DevOps Activities
4.9.1 Configure CI/CD Pipeline for New Module Repository
5 Internal Documentation
5.1 SF experts exchange topics
5.2 App Creation Workshop
5.2.1 Business Process Flow & Epics
5.3 Authentication POC - tooling
5.4 Configure SQL Server on Ubuntu 20.04 VDI
5.5 Configuration of Microk8s/Ubuntu VDI
5.6 Enumerations(Enums)
5.7 Functional Specifications
5.7.1 Access Management Module
5.7.2 PermissionGroups file
5.7.3 Access Management API
5.7.4 Access Management Entities
5.7.5 Access Management Use Cases
5.7.6 Access Management Permissions
5.7.7 Access Management - RBAC Overview
5.7.8 Factory Module
5.7.9 Detailed Information on Hierarchies
5.7.10 Factory Module APIs
5.7.11 Factory Module Documents
5.7.12 Factory Module Entities
5.7.13 Factory Module Messages
5.7.14 Wireframes Factory Modeling
5.7.15 Factory Module Permissions
5.7.16 Material Management Module
5.7.17 Material Management Module APIs
5.7.18 Material Management Module Documents
5.7.19 Material Management Module Entities
5.7.20 Material Management Module Messages
5.7.21 Material Management Module Use Cases
5.7.22 Material Management Module Permissions
5.7.23 Order Management Module
5.7.24 Order Management Module APIs
5.7.25 Order Management Module Documents
5.7.26 Order Management Module Entities
5.7.27 Order Management Module Messages
5.7.28 Order Management Module Use Cases
5.7.29 Order Management Module Permissions
5.7.30 TrackAndTrace Module
5.7.31 TrackAndTrace Module APIs
5.7.32 TrackAndTrace Module Entities (2207 version)
5.7.33 TrackAndTrace Module Messages
5.7.34 TrackAndTrace Module Use Cases
5.7.35 TrackAndTrace Module Permissions
5.8 Introducing Kustomize
5.8.1 ModMOM - SqlServer
5.9 List Persistence - Configuration & Characteristics
5.10 Local Development Environment Setup in Linux VDI
5.11 Model configuration
5.12 Metadata Model Query Definitions
5.13 Model Configuration
5.13.1 Accessibility Configuration of CO Fields
5.14 ModularMOM 2.0 Private Registry
5.15 ModularMOM 2.x Build and Images Versioning
5.16 Modular MOM on Cloud
5.17 Nightly Test Run Follow-Up Plan 2.x
5.18 Non Functional Requirements
5.18.1 NFR Dashboards
5.18.2 NFR Parking Lot
5.18.3 Supportability - Diagnostics
5.18.4 Supportability - Documentation
5.18.5 Supportability - Logging
5.18.6 Supportability - Tools
5.19 Observability Tools
5.20 Openshift Internal Project Links
5.21 [Partially Obsolete] Metadata Module Architectural Concept
5.22 Platform & Model Versioning


5.23 Postman Examples
5.24 Production Environment Allocation
5.25 Required Fields on Persistent Entities
5.26 SaaS Architecture
5.26.1 Product Definition
5.26.2 User Workflows
5.27 Sample Data (version 2112)
5.27.1 Sample data for Modular MOM 2.X - 2112 release
5.28 Selection Values
5.29 Solution Structure
5.30 SQL Server on deployment
5.31 System Objects & Persistence
5.32 Technological Upgrade
5.32.1 Upgrade to .NET 6
5.33 User code execution risk & mitigation
6 Test Activities
6.1 Code coverage using Unit Test
6.2 Calculation of Code Coverage in Pipeline
6.3 Debugging BDD API Framework and Common Steps
6.4 Executing BDD API Tests with AspNetCore Test Server
6.4.1 Executing BDD API Tests with AspNetCore Test Server in the Release Pipeline
6.4.2 Executing BDD API Tests with AspNetCore Test Server on VDI
6.5 How to run BDD UI against deployed environment
6.6 Test Strategy (external link)
7 Platform Architecture
7.1 Internal Release Notes
7.1.1 Platform Version 1.0.14 (latest)
7.2 Third-Party Software Used in Platform
7.3 Configuration Layer (Former Governance Layer)
7.3.1 Application Configuration and Deployment
7.3.2 Appsettings.json - Messaging Configuration
7.4 Information Layer
7.4.1 Cache Strategy
7.4.2 Query Strategy
7.5 Integration Layer
7.5.1 Dapr Analysis
7.5.2 Message resiliency - Detailed Design
7.5.3 Module name resolution
7.6 Observability Layer
7.6.1 Instrumenting Applications
7.6.2 OpenTelemetry
7.6.3 Observability Architecture
7.6.4 Reference Materials
7.6.5 Distributed Tracing
7.6.6 Metrics
7.6.7 Structured Logging
7.6.8 How to use observability data?
7.7 Runtime Layer
7.7.1 API Interaction
7.7.2 Exception Framework
7.7.3 Gateway Interaction - Pipeline call-chain
7.7.4 Healthchecks - K8 Healthiness probes
7.7.5 Localization Support for Metadata Runtime components and Platform
7.7.6 Spike: Localization Support for Metadata Runtime components
7.7.7 Unique Id Generation
7.8 Security Layer
7.8.1 Authenticated Api Request via Postman
7.8.2 Authorization Model
7.8.3 Dex Evaluation
7.8.4 Emissary Ingress (Ambassador API Gateway)
7.8.5 Enable Authentication in Business Module
7.8.6 Identity Brokering & SamAuth Integration
7.8.7 Oidc-Client library evaluation
8 Modules Architecture
8.1 ModMOM Modules Status
8.2 Application Catalog
8.3 Developing a Metadata Module
8.4 Configure CI/CD Pipeline for New Module Repository
8.5 Helm Chart for Module Deployments
8.6 Access Management
8.6.1 Access Management App
8.6.2 How to initialize Access Management
8.7 Factory Module
8.7.1 Factory App
8.8 Material Management Module
8.8.1 Material Management App
8.9 Order Management Module
8.9.1 Order Management App
8.10 TrackAndTrace Module


```
8.10.1 Track And Trace App
8.11 Modular MOM Configurator
8.11.1 Configurator - Architecture
8.11.2 API Specification
8.11.3 Git Integration
8.11.4 UI
```
Organization Modular MOM

Feature Team "Mauna Kea" (Genoa - Mountain)

Feature Team "Monviso" (Genoa - Mountain)

Feature Team "Pratapgad" (Pune - Fort)

Feature Team "Shivneri" (Pune - Fort)

Feature Team "Sinhagad" (Pune - Fort)

Photogallery

Distribution Lists

TimeZone issues - find a meeting slot

Agile Scrum Practice

Agile Scrum Practice Modular MOM2

User Story - Definition of Ready

User Story - Definition of Done

User Story - Definition of Done (checklist)

Overall Retrospectives (Sprint 1 to Sprint 23)

Scrum Events

TFS Azure DevOps

Communication Rules

Introduction to Opcenter Modular Manufacturing

Acronyms Definition

Glossary

Before you start

First Steps for Startup Users

Common Guidelines

```
Best practices for Git Branching and Merging
```
```
Building Redis on Windows for Development
```
```
[Obsolete] Deploy in Openshift from Feature Branch
```
```
Enabling LongPath Support in Windows
```
```
Git Procedures
```
```
Interfacing with ModularMOM Adopter Startups
```

```
Localization(L10N) support provided by Siemens Web Framework (SWF)
```
```
Monitoring Traces and Logs in Development Environment
```
```
Procedure for Pull Request
```
```
Viewing Logs from OpenShift with Kibana
```
CI/CD Processes and Practices

Process And Phases

```
Modular Manufacturing 2112
```
```
ModularMOM 2.0 Private Registry
```
```
ModularMOM 2.x Build and Images Versioning
```
```
Authentication POC - tooling
```
Integration and Deployment Tasks

```
Prepare
```
```
Configuration of Microk8s/Ubuntu VDI
Configure SQL Server on Ubuntu 20.04 VDI
Local Development Environment Setup in Linux VDI
Configure CI/CD Pipeline for New Module Repository
```
```
Develop
```
```
Deploy
```
```
[Obsolete] Deploy in Openshift from Feature Branch
```
CI/CD Configuration

```
Build
```
```
Build parameters
Variable groups
NET Version X
Build features
Feature: Code analysis
Feature: Digital signature of binaries
Feature: Publish a stable version
Release
Deployment
Helm Chart for Module Deployments
ModularMoM Helm Catalog
Delivery
Monitoring
Observability Tools
```
```
Release
```
```
Deployment
```
```
Helm Chart for Module Deployments
```
Delivery

Monitoring

```
Observability Tools
```
Development Activities

```
Create a Feature Branch
```

```
Run a development build
```
```
Deploy to Openshift environment
```
Integration Activities

```
Manage Pull Request
```
```
Check/verify the integration release
```
```
Deploy an integration release to a dev environment
```
Delivery Activities

```
Run a stable build
```
DevOps Activities

```
Configure CI/CD Pipeline for New Module Repository
```
Internal Documentation

SF experts exchange topics

App Creation Workshop

```
Business Process Flow & Epics
```
Authentication POC - tooling

Configure SQL Server on Ubuntu 20.04 VDI

Configuration of Microk8s/Ubuntu VDI

Enumerations(Enums)

Functional Specifications

```
Access Management Module
```
```
PermissionGroups file
```
```
Access Management API
```
```
Access Management Entities
```
```
Access Management Use Cases
```
```
Access Management Permissions
```
```
Access Management - RBAC Overview
```
```
Factory Module
```
```
Detailed Information on Hierarchies
```
```
Factory Module APIs
```
```
Factory Module Documents
```
```
Factory Module Entities
```
```
Factory Module Messages
```
```
Wireframes Factory Modeling
```
```
Factory Module Permissions
```

```
Material Management Module
```
```
Material Management Module APIs
```
```
Material Management Module Documents
```
```
Material Management Module Entities
```
```
Material Management Module Messages
```
```
Material Management Module Use Cases
```
```
Material Management Module Permissions
```
```
Order Management Module
```
```
Order Management Module APIs
```
```
Order Management Module Documents
```
```
Order Management Module Entities
```
```
Order Management Module Messages
```
```
Order Management Module Use Cases
```
```
Order Management Module Permissions
```
```
TrackAndTrace Module
```
```
TrackAndTrace Module APIs
```
```
TrackAndTrace Module Entities (2207 version)
```
```
TrackAndTrace Module Messages
```
```
TrackAndTrace Module Use Cases
```
```
TrackAndTrace Module Permissions
```
Introducing Kustomize

```
ModMOM - SqlServer
```
List Persistence - Configuration & Characteristics

Local Development Environment Setup in Linux VDI

Model configuration

Metadata Model Query Definitions

Model Configuration

```
Accessibility Configuration of CO Fields
```
ModularMOM 2.0 Private Registry

ModularMOM 2.x Build and Images Versioning

Modular MOM on Cloud

Nightly Test Run Follow-Up Plan 2.x

Non Functional Requirements

```
NFR Dashboards
```
```
NFR Parking Lot
```

```
Supportability - Diagnostics
```
```
Supportability - Documentation
```
```
Supportability - Logging
```
```
Supportability - Tools
```
Observability Tools

Openshift Internal Project Links

[Partially Obsolete] Metadata Module Architectural Concept

Platform & Model Versioning

Postman Examples

Production Environment Allocation

Required Fields on Persistent Entities

SaaS Architecture

```
Product Definition
```
```
User Workflows
```
Sample Data (version 2112)

```
Sample data for Modular MOM 2.X - 2112 release
```
Selection Values

Solution Structure

SQL Server on deployment

System Objects & Persistence

Technological Upgrade

```
Upgrade to .NET 6
```
User code execution risk & mitigation

Test Activities

Code coverage using Unit Test

Calculation of Code Coverage in Pipeline

Debugging BDD API Framework and Common Steps

Executing BDD API Tests with AspNetCore Test Server

```
Executing BDD API Tests with AspNetCore Test Server in the Release Pipeline
```
```
Executing BDD API Tests with AspNetCore Test Server on VDI
```
How to run BDD UI against deployed environment

Test Strategy (external link)


Platform Architecture

Internal Release Notes

```
Platform Version 1.0.14 (latest)
```
Third-Party Software Used in Platform

Configuration Layer (Former Governance Layer)

```
Application Configuration and Deployment
```
```
Appsettings.json - Messaging Configuration
```
Information Layer

```
Cache Strategy
```
```
Query Strategy
```
Integration Layer

```
Dapr Analysis
```
```
Message resiliency - Detailed Design
```
```
Module name resolution
```
Observability Layer

```
Instrumenting Applications
```
```
OpenTelemetry
```
```
Observability Architecture
```
```
Reference Materials
```
```
Distributed Tracing
```
```
Metrics
```
```
Structured Logging
```
```
How to use observability data?
```
Runtime Layer

```
API Interaction
```
```
Exception Framework
```
```
Gateway Interaction - Pipeline call-chain
```
```
Healthchecks - K8 Healthiness probes
```
```
Localization Support for Metadata Runtime components and Platform
```
```
Spike: Localization Support for Metadata Runtime components
```
```
Unique Id Generation
```
Security Layer

```
Authenticated Api Request via Postman
```
```
Authorization Model
```

```
Dex Evaluation
```
```
Emissary Ingress (Ambassador API Gateway)
```
```
Enable Authentication in Business Module
```
```
Identity Brokering & SamAuth Integration
```
```
Oidc-Client library evaluation
```
Modules Architecture

ModMOM Modules Status

Application Catalog

Developing a Metadata Module

Configure CI/CD Pipeline for New Module Repository

Helm Chart for Module Deployments

Access Management

```
Access Management App
```
```
How to initialize Access Management
```
Factory Module

```
Factory App
```
Material Management Module

```
Material Management App
```
Order Management Module

```
Order Management App
```
TrackAndTrace Module

```
Track And Trace App
```
Modular MOM Configurator

```
Configurator - Architecture
```
```
API Specification
```
```
Git Integration
```
```
UI
```

# App Catalogue

## Background

This section has been added to document the Apps being developed and in process of development, based on Modular MOM 2, by Modular MOM team
as well as by other SUs. Each App getting listed here is expected to provide the information about about - team owning the app, capabilities being offered,
overview of data-model and best practices guide for others to reuse the app if required.


# Access Management App

## Ownership

Modular MOM SU

The User Management App creates and manages Users, Roles and Groups. It also provides the RBAC to modular MOM apps by allowing the user to
manage the permissions assigned to the roles (out of scope for Dec MVP).

Supported Use-Cases

```
Import Users
Create/Edit/Delete Users
Create/Edit/Delete Groups
Create/Edit/Delete Roles
Assign permissions to Roles
```

### 1.

### 2.

# Factory App

## Ownership

Modular MOM SU

The Factory App creates and manages factory hierarchy model based on ISA95 standard.

Supported Use-Cases

```
Create a hierarchy item
Delete a hierarchy item
```

# Failure Catalog App

## Ownership

Quality SU

The Failure Catalog App is developed by the MMOM Quality Engineering Team within the Quality Startup.
It manages the master data of all potential failures which can occur on the shop floor.
The failure structure can be downloaded from the Teamcenter Quality Failure Catalog over AIG.

Supported Use-Cases

```
Tree Overview of Failures
CRUD Operations
Release Workflow
Revisioning and History Overview
Drag & Drop of Failures
Assignment of template Actions
```
Implemented Attributes

```
ID: string
Version: integer
Name: string
Path: path within a tree
Description: string
Latest: bool
Status: Draft, Released, Deprecated)
Term Type: Place, Location, Position, Type, Condition
Classification: Main, Minor, Critical, etc.
```
Screenshot of the App


# Inspection Evaluation Shopfloor App

## Ownership

Quality SU

The Inspection Evaluation Shopfloor App is developed by the MMOM Quality Engineering Team within the Quality Startup.

Supported Use-Cases

```
Acquire measurement data
```
Screenshot of the App


# Inspection Planning Shopfloor App

## Ownership

Quality SU

The Inspection Planning Shopfloor App is developed by the MMOM Quality Engineering Team within the Quality Startup.
It creates and manages Inspection Operations and Inspection Definitions to plan quality inspections.
The Inspection Operations can be downloaded from the Teamcenter Quality CP&IP application over AIG.

Supported Use-Cases

```
Tree Overview of Inspection Operations and Inspection Definitions
CRUD Operations for all objects
Release Workflow
Revisioning and History Overview
Management of a central Characteristic library
```
Screenshot of the App


# Material Management App

## Ownership

Modular MOM SU


# Order Management App

## Ownership

Modular MOM SU


# Track And Trace App

## Ownership

Modular MOM SU


# Modular MOM Configurator

Modular MOM Configurator is a tool provided by the platform, which can be used to configure the Modular MOM ecosystem.

The tool provides an user interface where users can configure their Apps, Models, Environments, Deployments etc.

It also enables the users to design their Models such as Domain and Message Models, define Model dependencies, design the Structure and behavior of
the Configurable Objects


# API Specification

APIs for System Configuration and the MetadataConfigurator

```
API Inputs Source
Data
```
```
Processing Returns
```
```
Get list of models -
```
```
GET SystemConfig/CatalogModels
```
```
none models.
catalog.json
```
```
Select all models
```
```
Order by editability, name (editable
models first)
```
```
Array of:
```
```
Name, repoName, versions[],
isEditable, currentVersion
{major, minor, patch},
```
```
SemanticVersions[]{major,
minor, patch}
Get a model -
```
```
GET SystemConfig/CatalogModels/
{modelName}
```
```
QueryParms: none,
```
```
Body: none,
```
```
Route:
modelName, string,
case in-sensitive
```
```
mo
del
s.
cat
alo
g.
json
```
```
Select current version of model
specified. This is the first item in the
Versions array.
```
```
Name, repoName, versions[],
isEditable, currentVersion, composed
models
```
```
Get list of Configurable Fields :
```
```
GET ConfigurableObject/{modelName}/{objectN
ame}/Field?includeInherited=false
```
```
Route: modelName, string,
case sensitive, ObjectName
```
```
QueryParms: includeInherited
```
```
Body: none
```
```
Metadata
repository
```
```
Read fields from metadata
```
```
If includeInherited, include inherited and
overrides
```
```
Name, Label, FieldType,
AssociatedObjectType, IsList,
IsOverride, DefinedIn, Description
```
```
Get a Configurable Field :
```
```
GET ConfigurableObject/{modelName}/{objectN
ame}/Field/{fieldName}
```
```
Route: modelName, string,
case sensitive, ObjectName,
FieldName
```
```
QueryParms: None
```
```
Body: None
```
```
Metadata
repository
```
```
Read field from metadata Field details, DefinedIn, IsOverride
```
```
Get an object from a model:
```
```
GET ConfigurableObject/{modelName}/
{objectName}
```
```
Route:
modelName, string,
case sensitive,
objectName
```
```
QueryParms: none
```
```
Body: none
```
```
Metadata
repository
```
```
Read object from metadata Configurable Object details, excluding
fields, maps, logic etc.
```
```
Get the objects defined in a model, optionally
including objects in submodules:
```
```
GET ConfigurableObject/{modelName}/?
includeInherited=false
```
```
Route: ModelName
```
```
QueryParms:
includeInherited
```
```
Body: none
```
```
Me
tad
ata
rep
osi
tory
```
```
Location = an array of containing
folders up to the project root.
```
```
If (includeInherited == false &&
isFromComposedModel == true )
```
```
Skip item
```
```
[isFromComposedModel,
DefinedInModel, location,
Name] sorted by
```
```
isFromComposedModel(false
first), DefinedInModel,
location, Name
Add a Configurable Field to a Configurable Object :
```
```
POST ConfigurableObject/{modelName}/
{objectName}/Field/{fieldName}
```
```
Route: modelName, objectNa
me, fieldName
```
```
QueryParms: None
```
```
Request Body:
```
```
{
"name": "string",
"isList": boolean,
"dataTypeTrait": {
"fieldDataType": "string"
}
}
```
```
Metadata
repository
```
```
Create backing field "{fieldName}Field" in
{objectName} class
```
```
Create property {fieldName} in {objectName}
class
```
```
Create property in {objectName}.json
```
```
Added Field details
```
```
Get Configurable Object Hierarchy:
```
```
GET ConfigurableObject/{modelName}/
{objectName}/Hierarchy?
includeAncestors=false
```
```
Route: ModelName,
ObjectName
```
```
QueryParms:
includeAncestors
```
```
Body: none
```
```
Metadata
repository
```
```
Get list of objects that have {objectName} as
an ancestor
```
```
If (includeAncestors == true)
```
```
Add all of the ancestor objects up through
BaseObject
```
```
Ancestors and Descendants collection
in the output response.
```
```
{
```
```
"ancestors": [],
```
```
"descendents": []
```
```
}
Override a configurable object from composed
model
```
```
POST ConfigurableObject/{modelName}/
{objectName}/Override
```
```
Route: ModelName,
ObjectName
```
```
QueryParms: none
```
```
Body: None
```
```
Metadata
repository
```
```
Create new class file in {location}
Create new json sidecar in {location}
```
```
Configurable Object details, excluding
fields, maps, logic etc.
```
```
Create a new editable model, not based on
existing models
```
```
POST SystemConfig/CatalogModel/
{modelName}
```
```
QueryParms: none
Body: initial version,?
repoName, modelType
(Message/Domain)
Route: modelName
```
```
overrides.
models.
catalog.json,
overrides.
apps.config.
dev.json,
{modelName
```
```
Set {repoName} to {modelname}
Update overrides.models.catalog.json
Add an element into the models array
Add {initialVersion} to versions array.
Create new {reponame} repo
Perform logic currently done by Copy-From-
Template to create the new devspace
```
```
Name, repoName, versions[],
isEditable, currentVersion, composed
models
```

```
}.
modelconfig.
json
```
```
Update code/config/{modelName}.
modelconfig.json
Commit changes to devspace
```
Create a new version of an editable model

POST SystemConfig/CatalogModel/
{modelName}/Version/{version}

```
QueryParms: none
Body: none
Route: modelName, version
```
```
overrides.
models.
catalog.json,
{modelName
}.
modelconfig.
json
```
```
Update overrides.models.catalog.json
insert {newVersion} as element 0 into
versions array.
Create new tag in repo found in overrides.
models.catalog.json
Update version in code/config/{modelName}.
modelconfig.json
```
```
Name, repoName, versions[],
isEditable, currentVersion, composed
models
```
Create a new model to hold extensions of a non-
editable model

POST SystemConfig/CatalogModel/
{baseModelName}/Extend

```
QueryParms: none
Body: initial version,
repoName, modelName
Route: baseModelName
```
```
overrides.
models.
catalog.json,
{modelName
}.
modelconfig.
json
```
```
Update overrides.models.catalog.json
insert {newVersion} as element 0 into
versions array.
Create new tag in repo found in overrides.
models.catalog.json
Set {modelName} to {baseModelName}
_Extensions
Set {repoName} to {modelname}
Update overrides.models.catalog.json
Add an element into the models array
Add {initialVersion} to versions array.
Create new {reponame} repo
Perform logic currently done by Copy-From-
Template to create the new devspace
Add current version of {baseModelName} as
submodule to {modelName}
Update code/config/{modelName}.
modelconfig.json
Version and submodule
```
```
Name, repoName, versions[],
isEditable, currentVersion, composed
models
```
Delete a model

DELETE SystemConfig/CatalogModel/
{modelName}/{deleteRemote}

```
QueryParms: deleteRemote
Body: none
Route: {modelName}
```
```
overrides.
models.
catalog.json,
{modelName
}.
modelconfig.
json
```
```
Update overrides.models.catalog.json
Find and remove the element into the
models array
Remove the local and remote repos
```
```
200 - OK
```
Add a dependency to a model at a specific
position. Defaults to "append"

POST SystemConfig/CatalogModel/
{modelName}/Dependency

```
Route: {modelName}
QueryParms: none
Body:
DependencyType (Enum =
Model, Binary)
DependsOnName
Version
Position=-1
```
```
{modelName
}.
modelconfig.
json
```
```
If {position} == -1, position = array.length
Update code/config/{modelName}.
modelconfig.json
Add dependency at position in the array
indicated by {dependencyType}
```
```
200 - OK
```
Update the version of a dependency

PUT SystemConfig/CatalogModel/{modelName}
/Dependency

```
Route: {modelName}
QueryParms: none
Body:
DependencyType (Enum =
Model, Binary)
DependsOnName
NewVersion
```
```
{modelName
}.
modelconfig.
json
```
```
Update code/config/{modelName}.
modelconfig.json
Update version property of {modelName}
entry in array indicated by {dependencyType}
```
```
200 - OK
```
Remove a dependency to a model

DELETE SystemConfig/CatalogModel/
{modelName}/Dependency

```
Route: {modelName}
QueryParms: none
Body:
DependencyType (Enum =
Model, Binary)
DependsOnName
```
```
{modelName
}.
modelconfig.
json
```
```
Update code/config/{modelName}.
modelconfig.json
Add dependency from array indicated by
{dependencyType}
```
```
200 - OK
```
Update model

POST SystemConfig/Model/{modelName}

```
Route: {modelName}
```
```
QueryParms: none
```
```
Request body
```
```
{
"platformVersion": "string",
"description": "string",
"activeVersion": "string"
}
```
```
{modelName
}.
modelconfig.
json
```
```
Update {modelName}.modelconfig.json 200 - OK
```
Get Configurable Objects V2

POST ConfigurableObject/{modelName}

```
Route: ModelName
```
```
QueryParms: None
```
```
Request body:
```
```
{
```
```
includeComposedModels:
bool = true
```
```
includeFramework: bool = true
```
```
isTypeOf: string = null
```
```
isSubclassOf: string = null
```
```
Sort: enum = Project
```
```
}
```
```
Metadata
repository
```
```
Get list of all objects in the model,
submodules and framework based on the
input filters.
```
```
Response object:
```
```
{
```
```
Location: "Framework",
```
```
Name: "BaseObject",
```
```
Inherits: "",
```
```
IsEditable: false,
```
```
IsFramework: true,
```
```
IsOverride: false,
```
```
definedInModel: "",
```
```
isFromComposedModel: true,
```
```
}
```
```
It will return an array of these response
objects.
```

Create the definition of a map between two Objects

POST ConfigurableObject/{modelName}/
{sourceObjectName}/Map/{mapName}

```
Route: modelName,
sourceObjectName,
mapName
QueryParms:
Body:
targetObjectName
className
```
```
Metadata
repository
```
```
Create new file "{sourceObjectName}_Maps"
if needed.
The location of this file is the same as the
{sourceObjectName}.cs file
Create new class called {className}
Inherited from CDOMap
With TSource and TTarget generics and
constraints
Add CDOMapName attribute for {mapName}
Add override method for InitializeFieldMaps()
including a call to base
```
```
ConfigurableObjectMap details.
```
Get a list of maps

GET ConfigurableObject/{modelName}/
{ObjectName}/Maps

```
Route: ModelName,
ObjectName
QueryParms: None
Body: None
```
```
Metadata
repository
```
```
Union of list of maps where ObjectName is the
source and where ObjectName is the target
Calculation of count of mapped fields
```
```
Name: CDOMapName
attribute (which should be
COMapName)
```
```
IsSource: bool
```
```
IsTarget: bool
```
```
Source: SourceObjectName
```
```
Target: TargetObjectName
```
```
MappedFieldCount: count of
fields mapped in this definition
```
```
TotalMappedFields: count of
fields mapped including
inherited
```
```
IsOverride: (Parent !=
"CDOMap") (which should
also be COMap)
```
Delete the definition of a map between two Objects

DELETE ConfigurableObject/{modelName}/
{sourceObjectName}/Map/{mapName}

```
Route: Model,
SourceObjectName,
mapName
QueryParms:
Body:
```
```
Metadata
repository
```
```
Remove CDOMapName attribute for
{mapName}
Find the class name for the {mapName}
Remove class called className
```
```
200 - OK
```
Override attributes of a field in a submodule

POST ConfigurableObject/{modelName}/
{objectName}/Field/{fieldName}/Override

```
Route: modelName,
objectName, FieldName
```
```
QueryParms:
Body:
ObjectType - Optional
FieldDefinition
```
```
Metadata
repository
```
```
If ({ObjectType} differs)
Create property {fieldName} in {objectName}
extension file with "new" keyword and the new
ObjectType
Create property in {objectName}.json
```
```
Field details as defined in GetField
```
Create/Update field mapping - Configure the
mapping of data to a target field

POST ConfigurableObject/{modelName}/
{sourceObjectName}/Map/{mapName}/Field

```
Route: Model,
SourceObjectName,
mapName
QueryParms:
Body:
SourcePropertyName
TargetPropertyName
MappingStrategy (Enum)
```
```
Metadata
repository
```
```
Add or replace a line to the InitializeFieldMaps
method in the map definition with the
targetFieldName:
mfieldMaps["{targetFieldName}"] = (src, tgt)
=> { tgt.{targetFieldName} =
{sourceExpression} }
```
```
200 - OK
```
Get the list of field mappings for a map

GET ConfigurableObject/{modelName}/
{sourceObjectName}/Map/{mapName}/Field?
includeInherited=false

```
Route: Model,
SourceObjectName,
mapName
QueryParms: includeInherited
Body:
```
```
Metadata
repository
```
```
Get the fields mapped explicitly in this
{mapName}
If ({includeInherited}) Add the fields mapped in
ancestor maps
```
```
mapName
targetFieldName
SourceExpression
IsOverride
```
Update the details of a configurable object in the
current model

PATCH

ConfigurableObject/{modelName}/{objectName}

```
Route: modelName, string,
case sensitive, objectName
QueryParms: none
Body:
Configurable object details
List of attributes to "clear"
```
```
Metadata
repository
```
```
For each entry in the "Clear" list, remove the
value of the attribute.
```
```
For each field in the detail that has a value,
update the object with that value.
```
```
Configurable Object details, excluding
fields, maps, logic etc.
```

# Build and Deployment

## Deployment Diagram


# Configurator - Architecture

## Architecture Diagram

Here is a high level architecture diagram that shows two modes of user interaction. The top one shows the user interaction through the Configurator tool
and the bottom one shows Advanced User mode where the user interacting with the code and Git server outside of the Configurator

## API Code Flow

The following diagram shows how the API invocation from a client flows through different layers of the codebase and persisted as C# and JSON files
before being pushed to a remote Git server.



# Git Integration


# Squashing Git Repositories for release

## Overview

Along with the Modular Mom Platform, several 'out of the box' modules will be included with the release. Each module is shipped in the form of a git
repository containing full source code. Shipping open code is necessary for the override mechanism built into the platform to work. At build time binaries
are created by weaving sources from out of the box modules and custom models extensions. Because only released versions of out of the box modules
are supported for override, the git repositories shipped to customers must have collapsed history that reflects only versions of the repository that are
available for consumption. To achieve this, the repositories are 'squashed' in which a collection of changes are consolidated and re-written to the
repository as a single change. Each of the consolidated changes (repository commits) then represents a known good release version and will have a
corresponding tag representing that version. The diagram below depicts this process. The left side shows a repository with it's full history and two different
points where a submodule references was first made, then updated. The right side shows the Squashed version of that same repository. Since there are 4
released versions, there are 4 corresponding commits only. Each subsequent commit represents all changes since the last released version. A repository
in this state can then be used as a submodule for other development ensuring it is only consumed by another module in a released state. In the full history
shown on the left, the submodule references are made to specific commit points or 'hashes'. These can be at any point in the evolution of the code base.
However, with shipped modules, the submodule relationships are established only on release versions. To achieve this semantic tags are added to each
'out of the box' repository and references between them are established by tags (versions) and not by specifying an underlying commit hash. In the image
below commit 11 referennce the common module at commit hash ea234. In the squashed version, it references the v2.0.0 (released) version of
M1_Common.

## Order of operations

Because there are many interrelationships among modules, rewriting their history must be done in a bottom up manner to preserve a relative relationships
to other repositories. For example, the image above, at commit #7, a submodule (M1_Common) was added to the model. The first tag release of the
M1_POC after the submodule was introduced is 1.0.2. Therefore in the squashed version of M1_POC on the right, the submodule reference must similarly
be present on version 1.0.2. On the left, at commit #11, the submodule was updated. Then next release after the submodule change was 1.0.3 and this is
the collapsed tag version where the submodule change is recorded. In the example above, two submodules are shown.


In the example below a third repository is included to demonstrate a typical use case.

The above hierarchy of repositories must be processed in the specific order outlined.

Squash use case - Main Module with one Sub Module

Each repository evolves independently. When a submodule reference is established, it's done so at a point in time that may or may not correspond to a
release version. When a main module is at release version 1.0.0 and a submodule is subsequently added, the effective version the submodule appears in
the main module is 2.0.0. This is because 2.0.0 is the first released version in which the submodule appears. The diagram below shows a full commit


history of the Main Module and the various places where the Submodule relationship was added or updated. In the squashed versions of these
repositories, these relationships must be reestablished relative to its effective version. The diagram below depicts the full and squashed history and the
relative relationships to the submodule.


# System Configurations


# UI


### 1.

### 2.

### 3.

### 4.

```
a.
b.
5.
a.
```
```
b.
```
```
c.
```
```
d.
```
```
i.
e.
6.
a.
i.
```
```
ii.
```
```
b.
i.
1.
```
### 1.

### 2.

### 3.

# Configurator Development

## Setup

```
Clone the Configurator repo (in this example to C:\Repos)
Create a directory for the runtime devspace (C:\Devspace)
Copy C:\Repos\Configurator\Devspace\MetadataRuntime to C:\Devspace\MetadataRuntime
Kick-off ui dependencies load \Repos\Configurator\UI\ConfiguratorUI directory
npm install
npm run build
Open C:\Repos\Configurator\API\Configurator.sln
Right-click on Siemens.MOM.Configurator.API project and Add a new item of type Application Settings called appsettings.development.
json
Set the devspace and local user. The userId isn't validated, so any string value should work.
```
```
appsetttings.development.json
```
### {

```
"Devspace": {
"rootPath": "C:\\Devspace",
"userId": "userId"
}
}
```
```
If you're running in a vdi, the settings in the appsettings.json should work without an override. If you're running outside of the vdi network,
add the following property to the appsettings.development.json file
```
```
appsetttings.development.json user data
```
### {

```
"SourceControl": {
"GitProfile": {
"Username": "swqa\\username",
"AccessToken": "cleartextpw"
}
}
}
```
```
Change C:\Repos\Configurator\API\Siemens.MOM.Configurator.Api\Properties\launchSettings.json (might have to run once to auto-gen
the file)
the ports are 5001 for https and 5000 for http
Build the solution
Setup UI runtime
If you are not going to be making changes to the UI frequently:
Open IIS manager and add a virtual directory called Configurator that points to C:
\Repos\Configurator\UI\ConfiguratorUI\out\site\configuratorUI.
This will allow you to open http:\\localhost\configurator to run the app much more quickly than the other method, but you have to
manually rebuild and reload.
If you are going to be making frequent changes
npm start
This will build and run the UI code in a "watched" mode so that any file changes are automatically rebuild and the UI
refreshed. It does take significantly longer to load the first time.
```
## Troubleshooting

Here are some things you can try to resolve some issues

nugets.framework or nuget.projectModel not found

This seems to crop up from time to time after a Visual Studio upgrade.

```
Open Visual Studio Installer
Using the "More" dropdown, select the Repair option.
You may need to restart your machine after this.
```
Problems inserting objects into solution

When one of the apis fails, the system can end up in an unstable and unknown state.


### 1.

### 2.

```
Delete your devspace user directory
Delete any existing user branches
```
If you're still hitting problems, you can open your appsettings.development.json file and specify a new "userId". This doesn't have to be a real Id, but it
should be unique, so a best practice is to include your name as part of the value or maybe add a few digits to the end of your current userId.


# Kubernetes Operators

This page describes some of the work on Kubernetes operators to support the Modular MOM 2 delivery.

## Tenant Operator

The Tenant Operator performs tasks associated with managing the lifecycle of tenants within the kubernetes environment.

```
Creating the tenant-specific namespace
Creating the tenant-specific databases
Deploying the tenant-specific infrastructure components into the namespace (such as Keycloak and the connections to Sumo Logic)
Deploying the components for either Modular Manufacturing or Quality products
In the event of a tenant termination action: Destroying all tenant-specific pieces in the kubernetes cluster
```
More details about the Tenant Operator can be found in the High-Level Design documentation.

## Technology

Following the best patterns available from the Kubernetes community, we are using a system called Operator SDK. This system adds a few extra helpers
to the defacto standard operator development system called Kubebuilder. All development is done in the Go programming language using go 1.17.x or
later.

The operator is able to handle both standard Kubernetes Namespaces and a new add-on called SubNamespaces provided by the Hierarchical
Namespace Controller (HNC). Most of the development will be taking place with the HNC. This provides some extra capabilities within a cluster that
simplify our SaaS operations.


# Quality MOM 2

```
Organization Quality MOM 2
Feature Team "Acquisition"
Definition of Ready (Acquisition Team)
Definition of Done (Acquisition Team)
Feature Team "Business"
Feature Team "Engineering"
Definition of Done (Engineering team)
Definition of Ready (Engineering team)
Story Points
Feature Team "Integration"
Definition of Done (Integration Team)
Definition of Ready (Integration team)
Functional Specification 2
Action Management
Action Management APIs
Action Management Entities
Action Management Messages
Failure Catalogue
Inspection Evaluation Shopfloor
IES - Module Permissions
Inspection Planning Shopfloor #
NonConformance Management
Variant Management
Internal Collaborative Content 2
Azure DevOps
Development
Access to database on OpenShift environment
Cheat sheets
Create and use local NuGet package
Working with MetadataRuntime
Optimistic Locking in ModMom Quality
Testing
UI BDD Tests
Development Runtime
Iteration Result
Operational Runtime (PLM)
```

# Organization Quality MOM 2


# Feature Team "Acquisition"

Our Standup meeting: Each morning at 9:15 except Iteration Planning Days.

```
Team Member Reiner Bögel (DI SW DM MOM R&D Q OQCC) <reiner.boegel@siemens.com>
```
```
Team Member Hafez Ezaiza (DI SW DM MOM R&D Q OQCC) <hafez.ezaiza@siemens.com>
```
```
Team Member Marcel Grosse (DI SW DM MOM R&D Q OQCC) <marcel.grosse@siemens.com>
```
```
Team Member Helmut Hähner (DI SW DM MOM R&D Q OQCC) <helmut.haehner@siemens.com>
```
```
Team Member Tahir Nazir (DI SW DM MOM R&D Q OQCC) <nazir@siemens.com>
```
```
Team Member Reichart, Dennis (DI SW DM MOM R&D Q OQCC) <dennis.reichart@siemens.com>
```
```
Team Member Dagmar Senft (DI SW DM MOM R&D Q OQCC) <dagmar.senft@siemens.com>
```
```
Team Member Ruben Sidon (DI SW DM MOM R&D Q OQCC) <ruben.sidon@siemens.com>
```
```
Product Owner Thomas Marx (DI SW DM MOM R&D Q QDA QDPO) <thomas.marx@siemens.com>
```
```
Scrum Master Gerald Brauneck (DI SW DM MOM R&D Q QDA ARCH) <gerald.brauneck@siemens.com>
```

# Definition of Ready (Acquisition Team)

Definition of Ready (DoR) is a list of criteria which have to be fulfilled by backlog items, before the item can be selected into the sprint/iteration.
For more information regarding the Siemens Guidelines see the link to the MOM Agile Practice Framework:
https://momwiki02.industrysoftware.automation.siemens.com/display/momqms/Reference+Guides#ReferenceGuides-DefinitionofReady

```
Item
```
```
The backlog item (user story/defect) should be small enough to be implemented within one iteration by one person
```
```
The backlog item (user story/defect) must have at least 1 acceptance criteria
```
```
The backlog item (user story/defect) and the acceptance criteria must be clearly understood of all involved team members
```
```
The backlog item (user story/defect) is estimated
```
```
A user story contains (Gherkin) test cases to follow BDD (Behavior Driven Development) approach (if possible)
```
```
Ist das ein Test Item unterhalb der User Story?
```
```
A user story must have a defined test case in Polarion (if possible)
```
```
Ist das ein Test Item unterhalb der User Story?
```

# Definition of Done (Acquisition Team)

The Definition of Done (DoD) describes the criteria which the development team must fulfill for the creation and completion of a product. The Definition of
Done is used to assess when work is complete on the product increment. The team must have a shared understanding what Done means. The Definition
of Done is also used to estimate how many items can be selected into the next sprint.

A list of criteria which are necessary to fulfill the Definition of Done will follow.

For more information regarding the Siemens Guidelines see the link to the MOM Agile Practice Framework:
https://momwiki02.industrysoftware.automation.siemens.com/display/momqms/Reference+Guides#ReferenceGuides-DefinitionofDone

```
Development
```
```
Acceptance
criteria All defined acceptance criteria are fulfilled
```
```
Coding
New Business Logic has been developed following the official development guidelines (SonarQube)
Code checked in and integrated into build
API Tests have been executed
No critical or high bugs are open
```
```
Unit Tests
Unit Tests will be provided, they will be carried out by using gated check-in builds. If they fail, check-in is not possible.
```
```
API Tests
API Tests will be provided
```
```
Code Review
Code review performed on new developments considered on every check-in. Evidence must be provided in Polarion for this
activity on implementation.
Pull requests will complete a story, a second developer must approve the PR, changes / recommendations must be
documented
```
```
SonarQube
Monitoring is part of the standard development activity. Each new development must not introduce additional issues with
respect to the targets.
After each check-in SonarQube must be carried out and its results must be proofed (should be done automatically by build
pipeline).
Analyze of SonarQube must be checked frequently for recommendations that do not block the check-in.
```
```
Availability
Changes are live on the QME-04 environment (Openshift)
```
```
Validation
```
```
Acceptance
Tests Review of the acceptance criteria on the deployment environment.
```

# Feature Team "Business"


# Feature Team "Engineering"


# Definition of Done (Engineering team)

The Definition of Done (DoD) describes the criteria which the development team must fulfill for the creation and completion of a product. The Definition of
Done is used to assess when work is complete on the product increment. The team must have a shared understanding what Done means. The Definition
of Done is also used to estimate how many items can be selected into the next sprint.

A list of criteria which are necessary to fulfill the Definition of Done will follow.

For more information regarding the Siemens Guidelines see the link to the MOM Agile Practice Framework:
https://momwiki02.industrysoftware.automation.siemens.com/display/momqms/Reference+Guides#ReferenceGuides-DefinitionofDone

```
Development
```
```
Acceptance
criteria All defined acceptance criteria are fulfilled
```
```
Coding
New Business Logic has been developed following the official development guidelines (SonarQube)
Code checked in and integrated into build
API Tests have been executed
No critical or high bugs are open
```
```
Unit Tests
Unit Tests will be provided, they will be carried out by using gated check-in builds. If they fail, check-in is not possible.
```
```
API Tests
API Tests will be provided
```
```
Code Review
Code review performed on new developments considered on every check-in. Evidence must be provided in Polarion for this
activity on implementation.
Pull requests will complete a story, a second developer must approve the PR, changes / recommendations must be
documented
```
```
SonarQube
Monitoring is part of the standard development activity. Each new development must not introduce additional issues with
respect to the targets.
After each check-in SonarQube must be carried out and its results must be proofed (should be done automatically by build
pipeline).
Analyze of SonarQube must be checked frequently for recommendations that do not block the check-in.
```
```
Availability
Changes are live on the QME-04 environment (Openshift)
```
```
Validation
```
```
Acceptance
Tests Finalizing acceptance test will be carried out by a developer who wasn’t involved in the development process
All acceptance criteria will be checked (given-when-then-approach)
Test case in Polarion must be executed by another developer (if existing)
```

# Definition of Ready (Engineering team)

Definition of Ready (DoR) is a list of criteria which have to be fulfilled by backlog items, before the item can be selected into the Sprint.
For more information regarding the Siemens Guidelines see the link to the MOM Agile Practice Framework:
https://momwiki02.industrysoftware.automation.siemens.com/display/momqms/Reference+Guides#ReferenceGuides-DefinitionofReady

```
The backlog item (user story/defect) should be small enough to be implemented within one iteration by one person
```
```
The backlog item (user story/defect) must have at least 1 acceptance criteria
```
```
The backlog item (user story/defect) and the acceptance criteria must be clearly understood of all involved team members
```
```
The backlog item (user story/defect) is estimated
```
```
A user story contains (Gherkin) test cases to follow BDD (Behavior Driven Development) approach
```
```
A user story must have a defined test case in Polarion (if possible)
```

# Story Points

The team has agreed on using story points which are not related to working days.
The reason for that is to have a more abstract view on the estimations for backlog items and considering the complexity of stories instead of estimating the
time needed.

To have a common understanding of the story points, the team agreed on the following reference stories.
All of them were estimated with a value of 3.
You can use it as a starting point for your estimations.

```
Story in
MyPolarion
```
```
Title Content of the story
```
```
DMQ-369 Create new Version of Failure Definition
Implementing the revision functionality for an existing meta model
concerns both backend and frontend
usage of a platform functionality that is already in use (possibility to check in other apps
on how to fulfill the requirements, copy-paste possible)
```
```
DMQ-3216 Upgrade to Modular MOM 2.x version for
all MOD Engineering Apps Upgrade of the platform version for all API projects
Testing the functionality after update is done
The work to do here was similar for all apps but despite from that needed some time to
check for any issues regarding the upgrade
```
```
DMQ-4137 Action Status "Template" as Flag
adding a new field to an existing meta model (backend + frontend)
modifying an existing enum (backend + frontend)
Using an UI element that wasn't used in any other app before
Modifying / extending tests (API tests, Unit tests, UI BDD tests)
```
Based on this, you can estimate stories by thinking about if the story to estimate has a higher or lower complexity.

For example if you have to do a small change in the UI only, this could be a 1.

If you have to add an element to the existing UI and backend where you are able to copy and paste an existing one, this could be a 2 (of course this could
also be a 1, but the (UI BDD) tests in mind, the effort could be a little higher).

If the story DMQ-4137 would also contain a complex business logic, this could be a 5 or 8 (as this would cause a more complex backend implementation
and testing).


# Feature Team "Integration"


# Definition of Done (Integration Team)

The Definition of Done (DoD) describes the criteria which the development team must fulfill for the creation and completion of a product. The Definition of
Done is used to assess when work is complete on the product increment. The team must have a shared understanding what Done means. The Definition
of Done is also used to estimate how many items can be selected into the next sprint.

A list of criteria which are necessary to fulfill the Definition of Done will follow.

For more information regarding the Siemens Guidelines see the link to the MOM Agile Practice Framework:
https://momwiki02.industrysoftware.automation.siemens.com/display/momqms/Reference+Guides#ReferenceGuides-DefinitionofDone

```
Development
```
```
Acceptance criteria
All defined acceptance criteria are fulfilled
```
```
Implementation
Script are implemented and reviewed
Preferences are identified and implemented
Workflows are identified and implemented
```
```
Unit Tests
Document created for Integration cases
Screenshots added where necessary
```
```
Integration
All dependent product limitations are listed
```
```
Code Review
Code review performed on new developments
```
```
The functionality has been tested for performance where applicable.
```
```
There must be no known open Bug by the time the US becomes resolved
```
```
Validation
```
```
Acceptance Tests
Finalizing acceptance test will be carried out by a Tester
All acceptance criteria w be checked (givillen-when-then-approach)
Test case in Polarion must be executed by tester(if existing)
```
```
Automation Tests
Automation Testing, we are doing regression testing (latest version).
Its feature DOD, once Automation testing is finished then only feature should be closed.
```

# Definition of Ready (Integration team)

Definition of Ready (DoR) is a list of criteria which have to be fulfilled by backlog items, before the item can be selected into the Sprint.
For more information regarding the Siemens Guidelines see the link to the MOM Agile Practice Framework:
https://momwiki02.industrysoftware.automation.siemens.com/display/momqms/Reference+Guides#ReferenceGuides-DefinitionofReady

```
Ordered in Product Backlog by Product Owner
User, Technical, and Architectural Stories discovered, refined, and estimated
Sprintable size (target is less than 1/3 of Sprint duration) achieved and readiness for acceptance reached (applicable)
Acceptance criteria developed and agreed upon
```
```
Dependencies are identified and implemented
```
```
Functional and non-functional requirements and designs elaborated and understood
```

# Functional Specification 2


# Action Management


# Action Management APIs

[http://api-gateway-qme-04.apps.openshift03.swqa.tst/actionapi/index.html](http://api-gateway-qme-04.apps.openshift03.swqa.tst/actionapi/index.html)


# Action Management Entities


# Action Management Messages


# Failure Catalogue


# Inspection Evaluation Shopfloor


# IES - Module Permissions

```
Name Effect Note Home Card Relevant
```
```
View Inspection Orders for Inspection Allows to view and filter the Inspection Order List
```
```
View Inspection Orders for Evaluation Allows to view and filter the Inspection Order List
```

# Inspection Planning Shopfloor #


# NonConformance Management


# Variant Management


# Internal Collaborative Content 2

```
Azure DevOps Content
Build pipelines
OpenShift Deployment
AWS Deployment
PLM Deployment
```
```
Environment Content
Overview
Repository Structure
Testing
Unit Tests
API Tests
BDD Tests
```

# Azure DevOps


# Development

Access to database on OpenShift environment

Cheat sheets

Create and use local NuGet package

Working with MetadataRuntime


### 1.

### 2.

### 3.

### 4.

```
a.
```
```
b.
```
```
c.
```
```
d.
```
# Access to database on OpenShift environment

You can use a port-forwarding feature to get access to databases located on OpenShift environments.

These are the steps to achieve this:

```
Download the OpenShift CLI (this will store the file oc.exe on your dev machine):
https://downloads-openshift-console.apps.openshift03.swqa.tst/
Copy the file oc.exe into the install folder of Git, this is usually
"C:\Program Files\Git\cmd"
By copying it to here, the necessary PATH variables are already set correctly.
Open a CMD instance and navigate to the folder where the oc.exe is located, in this example: "C:\Program Files\Git\cmd"
Get a token from OpenShift by opening this link:
https://console-openshift-console.apps.openshift03.swqa.tst/
```
```
Log in to OpenShift
```
```
Right-click on your user name on the upper right corner and select "Copy login command"
```
```
Log in again by clicking on "AD SWQA"
```
```
Click on "Display token"
```

### 4.

```
e.
```
### 5.

### 6.

a.
b.
c.
7.

### 8.

```
Copy the command from the section "Log in with this token" and execute it in the CMD instance that you've opened in step 3
```
```
The command "oc projects" lists all projects of the OpenShift environment, with "oc project" you can switch to a specific one:
```
```
After you've selected the project you want to access the database of, you can now execute the following command to set up a port forwarding:
```
```
oc port-forward sqlserver-0 5555:1433
```
```
sqlserver-0 ==> name of the SQL server instance of the OpenShift project
5555 => port that will be used on the local machine to connect the database
1433 => port of the remote SQL server on OpenShift
Open SQL Server Management Studio and type ", 5555" behind the server and instance name, for authentication select "SQL Server
authentication" and use "sa" as login:
```
```
To get the password, you can ask Alexander Kohl or someone else of the Mod Quality teams.
```

### 7.

### 8.

### 9.

```
a.
b.
```
```
If the connection succeeds, you are now able to access the SQL server's data on the OpenShift environment
If the connection fails, it is possible that the IPv6 address was used for the port forwarding, you can change this by:
deactivating IPv6 on the network interface of your dev machine
add "--address 0.0.0.0" to the port-forward command shown above => oc port-forward sqlserver-0 5555:1433 --address 0.0.0.0
```

# Cheat sheets

## Git

```
Check status of Git repo git status
```
```
Check out specific tag git checkout tags/{Tagname}
```
```
Get submodules (e.g. after cloning a repo referencing to the
submodules)
```
```
git submodule update --init --depth=1 --no-single-branch
```
```
Get latest changes of Git repo git fetch
```
```
Pull changes from remote to the local repository git pull
```
```
Push new branch and set as upstream git push --set-upstream origin <Name des neuen Branches>
```
```
Change of the upstream (e.g. after renaming the repo's
name on the Git server)
```
```
git remote set-url origin https://tfs05mom.industrysoftware.automation.siemens.com/MOM
/Quality/_git/<Repo-Name>
```
Hint: a very good learning web site for Git commands is the following: https://www.atlassian.com/git/tutorials

For example this is the page for a "git rebase": https://www.atlassian.com/git/tutorials/rewriting-history/git-rebase

## npm

```
Install npm packages npm i
npm install
npm ci (faster then "npm install" but only possible when package-lock.json exists)
```
```
Build UI npm run build
```
```
Start UI npm run start
```

### 1.

### 2.

### 3.

### 4.

# Create and use local NuGet package

If you want to try out a new functionality of an app that is used as a NuGet package in other apps (e.g. messaging apps like M1_ActionMessages), you can
create a local package on your dev environment.
You can then use that package in other apps (e.g. M1_Action) and try the new functionality before checking it in.

These are the steps how you can achieve this with Visual Studio 2022:

```
Open project settings and change the version number in section "Package"
```
```
Right-click on the app in the Solution Explorer and select "Pack"
```
```
Locate the .nupkg file that was created, usually in "..\bin\Debug", see Output of Visual Studio:
```
```
Open the app that will use the package in Visual Studio, e.g. M1_Action and open the NuGet Package Manager settings
```

### 4.

### 5.

### 6.

### 7.

### 8.

```
Add a new package source by browsing to the path where the .nupkg was stored and close the dialog with "OK"
```
```
Go to NuGet package in Package manager and select the local feed as package source and select the package, e.g. Siemens.MOM.
ActionMessages
Select the version that was created in first step in the "Version" drop down, then click on "Install"
```
```
Rebuild the project, it will now use the local NuGet package
```

### 1.

### 2.

### 3.

### 4.

### 5.

### 6.

# Working with MetadataRuntime

## Set up local development environment

tbd (cloning and setting up the MetadataRuntime repo locally)

## Debug a platform based API

With the following steps, you can run and debug a Modular MOM based API (e.g. "M1_Action") on your local development environment:

```
Build and start the API by executing the script StartModelAPI.cmd (a CMD window will open after the script is finished, you can directly close
this).
This will create all the necessary files and folders for the models you want to debug.
The folders will be created in the local repository of MetadataRuntime in the folder structure.
\MetadataRuntime\Platform\GeneratedModels_<Appname>, for example: .\MetadataRuntime\Platform\GeneratedModels_M1_Action
Open the solution "Platform.sln" of the MetadataRuntime repository in Visual Studio
```
```
Set "Siemens.MOM.Platform.Api" as the startup project, it should look like this:
```
```
Edit modelsettings.json so that it points to the specific directories of the app you want to debug.
Therefore you have to replace the entries with the name of the app to debug.
For example you want to debug "M1_Action", the file looks like this:
```
### {

```
"ModularMOM": {
"Metadata": {
"domainmodellocation": "..\\..\\GeneratedModels_M1_Action\\bin\\M1_Action.dll",
"inputmodellocation": "..\\..\\GeneratedModels_M1_Action\\bin\\M1_Action.Dto.dll",
"inputmodelnamespace": "M1_Action.Dto",
"requestmodelnamespace": "M1_Action.Request",
"responsemodelnamespace": "M1_Action.Response",
"writemodellocation": "..\\..\\GeneratedModels_M1_Action\\bin\\M1_Action.Write.dll",
"writemodelnamespace": "M1_Action.Write",
"messagemodellocation": "..\\..\\GeneratedModels_M1_Action\\bin\\M1_Action.Proto.dll",
"messagemodelnamespace": "Siemens.MOM.MessageModel"
},
"ModelInstrumentation": {
"Providers": [
"M1_Action"
]
}
}
}
```
```
Run Siemens.MOM.Platform.Api by clicking the Play button in Visual Studio:
```
```
From the Visual Studio instance where the Siemens.MOM.Platform.Api is running in debug mode, open a file from the app's repository you want
to debug by selecting File > Open > File...
For example you can navigate to .\M1_Action and open a service file from that repository.
You are now able to set breakpoints that will be hit.
```
## Update local development environment to new platform version


### 1.

### 2.

### 3.

### 4.

### 5.

### 6.

### 7.

### 1.

### 2.

### 3.

This is necessary if there's a new version of MetadataRuntime where apps you want to run/debug locally are referencing to.

If you do not update the local MetadataRuntime, you'll run in some failures when building other apps using BuildAndStartApi.ps1 or StartModelAPI.cmd

Prerequisites: MetadataRuntime is already checked out on local dev environment

```
Stop already running dotnet instances
Run CleanAll.ps1 script in .\MetadataRuntime
Open console in same directory
Run git fetch
Run git checkout tags/<Tagname> where <Tagname> is the specific tag for the new version, e.g. git checkout tags/v2.1.8
Drop local database to avoid data inconsistencies
Run BuildAll.ps1 script in .\MetadataRuntime
```
Possibly you experience some failures after that. This may occur due to some inconsistencies in the local repo.

To solve this, you can alternatively do the following:

```
Delete the local MetadataRuntime repository
Run git clone to get the sources again: git clone https://tfs05mom.industrysoftware.automation.siemens.com/MOM/ModularMOM/_git
/MetadataRuntime
Run git checkout tags/<Tagname>, e.g. git checkout tags/v2.1.8
```

# Optimistic Locking in ModMom Quality

## How To Implement Optimistic Locking

## Data Layer

```
Add a field "EditToken" to the entity
..
```
## UI


# Testing

UI BDD Tests


# UI BDD Tests

## Starting with UI BDD Tests

Markus recorded a session (in German) about how to start with UI BDD tests (using Cucumber).

This is helpful to get a quick overview on that.

The video can be found here: https://splm.sharepoint.com/:v:/r/sites/SUQualityART/Shared%20Documents/Quality%20Mod%20Quality/Development
/Testing/Quality%20SU%20-%20UI%20BDD%20Tests%20mit%20SWF6.0.mp4?csf=1&web=1


# Development Runtime

```
Url Purpose Content Notes
```
```
End-To-
End
testing
```
```
http://api-gateway-qualitysu-
01.apps.openshift03.swqa.
tst/
```
```
For Tester to execute end to end tests.
```
```
UI BDD
testing
```
```
http://api-gateway-qualitysu-
02.apps.openshift03.swqa.
tst/
```
```
For Tester to execute ui bdd tests.
```
```
Iteration
Result
```
```
http://api-gateway-qualitysu-
03.apps.openshift03.swqa.
tst/
```
```
To all folks who are interested in the
QualityStartUp progress.
```
```
Quality Iteration result and last
ModMom 2 Iteration result*)
*) please see notes
```
```
This project is only available for
InspectionAndEvaluation for a
limited time.
```
```
Release http://api-gateway-qualitysu-
04.apps.openshift03.swqa.
tst/
```
```
For PO, PrM, and others who need a view
on the current Release status.
```
```
Last Release of Quality and ModMom
2
```
```
Upgrade http://api-gateway-qualitysu-
05.apps.openshift03.swqa.
tst/
```
```
For Developer to test current upgrade
state of apps an ui.
```
```
Latest http://api-gateway-qualitysu-
06.apps.openshift03.swqa.
tst/
```
```
For Developer, PO, and others who need
immediate view on current development
state.
```
```
Closed Quality US are immediately
available and last ModMom 2 Iteration
result
```
```
swagger.json urls:
```
```
factorymodelingapi
```
```
materialmodelingapi
```
```
ordermanagementapi
```
```
trackandtraceapi
```
```
actionapi
```
```
nonconformancemanagementapi
```
```
inspectionplanningshopfloorapi
```
```
failurecatalogapi
```
```
variantmanagementapi
```
```
inspectionevaluationshopfloorapi
```
## Using Swagger UI

```
Install Swagger-UI Chrome Extension from here
Login by accessing one of the backend urls listed in column "Swagger" above
Copy backend url into the appropriate field
```


# Iteration Result

## I-2208-1 (Platform 2.1.10)

```
Module Version
```
```
Siemens.MOM.ActionMessages 2.0.0 https://artifacts.industrysoftware.automation.siemens.com/artifactory/nuget-local/Siemens.MOM.
ActionMessages.2.0.0.nupkg
```
```
Siemens.MOM.
FailureCatalogMessages
```
```
2.0.0 https://artifacts.industrysoftware.automation.siemens.com/artifactory/nuget-local/Siemens.MOM.
FailureCatalogMessages.2.0.0.nupkg
```
```
Siemens.MOM.
NonConformanceManagementMessa
ges
```
```
2.0.8 https://artifacts.industrysoftware.automation.siemens.com/artifactory/nuget-local/Siemens.MOM.
NonConformanceManagementMessages.2.0.8.nupkg
```
```
Action App 2.0.3 docker pull lanterna.industrysoftware.automation.siemens.com/qme-stable/modmom/action:2.0.3
```
```
InspectionPlanningShopfloor App 2.0.4 docker pull lanterna.industrysoftware.automation.siemens.com/qme-stable/modmom
/inspectionplanningshopfloor:2.0.4
```
```
InspectionEvaluationShopfloor App 2.0.3 docker pull lanterna.industrysoftware.automation.siemens.com/qme-stable/modmom
/inspectionevaluationshopfloor:2.0.3
```
```
FailureCatalog App 2.0.3 docker pull lanterna.industrysoftware.automation.siemens.com/qme-stable/modmom
/failurecatalog:2.0.3
```
```
NonConformanceManagement App 2.0.3 docker pull lanterna.industrysoftware.automation.siemens.com/qme-stable/modmom
/nonconformancemanagement:2.0.3
```
```
VariantManagement App 2.0.3 docker pull lanterna.industrysoftware.automation.siemens.com/qme-stable/modmom
/variantmanagement:2.0.3
```
```
Action Ui 1.0.2 docker pull lanterna.industrysoftware.automation.siemens.com/qme-stable/modmom/quality-
action:1.0.2
```
```
InspectionPlanningShopfloor Ui 1.0.1 docker pull lanterna.industrysoftware.automation.siemens.com/qme-stable/modmom/quality-
inspection-planning-shopfloor:1.0.1
```
```
InspectionEvaluationShopfloor Ui 1.0.1 docker pull lanterna.industrysoftware.automation.siemens.com/qme-stable/modmom/quality-
inspection-evaluation-shopfloor:1.0.1
```
```
FailureCatalog Ui 1.0.1 docker pull lanterna.industrysoftware.automation.siemens.com/qme-stable/modmom/quality-
failure-catalog:1.0.1
```
```
NonConformanceManagement Ui 1.0.1 docker pull lanterna.industrysoftware.automation.siemens.com/qme-stable/modmom/quality-non-
conformance-management:1.0.1
```
```
VariantManagement Ui 1.0.1 docker pull lanterna.industrysoftware.automation.siemens.com/qme-stable/modmom/quality-
variant:1.0.1
```

# Operational Runtime (PLM)

```
Link Purpose Content
```
```
Release or PI
Demo (2204)
```
```
AWS
```
```
Release or PI
Demo (2112)
```
```
AWS
```
```
https://modularmom.mod.sws.siemens.com
/inspectionplanningshopfloorui/
```
```
To all ModMom folks(?) who are interested in
ModMom progress
```
```
Last Releases of Mom Quality and
ModMom 2
```

# APS MOM 2


# Security


# Security Tasks

## Identified

```
Infrastructure / WAF configuration per web site / cf CloudFormation templates used for Website
```
```
Development / microservice rest api error management / cf wiki equipment ms
```
```
Bucket Policy for restricted siemens & cloudfront access (cf Bug 122417: CloudFront - invalid preprod / prod HomePage origin )
```
```
Operational common vulnerability handling in multi-bucket design (ref Bug 125319: preprod - Invalid Cognito User Pool configuration Bug 124603:
S3 Bucket hosting the SPA is public)
```

# Security Training

```
Awareness Training
Security Training
```
## Awareness Training

https://plconnect.industrysoftware.automation.siemens.com/org/MOM/PSS/GetStarted/Awareness%20Training.aspx

Legend: M=Mandatory; R=Recommended (few Feature Team members is enough)

```
Course Title Duration (min) Feature Team
Member
```
```
PO / SM
```
```
PSS-BT General Introduction 140 M M
```
```
PSS-BT Secure Coding 20 M
```
```
PSS-BT Security Threat and Risk Analysis 20 M M
```
```
PSS-BT Security Testing 20 M
```
```
PSS-BT Security Mechanisms 101 20 R M
```
```
PSS-BT Security Incident and Vulnerability Management 20 R M
```
```
PSS-BT Security Business Targets and Requirements 20 R R
```
```
PSS-BT Secure Supplier and Component Selection 20 M
```
```
PSS-BT Secure Field Service 20
```
```
PSS-BT Secure Configuration and Hardening 20 R
```
```
PSS-BT Secure Architecture and Design 20 M
```
```
Duration (h) 4 4
```
## Security Training

https://plconnect.industrysoftware.automation.siemens.com/org/MOM/PSS/GetStarted/Security%20Training.aspx

Legend: M=Mandatory; R=Recommended (optional, or few Feature Team members is enough)

```
Course Title Duration (min) Feature Team Member PO / SM
```
```
AWA 101 - Fundamentals of Application Security 60 M M
```
```
COD 101 - Fundamentals of Secure Development 60 M
```
```
COD 141 - Fundamentals of Secure Database Development 110 R
```
```
COD 152 - Fundamentals of Secure Cloud Development 30 M
```
```
COD 153 - Fundamentals of Secure AJAX Code 35 M
```
```
COD 215 - Creating Secure Code - .NET Framework Foundations 90 M
```
```
COD 253 - Creating Secure AWS Cloud Applications 60 M
```
```
COD 255 - Creating Secure Code Web API Foundations 120 M
```
```
COD 314 - Creating Secure C# Code 90 M
```
```
COD 351 - Creating Secure HTML5 Code 90 M
```
```
COD 352 - Creating Secure JQuery Code 90 M
```
```
DES 101 - Fundamentals of Secure Architecture 60 M M
```
```
DES 201 - Fundamentals of Cryptography 120 R
```
```
DES 212 - Architecture Risk Analysis and Remediation 60 M M
```
```
DES 311 - Creating Secure Application Architecture 120 R M
```
```
ENG 211 - How to Create Application Security Design Requirements 60 R M
```

```
ENG 311 - Attack Surface Analysis and Reduction 60 R
```
```
ENG 312 - How to Perform a Security Code Review 60 M
```
```
TST 101 - Fundamentals of Security Testing 120 R
```
```
TST 201 - Classes of Security Defects 60 R
```
```
TST 211 - How to Test for the OWASP Top 10 90 R
```
```
DES 214 - Securing Network Access 30 R
```
```
DES 215 - Securing Operating System Access 30 R
```
```
DES 216 - Securing Cloud Instances 30 M
```
```
DES 222 - Applying OWASP 2017: Mitigating Injection 12 R
```
```
DES 223 - Applying OWASP 2017: Mitigating Broken Authentication 12 R
```
```
DES 224 - Applying OWASP 2017: Mitigating Sensitive Data Exposure 12 R
```
```
DES 225 - Applying OWASP 2017: Mitigating XML External Entities 12 R
```
```
DES 226 - Applying OWASP 2017: Mitigating Broken Access Control 12 R
```
```
DES 227 - Applying OWASP 2017: Mitigating Security Misconfiguration 12 R
```
```
DES 228 - Applying OWASP 2017: Mitigating Cross Site Scripting 12 R
```
```
DES 229 - Applying OWASP 2017: Mitigating Insecure Deserialization 12 R
```
```
DES 230 - Applying OWASP 2017: Mitigating Use of Components with Known Vulnerabilities 12 R M
```
```
DES 231 - Applying OWASP 2017: Mitigating Insufficient Logging & Monitoring Vulnerabilities 12 R
```
```
DES 292 - Architecture Risk Analysis & Remediation for IoT Embedded Systems 30 R
```
```
ENG 205 - Fundamentals of Threat Modeling - NEW 60 R
```
Trainings course may evolve: if a training is not available, just note it.
Duration does not include exam duration, anticipate overtime to pass exams.


# Technical Knowledge Base


# AWS

This section contains a set of page related to the following AWS specific topics

```
ElastiCache (link to AWS Elasticache https://aws.amazon.com/elasticache/)
SWF UI builder on AWS ECS with Fargate Container (link to AWS Fargate https://aws.amazon.com/fargate/?whats-new-cards.sort-by=item.
additionalFields.postDateTime&whats-new-cards.sort-order=desc&fargate-blogs.sort-by=item.additionalFields.createdDate&fargate-blogs.sort-
order=desc)
```

# ElastiCache

Why do we need ElastiCache?

DynamoDB is very popular as a NO-SQL resource. But due to throttling of requests, there is an increase in the response times causing delay during real-
time application. In order to reduce the effect of throttling, one has to increase the throughput on the DynamoDB to handle burst traffic when sustained
traffic could be handled with lesser throughput.

So in order to reduce the response time, design can be tweaked to introduce a caching layer for read. This is where ElastiCache can be really helpful.

Introduction

Amazon ElastiCache works as an in-memory data store and cache to support the most demanding applications requiring sub-millisecond response times.
Amazon ElastiCache offers fully managed Redis and Memcached.

1) Memcached

It is an in-memory key value data store. The Memcached is very easy and simple to use. It is essentially equivalent to putting and getting strings in
memory.

Benefits:

```
Improves application performance
Reduces operational costs
Memcached is not transactional, therefore API can't participate in a transaction. It does provide certain optimistic lock mechanisms.
```
Shortcomings:

```
It is volatile(If entry reaches expiration or Memcached is full, data will be deleted)
It does not provide any availability and persistence options.
```
These shortcomings is the reason why Memcached is mainly used for easily recoverable data.

2) Redis: Remote Dictionary Server

It is the most popular key-value data store. It is usually preferred over Memcached because it supports transactions.


### 1.

### 2.

### 3.

### 4.

### 5.

### 6.

### 7.

The Redis engine is very fast. It is highly available and has persistence options. Unlike Memcached, it has more sophisticated data types.

Redis allows a primary node(master) and 0 or more replicas that asynchronously replicate data from the primary. Primary will accept read/write but replicas
are read only nodes. All of this has a lot of overhead which is managed by AWS.

The syncing is monitored in real time. If one of the replicas fail, AWS detects it and syncs it with the primary and that new replica has the same endpoint as
the old one,

IF MULTI AZ IS AVAILABLE and the primary fails at any point of time, the most up-to date replica becomes the primary node. No change needs to be
done for the endpoint.

Steps to create a Redis cache on AWS

```
Setup a VPC with at least two subnet groups, a route table, and a security group.
Create an Elasticache Subnet Group pointing to the two subnet groups created in step 1.
Create your Elasticache instance with it pointed to the Elasticache Subnet Group created in step 2.
Create your C# Lambda function and use a 3rd party library to connect to Elasticache. For Redis, I used StackExchange.Redis 1.2.1
successfully with .Net Core 1.0. More recent versions will not work with .Net Core 1.0.
Associate your Lambda with the same VPC, subnets, and security group.
Associate your Lambda function with an IAM Role that allows you to execute the Lambda. Something like AWSLambdaFullAccess and
AWSLambdaVPCAccessExecutionRole will work.
Test your Lambda for connectivity.
```
[http://fitsofury.blogspot.com/2018/02/aws-connect-to-elasticache-redis.html](http://fitsofury.blogspot.com/2018/02/aws-connect-to-elasticache-redis.html) (example to connect lambda with Redis)


# SWF UI builder on AWS ECS with Fargate Container

```
What is AWS ECS and Fargate?
Purpose of analyzing AWS ECS with Fargate
Setting up dev environment
AWS ECS component
Steps to set up UI builder application on AWS ECS with fargate
AWS CloudFormation for AWS ECS using Fargate
Analysis Outcome
```
## What is AWS ECS and Fargate?

Amazon Elastic Container Service (Amazon ECS) is a highly scalable, high-performance container orchestration service that supports Docker containers
and allows you to easily run and scale containerized applications on AWS. Amazon ECS eliminates the need for you to install and operate your own
container orchestration software, manage and scale a cluster of virtual machines, or schedule containers on those virtual machines.

AWS Fargate is a compute engine for Amazon ECS that allows you to run containers without having to manage servers or clusters.

## Purpose of analyzing AWS ECS with Fargate

Analysis of AWS Servies (Fargate, ECS) to host POC application in containers.

Deploy POC Application with UI builder on one of container.

## Setting up dev environment

1. Install docker runtime from https://hub.docker.com/?overlay=onboarding.
2. Install AWS Tools for PowerShell (https://www.powershellgallery.com/packages/AWSPowerShell/3.3.590.0(current version))


AWS ECS component

Steps to set up UI builder application on AWS ECS with fargate

1. Create docker image [ We should have a working copy of SWF OOTB app before we start the below steps to create docker image ]
a. Open application folder using VS code editor.
b. Add file Dockerfile and .dockerignore
c. Open Dockerfile and add following content-

```
Dockerfile
```
```
FROM gitlab.industrysoftware.automation.siemens.com:4567/apollo/tooling/docker-node:10.16.3
# FROM node:10-alpine
```
```
# Create app directory
WORKDIR /usr/src/app
```
```
# Install dependencies
COPY package*.json ./
RUN npm config set registry https://artifacts.industrysoftware.automation.siemens.com/artifactory/api/npm/npm/
RUN npm ci --only=production
```
```
# Bundle app source
COPY..
```
```
# afx-darsi port
EXPOSE 8075
# afx-gateway port
EXPOSE 3000
```
```
# script to run from package.json
CMD [ "npm" ,"run","startapp" ]
```

d. Open .dockerignore file and add following content-

```
.dockerignore
```
```
#Following folder will be excluded from image
node_modules
```
e. Open PowerShell and navigate to application root directory (standalone PowerShell console or PowerShell console in VS code).
f. Run following command to build docker image

```
> docker build -t testrepo
```
Note- testrepo is name for repository. You may change as per your need.

2. Now your docker image is created and you need to upload image to Amazon Elastic Container Registry (AWS ECR).
a. Login to AWS and navigate to ECR under AWS ECS.
b. Click on 'Create Repository' button and provide repository name (testrepo may be).
c. After successfully creating repository, go to PowerShell console.
d. Login to AWS from PowerShell using below command

```
> Invoke-Expression -Command (Get-ECRLoginCommand -Region eu-west-1). Command
```
Note- -Region eu-west-1 may change as per region selected.
Easiest way to get this command is to open repository in AWS and click on View push commands.
e. Add cloud specific tag to image

```
> docker tag testrepo:latest 609995600587.dkr.ecr.eu-west-1.amazonaws.com/testrepo:latest
```
Note- Easiest way to get this command is to open repository in AWS and click on View push commands.
f. Push image to AWS cloud repository

```
>docker push 609995600587.dkr.ecr.eu-west-1.amazonaws.com/testrepo:latest
```
g. Login to AWS web console and go to Amazon ECS> ECR.
h. Open newly created repository image and note down image URL ex-609995600587.dkr.ecr.eu-west-1.amazonaws.com/testrepo:latest

3. Go to Amazon ECS>Task Definitions.
a. Click on create new task definition.
b. Select Fargate option and click on next step button.
c. On 'Configure task and container definitions' provide configuration as given below

```
Parameter Value
```
```
Task Definition Name <Name of Task>
```
```
Requires Compatibilities FARGATE
```
```
Task Role ecsTaskexecutionRole
```
```
Network Mode awsvpc
```
```
Task execution role ecsTaskexecutionRole
```
```
Task memory (GB) 0.5GB
```
```
Task CPU (vCPU) 0.25 vCPU
```
d. Click on Add container and provide below information


```
Parameter Value
```
```
Container name <Name of container>
```
```
Image <repository-url/image-tag> URL from step 2
```
```
Memory Limits (MiB) Soft limit 256
```
```
Port mappings Port- 3000 Protocol- tcp
Port- 8075 Protocol- tcp
```
e. Click Add button to add container.
f. Scroll down and click Create button to complete Task definition.

4. Go to Amazon ECS>Clusters and click Create Cluster.
a. Select Networking only option and click next step.
b. Provide Cluster name and tick Create VPC.
c. Add tag "Owner":<creator/owner> and click create.
5. Open newly created cluster and click Create button in Services tab.
a. Configure service with following values:

```
Parameter Value
```
```
Launch type FARGATE
```
```
Task Definition Family: <task definition> created in step 3
Revision: <x>(latest)
```
```
Cluster <newly created cluster> created in step 4
```
```
Service name <Name of service>
```
```
Service type REPLICA
```
```
Number of tasks 1
```
```
Minimum healthy percent 100
```
```
Maximum percent 200
```
```
Deployment type Rolling update
```
```
Placement Templates Custom
```
b. Click on next step
c. Configure network with following options-

```
Parameter Value
```
```
Cluster VPC <keep default value as it is>
Note- only change to use already created VPC
```
```
Subnets <Select both available subnets>
```
d. Security groups: <click on edit button>
e. Click on Add rule button on Configure security groups page.
f. Create custom TCP on port 3000 to access from source (0.0.0.0 for public access)
g. Create custom TCP on port 8075 to access from source (0.0.0.0 for public access)
h. Hit 'Save' button.
i. Keep all other fields as default value and click on next step
j. On Set Auto Scaling (optional) page, click next step button.
k. Review configuration and click on create service.
l. On service page, it will show task creation status. Service is accessible once 'Running count' becomes 1.

6. Application container is successfully created and running.
7. To access application
a. Go to Task tab and click on running task id (ex- c6bf98a3-00b2-4cec-bff8-f2bcff78ea02).
b. On task details page, find ENI id (network interface) in Network section and click on it.
c. Use IPv4 Public IP to access your application from browser
ex- 34.242.219.225:3000/#

Congratulations, your application up and running on AWS ECS.


AWS CloudFormation for AWS ECS using Fargate

```
ECS cloudformation template
```
### {

```
"AWSTemplateFormatVersion": "2010-09-09",
"Description": "Creating ECS service",
"Parameters": {
"ContainerName": {
"Type": "String",
"Description": "Name of container",
"Default": "demo-app-container"
},
"ClusterName": {
"Type": "String",
"Description": "Name of cluster",
"Default": "demo-app"
},
"TaskName": {
"Type": "String",
"Description": "Name of task",
"Default": "demo-app-task"
},
"ImageURL": {
"Type": "String",
"Description": "Repository Image URL like 609995600587.dkr.ecr.eu-west-1.amazonaws.com/testrepo:
latest"
},
"ServiceName": {
"Type": "String",
"Default": "demo-app-service"
}
},
"Resources": {
"cluster": {
"Type": "AWS::ECS::Cluster",
"Properties": {
"ClusterName": {
"Ref": "ClusterName"
}
}
},
"taskdefinition": {
"Type": "AWS::ECS::TaskDefinition",
"Properties": {
"ContainerDefinitions": [
{
"Name": {
"Ref": "ContainerName"
},
"Image": {
"Ref": "ImageURL"
},
"PortMappings": [
{
"HostPort": 3000,
"Protocol": "tcp",
"ContainerPort": 3000
},
{
"HostPort": 8075,
"Protocol": "tcp",
"ContainerPort": 8075
}
],
"MemoryReservation": 256,
"Essential": "true"
}
```

### ],

"Memory": "512",
"TaskRoleArn": "arn:aws:iam::609995600587:role/ecsTaskexecutionRole",
"ExecutionRoleArn": "arn:aws:iam::609995600587:role/ecsTaskexecutionRole",
"Family": {
"Ref": "TaskName"
},
"RequiresCompatibilities": [
"FARGATE"
],
"NetworkMode": "awsvpc",
"Cpu": "256"
}
},
"service": {
"Type": "AWS::ECS::Service",
"Properties": {
"Cluster": {
"Ref": "cluster"
},
"DeploymentConfiguration": {
"MaximumPercent": 200,
"MinimumHealthyPercent": 100
},
"DesiredCount": 1,
"LaunchType": "FARGATE",
"TaskDefinition": {
"Ref": "taskdefinition"
},
"ServiceName": {
"Ref": "ServiceName"
},
"NetworkConfiguration": {
"AwsvpcConfiguration": {
"AssignPublicIp": "ENABLED",
"SecurityGroups": [
{
"Ref": "InstanceSecurityGroup"
}
],
"Subnets": [
{
"Ref": "Subnet1"
}
]
}
},
"SchedulingStrategy": "REPLICA"
}
},
"VPC": {
"Type": "AWS::EC2::VPC",
"Properties": {
"CidrBlock": "10.0.0.0/16",
"EnableDnsHostnames": "true"
}
},
"Subnet1": {
"Type": "AWS::EC2::Subnet",
"Properties": {
"VpcId": {
"Ref": "VPC"
},
"CidrBlock": "10.0.0.0/24"
}
},
"InternetGateway": {
"Type": "AWS::EC2::InternetGateway"
},
"GatewayAttachment": {
"Type": "AWS::EC2::VPCGatewayAttachment",


"Properties": {
"InternetGatewayId": {
"Ref": "InternetGateway"
},
"VpcId": {
"Ref": "VPC"
}
}
},
"Role": {
"Type": "AWS::IAM::Role",
"Properties": {
"AssumeRolePolicyDocument": {
"Version": "2008-10-17",
"Statement": [
{
"Sid": "",
"Effect": "Allow",
"Principal": {
"Service": "ecs.amazonaws.com"
},
"Action": "sts:AssumeRole"
}
]
},
"ManagedPolicyArns": [
"arn:aws:iam::aws:policy/service-role/AmazonEC2ContainerServiceRole"
]
}
},
"InstanceSecurityGroup": {
"Type": "AWS::EC2::SecurityGroup",
"Properties": {
"GroupDescription": "Allow http to client host",
"VpcId": {
"Ref": "VPC"
},
"SecurityGroupIngress": [
{
"IpProtocol": "tcp",
"FromPort": 80,
"ToPort": 80,
"CidrIp": "0.0.0.0/0"
},
{
"IpProtocol": "tcp",
"FromPort": 3000,
"ToPort": 3000,
"CidrIp": "0.0.0.0/0"
}
]
}
},
"myRouteTable": {
"Type": "AWS::EC2::RouteTable",
"Properties": {
"VpcId": {
"Ref": "VPC"
}
}
},
"myRoute": {
"Type": "AWS::EC2::Route",
"Properties": {
"RouteTableId": {
"Ref": "myRouteTable"
},
"DestinationCidrBlock": "0.0.0.0/0",
"GatewayId": {
"Ref": "InternetGateway"
}


### }

### },

```
"SubnetRouteTableAssociation": {
"Type": "AWS::EC2::SubnetRouteTableAssociation",
"Properties": {
"RouteTableId": {
"Ref": "myRouteTable"
},
"SubnetId": {
"Ref": "Subnet1"
}
}
}
},
"Outputs": {
"Cluster": {
"Value": {
"Ref": "cluster"
}
}
}
}
```
Analysis Outcome

```
It is possible to host UI builder (afx-gateway and afx- darsi) on AWS ECS using fargate.
Changes required is very less. We need to just change package.json file to modify existing script commands to make them compatible with Linux
environment (Bash commands).
Deployment is quite easy. It is straight forward procedure and takes 10-15 minutes to get deployed.
From maintenance point of view, its very easy to create revision of Task definition. Modifying service with forcefully deploy option takes care of
deploying and running task with new revision.
Security and instance management is AWS fargates responsibility, no overhead on devops.
Managing resources and scalability at run-time is automated with AWS fargate
For load balancing, we need to create load balancer separately and attach to ECS service. AWS ECS supports Application Load Balancer,
Network Load Balance and Classic Load Balancer. Load balancing settings can only be set on service creation.
Running fargate is cheap if workload(CPU or memory requirements) is less compared to EC2 instance. But if workload is more and consistent,
aws fargate is costly compared to EC2.( as per references: https://www.trek10.com/blog/fargate-pricing-vs-ec2/ , https://containersonaws.com
/introduction/ec2-or-aws-fargate/#use-cases)
```

# Developer's Jumpstart for ModMOM 1.x

Need to develop ModMOM services? Don't panic... just read on.

## Prerequisites

```
Opcenter Execution Foundation 3.0 installed & configured on your dev machine (3.1 and next versions are not supported). Foundation version: V1
0.00.00.00_01.07.00.53
An account in the RevMOM TFS Project
Optional: VDI which can run Docker & Kubernetes
```
## Developing a new ModMOM Microservice

```
With the help of an architect, identify the scope and name and short name of your microservice.
Example: Manage Plant Orders, Name: PlantOrder, Short Name: PO
This exact name is going to be used in many of the following steps. It is good practice, and will also avoid trouble, to use the very same
name everywhere, avoiding variations in casing, singular/plural, dashes, etc.
Create two new git repositories in TFS:
One for the backend service, (Name: Revolution_MOM_MS_PlantOrder), taking the Revolution_MOM_MS_Dispatch as example (you
do this in TFS, Code Tab, from the repo dropdown on the upper left).
Clone the new repository to your dev machine.
Add Revolution_MOM_Microservices_CF, and Revolution_MOM_Template as a sub-modules, by executing the following
command from the git command line, from the root directory of the newly created repo (see in-depth info here: Working with Git
submodules in Visual Studio):
```
```
git command
```
```
git submodule add --name Shared --force https://tfs01mom.industrysoftware.automation.
siemens.com/tfs/DefaultCollection/Revolution_MOM/_git/Revolution_MOM_Microservices_CF Shared
git submodule add --name Microservice --force https://tfs01mom.industrysoftware.automation.
siemens.com/tfs/DefaultCollection/Revolution_MOM/_git/Revolution_MOM_MS_Template
Microservice
```
```
Copy the .gitignore file from the Revolution_MOM_MS_Dispatch repo
Copy the NuGet.Config file from the Revolution_MOM_MS_Dispatch repo
Copy the Tools folder from the Revolution_MOM_MS_Dispatch repo
Copy and adapt the ServiceName.props and ServiceConfiguration.jconfig files from the Revolution_MOM_MS_Dispatch
repo (set your own ServiceName and ServiceShortName, AssemblyPrefix and NamespacePrefix are Siemens.RevMOM)
Folder structure inside the git repository:
App\<appname>\<app root folder>
FBs\<fbname>\<fb root folder>
"TO_BE_REVIEWED" One for the UI (Name: Revolution_MOM_MS_PlantOrder_UI), taking the Revolution_MOM_MS_PlantOrder_UI
as example
```
```
Develop the business logic of your microservice in Opcenter Execution Foundation Project Studio:
First, develop a FB (Example Name: PlantOrderFB, naming schema: "<ServiceName>FB"), prefix: Siemens.RevMOM, domain
acronym: abbreviation of service name). In RevMOM only "MasterData" domain FB is supported.
Put it inside a folder named "FBs", like it is done in the Revolution_MOM_MS_Dispatch repo. VS Solution Path Example:
FBs\PlantOrderFB\PlantOrderFB.sln
Make sure the project dependencies in the solution are set correctly, in order to guarantee the right build order (otherwise the
build in TFS will fail)
Make sure the "Copy to Engineering Target Folder" in the manifest section of the Installer Project is checked (otherwise the
build in TFS will fail)
Second, develop an APP referencing your FB (Example Name: PlantOrderApp, naming schema: "<ServiceName>App").
Put it inside a folder named "App", like it is done in the Revolution_MOM_MS_Dispatch repo. VS Solution Path Example:
App\PlantOrderApp\PlantOrderApp.sln
Develop Unit Tests for your Functional Block, following the example in PlantOrder
Develop a UI for the APP with Apollo
Create a TFS Build Definition for your service, cloning the Dispatch/ Build Dispatch Microservice Build
Note: it will not be a real copy of the build, the Build Definition references a TFS Task Group which will be shared by all Build Definitions
Put the Build Definition into a new folder, example for the folder name: PlantOrder
Change the source repository to your new repo, in our case it will be: Revolution_MOM_MS_PlantOrder
Adapt the build parameters to the new service:
Build Variable "ServiceName", example value: PlantOrder
Execute the build.
In case you encounter errors due to missing files, copy them from the repository Revolution_MOM_MS_Dispatch.
"NOT_NEEDED_ANYMORE" Clone another build definition: PlantOrders / Prepare to Build PlantOrder App Container. Name it e.g. "Prepare to
Build PlantOrder App Container"
Note: This secondary build is a workaround which is needed to build the Linux Container Image. The agent which builds the Linux
Container Image must run on a Linux host. The build however must run on a Windows host, because MSBuild and the C# compiler run
only on Windows. Moreover, artifacts which are published on a Windows File Share ("dropfolder") cannot be read by Linux agents. To
```

### 1.

### 2.

### 3.

### 1.

### 2.

```
publish into a dropfolder is however useful and is the company standard. So we introduce this secondary build, which is triggered by the
primary build. It reads the artifacts produced by the primary build from the drop folder and puts them onto a TFS-Internal publish location,
from which it can be read by the Linux agent. This secondary build is used as a source for the Release, which builds the container from
the artifacts the build produced. It is worth mentioning that also the container images are artifacts and therefore conceptually should be
produced by builds. However, for technical reasons related to TFS, a build cannot take artifacts from the TFS-internal publish location
from another build as input, only Releases can do that. For this reason, the construction of the container images is modeled as a
Release in TFS. The "real" Release is a TFS environment in the Release Definition which is triggered by the successful completion
"Build Environment Image" environment.
In the "Trigger Container Build" task of the primary build, set BuildID to the ID of the sedondary build (you can take it from the URL). In
this way, the primary build will trigger the secondary build.
"TO_BE_REVIEWED" Copy the Release Definition(s) from the Dispatch Service and adapt them to your service.
Clone the "K8s PlantOrder Service" Release Definition and rename it to e.g. "K8s <servicename> Service"
Remove the primary artifact and add a new one, based on Build Definition "Dispatch /Build Dispatch Microservice". Put "drop" as the
source alias
In the "Build Container Images" set "SourceImageName" and "TargetRepositoryName" tasks, put e.g. "revmom-<servicename>" (all
lowercase mandatory)
In the Release Definition Variables, set ServiceName to <servicename> (all lowercase mandatory)
Once a basic version of your service and pipeline are running, activate a branch policy on the master branch
Implement Component Tests, following the instructions in Component Tests
```
Invoking the service's REST API with PowerShell

```
Powershell
```
```
# Query an entity
Invoke-RestMethod -Uri http://localhost:5000/odata/PlantOrder
```
```
# Invoke a command
Invoke-RestMethod -Method Post -Headers @{ 'content-type' = 'application/json' } -Uri http://localhost:5000
/odata/CreatePlantOrder -Body '{"command":{"name":"Peter"}}'
```
```
# Perform scaffolding
Invoke-RestMethod -Method Post -Headers @{ 'content-type' = 'application/json' } -Uri http://localhost:5000
/odata/ScaffoldDatabase -Body '{"command":{"name":"Peter"}}'
```
Playing around with Containers and Kubernetes on your VDI

The pipeline on TFS will deploy your service to a shared Kubernetes cluster on AWS. For local testing, the service can run on your VDI as an ASP.NET
Core server directly on Windows. However if you want a local deployment inside a container, or with Kubernetes, on your VDI, you need a special VDI
which supports Docker and Kubernetes.

Background information: Our target are Linux containers. A Windows machine can run Linux containers only inside a Virtual Machine (i.e. a VM inside the
VDI). In order for this to work, the VDI needs to have Hyper-V enabled. Hyper-V support needs to be enabled from the IT staff. There are ready-to-use VDI
templates available. Ask the IT staff if it is possible to enable Hyper-V support in your existing VDI.

The same thing is true also for the vCloud infrastrucure, i.e. Docker can run on vCloud VMs, but Hyper-V support must be enabled by the IT staff.

The docker edition to be used with Windows 10 (which is the OS you normally have on your VDI) is called "Docker Desktop".

```
Download "Docker Desktop" stable version for windows from the internet (Kubernetes is built-in)
Install it (option by default), then restart VDI.
Configure Docker Desktop (Right click on docker icon in taskbar Settings)
Resources set memory to 4 GB (suggestion)
Kubernetes set Enable
Apply & Restart
Launch Power Shell Administrator to send command to Docker Desktop
"docker version" to verify if Docker is running
Download Helm client from the Releases page: https://github.com/helm/helm/releases/latest
Extract in a dedicated folder (ie: C:\Helm)
Set environment variables by command line in power shell:
[Environment]::SetEnvironmentVariable("Path", $env:Path + ";<helm path>", "Machine")
[Environment]::SetEnvironmentVariable("HELM_EXPERIMENTAL_OCI", 1, "Machine")
Restart Power Shell Administrator
Install AWS CLI for windows 64 bit from the internet
```
Regarding Windows Server, it doesn't make much sense to execute Linux containers on Windows server. It is better to use directly Linux as host OS, e.g.
Ubuntu.


# Adding the ASP.NET Core Server Project manually

Should there be cases where the generic ASP.NET Core Server project in the Revolution_MOM_MS_Template repo is not sufficient for some reason, it is
possible to create an own ASP.NET Core Server project manually.

```
Copy the MaterialService folder from the Revolution_MOM_MS_Material repo and rename it to e.g. PlantOrderService
Note: The command handlers from the FB and APP must be recompiled as .NET Standard. For this purpose, inside the MaterialService
/ PlantOrderService folder there are again C# projects for the command handlers.
Remove all the XXX_CommandHandler subfolders but two:
the Material.CommandHandler, which is the APP. Rename it to e.g. PlantOrderAPP.CommandHandler
one of the FB _CommandHandler subfolders. Rename it to e.g. PlantOrderFB.CommandHandler
Rename MaterialService.sln to e.g. PlantOrderService.sln
adjust the references.targets file (remove references to FBs others than yours). The item group referring to the model dlls should look
like this:
```
```
references.targets
```
```
<Reference Include="..\..\bin\$(Configuration)\*\*.PlantOrderFB.*.dll">
<HintPath>..\..\bin\$(Configuration)\*\*.PlantOrderFB.*.dll</HintPath>
<Private>True</Private>
</Reference>
<Reference Include="..\..\bin\$(Configuration)\*\*.PlantOrderApp.*.dll">
<HintPath>..\..\bin\$(Configuration)\*\*.PlantOrderApp.*.dll</HintPath>
<Private>True</Private>
</Reference>
```
```
remove all files in PlantOrderService\LambdaMicroservice\Controllers
open the solution in Visual Studio, remove all the unavailable projects
in the LambdaMicroservice project, remove all references to unavailable projects
remove the unit test project for the moment (however it is mandatory to implement unit tests, do it once the build works)
in the Startup.cs file of the LambdaMicroservice project, set the new service name, e.g. "PlantOrderManagement"
Start the LambdaMicroservice project locally in Visual Studio in order to verify that everything works well
```

# Component Tests

Component Tests are written in a similar way as Unit Tests, but they are executed with the real platform and database connected.

They are executed on the cloud, in AWS, in parallel, each test case with its own containerized database. The results are reported back to TFS.

## How to implement Component Tests for a Microservice

Generally speaking,

```
copy ComponentTest folder from the Revolution_MOM_PlantOrder sample git repository
rename all occurrences of "PlantOrder" or "PO" to the name of your service
implement your test methods
make sure they all work when running them locally on your VDI (to use a local database, uncomment the relative line in AssemblyInitialize.cs in
order to set the environment variable "databaseEndpointInformation")
in TFS, create a new Auxiliary build, cloning "Auxiliary build for PlantOrder ComponentTest Release (publish to TFS)" from the PlantOrder folder,
and changing all occurrences of "PlantOrder" or "PO" to the name of your service
grab the ID of the auxiliary build (can be seen in the URL when editing it) and use it in the "Trigger Component Test" task of the build (-BuildID in
arguments)
create a new TFS Release, cloning the "Run PlantOrder Component Tests on ECS" release and substituting all occurrences of "PlantOrder" and
"PO" with the name of your service
use the newly created Auxiliary build as Artifact for the Release, and enable the continuous deployment trigger
```
After a successful execution of a build for your service, the Auxiliary build is supposed to be triggered, which will in turn trigger the Component Test. The
test results will be visible in the "Tests" tab of the Release.

## Folder Structure in the git repository

Different from the Visual Studio solution of the ASP.NET Core service itself, which is shared between all microservices, there is an individual VS solution of
the component test in each repository, under /ComponentTest/<ServiceName>/<ServiceName>.sln.

In order to be able to compile, the component test projects need VS project references to the shared projects under the /Microservice folder. Project
references can exist only between projects of the same VS solution, so the references projects are inserted also under the component test solution.

The Component Test .csproj files include the same references.targets file as the shared projects under /Microservice do, so in order for the relative paths
contained in references.targets to work, the component test projects must be on the same folder level as the shared projects. I.e. the "extra" level under
/ComponentTest must not be removed.

## Mode of operation

The component test for a Service is implemented as a TFS Release, named "Run <ServiceName> Component Test on ECS" which takes its input from a
build called "Auxiliary build for <ServiceName>".

It builds Linux container images for test execution and database and uploads them to the AWS Cloud, where all tests are executed in parallel on a AWS
ECS Cluster.

The TFS Release is divided in three major steps: Build Container Images, Execute Tests, Cleanup

Build Container Images

Two containers need to be built: The test container, containing the software under test, and the corresponding database container, containing the
scaffolded database which will be used for test execution.

The scaffolding of the database is done on the build server, i.e. the container with the base database is run on the build server, then another container
containing the scaffolding logic, which connects to the database in the first container, and scaffolds the database. After completion, the current state of the
database container is committed to a new container image.

Another step which is done on the build server is the extraction of the test cases. I.e. the test container is run with an option to the "dotnet vstest" tool
which makes it only print all the test cases it found inside the test DLL. This file is extracted and afterwards will be used to create as many tasks in AWS
ECS as there are test cases, and instruct each task with the test case to execute.

For the scaffolding and the test case extraction, two special images, derived from the generic test container image, are built.

The scaffolding operation is itself implemented as a test method, and the test results will be published to TFS when done.

The steps in sequence are:

```
Build container test execution image
Create dynamic dockerfile for scaffolding container image
Build container test scaffolding image - this is the image for the container which will execute the scaffolding on the database container
Build container test extraction image - this is the image for the container which will extract the list of test cases from the test dll
Run container test case extraction
Run database for scaffolding
Run test container on build machine for scaffolding
Publish Scaffolding Test Results
```

```
Commit database image
Push test container Image - i.e. upload the image to the docker registry in AWS ECR
Push database container Image
```
Execute Tests

The tests themselves are executed on AWS ECS. The necessary infrastructure on AWS is instantiated ad hoc using AWS CloudFormation.

For every test method found in the list of test methods, an ECS task is created and run. This is done by a bash script which leverages the AWS CLI to
interact with ECS.

Every test container running in an ECS task, when finished, uploads its results to an S3 bucket.

After all tasks have been started, the script waits for all of them to complete.

The test results are downloaded from the S3 bucket and published to TFS.

Cleanup

All the created resources need to be cleaned up after every test run.

```
The AWS CLoudFormation stack must be deleted
The database and scaffolding containers running on the build server, and their relative images, must be removed
The docker virtual network must be removed
...
```
Reasoning behind design choices in TFS

In general, the intended use of TFS Releases is to implement deployments to environments, so its use for tests represents a workaround.

This is due to some limitations in TFS regarding multi-stage builds, and workloads which need to be executed on Linux hosts vs. workloads which need to
be executed on Windows hosts.

The build of the assemblies must be executed on a Windows host, but the container images which are needed for the component test can be built only on
Linux hosts.

Troubleshooting

In case of problems with the execution of the component test, here some tips on how to investigate what's going on.

bash inside a test container

You can have a command shell inside a container by running the image with the command which you can find in the log files on TFS, plus "-i -t --
entrypoint=bash" in order to express that you want an interactive bash:

```
enter inside a test container
```
```
docker run --name test-scaffold-workorder-144215-3499 --hostname test-scaffold-workorder-144215-3499 --env
databaseEndpointInformation=EmptyDb-workorder-144215-3499 --env ProgramData=/tmp --env
ScaffoldTestDll=PlantOrderService.Deployment.ComponentTest.dll -i -t --entrypoint=bash scaffolding-container-
workorder:144215
```
Inspect scaffolded database

Extract the exact name of the docker image with the scaffolded database from the log of the TFS Component Test Release. Run the container on your VDI
and connect to ot with SQl Server Management Studio:

docker run -p 5013:1433 701215477833.dkr.ecr.eu-west-1.amazonaws.com/scaffolded-db:plantorder-144437

With SSMS, connect to: 127.0.0.1,5013


# Debugging a Microservice locally

There are several ways to debug your microservice on your Windows 10 development machine:

```
As a Windows Process (without containers)
Inside a Linux container
```
## Debugging a Microservice locally as a windows process (without containers)

The way this is supposed to work is just to press the "LocalServer" button in Visual Studio. Unfortunately, there is a problem with this approach: For some
reason we didn't completely understand yet, the controllers are not loaded correctly, and so command invocations and queries will fail with an 404 error.

The workaround for this is to publish and start the process manually, as shown here, and to attach then to the process form Visual Studio.

```
dotnet publish LambdaMicroservice\LambdaMicroservice.sln -c Release -o ..\..\published --runtime win10-x64 /p:
ServiceName=PlantOrder
.\LambdaMicroservice.exe
```
## Debugging a Microservice locally inside a Linux Container

In order for this to work, you need a VDI which supports Linux Containers. Ask the IT Staff.

to be continued...


# Kubernetes Cheat Sheet

```
Download helm charts from registry
```
```
# Establish port forwarding on your VDI (going directly to a remote helm registry will cause a http/https
conflict)
# (assuming that the helm registry is running on 172.27.3.104 (TFS build server goax504) on port 5000)
netsh interface portproxy add v4tov4 listenaddress=127.0.0.1 listenport=5000 connectaddress=172.27.3.104
connectport=5000
```
```
#Only in case you want to remove the previous portproxy setting, use this command
netsh interface portproxy delete v4tov4 listenaddress=127.0.0.1 listenport=5000
```
```
#If Kubernetes is not installed, instal DockerDesktop (with Linux containers), and then instal Kubernetes from
DockerDesktop console
#Check that kubectl (Kubernetes client command), is pointing to the context docker-for-desktop:
kubectl config get-contexts
kubectl config use-context docker-for-desktop
```
```
# Download chart from remote repository
helm chart pull localhost:5000/revmom-plantorder:1.0.2037
```
```
# Write helm chart to a directory
helm chart export localhost:5000/revmom-plantorder:1.0.2037
```
```
# Install or upgrade K8s service on your VDI
# you need to override setting for service.type=LoadBalancer, because external Load Balancers are available
only on the cloud, not on your VDI
helm upgrade --install --set service.type=ClusterIP revmom-plantorder revmom-plantorder
```
```
# if you have less memory available or assigned to Docker for Windows than the service requires, you can either
assign more memory to Docker for Windows
# (Settings/Resources/Memory), or you can override the service's default memory request:
helm upgrade --install --set resources.requests.memory=2096 --set service.type=ClusterIP revmom-plantorder
revmom-plantorder
```
```
#In case of upgrade of the docker image, you need to restart the running pods in this way
helm upgrade --recreate-pods --install --set service.type=ClusterIP revmom-plantorder revmom-plantorder
```
```
#The above commands should pull the docker image and run the right container through Kubernetes
#Check that the docker image has been pulled
docker image ls
```
```
#If the image revmom-plantorder is not in the list, pull it manually from AWS
#insert your credential with the AWS CLI command:
aws configure
```
```
#Get the AWS login for Docker, copy and paste the result to login
aws ecr get-login --no-include-email --region eu-west-1
docker login -u AWS -p ...
```
```
#Pull the docker image from AWS (if not already pulled by helm)
docker pull 609995600587.dkr.ecr.eu-west-1.amazonaws.com/revmom-plantorder:1.0.2037
```
```
#Restart DockerDesktop by selecting the proper button, and wait until DockerDesktop and Kubernetes are running
```
```
#Verify that the microservice container is running in kubernetes
kubectl get pods
#Result:
#NAME READY STATUS RESTARTS AGE
#revmom-plantorder-6cf94bd495-qmlcp 1/1 Running 0 18h
```
```
kubectl describe pods revmom-plantorder-6cf94bd495-qmlcp
```

#Get the local machine IP address
$env:HostIP = (Get-WmiObject -Class Win32_NetworkAdapterConfiguration | where {$_.DefaultIPGateway -ne $null}).
IPAddress[0]

#Set the container port-forward
kubectl --namespace default port-forward --address localhost,$env:HostIP revmom-plantorder-6cf94bd495-
qmlcp 8080:5000

#In order to test if the microservice works, launch the following commands on another PowerShell
Invoke-RestMethod -Uri [http://localhost:8080/odata/$metadata](http://localhost:8080/odata/$metadata)
Invoke-RestMethod -Uri [http://localhost:8080/odata/PlantOrder](http://localhost:8080/odata/PlantOrder)
Invoke-RestMethod -Method Post -Headers @{ 'content-type' = 'application/json' } -Uri [http://localhost:8080](http://localhost:8080)
/odata/CreatePlantOrder -Body '{"command":{"name":"PL01","NId":"PL01"}}'
Invoke-RestMethod -Method Post -Headers @{ 'content-type' = 'application/json' } -Uri [http://localhost:8080](http://localhost:8080)
/odata/ScaffoldDatabase -Body '{"command":{"name":"Peter"}}'

Interact with cluster from terminal

# download kubeconfig files from S3 bucket
aws s3 cp s3://dev-rm-s3-cf/kubeconfigs/kubeconfig-ged1 kubeconfig-ged1
aws s3 cp s3://dev-rm-s3-cf/kubeconfigs/kubeconfig-ged2 kubeconfig-ged2

# create handy aliases, to avoid a lot of typing
alias kuged1="kubectl --kubeconfig ~/kubeconfig-ged1"
alias kuged2="kubectl --kubeconfig ~/kubeconfig-ged2"
alias heged1="helm --kubeconfig ~/kubeconfig-ged1"
alias heged2="helm --kubeconfig ~/kubeconfig-ged2"

# sample command with alias
kuged1 get pods

# add IAM user as admin for the cluster (from a bash)
# cannot stand vi, I prefer nano
export KUBE_EDITOR="nano"
kuged1 edit -n kube-system configmap/aws-auth

# Please edit the object below. Lines beginning with a '#' will be ignored,
# and an empty file will abort the edit. If an error occurs while saving this file will be
# reopened with the relevant failures.
#
apiVersion: v1
data:
mapRoles: |

- groups:
- system:bootstrappers
- system:nodes
rolearn: arn:aws:iam::609995600587:role/eksctl-revmom-ged1-nodegroup-ng-1-NodeInstanceRole-15YE9WB8PAZB0
username: system:node:{{EC2PrivateDNSName}}
mapUsers: |
- userarn: arn:aws:iam::609995600587:user/harshila.jagtap
username: harshila
groups:
- system:masters
- userarn: arn:aws:iam::609995600587:user/PeterBalzer
username: peter
groups:
- system:masters
kind: ConfigMap
metadata:
creationTimestamp: "2019-12-09T07:36:25Z"
name: aws-auth
namespace: kube-system
resourceVersion: "7329398"


Cluster on AWS EKS

```
Manage EKS Cluster
```
```
eksctl create cluster -f eks-cluster.yaml --kubeconfig C:\\Users\\itr00626/.kube/config-aws
eksctl get cluster
eksctl delete cluster -f eks-cluster.yaml
eksctl scale nodegroup --cluster revmom --nodes=2 --name=ng-1-workers
```
Make sure kubectl points to the correct config file for the cluster on AWS EKS.

```
Manage K8s nodes
```
```
kubectl get nodes
```
Connect to K8s cluster EC2 node with PuTTY from VDI

Get the private key (.pem) file from Balzer, Peter. Follow generic instructions on how to connect to a Linux instance from Windows using PuTTY (https://doc
s.aws.amazon.com/AWSEC2/latest/UserGuide/putty.html?icmpid=docs_ec2_console).

On the AWS EC2 Console, select the instance you want to connect to, select Actions/Networking/Change Security Groups. Assign a security group which
has inbound port 22 open.

In PuTTY, as Host Name, put ec2-user@<IP address of the EC2 node>.

Testing for TCP Connectivity

```
Testing for TCP Connectivity
```
```
# Install netcat on Amazon Linux
sudo yum install nmap
```
```
# Install netcat on Ubuntu (used for containers)
apt-get update
apt-get install netcat
```
```
# Test TCP Connection
nc -zv <ip address> <port>
```
curl -d "{"command":{"channel":1,"message":"test"}}" -H content -type:application/json -X POST [http://localhost:5000/odata/ScaffoldDatabase](http://localhost:5000/odata/ScaffoldDatabase)


### 1.

### 2.

### 3.

### 4.

### 5.

### 1.

### 2.

### 3.

### 4.

### 5.

### 6.

### 1.

### 2.

### 3.

### 4.

# Minishift Scenario Guidelines

The Minishift Scenario is based on a VDI with nested virtualization enabled.

With this scenario a microservice/UI developer can run/test a component locally in an OpenShift 3.11 environment.

## Prerequisites

```
VDI should be installed with Windows 10 Pro version >= 1709
IT must enable options to permit nested virtualization to have DNS enabled IP.
Install Git (if it isn’t already installed)
Uninstall Docker desktop (if installed)
Hyper – V feature must be turned off.
```
```
To be sure the Hyper-V feature is disabled:
```
```
Open Windows PowerShell (Admin)
Execute: PS> Get-ExecutionPolicy
If the response is ‘Restricted’, execute: PS> Set-ExecutionPolicy unrestricted
Download DG readiness tool from here: https://www.microsoft.com/en-us/download/details.aspx?id=53337
Unzip dgreadiness_v3.6.zip
Execute: PS>DG_Readiness.ps1 –Disable
```
## Setup environment

1. Install Virtual Box

```
Download VirtualBox from here: https://www.virtualbox.org/wiki/Downloads
Run the downloaded executable file to install VirtualBox (with default options)
```
2. Install minishift

```
Download Minishift from here: https://github.com/minishift/minishift/releases
Unzip the Minishift package previously downloaded in a dedicated folder <minishift-path>
```
3. Install Docker Toolbox

```
Download the Docker Toolbox package from here: https://github.com/docker/toolbox/releases
Run the downloaded executable file:
Install only ‘Docker compose for Windows’ component by selecting it (deselect all other components)
```
4. Install helm

```
Download Helm client “Windows amd64” version 3.0.1 or later, from the Releases page: https://github.com/helm/helm/releases/latest
Extract in a dedicated folder <helm-path>
Set environment variables by command line in power shell:
[Environment]::SetEnvironmentVariable("Path", $env:Path + ";<helm-path>", "Machine")
[Environment]::SetEnvironmentVariable("HELM_EXPERIMENTAL_OCI", 1, "Machine")
```
5. Install Visual Studio Code.

```
Install Visual Studio Code, run it and select the following Extensions and related dependencies (you can use Ctrl+Shift+X accelerator):
```
```
Docker 1.0.0
Kubernetes 1.1.1
OpenShift Extension Pack 0.0.2
Excel viewer 2.1.34 (optional)
```

### 1.

### 2.

### 3.

6. Configure environment

```
Add the following paths to the Path environment variable of your VDI:
C:\Program Files\Git\cmd;
C:\Program Files\Docker Toolbox
C:\<helm-folder>\windows-amd64
C:\<minishift-path>\minishift-1.34.2-windows-amd64
C:\Users\<your user>\.vs-kubernetes\tools\kubectl
C:\Users\<your user>\.minishift\cache\oc\v3.11.0\windows
```
7. Minishift setup

```
Before to start minishift it is needed to setup it.
from Windows Powershell execute: PS> minishift setup
reboot the VDI (as required)
enter again the same command: PS> minishift setup
Disable Hyper-V (it was resumed by minishift setup)
Open Apps & Features System settings
Go to Programs and Features
Click on Turn Windows features on or off
Deselect ‘Hyper-V’ feature
Select ‘Window Subsystem for Linux’ feature
Reboot the VDI machine
```
8. Minishift start

```
Start the minishift VM via VirtualBox. From Windows Powershell execute:
```
```
PS> minishift start –-vm-driver virtualbox –-profile minishift
```
```
Set the environment variables so that the local docker client can be used from command line.
Execute: PS> minishift docker-env
Set the following environment variables as they are listed in the command result:
DOCKER_TLS_VERIFY = 1
DOCKER_HOST=<docker host address>
DOCKER_CERT_PATH = “C:\Users\<your user>\.minishift\certs”
Check if the oc client envinroment variable is correctly set:
Execute: PS> minishift oc-env
Read the PATH environment variable value. If it is not the same value specified at step 6 in the previous chapter, set the
variable with the result of the command.
After environment variables changes, please remember to close and open any Powershell window in order to use the new values.
```
RUN TIME

1. Check minishift

```
All the components must be running. You can check it by executing:
```
```
PS>oc get pods
```
```
If the command returns error, then stop and restart minishift by executing:
```
```
PS> minishift stop
PS> minishift start –-vm-driver virtualbox -–profile minishift
```
2. Open OpenShift console

```
In order to check the behavior of the loaded containers in the minishift VM, open OpenShift web console.
```
```
Get the server address by executing:
```
```
PS> oc version
```

```
The <server address> is in the Server value (ie: 192.168.99.100:8443)
```
```
open the Open Shift web console at address https://<server address>/console
(in case of error opening the page) click on Advanced and the follow link
Login with user: developer and password: developer.
```
3. Microservice installation

```
Get the microservice installation ZIP file. You can find it in \\dropfolder\df\df24\revmom-microservices The name of the microservice installation
file is:
```
```
revmom-<service name>_<release version>.zip
```
```
Unzip the installation ZIP file in the <working-folder>.
Configure microservice environment variables in the following file:
```
```
helm\revmom-<service name>\templates\deployment.yaml
```
```
In case of UI microservice, configure the enpoints in the following file:
```
```
docker\bin\assets\config\<service name>-config.json
```
```
In case of microservice is already installed, execute in <working folder>:
```
```
PS> UninstallMicroservice.ps1
```
```
From Windows Powershell execute in <working folder>
```
```
PS> InstallMicroservice.ps1
```
Note: in alternative from File Explorer you can double click on any .sh script to run it with GIT Bash

If the command execution fails, check that OpenShift is running (refer to point 1 ‘Check minishift’)

Because minishift resources are limited, you can run up to 4 microservices at the same time.

When the limit is reached new instances will not start, in this case you have to uninstall another running microservice.

4. Microservice test

```
Check if microservice is really installed on minishift VM.
Via OpenShift console
Via Visual Studio Code in the Docker, Kubernetes and OpenShift sections
```
```
Test the runtime microservice using Postman
Get the name of the Route of the microservice. Execute:
```
```
PS> oc get routes
```
```
Verify that the microservice works with the postman command:
```
```
GET http://<microservice host name>/odata/$metadata
```
```
Where <microservice host name> is the HOST/PORT, corresponding to the microservice, in the result of the ‘oc get routes’ command
```
```
Test the command using the POST command.
```
```
Test UI runtime microservice via browser
Open OpenShift console
Select Applications/Routes
Select the Hostname of any UI microservice, to open the corresponding UI
```
5. Get the logs

Via Open Shift web console:


```
In the Overview page select the container
Then select the microservice
Then select the pod
From the menu select Logs
```
Via command line:

```
Get the container id by executing:
```
```
PS> docker ps.
```
```
The container ids are listed in the first column. Look for that one with name like ‘K8s_revmom-<microservice name>_*’
```
```
Get the microservice logs by executing:
```
```
PS> docker logs <container id>
```
```
If you wanto to save the logs in csv file, then execute:
```
```
PS> docker logs <container id> > <filename>.csv
```
Via Visual Studio Code:

```
Active docker component
Right click on the container (look for it in the top left box area)
Select ‘View logs’ in the context menu
```

### 1.

### 2.

# Working with Git submodules in Visual Studio

If you apply modifications to a git submodule which is mounted in your microservice git repository, and want them to become effective in the TFS pipeline,
you have to follow a two-step approach.

The key point here is that the mounting microservice repository points to a specific version of the mounted repository, and not automatically to the last
version. So the two steps are:

```
Update the submodule itself
Update the mounting microservice repository to point to the updated version of the submodule.
```
First, you need to push the modifications of the submodule to the remote (TFS) repository of the submodule. Therefore, in the Visual Studio Team
Explorer, you connect to the Local Git Repository of the submodule inside your Microservice repository. If you have cloned more than one microservice
repository locally on you dev machine, the submodules of these repositories will appear with as many instances as there are microservice repositories. E.
g. if you have three microservices repositories, you will see three instances of each submodule which is mounted into these repositories in the list of Local
Git Repositories in the Visual Studio Team Explorer.

If the submodule you modified doesn't appear yet in the list of Local Git Repositories in the Visual Studio Team Explorer, you need to add it manually
performing the "Add" operation, specifying the local path of the submodule on your file system inside the mounting repository.

According to the branch policy of the submodule, you can push your modifications directly to the remote (TFS) repository or you need to perform a pull
request.

At this point, the submodule itself is updated on TFS, but the mounting repository doesn't yet point to the last version of the submodule.

In order to achieve this, you need to push (or perform a pull request) a modification of the mounting repository to TFS, containing the update of the mount
point.


### 1.

### 2.

### 3.

### 4.

### 5.

### 6.

### 7.

### 8.

Steps to update submodule in the repository using command line :

If you have changes in the submodules:

```
Clone the repository
For eg : update “Microservice” submodule àgo to the folder path of Microservice and open GitBash at this location
git checkout master
git stash
git stash drop // for removing possible local changes
git pull
After checking the changes, we can see the submodule in differences..(GUID changes if we see the modified changes)
Commit it to the branch
```
If you don't have changes in the submodules:

git submodule foreach git pull origin master

Steps to get the correct version of the submodules

git submodule update --recursive


### 1.

### 2.

### 3.

# Generate token for ModularMOM via Postman using Token

# Service

In case you need to generate token for dev / test activities, you can follow below mentioned steps to generate token via Postman for ModularMOM [ These
steps consider our token service deployed on OS02 project ]

```
Select Verb (GET / POST) & Enter the URL / endpoint in postman Ex.: http://webapp.test.com/odata/yourendpointname
Go to ‘Authorization’ tab -> Select Type as ‘OAuth 2.0’. On right panel click ‘Get New Access Token’
Fill in values as mentioned here – The values shown in image are as per the OS02 project token service we have deployed. If you are running
token service on your own VDI, fill in values as set in your Config.cs / appSettings file.
```
```
Grant Type: Client Credentials
```
```
[Allowed in token service when suing Postman]. Our token service also supports “Implicit” flow which is used by our web apps via msal library
```
```
Access Token URL: https://{tokenservice endpoint}/connect/token (Note - follow the Image below)
```
```
You can get entire list of endpoints exposed by token service here: https://<<token service url>>/.well-known/openid-configuration
```
(for example, https://token-public-api-momosprj07.apps.openshift01.swqa.tst/connect/token)

```
Client ID: testclient
```
```
This value is only for Postman, for web apps we use different client Id: auth-code-client
```
```
Client Secret: SiemensMOM
```
```
This value will depend as per configuration of env variable on deployed instance. (Specified value will work only for OS02 project)
```
```
Scope: ModularMOM
```
```
Use this value only unless you modify in token service code.
```
```
Client Authentication: Send client credentials in body
```

If you have an error, check in Settings if SSL certificate validation has been disabled


4. Click on ‘Request Token’
5. You should see the generated token like below –
6. Click on ‘Use Token’. The value will be set in your request Headers : Authorization OR You can also copy this token and use as needed.


# Modular MOM web apps development using SWF

Versions

Updated afx & mom-ui packages and verified for all 3 web apps.
"dependencies": {
"afx": "4.0.0",
"live-server": "latest",
"mom-images": "0.51.0",
"mom-ui": "1.5.1",
"msal": "1.2.1"
}

Environment variables and values in web apps

Areas to update in dev – docker entry file

Environment variables

Whenever we are introducing a new endpoint in any web app we need to update the same at below mentioned locations & artifacts -

a) Webapp config file - for eg. supervise-config.json. Add placeholders for the required URL

```
"api": {
"logsService": "$LOG_BACKEND_ENDPOINT",
},
```
b) Openshift deployment template for UI apps

```
Template Location: https://tfs01mom.industrysoftware.automation.siemens.com/tfs/defaultcollection/Revolution_MOM/Pratapgad%20Team/_git
/Revolution_MOM_Microservices_CF?path=%2FOpenshift%2FTemplates&version=GBmaster
```
```
Template filename: siemens_modularmom_ui_template.yaml
```
```
Under "kind: DeploymentConfig" look for "template: spec: containers: env:". Add a new placeholder as mentioned in webapp config file.
```
```
Navigate to end of file look for "parameters:" section and define a new parameter for your endpoint. Make sure it has a default value and required
flag set to "false"
```
```
For ex:.
```
```
name: TRACEANDTRACK_ENDPOINT
description: "Public endpoint where to connect to the TRACEANDTRACK backend microservice."
displayName: Where to connect to the TRACEANDTRACK microservice.
value: "traceandtrack-public-api"
required: false
```
c) On UI web app git repository root Navigate to "Containerization"

```
Edit file "dockerfile" Add new endpoint details
```
```
Ex:. ENV LOG_BACKEND_ENDPOINT="http://<service url>/odata/"
```

Batch job – error messages


# Http request methods for Modular MOM web apps

We have introduced a new HTTPRequest wrapper on top of the Http service provided by SWF.

This service is meant to intercept any HTTP call from web app to our Business microservice and inject the token generated for respective user. The
service takes care of requesting a refresh token as well depending on token expiry when HTTP call is issued.

From now on please do not use MomHttpRequests provided by MOMUI kit [ MomHttpGet, MomHttpPost,... ] for any API call from our web apps, instead
use this method “ModularMOMHttpRequest” exposed in file “ModularMOMHttpInterceptor.js”. Currently all the apps have already been modified to use this
interceptor.

Below are some examples that demonstrate how to use this method. This supports all HTTP verbs exposed by HttpRequest under utils.

GET

```
"getData": {
"actionType": "ModularMOMHttpRequest",
"inputData": {
"url": "{{ctx.appConfig.backendService}}",
"config": {
"withCredentials": false,
"method": "GET"
}
}
}
```
### POST

```
"postData": {
"actionType": "ModularMOMHttpRequest",
"inputData": {
"url": "{{ctx.appConfig.backendService}}",
"data": {
<<request body goes here>>
},
"config": {
"withCredentials": false,
"method": "POST"
}
}
}
```
Request Template defined under actionTemplateDefs.json:

```
"ModularMOMHttpRequest": {
"actionType": "JSFunctionAsync",
"method": "ModularMOMHttpCall",
"inputData": {
"url": "{{inputData.url}}",
"data": "{{inputData.data}}",
"config": "{{inputData.config}}",
"options": "{{inputData.options}}"
},
"deps": "js/ModularMOMHttpInterceptor"
}
```

# Guidelines for declaring MS permissions via Permission.

# xml

Modular MOM Business microservices need to define the functional rights they expose in terms of business functionality. These permissions / rights are
available on Admin app under “Role – Permission mapping” option. Permissions are owned by microservices and also linked to Modules in system.

Which micro-services should have Permission:

Any business Micro-service that exposes endpoints for consumption from UI apps must declare the permissions based on business functionality it is
supporting. [ These permissions are resolved by engine for each user action from the role claims passed in auth token ]

For the endpoints / functionality which are called internally by runtime engine / other microservice do not need to declare permissions as the engine uses
system token (with all functional rights) to execute actions.

How do we define Permission for a microservice:

We need to add a “permission.xml” file at repository root. This xml should define the Name of permission it is exposing on UI along with the Functional
rights on objects in system [ invoke / read / execute On Entity / Commands / Reading Functions ]

For every business microservice this data from xml is combined into the Manifest.xml file when we build the MS project and becomes a part of the
generated assemblies / artifacts.

How is this imported in Permission MS and processed by engine:

Upon the initialization of MS if there are any permissions defined in its manifest file, a call for “ImportPermissions” is sent to “Permission Microservice”
internally. Upon successful import, these permissions are populated in database of Permission MS which is then fetched and displayed on UI on ‘ Role –
Permission mapping’ request.

Except for Permission MS itself where Permissions are imported via DbInit files placed as artifacts by dev. ImportPermissions call is skipped in this case as
an exceptional scenario.

When we try to request for a specific page / functionality from UI, the token accompanied with API request contains roles claim that respective user has in
system. These roles are then used to fetch the associated permissions via Permission Gateway and then validated against the request.

List of currently exposed permissions by Microservices:

https://momwiki02.industrysoftware.automation.siemens.com/display/RevMOM/User+Permissions

Circuit recordings :

In below link of circuit recordings, we can find the explanation provided by Matteo on how to create “Permission.xml”, which location we need to place it
and many other details also.

https://circuit.siemens.com/#/conversation/45d842c6-c945-437e-b63e-9d14dabcbd48

## Examples:

Scenario: Read an entity –

Refer securable object name from – App -> POMModel project -> Model explorer -> Data Model -> { Select Model} -> Model Details -> Full name [Notice
the ReadingModel in entity name]

```
<?xml version="1.0" encoding="utf-8"?>
<permissions>
<permission>
<name>User Role View</name>
<functionrights>
<functionright>
<type>Entity</type>
<securableobjectname>Siemens.RevMOM.PermissionApp.PermissionApp.PMPOMModel.DataModel.
ReadingModel.Role</securableobjectname>
<operationname>read</operationname>
</functionright>
</functionrights>
```

```
</permission>
</permissions>
```
Scenario: Invoke a command –

Refer securable object name from – App -> POMModel project -> Model explorer -> Commands -> { Select Command} -> Model Details -> Full name
[Notice the Publishedl in command name]

```
<?xml version="1.0" encoding="utf-8"?>
<permissions>
<permission>
<name>User Role Add</name>
<functionrights>
<functionright>
<type>Command</type>
<securableobjectname>Siemens.RevMOM.PermissionApp.PermissionApp.PMPOMModel.
Commands.Published.CreateRole</securableobjectname>
<operationname>invoke</operationname>
</functionright>
</functionrights>
</permission>
</permissions>
```
Scenario: Invoke a Composite command –

Refer securable object name from – App -> POMModel project -> Model explorer -> Commands -> { Select Composite Command } -> Model Details ->
Full name [Should NOT have Published in name]

```
<?xml version="1.0" encoding="utf-8"?>
<permissions>
<permission>
<name>Production Order Launch</name>
<functionrights>
<functionright>
<type>Command</type>
<securableobjectname>Siemens.RevMOM.LaunchApp.LaunchApp.LHPOMModel.Commands.ProcessProductionOrder<
/securableobjectname>
<operationname>invoke</operationname>
</functionright>
</functionrights>
</permission>
</permissions>
```
Scenario: Invoke a Reading Function -

The permission format for a reading function follow the same schema of the command and entity, simply the verb is “execute” and the type is
“reading_function”. Where the name of the reading function is its full name.

```
<permission>
<name>Permission Execute Reading Function</name>
<functionrights>
<functionright>
<type>reading_function</type>
<securableobjectname>RevMOMTestApp.RevMOMTestApp.RevMOMTestApp.RMPOMModel.
ReadingFunctions.ReadFakeEntity</securableobjectname>
<operationname>execute</operationname>
</functionright>
</functionrights>
</permission>
```

Multiple functional rights under one permission can be defined –

Scenario: Requirement to invoke multiple objects to achieve a business functionality [ Ex.: Role – Permission assignment needs calls to fetch
module, Fetch Permission based on Module and Map Permission to roles ]

```
<?xml version="1.0" encoding="utf-8"?>
<permissions>
<permission>
<name>User Role / Permission Assignment</name>
<functionrights>
<functionright>
<type>Command</type>
<securableobjectname>Siemens.RevMOM.PermissionApp.PermissionApp.PMPOMModel.
Commands.Published.RoleToPermissionsMapping</securableobjectname>
<operationname>invoke</operationname>
</functionright>
<functionright>
<type>Command</type>
<securableobjectname>Siemens.RevMOM.PermissionApp.PermissionApp.PMPOMModel.
DataModel.ReadingModel.Module</securableobjectname>
<operationname>read</operationname>
</functionright>
<functionright>
<type>Command</type>
<securableobjectname>Siemens.RevMOM.PermissionApp.PermissionApp.PMPOMModel.
Commands.Published.GetPermissionsForRoleMapping</securableobjectname>
<operationname>invoke</operationname>
</functionright>
</functionrights>
</permission>
</permissions>
```
Scenario: Single Permission but have different behavior in different web app [ In supervise app – Work order is fetched from entity however in
Operate app – Work order is fetched via a FB command as it aggregates data from multiple MS ]. Both objects (entity & command) are owned by
same MS though.

```
<?xml version="1.0" encoding="utf-8"?>
<permissions>
<permission>
<name>Work Order View</name>
<functionrights>
<functionright>
<type>Entity</type>
<securableobjectname>Siemens.RevMOM.DispatchApp.DispatchApp.DPPOMModel.DataModel.ReadingModel.
WorkOrder</securableobjectname>
<operationname>read</operationname>
</functionright>
<functionright>
<type>Command</type>
<securableobjectname>Siemens.RevMOM.DispatchApp.DispatchApp.DPPOMModel.Commands.
TraceWokOrderProducedMaterial</securableobjectname>
<operationname>invoke</operationname>
</functionright>
</functionrights>
</permission>
</permissions>
```
Multiple Permissions in same permission.xml file can be defined:

```
<?xml version="1.0" encoding="utf-8"?>
<permissions>
<permission>
<name>User Role View</name>
```

<functionrights>
<functionright>
<type>Entity</type>
<securableobjectname>Siemens.RevMOM.PermissionApp.PermissionApp.PMPOMModel.
DataModel.ReadingModel.Role</securableobjectname>
<operationname>read</operationname>
</functionright>
</functionrights>
</permission>
<permission>
<name>User Role Add</name>
<functionrights>
<functionright>
<type>Command</type>
<securableobjectname>Siemens.RevMOM.PermissionApp.PermissionApp.PMPOMModel.
Commands.Published.CreateRole</securableobjectname>
<operationname>invoke</operationname>
</functionright>
</functionrights>
</permission>
</permissions>


### 1.

### 2.

### 1.

### 2.

# Adding Nuget packages in ModularMOM MS

Package Source/Feeds to be used for all the nuget packages :

```
UAF DEV FEED : https://tfs01mom.industrysoftware.automation.siemens.com/tfs/DefaultCollection/_packaging/UAF_Feed/nuget/v3/index.json
REVMOM THIRD PARTS : https://tfs01mom.industrysoftware.automation.siemens.com/tfs/DefaultCollection/_packaging
/UAF_RevMOM_ThirdParties_feed/nuget/v3/index.json
REVMOM : https://tfs01mom.industrysoftware.automation.siemens.com/tfs/DefaultCollection/_packaging/UAF_RevMOM_feed/nuget/v3/index.json
UAF THIRD PARTS : https://tfs01mom.industrysoftware.automation.siemens.com/tfs/DefaultCollection/_packaging/UAF_ThirdParties/nuget/v3
/index.json
```
Steps to include any 3rd party nuget package in Business MS :

```
Identify the dependencies of the newly added NuGet package.
Check if all of them are present in the TFS feed. TFS feed: https://tfs01mom.industrysoftware.automation.siemens.com/tfs/DefaultCollection
/Revolution_MOM/_packaging?feed=UAF_RevMOM_ThirdParties_feed&_a=feed
The packages that are not present there download them and keep the .nupkg files in a separate folder.
Add the folder as package source in NuGet config.
Add the nuget package to FB/App handler projects in Project Studio from above mentioned source and try to build FB/APP projects.
For Lambda MS template , follow below mentioned steps to add the nuget package.
```
```
Manually add a file called "nuget.targets" at the root of your service(Usually the repo folder)
There you have to put something like :
```
```
Sample content
```
```
<Project xmlns="http://schemas.microsoft.com/developer/msbuild/2003">
<ItemGroup>
<PackageReference Include="<my nuget>" Version="<the version>" />
</ItemGroup>
</Project>
```
```
In this way, all the “Handler” projects inside the MS_Template will load the new targets and you can compile the service.
For reference , follow this link https://tfs01mom.industrysoftware.automation.siemens.com/tfs/defaultcollection/Revolution_MOM/Rajgad%20Team
/_git/Revolution_MOM_MS_Permission?path=%2Fnuget.targets&version=GBmaster
```
```
If build is successful, share the newly added .nupkg files with Matteo.
Once the packages are added in the feed, wait for some time, remove the local folder source from NuGet config and run the build again. The
build should be successful.
```
Steps to include any third party packages in Infrastructure MS :

```
Identify the dependencies of the newly added NuGet package.
Check if all of them are present in the TFS feed.
TFS feed: https://tfs01mom.industrysoftware.automation.siemens.com/tfs/DefaultCollection/Revolution_MOM/_packaging?
feed=UAF_RevMOM_ThirdParties_feed&_a=feed
The packages that are not present there download them and keep the .nupkg files in a separate folder.
Add the folder as package source in NuGet config.
Run Build.
If build is successful, share the newly added .nupkg files with Matteo.
Once the packages are added in the feed, wait for some time, remove the local folder source from NuGet config and run the build again. The
build should be successful.
```
Please Note : Microsoft packages that are not part of the netstandard or netcore core libraries are 3rd party packages too.

Adding a 3rd party nuget package is not a recommended practice, so it has to be done only in cases of real need. If the nuget package is not in our TFS
feeds, you need to notify Bardini, Matteo and/or Trifoglio, Giuliano , so that the package clearing, potential runtime issues are addressed and to identify
the overall usability of the package in current scenario.


### 1.

### 2.

### 3.

### 4.

### 5.

### 6.

```
a.
```
```
b.
c.
```
### 1.

### 2.

# Getting started with OpenShift CLI

## Steps how to Setup the Openshift Client (oc command)

```
Go to https://github.com/openshift/origin/releases
Click on the latest release e.g. v3.11.0
Download the openshift-origin-client-tools-v3.11.0-0cbc58b-windows.zip
Unzip the openshift-origin-client-tools-v3.11.0-0cbc58b-windows.zip file and rename the folder to OpenshiftClient just for readability
Copy the above folder to the location where you want to have this command in your VDI e.g. C:\Program Files (x86)\OpenshiftClient
Setup environment variables
Open the control panel and click on System. Click on Advanced system settings on the left or open the Advanced tab of System
Properties. Click on the button labeled Environment Variables at the bottom. Look for the option in User variables section for Path
Click on New and provide the path of your OpenshiftClient folder path e.g. C:\Program Files (x86)\OpenshiftClient and Click on OK.
The following are a snap of the steps mentioned above.
```
## Login in OpenShift CLI using SWQA Credentials

```
Login using swqa credentials in https://console-openshift-console.apps.openshift01.swqa.tst/topology/ns/default
Click on Copy Login Command from the console
```

### 1.

### 2.

```
a.
b.
c.
d.
e.
3.
```
3. Copy & run the oc login command e.g. oc login --token=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx --server=https://api.openshift01.swqa.tst:
6443

oc login --token=h1zaubQJf7-bpjzyifmCT6vjUwCTgFPKRRmiPBXtDFI --server=https://api.openshift01.swqa.tst:6443

List of useful oc commands

```
Official Red Hat OpenShift CLI Commands, https://docs.openshift.com/container-platform/4.1/cli_reference/developer-cli-commands.html
Following are some useful commands which can be useful for the beginner in OpenShift,
oc version
oc get project
oc get template
oc delete template template_name e.g. oc delete template siemens-modularmom-template
oc process template/template_name -p parameter_list e.g. oc process template/siemens-mom-engine-ms-template -p NAME=permission
Our wiki page Modular MOM Documentation https://momwiki01.industrysoftware.automation.siemens.com/display/MMADoc
/How+to+Install+and+Configure+Modular+MOM+Components is a good starting point to understand how we can install the different components
in OpenShift.
```

### 1.

### 2.

### 3.

### 1.

### 2.

### 3.

### 4.

### 5.

### 6.

### 7.

### 8.

### 9.

### 10.

### 11.

### 12.

### 1.

### 2.

### 3.

### 4.

### 5.

### 6.

### 7.

# Steps to Update Engine references in Modular MOM project

Engine references can be updated for :

```
Business MS
Infrastructure MS
```
How to check whether we are referring to the appropriate Engine version :

```
Go to git repository "Revolution_MOM_MS_Template"
Traverse till the Path : Revolution_MOM_MS_Template/LambdaMicroservice/LambdaMicroservice/LambdaMicroservice.csproj
Check for Package Reference "ModularMOM.Runtime.Engine" version value
```
Steps to update Engine references for Business MS are :

```
Select the Business MS for which you want to update the Engine references
Create your development branch by cloning it from the master branch
Update submodules "Microservice" and "Shared". This way we will get the latest pull from both the repositories
Update references in FB (if needed depending on references used) Open Manage NuGet Packages for solution and update to the required
version needed.
Clean & Build FB
Update references in App (if needed depending on references used) Open Manage NuGet Packages for solution and update to the required
version needed.
Clean & Build App
Build Lambda Microservice
Publish
Do a sanity check
Verify changes should have only submodules and reference changes(if any)
Commit the changes to the feature branch and raise pull request
```
Steps to update Engine references for Infrastructure MS are :

```
Select the Infrastructure MS for which you want to update the Engine references
Create your development branch by cloning it from the master branch
Update references in Solution Open Manage NuGet Packages for solution and update to the required version needed.
Clean & Build Solution
Do a sanity check
Verify changes has only references related changes
Commit the changes to the feature branch and raise pull request
```

# How to setup the SQL Server sa user on Opcenter X VDI

Please use this procedure in order to authorize your swqa account to logon SQL (and also to reset sa pwd) on HostNameofVDI.swqa.tst

```
Open SQL Server Configuration Manager
Stop the SQL Server Instance you need to recover the SA password
Open the properties on the SQL Server Instance and click on the Advanced tab
Change the Startup parameter by adding-m;at the begging of the line and click OK
Start the SQL Service Instance
Open the command prompt(run as administrator)
Run sqlcmd and press enter
```
EXEC sp_addsrvrolemember 'SWQA\YourUsername', 'sysadmin';
GO

```
Open SQL Server Configuration Manager
Stop the SQL Server Instance
Open the properties on the SQL Server Instance and click on the Advanced tab
Change the Startup parameter by removing the-m;at the begging of the line and click OK
Start the SQL Service Instance
Open Microsoft SQL Server Management Studio and login with the account you added
Under the DB, expand Security, then Logins
Open the properties for the sa account, and reset the password
```

# Tips to design a Use Case flow diagram

```
Try to understand the overall flow and identify all the Apps and/or Micro-services involved in the Use Case.
Draw.io is now integrated with confluence, so it's more intuitive to edit the flow diagram within the confluence editor itself.
For more details around draw.io ,follow https://drawio-app.com/learning/
Mention the MS/App name in the rectangular boxes.
For every Http call, put solid and dashed arrows to show when the call starts and when it ends.
Color coding can be used to differentiate between a MS and a Queue, Green color can be used to represent a Queue and for MS/App no color is
required.
Keep in mind that there won't be a return arrow in case of a message published to Queue( since the message is not expected to return a
response).
Follow naming conventions for depicting the API calls, try to stick to actual API endpoints names for more clarity.
API endpoint parameters can be skipped to reduce clutter and increase readability.
It's encouraged to add comments in the diagram ,whenever the need arises.
Keep a watch on this space , which is used to add Use Case diagrams : https://momwiki02.industrysoftware.automation.siemens.com/display
/RevMOM/Use+cases?src=contextnavpagetreemode
```
Sample Use case diagram for reference:


# Step by step guide to run BDD automated tests for any UI

# microservices

In addition to the Js unit tests for the UI side, BDD tests make sure the functionality is tested as per the user expected it to function.

## How to setup BDD Automation for a new UI MS

the following steps can be referred when BDD automation has to be implemented for a new UI MS.

Folder Structure and scripts to include

```
Test folder, protractor.conf.js, protractor_debug.conf.js file, tsconfig.json file, .vscode folder should be the same level as src folder.
.vscode file contains the debug configuration for the bdd automated tests.
```
Inside .vscode folder create two files 1) launch.json 2) settings.json

```
Test folder contains three subfolders namely
features (which contains all the .feature files)
stepdefs (which contains all the .ts stepdef files)
pages (which contains all the .ts page files)
```
```
Include the following code in the package.json
"devDependencies": {
"bdd-test-framework": "4.0"
}
Include following scripts in the package.json
```
"bdd_build": "gulp --gulpfile node_modules/bdd-test-framework/gulpfile.js --cwd. all",

"bdd_test": "gulp --gulpfile node_modules/bdd-test-framework/gulpfile.js --cwd. createreportsfolder && protractor protractor.conf.js --
baseUrl=http://127.0.0.1:8080 --cucumberOpts.tags=~@ignore",

"bdd_report": "node node_modules/bdd-test-framework/test/util/createHTMLReport.js",

"bdd_report_launch": "start reports/e2e/cucumber-test-results.html",

"webdriverupdate": "webdriver-manager update",

"webdriverstart": "webdriver-manager start",


"bdd_framework_build": "cd node_modules/bdd-test-framework && npm run build",

"webdriver": "npm run webdriverupdate && npm run webdriverstart"

Testing the Feature files

Open three terminals in visual studio code and then run the following scripts in the given manner.

1. Terminal – 1:

```
npm install
npm run build
npm start
```
The above commands lets you run the UI app locally.

2. Terminal – 2:

```
npm run bdd_framework_build
```
Run this command, for the first time to build BDD Test Framework. All the cucumber steps will get linked with the bdd step defs available in the
framework.

```
npm run bdd_build
```
Run this command, for building custom stepdefs. All the custom cucumber steps in the test folder will get linked with the custom bdd step defs
available in the framework.You have to run this command only when you change stepDef or page files in addition to the first time.

3. Terminal – 3:

```
npm run webdriver
```
This starts selenium web-driver with which we can access browser automation APIs provided by browser vendors to control browser and run tests.
Please note that this script contains inbuilt selenium webdriver update and selenium webdriver start scripts.Run this command for the first time.

4. Terminal – 2:

```
npm run bdd_test
```
This command tests the feature files that are specified in protractor.conf.js file against the selenium webdriver that we just started.Run this
command, when there is a modification in the feature file and not in the stepdef. If there is a stepDef file change then run 'npm run bdd_build' which builds
the custom stepDefs and page files and then run this script to test those changes.

```
npm run bdd_report
```
Run this script whenever you want to generate a report for the features that you just run.It takes the data from results.json present inside reports
folder and create a dashboard presentation to view the results.the path can be absorbed from the terminal after running this script.open that path in any
browser to see the dashboard representation.

Common Issues you may face while doing BDD automation

```
npm run bdd_framework_build throws node conflict errors:
```
Solution : If you are seeing node conflict errors then you need to delete the “node” folder from location “C:
\Blah_Blah\latest\supervise\node_modules\protractor\node_modules@types“ (the location may vary as per your project) And then try running “npm run
bdd_framework_build”

Try to delete @types folder inside the protractor node modules and run the script again.

```
You may face an error regarding java as most of the times Java will not come pre-installed in VDI. So try to install java any version for the first
time and you are good to go.
Typescript error while doing npm run bdd_build"
```

Solution: Include "import chai = require('chai-as-promised');" in the file where the error is thrown along with other imports.

Debugging StepDefs and Page Files

```
Install "Cucumber (Gherkin) Full Support" plugin for Visual Studio Code
```
This plugin allows you to navigate from feature file to its method definition in stepdef file and then to the respective page file.The flow of the
framework is feature file step >> relevant Stepdefs file >> relevant Page file

For example: Step "Then I validate HomepageCard "Work Orders" is visible" in feature file will call >>function iValidateLocalText() in test
/stepdefs/superviseHomeStepDefs.ts file which in turn will call >>function checkText() in test/pages/superviseHomePage.ts file

```
When you run "npm run bdd_build" command as mentioned in section "Testing the feature files" , the typescript files are compiled in `javascript
files which reside in %ROOT%\dist folder, to debug you need to put a breakpoint in these js files
```
```
For example For the files test/stepdefs/superviseHomeStepDefs.ts, test/pages/superviseHomePage.ts mentioned above you will find
corresponding files in Dist folder
```
```
Dist/stepdefs/superviseHomeStepDefs.js
Dist/pages/superviseHomePage.js
To find the js file you need to debug:
```
```
Right click on the problematic cucumber feature step and select "Go to definition"
This will take you to the stepdef typescript file and corresponding js file in Dist folder is the file that you need to debug
Navigate to %ROOT%\dist folder in visual studio code and put breakpoint at code you want to Debug
Go to Debug mode in Visual Studio Code from left edge toolbar
Select "Launch E2E Tests" in launch configuration dropdown from Top
And click the "Run" button besides launch configuration dropdown
Additionally you may need to remove '--headless' configuration in chromeoptions from %ROOT%\protractor_debug.conf.js to see whats
happening on browser
```

### 1.

### 2.

### 3.

### 4.

### 5.

### 6.

### 7.

### 8.

### 9.

### 10.

### 11.

### 12.

### 13.

### 14.

### 15.

### 16.

### 17.

# Event Implementation Guide for ModularMOM MS

Steps to follow for implementing events in ModularMom MS :

```
Update your services to the latest submodule of Microservice.
Identify the use case where you need to implement event, finalize the event parameters.
for the common event , create a Repository (For every common event there has to be a separate Repository), gave a name following this pattern
Revolution_MOM_EVT_{Event_Name}. As an example, for the event WorkOrderCreated, the resulting repository name will be:
Revolution_MOM_EVT_WorkOrderCreated”
Clone this repository and Following Revolution_MOM_EVT_WorkOrderCreated structure, create a FB (name it something like this
'WorkOrderCreatedFB').
In this FB, define the event (name could be 'WorkOrderCreated') and its parameters. Also don't forget to check the 'Copy to Engineering Folder' in
installer project properties. Commit the code.
Now , clone the Repository where you want to use this event. for instance let's assume that Dispatch need to fire 'WorkOrderCreated' event and
'TNT' would listen to this event.
So clone the Dispatch Repository, in order to do so, go to the FBs folder and add a submodule using git command , for eg. : git submodule add --
name WOCreatedCommonEvent --force https://tfs01mom.industrysoftware.automation.siemens.com/tfs/DefaultCollection/Revolution_MOM/_git
/Revolution_MOM_EVT_WorkOrderCreated WOCreatedCommonEvent
The resulting folder structure should be the same of the other FB (the MS one), so FBs\{Your event}\{EventFb solution} (for example,
FBs\WOCreatedCommonEvent\WorkOrderCreatedFB.sln). If the resulting folder structure is not the following, please check on the original
common event repo structure and verify if there are differences in the folder and files structures. Now, open the 'WorkOrderCreatedFB' solution
and build it, then Open 'DispatchMS' FB and from project references add 'WorkOrderCreatedFB' package.
Once you've done it, you should find the 'WorkOrderCreated' event inside 'DispatchMS' FB Model project.
In 'DispatchMS' FB , open the relevant handler file and add the logic to fire the event by passing values to the event parameters. Here you might
need to add a reference to the Event Model artifact dll, containing 'WorkOrderCreated' (the one created by 'WorkOrderCreatedFB').
Build 'DispatchMS' and then open 'DispatchMS' App project , reload the FB package (after reloading ,if 'WorkOrderCreatedFB' package is not
installed, try to uninstall 'DispatchMS' FB package and then install again, it will automatically install the dependent 'WorkOrderCreatedFB'
package ).
Rest of the steps for 'DispatchMS' are unchanged , like building Lambda MS and then publishing it locally for testing.
Now switch to TNT , first clone the repository and then follow the same steps as mentioned above (step 5,6,7).
Then inside 'TNTMS' FB project, using model editor, create a event handler for 'WorkOrderCreated' event.
Import this event handler in EventHandler project and write you logic. Build the 'TNTMS' FB.
Open 'TNTMS' App project and follow the same step as described in step 9 above. Also ensure that the predefined subscription to
'WorkOrderCreated' event handler in Model editor is set to true. (You can set this true by selecting the event handler in model project and going to
its properties). Build the App project.
Again rest of the steps for 'TNTMS' are unchanged , like building Lambda MS and then publishing it locally for testing.
```
As part of POC : Dispatch , TNT and common event has been used , please find below the tfs links for the same:

https://tfs01mom.industrysoftware.automation.siemens.com/tfs/DefaultCollection/Revolution_MOM/_git/Revolution_MOM_EVT_WorkOrderCreated

https://tfs01mom.industrysoftware.automation.siemens.com/tfs/DefaultCollection/Revolution_MOM/Rajgad%20Team/_git/Revolution_MOM_MS_Dispatch?
path=%2FFBs%2FDispatchFB%2FDispatchFB%2FDispatchFB.CommandHandler%2FDispatchFB.MSModel%2FCommands%2FGetWorkOrderHandler.
cs&version=GBTestCommonEvent_Dispatch

https://tfs01mom.industrysoftware.automation.siemens.com/tfs/DefaultCollection/Revolution_MOM/Rajgad%20Team/_git
/Revolution_MOM_MS_TrackAndTrace?path=%2FFBs%2FTraceNTrackFB%2FTraceNTrackFB%2FTraceNTrackFB.EventHandler%2FTraceNTrackFB.
MSModel%2FEvents%2FWorkOrderCreated%2FWorkOrderCreated_Handler.cs&version=GBTestCommonEvent_TNT


# MIOUMC0x: Installation of Additional Resources

This section consists of the following chapters:

```
Chapter Description
```
```
How to Install and Configure UMC Contains info on how to
```
```
install and configure the User Management component (UMC) according to the following scenarios:
stand-alone scenario
redundant scenario.
configure the UMC web components to use HTTP or HTTPS
configure the Windows session authentication
configure the smart card authentication.
```
```
UMC Endpoints Configuration Info on configuring the UMC endpoints.
```
```
How to Install and Configure MIO Contains info on how to install and configure Opcenter Connect MOM (formerly, MIO).
```

### 1.

### 2.

### 3.

### 4.

### 5.

# How to Install and Configure UMC

Follow this workflow to install and configure UMC:

```
Satisfy the UMC Prerequisites.
Install or Upgrade UMC on the Windows Server 2016 machine.
Configure UMC according to the redundant scenario.
Optionally, you can configure the Windows session authentication to log in to UMC using the current Windows session.
Optionally, you can configure the smart card authentication to log in to UMC using the personal smart card.
```

### 1.

### 2.

### 3.

### 4.

### 5.

### 6.

### 7.

### 8.

### 9.

### 10.

### 1.

### 2.

### 3.

### 4.

### 5.

### 6.

### 7.

### 8.

### 9.

### 10.

# UMC Prerequisites

These are the UMC prerequisites:

```
Windows Server 2016
Microsoft Framework:
Microsoft .NET Framework 4 Client Profile
Microsoft .NET Framework 4 Extended.
Microsoft IIS 7.5, 8, 8.5, or 10
```
```
Application Request Routing 3.0 and its prerequisites
IIS Prerequisites.
```
## Application Request Routing 3.0

Follow this procedure to install the WebSocket Protocol server role:

```
Open Server Manager.
Select Add Roles and Features.
Click Next.
Select Role-based or feature-based installation.
Select the current server from the pool.
Expand Web Server (IIS) Web Server Application Development.
Select the WebSocket Protocol role.
Click Next.
Click Install.
After installing the WebSocket Protocol server role, download and install:
Internet Information Services (IIS) 10.0 Express
Application Request Routing 3.0.
```
## IIS Prerequisites

Verify the following roles and features are installed for the Windows Server 2016 machine:

```
Open Server Manager.
In the left pane, click Dashboard.
Click Add Roles and Features.
Click Role-based or feature-based installation and click Next.
Select a server from the list and click Next.
Expand Web Server (IIS).
Verify the following roles are installed under Web Server:
```
```
Common HTTP Features:
Default Document
Directory Browsing
HTTP Errors
Static Content
Health and Diagnostics:
HTTP Logging
Performance:
```
```
Static Content Compression
Security:
```
```
Request Filtering
Windows Authentication
Application Development:
```
```
.NET Extensibility 4.6
ASP.NET 4.6
ISAPI Extensions
ISAPI Filters
Verify the following roles are installed under Management Tools:
IIS Management Console
IIS Management Scripts and Tools
Management Service
Click Next.
Verify the following features are installed:
```
```
.NET Framework 3.5 Features:
.NET Framework 3.5 (includes .NET 2.0 and 3.0)
```
```
In case of IIS 7.5, .json must be added to the MIME types.
```

### 10.

### 11.

### 12.

```
.NET Framework 4.6 Features:
.NET Framework 4.6
ASP.NET 4.6
WCF Services:
TCP Port Sharing
Windows Defender Features:
Windows Defender
GUI for Windows Defender
Windows PowerShell:
Windows PowerShell 5.1
Windows PowerShell 2.0 Engine
Windows PowerShell ISE
Verify that WoW64 Support is installed.
Close Server Manager.
```

### 1.

### 2.

### 3.

### 4.

### 1.

### 2.

### 3.

### 4.

### 5.

### 6.

### 7.

### 8.

### 9.

### 10.

### 11.

# Installing or Upgrade UMC

The UMC prerequisites must be satisfied before you start installing UMC. Follow this procedure to install UMC:

Note:

```
Open the Temp folder at the Windows Server 2016 machine.
Follow the path: UMC_02.08.01.00_04.03.0002\Delivery\BUNDLE\UMC\.
Run Start.exe.
Choose the default settings in the installer.
```
How to upgrade UMC

```
Check the version of installed UMC via Control Panel Programs and features
Open this location on the machine C:\Program Files\Siemens\UserManagement\BIN
Run this file as administrator REMOVE_IdP_WebUI_configuration.bat
Open the new version package you want to install, for example: Follow the path: UMC_02.09.02.00_04.05.0003\Delivery\BUNDLE\UMC\
Run Start.exe [ system will prompt for restart, allow it to restart then follow the setup wizard ]
From the bin location mentioned in step 2 run "umconf.exe" on next screen select option "Upgrade" and use command "umconf -U"
Upon completion of installation navigate to this path again C:\Program Files\Siemens\UserManagement\BIN
Execute this batch file IdP_WebUI_configuration.bat
Copy the content of the modularmom_umc_patch_1.0.0.zip file to the machines of your Windows application cluster
Unzip the modularmom_umc_patch_1.0.0.zip file and execute the .bat file as administrator
Reset / refresh IIS
```

### 1.

### 2.

### 3.

### 4.

# Cleaning UMC Configuration Up

To remove the existing UMC configuration, follow this procedure:

```
Open PowerShell as Administrator.
Go the following directory: C:\Program Files\Siemens\UserManagement\BIN\.
Remove the UMC web components configuration by running this command:
```
```
.\REMOVE_IdP_WebUI_configuration.bat
```
```
Clean the UMC configuration DB by running this command:
```
```
.\UMConf.exe -D -f
```

### 1.

### 2.

### 3.

### 4.

### 5.

### 6.

# How to Configure UMC

Follow this workflow to configure UMC:

```
Clean the UMC configuration unless you configure UMC at the machine where UMC was not configured previously.
Configure the machine as the UMC priority ring server.
```
```
Configure the UMC web components at the priority ring server.
For the redundant scenario, configure the other machine as the UMC secondary ring server.
Configure the UMC web components at the secondary ring server.
If you use the redundant scenario, verify that the scenario was configured properly.
```
```
The priority ring server machine will operate according to the stand-alone scenario if no other machine is configured as the UMC
secondary ring server.
```

### 1.

### 2.

### 3.

### 4.

# Configuring UMC Priority Ring Server

Follow this procedure to configure the machine you have elected as the UMC priority ring server:

```
Open Power Shell as Administrator.
Go to the following directory: C:\Program Files\Siemens\UserManagement\BIN\.
Run the following command:
```
```
.\UMConf.exe
```
```
Enter the values accordingly:
UM domain name: MMOM
UM administrator user name: UMCadmin
Password: SwqaMe$1
Retype Password: SwqaMe$1
Service account user name (domain\user): .\Administrator
Password: SwqaMe$1
Do you want to configure provisioning service? [y/n ] n
```

### 1.

### 2.

### 3.

### 4.

### 5.

# Configuring UMC Secondary Ring Server

Follow this procedure to configure the machine you have elected as the UMC secondary ring server:

```
Install UMC on the machine as described in Installing or Upgrade UMC.
Open Power Shell as Administrator.
Go to the following directory: C:\Program Files\Siemens\UserManagement\BIN\.
Get the fingerprint code of the UMC priority ring server by running this command:
```
```
.\umconf -fingerprint -c MIOUMC0x
```
```
Join the UMC priority ring server by running this command:
```
```
.\umconf -j -m 1 -c MIOUMC0x -u UMCadmin -p 'SwqaMe$1' -s .\Administrator 'SwqaMe$1' -v -b -fp
<fingerprint code>
```

# How to Configure UMC Web Components

The procedures on configuring the UMC web components vary depending on which protocol you want to use – HTTP or HTTPS.

```
Configure UMC to Use HTTP
Configure UMC to Use HTTPS
```

### 1.

### 2.

### 3.

### 1.

### 2.

### 3.

### 4.

### 5.

# Configuring UMC to Use HTTP

Follow the specified procedure to configure the UMC web components to use the HTTP protocol.

## Procedure

```
Open Power Shell as Administrator.
Go to the following directory: C:\Program Files\Siemens\UserManagement\BIN\.
Run the following command:
```
```
.\IdP_WebUI_configurator.bat
```
## Checking Web Components Configuration

Make sure the UMC web components were configured correctly following this procedure:

```
Open Internet Information Services (IIS) Manager.
Expand the Default Web Site entry and select the UMC entry.
In the right part of Internet Information Services (IIS) Manager, click Browse.
The Login page of UMC will open in the browser.
Log in to UMC as UMCadmin – the UMC home page appears if the web components were configured correctly.
```

### 1.

### 2.

### 3.

### 4.

### 5.

### 6.

### 1.

### 2.

### 3.

### 1.

### 2.

### 3.

### 4.

### 5.

### 6.

### 1.

### 2.

### 3.

### 4.

### 5.

### 1.

### 2.

### 3.

### 4.

# How to Configure UMC to Use HTTPS

Follow this workflow to configure UMC to use HTTPS:

```
Add your swqa account to the Administrators group.
Disable UAC in the registry.
Configure the default web site to use the SSL certificate.
Configure the UMC web components to use HTTPS.
Check the web components to be configured properly.
Check that the connection is secure.
```
## Adding SWQA Account to Administrators Group

```
Log in to the UMC server machine as Administrator.
Open Control Panel User Accounts Manage User Accounts.
Add your swqa account to the Administrators group.
```
## Disabling UAC in Registry

```
Open the Windows Registry.
Navigate to the following folder: HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Windows\CurrentVersion\Policies\System\.
In the right pane, select the EnableLUA DWORD key.
If there is no such key – create it.
Set the EnableLUA DWORD key value to "0".
Restart the machine.
```
## Configuring Default Web Site to Use SSL Certificate

```
Open Internet Information Services (IIS) Manager.
In the left pane, select the Default Web Site entry in the tree.
In the right pane, click Bindings.
Add the HTTPS binding and select the SSL certificate that will be trusted by the machines in the swqa domain.
```
```
Restart the machine.
```
## Configuring UMC Web Components to Use HTTPS

```
Log in to the machine with your swqa account.
```
```
Open Power Shell as Administrator.
Go to the following directory: C:\Program Files\Siemens\UserManagement\BIN\.
```
```
Using the swqa account is crucial. If you use the Windows local user, the web components will fail to be configured to use the HTTPS
protocol properly.
```
```
The FQDN name must be used for the secure connection. Using the swqa account in the next steps will allow you to set the UMC web
components to use the FQDN.
```

### 4.

### 1.

### 2.

### 3.

### 4.

### 5.

### 1.

### 2.

### 3.

### 4.

```
Run the following command:
```
```
.\IdP_WebUI_configurator.bat
```
Checking Web Components Configuration

Make sure the UMC web components were configured correctly following this procedure:

```
Open Internet Information Services (IIS) Manager.
Expand the Default Web Site entry and select the UMC entry.
In the right part of Internet Information Services (IIS) Manager, click Browse.
The Login page of UMC will open in the browser.
Log in to UMC as UMCadmin – the UMC home page appears if the web components were configured correctly.
```
Checking Connection to Be Secure

To verify that the UMC web components were configured to use the HTTPS protocol properly, follow this procedure:

```
Open the following URL from a remote machine that is in the same domain with the UMC machines: https://MIOUMC0x.SWQA.TST.
Make sure the browser perceives the connection as a secure one.
Log in to UMC as UMCadmin.
Make sure the browser perceives the connection as a secure one again.
```

### 1.

### 2.

### 3.

### 4.

# Verifying UMC Redundant Scenario

To verify the redundant scenario, follow this procedure:

```
Open the UMC secondary ring server URL.
Log in to UMC as UMCadmin.
In the upper-right corner, click the main menu and select Event Log.
Compare the event logs with the event logs of the UMC priority ring server – the logs must be the same.
```

### 1.

### 2.

### 3.

### 4.

### 5.

# How to Configure Strong Authentication Level For Smart

# Card

In order to set only smart card login for admin app , the authentication level of smart card authentication should be strong and for Password authentication
and windows authentication it should be weak/standard.

By default, the smart card authentication option is disabled in UMC. Follow this procedure to enable the option:

```
Log in to UMC.
In the upper-right corner, click the main menu and select IDP Configuration.
In the Authentication options tab, select authentication level as strong for Smart Card Authentication.
Select authentication level as weak for other two authentication methods.
```
```
Click Save all changes.
```

### 1.

### 2.

### 3.

### 1.

### 2.

### 3.

### 4.

### 5.

# How to Authenticate with Windows Session

There are two types of the Windows users that can be used to log in to UMC:

```
Windows local users
Windows Active Directory (AD) users
```
## How to Log In as Windows Local User

Follow this workflow to log in to UMC as the Windows local user:

```
Import the Windows local user.
Open the URL of UMC.
Click Use your current Windows session to Login.
```
## How to Log In as AD User

Follow this workflow to log in to UMC from a remote machine – that is in the same domain with the UMC server machine – as the AD user:

```
Configure the service provisioning.
Add the domain to the whitelist.
Import the AD user.
Open the URL of UMC.
Click Use your current Windows session to Login.
```
```
You can log in as the Windows local user to UMC at the UMC server machine only.
```

### 1.

### 2.

### 3.

### 4.

### 5.

### 6.

# Importing Windows Local Users

This procedure can only be used to import a local machine user on the UMC priority ring server or UMC secondary ring server machines:

```
Run Power Shell as Administrator.
Navigate to the folder where the umx utility is located.
```
```
Type the following command:
```
```
.\umx -x commandUserName commandUserPassword -I -u -w userName
```
```
Type the name of the UMC user that is executing the command and the user password instead of commandUserName and commandUserPasswo
rd respectively.
Type the name of the Windows user that you want to import instead of userName.
Run the command.
```
```
The default folder of the umx utility is C:\Program Files\Siemens\UserManagement\BIN.
```

### 1.

### 2.

### 3.

### 4.

### 5.

### 6.

# Configuring Service Provisioning

If the service provisioning was not configured while installing and configuring UMC, follow this procedure to configure the service provisioning:

```
Run Power Shell as Administrator.
Navigate to the folder where the umconf utility is located.
```
```
Type the following command to set the account under which the provisioning service will run:
```
```
.\umconf -P -u name -p password -f
```
```
Type the user name preceded by the domain instead of name.
```
```
Type the user password instead of password.
Run the command.
```
```
The default folder of the umconf utility is C:\Program Files\Siemens\UserManagement\BIN.
```
```
For the SWQA machines, the formatting must be the following: swqa\<username>. It could be a good idea to use your own SWQA
user, as access to the Active Directory is required for this account. I.e. a local system account will not work.
```

### 1.

### 2.

### 3.

### 4.

### 5.

# Adding Domain to Whitelist

Follow the procedure to add a domain to the whitelist:

```
Run Power Shell as Administrator.
Navigate to the folder where the umconf utility is located.
```
```
Type the following command:
```
```
.\umconf -c -w -d name
```
```
Type the domain name instead of name.
Run the command.
```
```
The default folder of the umconf utility is C:\Program Files\Siemens\UserManagement\BIN.
```

### 1.

### 2.

### 3.

### 4.

### 5.

### 6.

# Importing AD Users

This procedure describes how to import one or more Active Directory (AD) users (Windows domain users) to UMC:

```
Log in to UMC
Click Import Users.
Select the domain that you want to import the user from.
```
```
Enter the name of the user that you want to import and click Search.
Select the user and click Add.
Click Import.
```

### 1.

### 2.

### 3.

### 4.

### 5.

### 6.

### 7.

### 8.

# How to Authenticate with Smart Card

Follow the specified workflow to log in to UMC using the personal smart card. Also, make sure the prerequisites are satisfied.

## Prerequisites

The UMC web components must be configured to use the HTTPS protocol.

## Workflow

```
Configure the client machine.
Enable the smart card authentication option.
Enable the PKI filter.
```
```
Add the new user to UMC according to the PKI filter you selected.
Open the URL of UMC at the client machine.
Click Use your smart card or personal certificate to Login.
```
```
Select the certificate.
Enter your pin.
```
```
The smart card authentication can only be configured if the UMC web components were configured to use HTTPS.
```
```
The Authenticate using CN filter is used in this workflow. For details on the other filters, refer to the UMC documentation.
```

### 1.

### 2.

### 3.

### 4.

### 5.

### 6.

### 7.

# How to Configure Client Machine

Follow this workflow to configure the client machine to use the smart card authentication:

```
If you want to use a virtual machine (VM) as the client, connect to the VM using the Remote Desktop Connection application.
```
```
Enable the smart card device for your remote session.
Copy the PKI_CardOS folder from \\areames.siemens.com\SoftwarePool\ to the client machine.
Install PKI Basic Client on your client machine.
Restart the client machine.
After restarting the client machine, the icon in the System Tray must be yellow without any error sign.
```
```
Disable the CRL check.
```
```
The smart card authentication will not work if you use the VMware Horizon Client to connect to the VM.
```

### 1.

### 2.

### 3.

### 4.

### 5.

### 6.

# Installing PKI Basic Client

Follow this procedure to install the public key infrastructure (PKI) basic client:

```
At the client machine, open the PKI_CardOS folder and run ST0000280.msi.
Click Next.
```
```
Click Next.
Select the Complete setup type.
Click Next.
Click Install.
```

### 1.

### 2.

### 3.

### 4.

### 5.

# Enabling Smart Card for Remote Session

Follow these steps to enable the smart card device for the remote desktop session:

```
Open the Remote Desktop Connection application.
Select the Local Resources tab.
Click More.
Select Smart cards.
```
```
Click OK.
```

### 1.

### 2.

### 3.

### 4.

### 5.

# Disabling CRL Check

Follow these steps to disable the certificate revocation list (CRL) check at the client machine:

```
Copy Modify CRL Check.zip to the client machine and unzip it.
Run Power Shell as Administrator.
Navigate to the Modify CRL Check folder.
Run the following command:
```
```
.\DisableCRLCheck.bat
```
```
If the CRL check was disabled at the UMC server machine, enable it by running the following command:
```
```
.\EnableCRLCheck.bat
```

### 1.

### 2.

### 3.

### 4.

# Enabling Smart Card Authentication Option

By default, the smart card authentication option is disabled in UMC. Follow this procedure to enable the option:

```
Log in to UMC.
In the upper-right corner, click the main menu and select IDP Configuration.
In the Authentication options tab, select Smart Card Authentication.
```
```
Click Save all changes.
```

### 1.

### 2.

### 3.

### 4.

### 5.

# Enabling PKI Filter

```
Log in to UMC.
In the upper-right corner, click the main menu and select Account Policies.
Switch to the Advanced tab.
Select the PKI filter.
```
```
Click Save.
```

### 1.

### 2.

### 3.

### 4.

### 5.

# Adding User to UMC to Log In with Smart Card

This procedure describes how to add the user to UMC according to the Authenticate using CN filter so that the the smart card owner can log in to UMC
using smart card credentials:

```
Right-click the yellow icon in the System Tray.
```
```
Select About CardOS API.
Select Broadcom Corp Contacted Smart Card 0.
```
```
Select the certificate that was issued to you.
Go to the Details tab.
```

### 6.

### 7.

### 8.

### 9.

### 10.

### 11.

### 12.

### 13.

### 14.

```
Select Subject in the Field column.
```
```
Copy the second name and first name that are specified after CN =.
Log in to UMC as UMCadmin.
In the upper right corner, click the main menu and select Users.
In the upper left corner, click Add User.
Paste the copied second name and first name into the User Name field.
Type any password.
Enable the user by selecting the check box in the Enabled column.
In the right part of the page, click Update.
```

# UMC Endpoints Configuration

UMC Endpoint switch:

From TFS, edit the following file:

Revolution_MOM_Microservices_CF/Openshift/Environments/momosprj0x.env

Delete the line:

UMC_AUTH_ENDPOINT='https://172.23.196.164/IPSimatic-Logon'

Change the following row:

UMC_ENDPOINT=http://172.23.196.164/

To:

UMC_ENDPOINT=http://mioumc0x.swqa.tst/

Commit the changes on master.

Reinstall microservices:

Repeat the following steps for token, permission, and every component service you want to run on momosprj0x.

On TFS Build and Release->Releases->OS4 <ServiceName> Service+Release

Open the new release and manually deploy the release to Openshift DevTest x

Deploy will uninstall the running microservice and install the new one, automatically.

Once the microservice installation in OpenShift has been finished, the microservice must be resumed manually.


# How to Install and Configure MIO


# MIO Prerequisites

Install .NET Framework 4.7.1 Developer Pack: NDP471-DevPack-ENU.exe

Verify that the servername of SQL Server is MIOUMC0x

Verify that sa SQL user has the password SwqaMe$1


# MIO Installation

Download and extract the last version of "Opcenter Connecy MOM 3.1.zip" from GTAC

Edit the file OpcenterConnectMOMSetupParameters.xml and change MIOUMC0x to the right UMC host

<SetupParameters>

<Property Id="DBTYPE" Value="S" Description="Database Type S for MSSQL O for Oracle"/>

<Property Id="DBSERVER" Value="MIOUMC0x" Description="Database Server Address"/>

<Property Id="DBPORT" Value="1433" Description="Database Port"/>

<Property Id="DBSCHEMA" Value="Mio_Sch" Description="Database Schema"/>

<Property Id="DBNAME" Value="MIODB" Description="Database Instance Name"/>

<Property Id="DBUSER" Value="MIOUser" Description="Database User"/>

<Property Id="AUTHDOMAIN" Value="localhost" Description="Authentication Domain"/>

<Property Id="PRILICENSESERVER" Value="28000@GOAPLMLIC03" Description="Primary License Server"/>

<Property Id="SECLICENSESERVER" Value="28000@GOAPLMLIC02" Description="Secondary License Server"/>

<Property Id="SSL" Value="N" Description="Use SSL Y for Yes N for No"/>

<Property Id="ORACLEPATH" Value="C:\Oracle.ManagedDataAccess.dll" Description="Oracle Data Access DLL Path"/>

<Property Id="CONFIGUSER" Value="Administrator" Description="Configuration Service User"/>

<Property Id="PASSPHRASE" Value="SwqaMe$1" Description="Passphrase for encryption"/>

<Property Id="HOSTS" Value="localhost" Description="Configuration Service Hosts"/>

<Property Id="CHANNELSOURCE" Value="MIOChannelSource" Description="Channel Source Instance Name"/>

<Property Id="PORTVALUE" Value="13024" Description="Client Gateway Port"/>

<Property Id="CLIENTNAME" Value="MIOClient" Description="Client Gateway Name"/>

<Property Id="ESINSTALLDIR" Value="C:\MIO31\" Description="Enterprise Services Install Directory"/>

<Property Id="DBPWD1" Value="Cam1star" Description="DB Password used with UI Install"/>

<Property Id="DBPWD2" Value="Cam1star" Description="DB Confirm Password used with UI Install"/>

<Property Id="CONFIGPWD1" Value="SwqaMe$1" Description="Config Service Password used wtih UI install"/>

<Property Id="CONFIGPWD2" Value="SwqaMe$1" Description="Config Service Confirm Password used with UI Install"/>

<Property Id="DONOTDELETE" Value="" Description="Do not delete this property"/>

</SetupParameters>

Select OpcenterConnectMOMsetup.msi and click on Install.

Install all of MIO Components and select Create DB.

Click Next and leave default

Click Naxt and insert sa credentials (sa SWQA!n1)

Powershell:

```
iisreset
```

### 1.

### 2.

### 3.

### 4.

# MIO Installation Verification

Follow these steps to perform the Installation Functionality test:

```
Navigate to C:\Program Files\Opcenter Connect MOM\Channel Adapter Host\test data.
Copy miofiletest001.xml.
Navigate to C:\Program Files\Opcenter Connect MOM\Channel Adapter Host\inbound.
Paste miofiletest001.xml. The application populates the C:\Program Files\Opcenter Connect
```
MOM\Channel Adapter Host\outbound folder with a copy of the original XML file with a completion message and time stamp information added.


# Convert an object to a B2MML (ERP)

For the above conversion you need:

1. Describe and agree on the structure of the document (examples)
2. Open B2MML-V0600-ProductionPerformance.xsd (Revolution_MOM_MS_ERP\FBs\ERPFB\ERPFB\ERPFB.
CommandHandler\XML_B2MML_Tools_FB\Schema\), find and uncomment your type

```
<!-- Transaction Elements -->
```
```
<!--<xsd:element name = "GetProductionPerformance" type = "GetProductionPerformanceType"/>
<xsd:element name = "ShowProductionPerformance" type = "ShowProductionPerformanceType"/>-->
<xsd:element name = "ProcessProductionPerformance" type = "ProcessProductionPerformanceType"/>
<!--<xsd:element name = "AcknowledgeProductionPerformance" type = "AcknowledgeProductionPerformanceType"/>-->
<xsd:element name = "ChangeProductionPerformance" type = "ChangeProductionPerformanceType"/>
<!--<xsd:element name = "RespondProductionPerformance" type = "RespondProductionPerformanceType"/>
<xsd:element name = "CancelProductionPerformance" type = "CancelProductionPerformanceType"/>
<xsd:element name = "SyncProductionPerformance" type = "SyncProductionPerformanceType"/>-->
```
3. Add your own fields to the B2MML-V0600-Extensions.xsd file, according to the document in step 1. (more details and more)
4. Regenerate Siemens.RevMOM.ERPFB.CommandHandler.XML_B2MML_Tools_FB.ProductionPerformance class via Developer Command Prompt for
VS 2019 (example)

```
xsd B2MML-V0600-ProductionPerformance.xsd B2MML-V0600-AllExtensions.xsd /classes /language:CS
```
5. Now you can create an instance for converting:

```
Example
```
```
var processProductionPerformance = new ProcessProductionPerformanceType
{
//TODO this property will be defined from other MS in future
releaseID = "0.0.2",
ApplicationArea = new TransApplicationAreaType
{
Sender = new TransSenderType {
AuthorizationID = new IdentifierType { Value = "Source System" }
},
Receiver = new TransReceiverType[] {
new TransReceiverType{
ID = new IdentifierType[] { new IdentifierType {Value = "ModularMOM"} }
}
},
CreationDateTime = new DateTimeType {
Value = evt.Timestamp.DateTime
}
},
DataArea = new ProcessProductionPerformanceTypeDataArea
{
Process = new TransProcessType(),
ProductionPerformance = new ProductionPerformanceType[] {
new ProductionPerformanceType {
ProductionResponse = new ProductionResponseType[]{
new ProductionResponseType {
ID = new IdentifierType { Value = evt.ProductionOrderName},
SegmentResponse = new SegmentResponseType[] {
new SegmentResponseType{
ID = new IdentifierType {Value = evt.OperationName},
WorkCenterName = evt.WorkCenterName,
TimeStamp = evt.Timestamp.DateTime,
MaterialActual = new MaterialActualType [] {
new MaterialActualType {
//TODO this property will be defined from other MS in
future
MaterialUse = new MaterialUseType{Value = "Produced" },
```

```
Quantity = new QuantityValueType[]{
new QuantityValueType
{
QuantityString = new QuantityStringType{ Value
= evt.Quantity.ToString()},
UnitOfMeasure = new UnitOfMeasureType{ Value =
evt.Unit }
} } } } } } } } } } }
```
### };

6. Convert it:

```
string WIPConfirmationB2MML = XMLHelper.SaveXmlToString<ProcessProductionPerformanceType>
(processProductionPerformance);
```
7. and validate:

```
XMLHelper.XMLValidation<ProcessProductionPerformanceType>(WIPConfirmationB2MML);
```
You can also see sample code in ERP MS (WIPConfirmation_Handler).


### 1.

### 2.

### 3.

### 4.

```
a.
5.
a.
```
```
6.
a.
b.
7.
a.
```
### 8.

# How to seed data using xxx.dbInit.xml

In some of the scenario we might need to seed data in Entities at the time of installation. For doing this we can leverage functionality provided by platform.

## Steps

```
Open FB solution in visual studio.
Navigate to <xxx>FB.Installer project and find config folder.
Open file Siemens.RevMOM.<xxx>FB.Dbinit.xml.
Now add new section element in sections element, set following attribute to the section element
<Section engineeringLevel="LibraryFunctionalBlock" implementationName="<identifier>" domainName="MasterData">
Add 'Entity' element to the section. Each entity element represent a record in DB table. Set type attribute.
<Entity type="Siemens.RevMOM.MasterData.PermissionFB.MSModel.DataModel.Permission"> You can get full type name from model
properties.
Add one or more 'Property' elements as per requirement. It has following attributes
<Property name="Name" kind="Plain" value="User Role View" />
Make sure that we are not missing mandatory properties else DBInit might fail.
To add reference to other entity, please provide logical key of that entity
<Property name="Microservices" kind="ManyToManyReference"> 'Kind' is type of references which we configure while creating
relationships in model designer.
<LogicalKey entityType="Siemens.RevMOM.MasterData.PermissionFB.MSModel.DataModel.MicroService"> Note that entityType is
of other entity defined by relationship
<Property name="Name" kind="Plain" value="Permission" /> value should uniquely identify record for reference. Add more
properties in case of composite logical key.
</LogicalKey>
</Property>
Save the file and build solution.
```
```
Example
```
```
<?xml version="1.0" encoding="utf-8" ?>
<Sections>
<Section engineeringLevel="LibraryFunctionalBlock" implementationName="MappedUserData" domainName="MasterData">
<Entity type="Siemens.RevMOM.MasterData.PermissionFB.MSModel.DataModel.MappedUsers">
<Property name="UserId" kind="Plain" value="2" />
</Entity>
</Section>
```
```
<Section engineeringLevel="LibraryFunctionalBlock" implementationName="MicroserviceData" domainName="
MasterData">
<Entity type="Siemens.RevMOM.MasterData.PermissionFB.MSModel.DataModel.MicroService">
<Property name="Name" kind="Plain" value="Permission" />
</Entity>
</Section>
```
```
<Section engineeringLevel="LibraryFunctionalBlock" implementationName="ModulesData" domainName="MasterData">
<Entity type="Siemens.RevMOM.MasterData.PermissionFB.MSModel.DataModel.Module">
<Property name="Name" kind="Plain" value="Module" />
<Property name="Microservices" kind="ManyToManyReference">
<LogicalKey entityType="Siemens.RevMOM.MasterData.PermissionFB.MSModel.DataModel.MicroService">
<Property name="Name" kind="Plain" value="Permission" />
</LogicalKey>
</Property>
</Entity>
</Section>
```
```
<Section engineeringLevel="LibraryFunctionalBlock" implementationName="PermissionsData" domainName="
MasterData">
<Entity type="Siemens.RevMOM.MasterData.PermissionFB.MSModel.DataModel.Permission">
<Property name="Name" kind="Plain" value="User Role View" />
<Property name="Microservices" kind="ManyToManyReference">
<LogicalKey entityType="Siemens.RevMOM.MasterData.PermissionFB.MSModel.DataModel.MicroService">
<Property name="Name" kind="Plain" value="Permission" />
</LogicalKey>
</Property>
</Entity>
<Entity type="Siemens.RevMOM.MasterData.PermissionFB.MSModel.DataModel.Permission">
<Property name="Name" kind="Plain" value="User Role Add" />
```

<Property name="Microservices" kind="ManyToManyReference">
<LogicalKey entityType="Siemens.RevMOM.MasterData.PermissionFB.MSModel.DataModel.MicroService">
<Property name="Name" kind="Plain" value="Permission" />
</LogicalKey>
</Property>
</Entity>
<Entity type="Siemens.RevMOM.MasterData.PermissionFB.MSModel.DataModel.Permission">
<Property name="Name" kind="Plain" value="User Role Edit" />
<Property name="Microservices" kind="ManyToManyReference">
<LogicalKey entityType="Siemens.RevMOM.MasterData.PermissionFB.MSModel.DataModel.MicroService">
<Property name="Name" kind="Plain" value="Permission" />
</LogicalKey>
</Property>
</Entity>
<Entity type="Siemens.RevMOM.MasterData.PermissionFB.MSModel.DataModel.Permission">
<Property name="Name" kind="Plain" value="User Role Remove" />
<Property name="Microservices" kind="ManyToManyReference">
<LogicalKey entityType="Siemens.RevMOM.MasterData.PermissionFB.MSModel.DataModel.MicroService">
<Property name="Name" kind="Plain" value="Permission" />
</LogicalKey>
</Property>
</Entity>
<Entity type="Siemens.RevMOM.MasterData.PermissionFB.MSModel.DataModel.Permission">
<Property name="Name" kind="Plain" value="User Role / User Assignment" />
<Property name="Microservices" kind="ManyToManyReference">
<LogicalKey entityType="Siemens.RevMOM.MasterData.PermissionFB.MSModel.DataModel.MicroService">
<Property name="Name" kind="Plain" value="Permission" />
</LogicalKey>
</Property>
</Entity>
<Entity type="Siemens.RevMOM.MasterData.PermissionFB.MSModel.DataModel.Permission">
<Property name="Name" kind="Plain" value="User Role / Permission Assignment" />
<Property name="Microservices" kind="ManyToManyReference">
<LogicalKey entityType="Siemens.RevMOM.MasterData.PermissionFB.MSModel.DataModel.MicroService">
<Property name="Name" kind="Plain" value="Permission" />
</LogicalKey>
</Property>
</Entity>
</Section>

<Section engineeringLevel="LibraryFunctionalBlock" implementationName="RolesData" domainName="MasterData">
<Entity type="Siemens.RevMOM.MasterData.PermissionFB.MSModel.DataModel.Role">
<Property name="Name" kind="Plain" value="Admin" />
<Property name="Users" kind="ManyToManyReference">
<LogicalKey entityType="Siemens.RevMOM.MasterData.PermissionFB.MSModel.DataModel.MappedUsers">
<Property name="UserId" kind="Plain" value="2" />
</LogicalKey>
</Property>
<Property name="Permissions" kind="ManyToManyReference">
<LogicalKey entityType="Siemens.RevMOM.MasterData.PermissionFB.MSModel.DataModel.Permission">
<Property name="Name" kind="Plain" value="User Role View" />
</LogicalKey>
<LogicalKey entityType="Siemens.RevMOM.MasterData.PermissionFB.MSModel.DataModel.Permission">
<Property name="Name" kind="Plain" value="User Role Add" />
</LogicalKey>
<LogicalKey entityType="Siemens.RevMOM.MasterData.PermissionFB.MSModel.DataModel.Permission">
<Property name="Name" kind="Plain" value="User Role Edit" />
</LogicalKey>
<LogicalKey entityType="Siemens.RevMOM.MasterData.PermissionFB.MSModel.DataModel.Permission">
<Property name="Name" kind="Plain" value="User Role Remove" />
</LogicalKey>
<LogicalKey entityType="Siemens.RevMOM.MasterData.PermissionFB.MSModel.DataModel.Permission">
<Property name="Name" kind="Plain" value="User Role / User Assignment" />
</LogicalKey>
<LogicalKey entityType="Siemens.RevMOM.MasterData.PermissionFB.MSModel.DataModel.Permission">
<Property name="Name" kind="Plain" value="User Role / Permission Assignment" />
</LogicalKey>
</Property>
</Entity>
</Section>


```
</Sections>
```
When service is installed you can verify database for DBinit file.


### 1.

### 2.

### 3.

```
a.
```
```
b.
```
```
c.
```
```
d.
```
### 4.

# Working with Localization file for Web App

## Description

Our target is to have a common dictionary for all UI labels, error messages being used in each app. For building dictionary we chose to use CSV file format
as its easy to maintain and update.
We can visualize CSV data in tabular format and we can apply filters. Each row in CSV represent a resource(key and its values) and each column is a
language. Currently we have only English (column header 'en')

## How to add dictionary file in web apps

```
Navigate to web app folder.
Add sub module using following command. Please replace <app root folder> with your app folder e.g. Operate_UI/LocalizationResources
```
```
git submodule add --name LocalizationResources --force https://tfs01mom.industrysoftware.automation.
siemens.com/tfs/DefaultCollection/Revolution_MOM/_git/Revolution_MOM_UI_LocalizationResources
LocalizationResources
```
```
Open package.json file and update as shown below
Add following dependency to the dependencies section.
```
```
"csv-parser": "latest"
```
```
Add new script in Scripts section.
"generate-resources": "node LocalizationResources/transformationModule/src/js/transformService.js <app specific localization file
name> <path for SWF files generation>"
Replace <app specific localization file name> with messages file name. All SWF localized files will be created with this name at
location given by<path for SWF files generation>
e.g. for Operator web app
"generate-resources": "node LocalizationResources/transformationModule/src/js/transformService.js operateUIMessages src/modules
/resources/src/i18n",
Modify existing build script.
```
```
"build": "npm run generate-resources && gulp --gulpfile node_modules/afx/gulpfile.js --cwd. site_<appname>".
```
```
This command will automatically generate SWF resource files first and then build web app along with resource files.
Run following command
```
```
>npm install
```

### 5.

### 1.

### 2.

### 3.

### 4.

### 5.

### 6.

```
Now build web app using below command. SWF resource files will be auto generated at given location.
```
```
>npm run build
```
Adding resource strings(key-value string message)

```
Clone common messages repository.
Navigate to "LocalizationResources\transformationModule\src\i18n\modularMomMessages.csv"
Add a row with with key and English value in comma separated format( You can also open this file in any CSV editor and record in tabular format).
ex-
```
```
"Key","en"
"workOrdersTitle","Work Order"
"workOrderOperations","Work Order Operations"
```
```
Then push changes to master branch.
Go to specific app, and update submodule using following command
```
```
git submodule update
```
```
Now build app and run it.
```

### 1.

### 2.

### 3.

### 4.

### 5.

# How to enable HA Endpoints on UI Services

In order to add readiness and liveness capabilities to a UI service, follow these steps:

```
Open a UI repo with Visual Studio, for example Revolution_MOM_MS_<component>_UI
Go to the following folder : <component>_UI/src/solution/src
Add a new folder named : healthiness
Add a new empty file named : liveness
Add a new empty file named : readiness
```
In case of doubt you can check the implementation in repo Revolution_MOM_MS_Admin_UI


### 1.

### 2.

### 3.

### 4.

# Practices to be followed while Creating Branches

1. Branches needs to deleted once task/feature is done. If it is missed, then all obsolete branches needs to be deleted at the sprint end/start.
2. Take a decision that, branch should be feature/task based, if possible, make task based, so early check-in would be

possible, so in minimal time, a incremental thing could be delivered.

Following Conventions can be followed while creating branches in git

```
Use issue tracker/Task IDs in branch names:
Advantages :
1.1: Minimal thinking, and has more advantages.
1.2: Searching and filtering becomes easier in the issue tracker. Once you know your issue/Task number, it becomes easy to find the
branch using auto complete in the local git tree/tfs.
1.3: The issues created in the issue tracker, in most cases, are used for tracking the team’s progress. It becomes easy to correlate the
relevant working branch with each task — especially when each developer is working on many issues at the same time.
You can categorize your task by having prefix like bug-, feature-, task-, or these can be abbreviated with (bug-, ftr-, tsk- respectively)
Add a short descriptor of the task.
3.1 : Issue/Task tracker ID itself is sufficient to identify a unique branch in a project in most cases, there could be chances that some more
nuance is needed. For example, there could be multiple branches needed to work on one issue, possibly by different people.
3.2 : Use a short, actionable descriptor of the task after the issue ID. This makes the branch name recognizable, distinct, and easy to search
for in case you don’t have the issue ID handy. Make sure that the descriptor is concise, but descriptive enough to give you an idea of what’s going
on in the branch.
Use hyphens/underscore as a separators.
4.1 : This is a little opinionated, but we can use alternate hypen(-) and underscore(_) before and after task id, or vice versa, to make it more
readable.
```
```
Example: Task 1225: Create a work Order
tsk-1225_CreateWO or,
tsk_1225-CreateWO
```
Finally, Name should not be too large, it need to be understandable.(Make sure char length is not greater than 28)


### 1.

### 2.

### 3.

### 4.

### 5.

### 6.

### 7.

### 8.

### 9.

### 1.

# Checklist for AC and Description

## Description

```
The Scope and the purpose of the functionality should be clear and descriptive.
All references should be updated e.g. wiki links (in case we do not have create a container page to support) if multiple pages are impacted we
need to update all impacted links.
The mockup should be attached to the user story as an when available and stakeholders of the user story need to be tagged in TFS.
UI impact needs to be defined e.g. icon change, new column name addition some behavioral aspect of UI changing, or being added.
The name of the icon which has been newly added should be mentioned so that it can be found in the Icon Repository.
The impacted endpoint should be mentioned.
The list of permissions (added/updated).
The API-related impact from end-user documentation requirements should be mentioned.
All relevant screenshots required for documentation should be attached if not available in the environment.
```
## AC

```
It should contain the scope of the business functionality
```

# Installation & Configuration of the Monitoring Stack for

# NFR Testing

To monitoring windows instance we will use following stack:

1) Grafana for visualization

2) InfluxDB as database to store metrics

3) Telegraf for collecting metrics

To setup this stack we should do next steps:

Install telegraf
1.1 Download telegraf dy command: wget https://dl.influxdata.com/telegraf/releases/telegraf-1.11.0_windows_amd64.zip "C:\Users\$($env:USERPROFILE)
\Downloads\telegraf-1.10.3.zip"
1.2 Extract telegraf: Expand-Archive -Path "C:\Users\$($env:USERPROFILE)\Downloads\telegraf-1.10.3.zip" -DestinationPath "C:\Program Files\Telegraf"
1.3 Change influxDB address and put following confuguration to the "C:\Program Files\Telegraf\telegraf.conf" file(this config file also collect metrics from
MsSQL server):
# Telegraf configuration
[global_tags]
[agent]
interval = "10s"
round_interval = true

metric_buffer_limit = 1000
flush_buffer_when_full = true
collection_jitter = "0s"

flush_interval = "10s"
flush_jitter = "0s"

debug = false
quiet = false
logfile = "/Program Files/telegraf/telegraf.log"

hostname = ""

### ###############################################################################

### # OUTPUTS #

### ###############################################################################

# Configuration for influxdb server to send metrics to
[[outputs.influxdb]]
urls = ["http://172.23.202.164:8086"] # required
database = "telegraf"
precision = "s"
timeout = "5s"

###############################################################################
# INPUTS #
###############################################################################

# Windows Performance Counters plugin.
# See more configuration examples at:
# https://github.com/influxdata/telegraf/tree/master/plugins/inputs/win_perf_counters

[[inputs.win_perf_counters]]
[[inputs.win_perf_counters.object]]
# Processor usage, alternative to native, reports on a per core.
ObjectName = "Processor"
Instances = ["*"]
Counters = [
"% Idle Time",
"% Interrupt Time",
"% Privileged Time",
"% User Time",
"% Processor Time"
]
Measurement = "win_cpu"
# Set to true to include _Total instance when querying for all (*).
#IncludeTotal=false

[[inputs.win_perf_counters.object]]
# Disk times and queues


ObjectName = "LogicalDisk"
Instances = ["*"]
Counters = [
"% Idle Time",
"% Disk Time",
"% Disk Read Time",
"% Disk Write Time",
"% User Time",
"% Free Space",
"Current Disk Queue Length",
"Free Megabytes",
"Disk Read Bytes/sec",
"Disk Write Bytes/sec"
]
Measurement = "win_disk"
# Set to true to include _Total instance when querying for all (*).
IncludeTotal=true

[[inputs.win_perf_counters.object]]
ObjectName = "System"
Counters = [
"Context Switches/sec",
"System Calls/sec",
"Processor Queue Length",
"Threads",
"System Up Time",
"Processes"
]
Instances = ["------"]
Measurement = "win_system"
# Set to true to include _Total instance when querying for all (*).
#IncludeTotal=false

[[inputs.win_perf_counters.object]]
# Example query where the Instance portion must be removed to get data back,
# such as from the Memory object.
ObjectName = "Memory"
Counters = [
"Available Bytes",
"Cache Faults/sec",
"Demand Zero Faults/sec",
"Page Faults/sec",
"Pages/sec",
"Transition Faults/sec",
"Pool Nonpaged Bytes",
"Pool Paged Bytes"
]
# Use 6 x - to remove the Instance bit from the query.
Instances = ["------"]
Measurement = "win_mem"
# Set to true to include _Total instance when querying for all (*).
#IncludeTotal=false

[[inputs.win_perf_counters.object]]
# more counters for the Network Interface Object can be found at
# https://msdn.microsoft.com/en-us/library/ms803962.aspx
ObjectName = "Network Interface"
Counters = [
"Bytes Received/sec",
"Bytes Sent/sec",
"Packets Received/sec",
"Packets Sent/sec"
]
Instances = ["*"] # Use 6 x - to remove the Instance bit from the query.
Measurement = "win_net"
#IncludeTotal=false #Set to true to include _Total instance when querying for all (*).

[[inputs.win_perf_counters.object]]
# Process metrics
ObjectName = "Process"
Counters = [
"% Processor Time",
"Handle Count",
"Private Bytes",
"Thread Count",
"Virtual Bytes",
"Working Set"
]


Instances = ["*"]
Measurement = "win_proc"
#IncludeTotal=false #Set to true to include _Total instance when querying for all (*).

# Read metrics from Microsoft SQL Server
[[inputs.sqlserver]]
## Specify instances to monitor with a list of connection strings.
## All connection parameters are optional.
## By default, the host is localhost, listening on default port, TCP 1433.
## for Windows, the user is the currently running AD user (SSO).
## See https://github.com/denisenkom/go-mssqldb for detailed connection
## parameters, in particular, tls connections can be created like so:
## "encrypt=true;certificate=<cert>;hostNameInCertificate=<SqlServer host fqdn>"
servers = [
"Server=172.23.202.164;Port=1433;User Id=telegraf;Password=@/>dss*sz4x!2<ne;app name=telegraf;log=1;",
]

## Optional parameter, setting this to 2 will use a new version
## of the collection queries that break compatibility with the original
## dashboards.
## Version 2 - is compatible from SQL Server 2012 and later versions and also for SQL Azure DB
query_version = 1

## If you are using AzureDB, setting this to true will gather resource utilization metrics
# azuredb = false

##Possible queries
##Version 2:
##- PerformanceCounters
##- WaitStatsCategorized
##- DatabaseIO
##- ServerProperties
##- MemoryClerk
##- Schedulers
##- SqlRequests
##- VolumeSpace
##- Cpu
## Version 1:
## - PerformanceCounters
## - WaitStatsCategorized
## - CPUHistory
## - DatabaseIO
## - DatabaseSize
## - DatabaseStats
## - DatabaseProperties
## - MemoryClerk
## - VolumeSpace
## - PerformanceMetrics

## A list of queries to include. If not specified, all the above listed queries are used.
# include_query = []

## A list of queries to explicitly ignore.
exclude_query = [ 'Schedulers' , 'SqlRequests' ]

1.4 Install Telegraf as a Service: telegraf.exe --service install --config "C:\Program Files\Telegraf\telegraf.conf"
1.5 Start telegraf: net start telegraf
Install InfluxDB
2.1 Download InfluxDB from offitial site: https://portal.influxdata.com/downloads/
2.2 Extract archive to somewhere. e.g C:\Program Files\InfluxData\influxdb-data
2.3 Create 3 folders: meta, data, wal in C:\Program Files\InfluxData\influxdb-data
2.4 Put following configuration to influxdb.conf:

### Welcome to the InfluxDB configuration file.

# The values in this file override the default values used by the system if
# a config option is not specified. The commented out lines are the configuration
# field and the default value used. Uncommenting a line and changing the value
# will change the value used at runtime when the process is restarted.

# Once every 24 hours InfluxDB will report usage data to usage.influxdata.com
# The data includes a random ID, os, arch, version, the number of series and other
# usage data. No data from user databases is ever transmitted.
# Change this option to true to disable reporting.
# reporting-disabled = false

# Bind address to use for the RPC service for backup and restore.
# bind-address = "127.0.0.1:8088"


### ###

### [meta]
###
### Controls the parameters for the Raft consensus group that stores metadata
### about the InfluxDB cluster.
###

[meta]
# Where the metadata/raft database is stored
dir = "C:/Program Files/InfluxData/influxdb-data/meta"

# Automatically create a default retention policy when creating a database.
# retention-autocreate = true

# If log messages are printed for the meta service
# logging-enabled = true

###
### [data]
###
### Controls where the actual shard data for InfluxDB lives and how it is
### flushed from the WAL. "dir" may need to be changed to a suitable place
### for your system, but the WAL settings are an advanced configuration. The
### defaults should work for most systems.
###

[data]
# The directory where the TSM storage engine stores TSM files.
dir = "C:/Program Files/InfluxData/influxdb-data/data"

# The directory where the TSM storage engine stores WAL files.
wal-dir = "C:/Program Files/InfluxData/influxdb-data/wal"

# The amount of time that a write will wait before fsyncing. A duration
# greater than 0 can be used to batch up multiple fsync calls. This is useful for slower
# disks or when WAL write contention is seen. A value of 0s fsyncs every write to the WAL.
# Values in the range of 0-100ms are recommended for non-SSD disks.
# wal-fsync-delay = "0s"

# The type of shard index to use for new shards. The default is an in-memory index that is
# recreated at startup. A value of "tsi1" will use a disk based index that supports higher
# cardinality datasets.
# index-version = "inmem"

# Trace logging provides more verbose output around the tsm engine. Turning
# this on can provide more useful output for debugging tsm engine issues.
# trace-logging-enabled = false

# Whether queries should be logged before execution. Very useful for troubleshooting, but will
# log any sensitive data contained within a query.
# query-log-enabled = true

# Validates incoming writes to ensure keys only have valid unicode characters.
# This setting will incur a small overhead because every key must be checked.
# validate-keys = false

# Settings for the TSM engine

# CacheMaxMemorySize is the maximum size a shard's cache can
# reach before it starts rejecting writes.
# Valid size suffixes are k, m, or g (case insensitive, 1024 = 1k).
# Values without a size suffix are in bytes.
# cache-max-memory-size = "1g"

# CacheSnapshotMemorySize is the size at which the engine will
# snapshot the cache and write it to a TSM file, freeing up memory
# Valid size suffixes are k, m, or g (case insensitive, 1024 = 1k).
# Values without a size suffix are in bytes.
# cache-snapshot-memory-size = "25m"

# CacheSnapshotWriteColdDuration is the length of time at
# which the engine will snapshot the cache and write it to
# a new TSM file if the shard hasn't received writes or deletes
# cache-snapshot-write-cold-duration = "10m"


# CompactFullWriteColdDuration is the duration at which the engine
# will compact all TSM files in a shard if it hasn't received a
# write or delete
# compact-full-write-cold-duration = "4h"

# The maximum number of concurrent full and level compactions that can run at one time. A
# value of 0 results in 50% of runtime.GOMAXPROCS(0) used at runtime. Any number greater
# than 0 limits compactions to that value. This setting does not apply
# to cache snapshotting.
# max-concurrent-compactions = 0

# CompactThroughput is the rate limit in bytes per second that we
# will allow TSM compactions to write to disk. Note that short bursts are allowed
# to happen at a possibly larger value, set by CompactThroughputBurst
# compact-throughput = "48m"

# CompactThroughputBurst is the rate limit in bytes per second that we
# will allow TSM compactions to write to disk.
# compact-throughput-burst = "48m"

# If true, then the mmap advise value MADV_WILLNEED will be provided to the kernel with respect to
# TSM files. This setting has been found to be problematic on some kernels, and defaults to off.
# It might help users who have slow disks in some cases.
# tsm-use-madv-willneed = false

# Settings for the inmem index

# The maximum series allowed per database before writes are dropped. This limit can prevent
# high cardinality issues at the database level. This limit can be disabled by setting it to
# 0.
# max-series-per-database = 1000000

# The maximum number of tag values per tag that are allowed before writes are dropped. This limit
# can prevent high cardinality tag values from being written to a measurement. This limit can be
# disabled by setting it to 0.
# max-values-per-tag = 100000

# Settings for the tsi1 index

# The threshold, in bytes, when an index write-ahead log file will compact
# into an index file. Lower sizes will cause log files to be compacted more
# quickly and result in lower heap usage at the expense of write throughput.
# Higher sizes will be compacted less frequently, store more series in-memory,
# and provide higher write throughput.
# Valid size suffixes are k, m, or g (case insensitive, 1024 = 1k).
# Values without a size suffix are in bytes.
# max-index-log-file-size = "1m"

# The size of the internal cache used in the TSI index to store previously
# calculated series results. Cached results will be returned quickly from the cache rather
# than needing to be recalculated when a subsequent query with a matching tag key/value
# predicate is executed. Setting this value to 0 will disable the cache, which may
# lead to query performance issues.
# This value should only be increased if it is known that the set of regularly used
# tag key/value predicates across all measurements for a database is larger than 100. An
# increase in cache size may lead to an increase in heap usage.
series-id-set-cache-size = 100

###
### [coordinator]
###
### Controls the clustering service configuration.
###

[coordinator]
# The default time a write request will wait until a "timeout" error is returned to the caller.
# write-timeout = "10s"

# The maximum number of concurrent queries allowed to be executing at one time. If a query is
# executed and exceeds this limit, an error is returned to the caller. This limit can be disabled
# by setting it to 0.
# max-concurrent-queries = 0

# The maximum time a query will is allowed to execute before being killed by the system. This limit
# can help prevent run away queries. Setting the value to 0 disables the limit.
# query-timeout = "0s"


# The time threshold when a query will be logged as a slow query. This limit can be set to help
# discover slow or resource intensive queries. Setting the value to 0 disables the slow query logging.
# log-queries-after = "0s"

# The maximum number of points a SELECT can process. A value of 0 will make
# the maximum point count unlimited. This will only be checked every second so queries will not
# be aborted immediately when hitting the limit.
# max-select-point = 0

# The maximum number of series a SELECT can run. A value of 0 will make the maximum series
# count unlimited.
# max-select-series = 0

# The maximum number of group by time bucket a SELECT can create. A value of zero will max the maximum
# number of buckets unlimited.
# max-select-buckets = 0

###
### [retention]
###
### Controls the enforcement of retention policies for evicting old data.
###

[retention]
# Determines whether retention policy enforcement enabled.
# enabled = true

# The interval of time when retention policy enforcement checks run.
# check-interval = "30m"

###
### [shard-precreation]
###
### Controls the precreation of shards, so they are available before data arrives.
### Only shards that, after creation, will have both a start- and end-time in the
### future, will ever be created. Shards are never precreated that would be wholly
### or partially in the past.

[shard-precreation]
# Determines whether shard pre-creation service is enabled.
# enabled = true

# The interval of time when the check to pre-create new shards runs.
# check-interval = "10m"

# The default period ahead of the endtime of a shard group that its successor
# group is created.
# advance-period = "30m"

###
### Controls the system self-monitoring, statistics and diagnostics.
###
### The internal database for monitoring data is created automatically if
### if it does not already exist. The target retention within this database
### is called 'monitor' and is also created with a retention period of 7 days
### and a replication factor of 1, if it does not exist. In all cases the
### this retention policy is configured as the default for the database.

[monitor]
# Whether to record statistics internally.
# store-enabled = true

# The destination database for recorded statistics
# store-database = "_internal"

# The interval at which to record statistics
# store-interval = "10s"

###
### [http]
###
### Controls how the HTTP endpoints are configured. These are the primary
### mechanism for getting data into and out of InfluxDB.
###

[http]
# Determines whether HTTP endpoint is enabled.
enabled = true


# Determines whether the Flux query endpoint is enabled.
# flux-enabled = false

# Determines whether the Flux query logging is enabled.
# flux-log-enabled = false

# The bind address used by the HTTP service.
# bind-address = ":8086"

# Determines whether user authentication is enabled over HTTP/HTTPS.
# auth-enabled = false

# The default realm sent back when issuing a basic auth challenge.
# realm = "InfluxDB"

# Determines whether HTTP request logging is enabled.
# log-enabled = true

# Determines whether the HTTP write request logs should be suppressed when the log is enabled.
# suppress-write-log = false

# When HTTP request logging is enabled, this option specifies the path where
# log entries should be written. If unspecified, the default is to write to stderr, which
# intermingles HTTP logs with internal InfluxDB logging.
#
# If influxd is unable to access the specified path, it will log an error and fall back to writing
# the request log to stderr.
# access-log-path = ""

# Filters which requests should be logged. Each filter is of the pattern NNN, NNX, or NXX where N is
# a number and X is a wildcard for any number. To filter all 5xx responses, use the string 5xx.
# If multiple filters are used, then only one has to match. The default is to have no filters which
# will cause every request to be printed.
# access-log-status-filters = []

# Determines whether detailed write logging is enabled.
# write-tracing = false

# Determines whether the pprof endpoint is enabled. This endpoint is used for
# troubleshooting and monitoring.
# pprof-enabled = true

# Enables authentication on pprof endpoints. Users will need admin permissions
# to access the pprof endpoints when this setting is enabled. This setting has
# no effect if either auth-enabled or pprof-enabled are set to false.
# pprof-auth-enabled = false

# Enables a pprof endpoint that binds to localhost:6060 immediately on startup.
# This is only needed to debug startup issues.
# debug-pprof-enabled = false

# Enables authentication on the /ping, /metrics, and deprecated /status
# endpoints. This setting has no effect if auth-enabled is set to false.
# ping-auth-enabled = false

# Determines whether HTTPS is enabled.
# https-enabled = false

# The SSL certificate to use when HTTPS is enabled.
# https-certificate = "/etc/ssl/influxdb.pem"

# Use a separate private key location.
# https-private-key = ""

# The JWT auth shared secret to validate requests using JSON web tokens.
# shared-secret = ""

# The default chunk size for result sets that should be chunked.
# max-row-limit = 0

# The maximum number of HTTP connections that may be open at once. New connections that
# would exceed this limit are dropped. Setting this value to 0 disables the limit.
# max-connection-limit = 0

# Enable http service over unix domain socket
# unix-socket-enabled = false

# The path of the unix domain socket.
# bind-socket = "/var/run/influxdb.sock"


# The maximum size of a client request body, in bytes. Setting this value to 0 disables the limit.
# max-body-size = 25000000

# The maximum number of writes processed concurrently.
# Setting this to 0 disables the limit.
# max-concurrent-write-limit = 0

# The maximum number of writes queued for processing.
# Setting this to 0 disables the limit.
# max-enqueued-write-limit = 0

# The maximum duration for a write to wait in the queue to be processed.
# Setting this to 0 or setting max-concurrent-write-limit to 0 disables the limit.
# enqueued-write-timeout = 0

# User supplied HTTP response headers
#
# [http.headers]
# X-Header-1 = "Header Value 1"
# X-Header-2 = "Header Value 2"

###
### [logging]
###
### Controls how the logger emits logs to the output.
###

[logging]
# Determines which log encoder to use for logs. Available options
# are auto, logfmt, and json. auto will use a more a more user-friendly
# output format if the output terminal is a TTY, but the format is not as
# easily machine-readable. When the output is a non-TTY, auto will use
# logfmt.
# format = "auto"

# Determines which level of logs will be emitted. The available levels
# are error, warn, info, and debug. Logs that are equal to or above the
# specified level will be emitted.
# level = "info"

# Suppresses the logo output that is printed when the program is started.
# The logo is always suppressed if STDOUT is not a TTY.
# suppress-logo = false

###
### [subscriber]
###
### Controls the subscriptions, which can be used to fork a copy of all data
### received by the InfluxDB host.
###

[subscriber]
# Determines whether the subscriber service is enabled.
# enabled = true

# The default timeout for HTTP writes to subscribers.
# http-timeout = "30s"

# Allows insecure HTTPS connections to subscribers. This is useful when testing with self-
# signed certificates.
# insecure-skip-verify = false

# The path to the PEM encoded CA certs file. If the empty string, the default system certs will be used
# ca-certs = ""

# The number of writer goroutines processing the write channel.
# write-concurrency = 40

# The number of in-flight writes buffered in the write channel.
# write-buffer-size = 1000

### ###

### [[graphite]]
###
### Controls one or many listeners for Graphite data.
###

[[graphite]]
# Determines whether the graphite endpoint is enabled.


# enabled = false
# database = "graphite"
# retention-policy = ""
# bind-address = ":2003"
# protocol = "tcp"
# consistency-level = "one"

# These next lines control how batching works. You should have this enabled
# otherwise you could get dropped metrics or poor performance. Batching
# will buffer points in memory if you have many coming in.

# Flush if this many points get buffered
# batch-size = 5000

# number of batches that may be pending in memory
# batch-pending = 10

# Flush at least this often even if we haven't hit buffer limit
# batch-timeout = "1s"

# UDP Read buffer size, 0 means OS default. UDP listener will fail if set above OS max.
# udp-read-buffer = 0

### This string joins multiple matching 'measurement' values providing more control over the final measurement name.
# separator = "."

### Default tags that will be added to all metrics. These can be overridden at the template level
### or by tags extracted from metric
# tags = ["region=us-east", "zone=1c"]

### Each template line requires a template pattern. It can have an optional
### filter before the template and separated by spaces. It can also have optional extra
### tags following the template. Multiple tags should be separated by commas and no spaces
### similar to the line protocol format. There can be only one default template.
# templates = [
# "*.app env.service.resource.measurement",
# # Default template
# "server.*",
# ]

###
### [collectd]
###
### Controls one or many listeners for collectd data.
###

[[collectd]]
# enabled = false
# bind-address = ":25826"
# database = "collectd"
# retention-policy = ""
#
# The collectd service supports either scanning a directory for multiple types
# db files, or specifying a single db file.
# typesdb = "/usr/local/share/collectd"
#
# security-level = "none"
# auth-file = "/etc/collectd/auth_file"

# These next lines control how batching works. You should have this enabled
# otherwise you could get dropped metrics or poor performance. Batching
# will buffer points in memory if you have many coming in.

# Flush if this many points get buffered
# batch-size = 5000

# Number of batches that may be pending in memory
# batch-pending = 10

# Flush at least this often even if we haven't hit buffer limit
# batch-timeout = "10s"

# UDP Read buffer size, 0 means OS default. UDP listener will fail if set above OS max.
# read-buffer = 0

# Multi-value plugins can be handled two ways.
# "split" will parse and store the multi-value plugin data into separate measurements
# "join" will parse and store the multi-value plugin as a single multi-value measurement.
# "split" is the default behavior for backward compatibility with previous versions of influxdb.
# parse-multivalue-plugin = "split"


### ###

### [opentsdb]
###
### Controls one or many listeners for OpenTSDB data.
###

[[opentsdb]]
# enabled = false
# bind-address = ":4242"
# database = "opentsdb"
# retention-policy = ""
# consistency-level = "one"
# tls-enabled = false
# certificate= "/etc/ssl/influxdb.pem"

# Log an error for every malformed point.
# log-point-errors = true

# These next lines control how batching works. You should have this enabled
# otherwise you could get dropped metrics or poor performance. Only points
# metrics received over the telnet protocol undergo batching.

# Flush if this many points get buffered
# batch-size = 1000

# Number of batches that may be pending in memory
# batch-pending = 5

# Flush at least this often even if we haven't hit buffer limit
# batch-timeout = "1s"

###
### [[udp]]
###
### Controls the listeners for InfluxDB line protocol data via UDP.
###

[[udp]]
# enabled = false
# bind-address = ":8089"
# database = "udp"
# retention-policy = ""

# InfluxDB precision for timestamps on received points ("" or "n", "u", "ms", "s", "m", "h")
# precision = ""

# These next lines control how batching works. You should have this enabled
# otherwise you could get dropped metrics or poor performance. Batching
# will buffer points in memory if you have many coming in.

# Flush if this many points get buffered
# batch-size = 5000

# Number of batches that may be pending in memory
# batch-pending = 10

# Will flush at least this often even if we haven't hit buffer limit
# batch-timeout = "1s"

# UDP Read buffer size, 0 means OS default. UDP listener will fail if set above OS max.
# read-buffer = 0

###
### [continuous_queries]
###
### Controls how continuous queries are run within InfluxDB.
###

[continuous_queries]
# Determines whether the continuous query service is enabled.
# enabled = true

# Controls whether queries are logged when executed by the CQ service.
# log-enabled = true

# Controls whether queries are logged to the self-monitoring data store.
# query-stats-enabled = false

# interval for how often continuous queries will be checked if they need to run
# run-interval = "1s"


### ###

### [tls]
###
### Global configuration settings for TLS in InfluxDB.
###

[tls]
# Determines the available set of cipher suites. See https://golang.org/pkg/crypto/tls/#pkg-constants
# for a list of available ciphers, which depends on the version of Go (use the query
# SHOW DIAGNOSTICS to see the version of Go used to build InfluxDB). If not specified, uses
# the default settings from Go's crypto/tls package.
# ciphers = [
# "TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305",
# "TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305",
# ]

# Minimum version of the tls protocol that will be negotiated. If not specified, uses the
# default settings from Go's crypto/tls package.
# min-version = "tls1.2"

# Maximum version of the tls protocol that will be negotiated. If not specified, uses the
# default settings from Go's crypto/tls package.
# max-version = "tls1.3"
2.5 Install InfluxDB as a service by command:
2.5.1 Download nssm tool from https://www.nssm.cc/download
2.5.2 Extract nssm.exe from the archive to somewhere. e.g C:\Program Files\InfluxData\influxdb-data
2.5.3 Open command line and run next commands
cd C:\Program Files\InfluxData\influxdb-data
nssm install
2.5.4 Add next parameters
To parameter "path" you need to add C:\Program Files\InfluxData\influxdb-data\influxd.exe
The second parameter will created automatically
To the third parametr yoy need to add arg: -config "C:\Program Files\InfluxData\influxdb-data\influxdb.conf"
In the last parameter you need to add name for service (for example influxDB_ServiceName)
2.5.5 Open Services on database PC find service with name influxDB_ServiceName and start it

3.1 Verify the firewall is off

4.1 Open SQL Server 2016 Configuration Manager and do next steps:
Open SQL Server Network Configuration and double click to Protocols for MOM10
Open TCP/IP properties and choose IP Adresses tab.
Open section IPAII with parameter TCP Dynamic Ports and TCP port
Delete variable from parameter TCP Dynamic Ports
Add 1433 to TCP Port
Click to Aplly and OK
Then you need to restart the database

After that you need to run request in database
USE master;
GO
CREATE LOGIN [telegraf] WITH PASSWORD = N'@/>dss*sz4x!2<ne';
GO
GRANT VIEW SERVER STATE TO [telegraf];
GO
GRANT VIEW ANY DEFINITION TO [telegraf];
GO

5.1 You can watch telegraf logs for checks that all fine
Go to folder C:\Program Files\telegraf
Open file telegraf.log
Log file need to consist message "Metric buffer overflow; 2 metrics have been dropped"


### 1.

### 2.

### 3.

### 4.

### 5.

### 1.

### 2.

### 3.

# Setting breadcrumb title using mom-config

## What we did before :

Previously, we used to set breadcrumb/header's titles for our states/pages using states.json file for static data. If we needed dynamic title for our state, we
used to call breadcrumb exposed methods from MOM UI (setHeaderTitle in mom.utils.service) in associated viewmodel and render dynamic title from ctx.

## Problem with that approach :

We used to render static part of breadcrumb title using localized text (i18n text) in states.json. We were not able to render text from i18n in states.json for
below scenario.

Consider a scenario of work orders state (page with all work orders in tabular data) and work order report state(report for selected work order) in Supervise
UI app.When we mention the sublocation for report page, we provide below breadcrumb config details for report sublocation

"breadcrumbConfig": {
"type": "navigate",
"selectionTitleField": "id",
"parent": "workOrder"
}

Here we mean that, while constructing breadcrumb for report state, take work orders sublocation as previous breadcrumb details in order to structure
breadcrumb for report in this manner SuperviseWork Orders Report

While rendering breadcrumbs for this state, when it is trying to get the breadcrumb details for work order sublocation, it is trying to find the headertitle from
'location' instead of sublocation.

For location part, mom service reads header title as simple string text and doesnt read from i18n. As a result, we were not able to localize the header title
text for the location.

If we had kept it hard coded to string, with multi language support coming, this static header title text would never get translated with language switch.

## Solution:

MOM UI has provided a way to set dynamic header titles for states by including a mom-config file in our UI. We can mention the sublocation/location
names as key and localized and dynamic data as value to the corresponding keys in json.

Steps to implement:

```
Import the mom ctx service in authenticator file and call its init method at the very beginning stage during authentication.
Create a json file named mom-config at module or solution level as per requirement and mention this config in kit.json
Set the titles for sublocations and locations as mentioned above.
Build the UI and verify mom-config file is copied in out folder automatically.
Run the UI app. Titles should be loaded (both static and dynamic) as per the values mentioned in mom config.
```
Note:
In case if you are mentioning any sublocation(A) as a parent to any other sublocation(B), then you need to set location title for former state(A) in mom-
config along with sublocations. You can set different titles for location and sublocation, if you want the title to be different when you navigate to its child
state.

## Advantage:

```
You can bring all header title related static and dynamic value setting at one place.
It ensures all static titles are localized.
There are whole lot of things we can configure using mom-config file other than title expressions.
```
## How to Use:


### 1.

### 2.

### 3.

### 1.

### 2.

### 3.

Currently, basic structure is implemented in all 3 UI apps. At the solution level, there is mom-config json file in all apps.
Config file would look something like below

For any new state in the app, you need to update mom-config file in below manner.

```
Locate the title expressions section in the json file.
Add new state's name as key and its corresponding headerTitle to be set as value (refer to the above pic).
If the header title value to be set for the state has to be localized, make an entry in i18n so that the text is rendered from common localized file
(similar process what we do in any viewModel for localization)
```
For any update in existing state title,

```
Locate the state name to be updated in the title expressions section of the json file.
Update the value against the identified state.
If the value being updated has to be localized, add new entry in i18n(if not already existing) and remove old entry (if not being used anymore).
```
Note : The title expression in mom-config is virtually divided into two sections through a line break : 1.Header title expression for location 2.Header title
expression for sublocations. It would be better if we can maintain that seggregation.

Reference for Implementation:

https://gitlab.industrysoftware.automation.siemens.com/mom/mom-ui/-/wikis/using-the-mom-context-service


# How to use Error parameters in Modular MOM?

Error parameters is way of providing additional information with error message in order to consume them and translate it into more meaningful messages.
There are 2 aspect of it, one is how to generate errors from API with error parameters and another is how to use them in Modular MOM web apps.

## Adding error parameters to API error response

To return error response from MS we can use return response with instance of ExecutionError class. This class provide following overloads.

```
ExecutionError(int code,string message)
ExecutionError(int code,string message,Category category)
ExecutionError(int code,string message,List<ExecutionErrorMessageParam> errorParameters)
ExecutionError(int code,string message,List<ExecutionErrorMessageParam> errorParameters,Category category)
```
We can overload to pass error message parameters. Each error parameter can have following properties

```
Name - name of parameter, ex- WorkOrderName,StartDate etc.
```
```
Datatype - Type of parameter value, ex- dateTime, enum etc
Value - Value of parameter
IsLocalizationRequired - This is flag to indicate that this param value needs to translated in local language. ex- status parameters can have value
'Ready','NotReady','InProgress','Completed'. This values need to translated before use. There is another scenario where we need to translate
value before use i.e. DateTime values. API returns UTC date time and it needs to be converted to localized format(similar to SWF).
```
```
Sample code snippet - API response
```
```
return new CollectThroughput.Response
{
Error = new ExecutionError((int)ErrorCodes.CollectThroughput_WorkOrderNotExist,
$"Work Order '{command.WorkOrderName}' does not exist. Throughput cannot be
collected.",
new List<ExecutionErrorMessageParameter> {
new ExecutionErrorMessageParameter("WorkOrderName","string",command.
WorkOrderName)
})
};
```
## Consuming error parameters in Modular MOM web apps

In order to display meaningful and correct API error messages in web apps, we need to process the response and generate localized error message.
To simplify it we have developed error handling mechanism which is common for all ModularMOMHttpCall. So developer just need to provide error

```
Please note name should not exceed 65 chars in length. ModularMOM web apps will not process beyond 65 chars.
```

message and enable automatic error handling capability.


### 1.

### 2.

To enable automatic error handling in web apps, perform following steps-

```
Add error message in modularMomErrorMessages.csv at Localization resources repo
Example-
"10052100"," WorkOrder '{WorkOrder}' started on '{startDate}' and having status '{status}' failed to launch."
```
```
In viewModel file, while calling ModularMOMHttpCall, add following parameter to config object
```
```
autoErrorHandling=true
```
```
Sample code snippet - viewModel
```
```
"saveDeclareProducedData": {
"actionType": "ModularMOMHttpRequest",
"inputData": {
"url": "{{ctx.operateConfig.api.collectingService}}{{ctx.operateConfig.commands.
collectThroughput}}",
"data": {
"command": {
"WorkOrderName": "{{ctx.collectThroughputContextInfo.data.workOrderName.
dbValue}}",
"WorkOrderOperationName": "{{ctx.collectThroughputContextInfo.data.operationName.
dbValue}}",
"WorkCenterName": "{{ctx.collectThroughputContextInfo.data.workCenterName.
dbValue}}",
"Throughput": "{{data.throughputCollected}}"
}
},
"config": {
"withCredentials": false,
"method": "Post",
"autoErrorHandling": true
}
}
}
```
```
By default error will not be handled, this is done in order to make sure existing flow works and we do not get 2 error messages (as earlier we used
to show error from viewModel).
If you see 2 error messages, please go to action in viewModel and set failure:[ {message: "" }]. This will stop SWF generated message and we
can have our custom message.
```
```
Message parameters placeholder names (i.e. WorkOrder, startDate, status) must match with parameter names returned by API
response
```

# How to Install, Upgrade, and Uninstall Fake Microservices

This section describes how to install, upgrade, and uninstall the following fake microservices for Modular MOM Version 0.0.5:

```
Account
MaterialFlow
Process.
```

### 1.

### 2.

### 3.

### 4.

### 5.

### 6.

# Installing and Upgrading Account MS

Follow these steps to install the Account MS:

```
In the ModularMOM folder on the Windows client, open (without unzipping it) the Components\Account\modularmom_account_1.0.6.0.zip file.
A certificate for signing and validating the internal service token must be made available for the container. You can use the certificate that you
created (or an already owned certificate) as described in Installing or Upgrading Openshift Secrets.
Move into the folder ModularMOM\Components\Account.
```
```
Run the following command:
```
```
For installation, run:
```
```
oc process siemens-modularmom-template \
-p NAME=account \
-p KAFKA_ENDPOINTS=my-cluster-kafka-bootstrap:9092 \
-p DISCOVERY_ENDPOINT=<endpoint value> \
-p DATABASE_ENDPOINT=<endpoint value> \
-p SERVICE_CODE=117 \
-p Service_Acronym=ACC \
-p PUBLIC_DNS=<public domain url> \
-p INTERNAL_DNS=<internal domain url> |
oc create -f -
```
```
For upgrade, run:
```
```
oc process siemens-modularmom-template \
-p NAME=account \
-p KAFKA_ENDPOINTS=my-cluster-kafka-bootstrap:9092 \
-p DISCOVERY_ENDPOINT=<endpoint value> \
-p DATABASE_ENDPOINT=<endpoint value> \
-p SERVICE_CODE=117 \
-p Service_Acronym=ACC \
-p PUBLIC_DNS=<public domain url> \
-p INTERNAL_DNS=<internal domain url> |
oc apply -f -
```
```
Run the following command:
```
```
oc start-build account --from-archive=modularmom_account_1.0.6.0.zip
```
```
Run the following command:
```
```
oc rollout resume dc/account
```
```
The Account microservice (MS) must be installed before the UI microservices (Admin, Supervise, and Operate).
```
```
If SQL Server was configured to listen on a specific TCP port, the TCP port number must be specified in the database endpoint
variable. Specify it after the server name and a comma in the commands below like this: DATABASE_ENDPOINT=MSSQLservernam
e,TCP-port-number.
```

### 1.

### 2.

### 3.

### 4.

### 5.

### 6.

# Installing and Upgrading MaterialFlow MS

Follow these steps to install the MaterialFlow MS:

```
In the ModularMOM folder on the Windows client, open (without unzipping it) the Components\MaterialFlow\modularmom_materialflow_1.
0.6.0.zip file.
A certificate for signing and validating the internal service token must be made available for the container. You can use the certificate that you
created (or an already owned certificate) as described in Installing or Upgrading Openshift Secrets.
Move into the folder ModularMOM\Components\MaterialFlow.
```
```
Run the following command:
```
```
For installation, run:
```
```
oc process siemens-modularmom-template \
-p NAME=materialflow \
-p KAFKA_ENDPOINTS=my-cluster-kafka-bootstrap:9092 \
-p DISCOVERY_ENDPOINT=<endpoint value> \
-p DATABASE_ENDPOINT=<endpoint value> \
-p SERVICE_CODE=116 \
-p Service_Acronym=MLF \
-p PUBLIC_DNS=<public domain url> \
-p INTERNAL_DNS=<internal domain url> |
oc create -f -
```
```
For upgrade, run:
```
```
oc process siemens-modularmom-template \
-p NAME=materialflow \
-p KAFKA_ENDPOINTS=my-cluster-kafka-bootstrap:9092 \
-p DISCOVERY_ENDPOINT=<endpoint value> \
-p DATABASE_ENDPOINT=<endpoint value> \
-p SERVICE_CODE=116 \
-p Service_Acronym=MLF \
-p PUBLIC_DNS=<public domain url> \
-p INTERNAL_DNS=<internal domain url> |
oc apply -f -
```
```
Run the following command:
```
```
oc start-build materialflow --from-archive=modularmom_materialflow_1.0.6.0.zip
```
```
Run the following command:
```
```
oc rollout resume dc/materialflow
```
```
The MaterialFlow microservice (MS) must be installed before the UI microservices (Admin, Supervise, and Operate).
```
```
If SQL Server was configured to listen on a specific TCP port, the TCP port number must be specified in the database endpoint
variable. Specify it after the server name and a comma in the commands below like this: DATABASE_ENDPOINT=MSSQLservernam
e,TCP-port-number.
```

### 1.

### 2.

### 3.

### 4.

### 5.

### 6.

# Installing and Upgrading Process MS

Follow these steps to install the Process MS:

```
In the ModularMOM folder on the Windows client, open (without unzipping it) the Components\Process\modularmom_process_1.0.6.0.zip file.
A certificate for signing and validating the internal service token must be made available for the container. You can use the certificate that you
created (or an already owned certificate) as described in Installing or Upgrading Openshift Secrets.
Move into the folder ModularMOM\Components\Process.
```
```
Run the following command:
```
```
For installation, run:
```
```
oc process siemens-modularmom-template \
-p NAME=process \
-p KAFKA_ENDPOINTS=my-cluster-kafka-bootstrap:9092 \
-p DISCOVERY_ENDPOINT=<endpoint value> \
-p DATABASE_ENDPOINT=<endpoint value> \
-p SERVICE_CODE=115 \
-p Service_Acronym=PRC \
-p PUBLIC_DNS=<public domain url> \
-p INTERNAL_DNS=<internal domain url> |
oc create -f -
```
```
For upgrade, run:
```
```
oc process siemens-modularmom-template \
-p NAME=process \
-p KAFKA_ENDPOINTS=my-cluster-kafka-bootstrap:9092 \
-p DISCOVERY_ENDPOINT=<endpoint value> \
-p DATABASE_ENDPOINT=<endpoint value> \
-p SERVICE_CODE=115 \
-p Service_Acronym=PRC \
-p PUBLIC_DNS=<public domain url> \
-p INTERNAL_DNS=<internal domain url> |
oc apply -f -
```
```
Run the following command:
```
```
oc start-build process --from-archive=modularmom_process_1.0.6.0.zip
```
```
Run the following command:
```
```
oc rollout resume dc/process
```
```
The Process microservice (MS) must be installed before the UI microservices (Admin, Supervise, and Operate).
```
```
If SQL Server was configured to listen on a specific TCP port, the TCP port number must be specified in the database endpoint
variable. Specify it after the server name and a comma in the commands below like this: DATABASE_ENDPOINT=MSSQLservernam
e,TCP-port-number.
```

### 1.

### 2.

# Uninstalling Account MS

Follow these steps to uninstall the Account MS:

```
Open the OpenShift Container Platform CLI.
Run the following command:
```
```
oc delete all -l app=account
```

### 1.

### 2.

# Uninstalling MaterialFlow MS

Follow these steps to uninstall the MaterialFlow MS:

```
Open the OpenShift Container Platform CLI.
Run the following command:
```
```
oc delete all -l app=materialflow
```

### 1.

### 2.

# Uninstalling Process MS

Follow these steps to uninstall the Process MS:

```
Open the OpenShift Container Platform CLI.
Run the following command:
```
```
oc delete all -l app=process
```

### 1.

### 2.

### 3.

### 4.

### 5.

### 6.

### 7.

### 8.

# How to Upgrade Foundation Installation with ModMOM

# custom DSL Plug-ins

These are the steps:

```
Download the latest version of ModularMOM.DSL.Plugins.Net31 from: https://tfs01mom.industrysoftware.automation.siemens.com/tfs
/DefaultCollection/Revolution_MOM/_packaging?feed=UAF_RevMOM_feed&_a=feed
Unzip in some folder. There you should find a "plugin" folder.
There a 3 files you need Simatic.Unified.dll, Simatic.Unified.Common.dll and SimaticIT.DSL.PluginEvent.dll
Verify if all the Foundation services are stopped, if not, stop them and disable them (we don't need them). You can find them going in Services
and check for SimaticIT.
If you have some project studio instances opened, please close them
Go on the foundation bin folder, the default path is C:\Program Files\Siemens\SimaticIT\Unified\bin
There you must copy Simatic.Unified.dll, Simatic.Unified.Common.dll. It will ask you to overwrite them, do it.
Now, move to "Plugin" folder (defualt path C:\Program Files\Siemens\SimaticIT\Unified\bin\Plugin) and copy SimaticIT.DSL.PluginEvent.dll [
you should already have done this for enabling the many to many when you copied SimaticIT.DSL.DataModelBuilder.dll)
```
After step 8 you are good to go.


### 1.

### 2.

### 3.

### 4.

### 5.

### 6.

### 7.

# How to Integrate Apps with Token MS Locally

These are the links to the repositories of the Modular MOM Apps (Apps) and Token microservice (MS):

```
Admin
Operate
Supervise
Token.
```
There are two ways to run the Apps locally. The first way is to use the Token MS locally. The second way is to use the Token MS from an OpenShift
scenario (OS).

## How to Use Token MS Locally

Follow these steps to integrate the Apps with the Token MS locally:

```
Clone the repositories of the three Apps and Token MS to local folders.
In the local Token MS repository, go the this folder: src IdentityServer Properties.
Open the launchSettings.json file.
```
```
In the file, specify the following endpoints:
POST_LOGOUT_REDIRECT_URIS – specify the endpoints of the Apps to communicate with the Token MS.
EDIRECT_URIS – specify the endpoints of the Apps to communicate with the Token MS.
UMC_ENDPOINT – the UMC environment that you want to use.
AUTH_ENDPOINT – this is the Token MS URL. It must be https://<loopback address>:44355/adfs.
DiscoveryServiceEndpoint – you can use either local Discovery MS or Discovery MS from specific OS. To use local Discovery MS, the
endpoint must be http://localhost:44329/. To use Discovery MS from specific OpenShift environment, specify the Discovery MS
endpoint and use the public endpoints in the Discovery MS instead of localhost:5000.
```
```
In the repository of any of the Apps, go this folder: src solution.
Open the webapp-config.json file.
Specify the endpoint of the Permission MS in the permissionService field.
```
```
In the launchSettings.json file, there are two sections – IIS Express and IdentityServer. In the next step, edit the endpoints in one of the
sections depending on how you will launch the Token MS – by using IIS Express or Identity Server.
```

### 8.

### 9.

### 10.

### 1.

### 2.

### 3.

### 4.

### 5.

```
Specify the URL of local Token MS in the authority field.
```
```
Repeat steps 6-9 for the other two Apps.
Run the Token MS.
```
How to Use Token MS from an OS

This way is recommend only if there are no changes in Permission, Discovery, and Token MSs:

```
In the repository of any of the Apps, go this folder: src solution.
Open the webapp-config.json file.
In the authority field, specify the endpoint of the Token MS from an OS followed by /adfs.
```
```
Go to the web console of the OpenShift environment that you are going to use.
In the Topology view, select the Token MS pod.
```

### 6.

### 7.

### 8.

### 9.

```
In the Actions menu, select Edit Deployment Config.
```
```
Go to the Environment tab.
To allow requests from local environment, add the variables of the local Apps right to REDIRECT_URIS and ALLOWED_CORS_ORIGINS (use
```
```
semicolon to add the new value).
Save the changes.
```

### 1.

### 2.

### 3.

### 4.

### 5.

# Getting started with UI Auto tests

Reference Page from Siemens Web Framework: https://gitlab.industrysoftware.automation.siemens.com/Apollo/afx/-/wikis/home

Reference Page for BDD Test Framework: https://gitlab.industrysoftware.automation.siemens.com/Apollo/bdd-test-framework/-/wikis/home

Prerequisites :

```
Download & install latest from https://code.visualstudio.com
Install Visual Studio Code Extension Cucumber (Gherkin) Full Support by Alexander Krechik (alexkrechik.cucumberautocomplete)
Install Java Azul Zulu JDK 8 (https://www.azul.com/downloads/zulu-community/?version=java-8-lts&os=windows&architecture=x86-64-
bit&package=jdk)
Install Node.js from here - (Node 12.14.0 LTS that installs NPM 6.13.4) remember to append Environment variable PATH to include "C:\Program
Files\nodejs".
Update Google Crome to the last release: chrome://settings/help
```
Configure UI environment for launch UI auto tests :

```
Download UI repository from the tfs ( eg. SupeviseUI link - Revolution_MOM_MS_Supervise_UI )
Open repository in any text editor ( eg. Visual Studio Code )
Navigate to folder that contains UI
Launch any console(eg. Git Bash). If you use Visual Studio Code then open console there
Execute 'npm install' command
Execute 'npm audit fix' command
Delete node_modules/protractor/node_modules/@types folder
Execute 'npm run bdd_framework_build'
Edit bddconfig.json file. Add password for users which will be used for login to UI, and insert the UI page URL (i. e."https://superviseui-public-ui-
momosprj0x.apps.openshift01.swqa.tst/")
Execute 'npm run bdd_build' command
Execute 'npm run webdriver' command (run it in separate console, this need to be running throughout test execution)
Delete '--headless' in protractor.conf.js if you want to see visually how UI auto test works
Execute 'npm run bdd_test' command.
```
notice: be attention with path to microservices (e.g. https at the beginning of microservice link)

Troubleshooting:

Due to a problem in the BDD Test Framework, during the test execution, the following error may happen:

```
TypeError: global.pageSearchAreaMap is not a function
at World.iVerifyRowSelected (C:\git\Supervise_UI\node_modules\bdd-test-framework\dist\stepdefs\splmTableWidgetStepDefs.js:55:42)
```
In this case edit the file .\node_modules\bdd-test-framework\dist\stepdefs\splmTableWidgetStepDefs.js and change any 'global.
pageSearchAreaMap()' to 'global.pageSearchAreaMap'


### 1.

### 2.

### 3.

# Generate Role based token via Postman from Token Service

Please follow below steps to generate Role based token via Postman for ModularMOM [ These steps consider our token service deployed on OS07 project
]

Pre-requisite: A role [with any desired name] must be created in Admin app with required permissions. This role must be configured in Token service
under environment variables "THIRDPARTY_ROLES"

Example:

"THIRDPARTY_ROLES": ["Admin","TestRole"]

```
Select Verb (GET / POST) & Enter the URL / endpoint in postman Ex.: http://webapp.test.com/odata/yourendpointname
Go to ‘Authorization’ tab -> Select Type as ‘OAuth 2.0’. On right panel click ‘Get New Access Token’
Fill in values as mentioned here – The values shown in image are as per the OS07 project token service we have deployed. If you are running
token service on your own VDI, fill in values as set in your Config.cs / appSettings file.
```
```
Parameter
Name
```
```
Sample value [as per OS07] Description
```
```
Grant Type Client Credentials
```
```
Access
Token URL
```
```
https://{tokenservice endpoint}/connect/token
```
```
(for example, https://token-public-api-momosprj07.
apps.openshift01.swqa.tst/connect/token)
```
```
You can use token service from any OS environment
```
```
Client ID thirdpartyclient Use this client ID only
```
```
Client Secret clientsecret Use the secret configured under environment variable
"THIRDPARTY_CLIENT_SECRET" in your target token service
```
```
Scope ModularMOM.ServiceClient Use this scope only
```
```
Client
Authentication
```
```
Send client credentials in body
```
```
Sample token request from postman
```
```
If you have an error, check in Settings if SSL certificate validation has been disabled
```
```
This process will provide you a token with role claim which is configured in Token MS.
```
```
We are also adding steps in API automation to allow fetching token with specified role and also capability to add permissions to this role from
automation scripts. For this purpose all env files are configured with a default role name "AutomationRole".
```
```
Be sure that a role named AutomationRole has been created in Admin UI and has the needed permissions, otherwise this flow generates a
correct token but incapable of doing something
```

4. Click on ‘Request Token’
5. You should see the generated token like below –


6. Click on ‘Use Token’. The value will be set in your request Headers : Authorization OR You can also copy this token and use as needed.


### 1.

### 2.

# SonarLint for Client Static Code Analysis on Visual Studio

In order to carry out the static code analysis, also check a possible sonarqube fix before committing, it is possible to install an extension of Visual Studio
2017/2019 named SonarLint for Visual Sudio 2017 (or 2019 ).

After installation it necessary to connect to the sonarqube server ( in our case https://momsonarqube02.industrysoftware.automation.siemens.com
/sonarqube) and bind the project you want to analyze.

Documentation details of sonarlint here

Take care of:

```
when bind a sonarcube project for a visual studio solution, all the project in the solution are automatically checked out, so take care to not commit
project that you do not intentionally modified
sonarlint results are displayed in output windows with a specific code Sxxx. In order to get a sonarlint errors it is necessary to open the file the you
want to check.
```

# How to set Automation Test Execution Timeout

If you want to manually change the automation test execution timeout because you are in the situation where you need to run multiple test cases on
demand on the same time please follow this picture:


### 1.

### 2.

### 1.

### 1.

### 2.

### 3.

### 4.

### 1.

### 2.

# Configure keyboard shortcut for ModularMOM UI Apps

Common UI Modules

```
A new module "momKeyboardShortcut" is available in commonUImodules for UI apps.
This module is responsible to bind keyboard shortcuts to commands in UI apps and fire the events binded to these command actions. A generic
method is created for the same in "modularMOMKeyboardShortcutService.js"
```
Operate App

```
JSON file named "modMOM-keyboardshortcut.json" is available in src folder. This JSON comprises of the keyboard shortcut configuration in key-
value pair where value is the shortcut configured key like "Alt+{configured key in JSON}"
```
Note: We recommend usage of shortcut keys alongwith "alt" key. If a command is disabled or configured in JSON but no permission associated
to user, then no action will be invoked evenif the keyboard shortcut is pressed.

Currently, keyboard shortcuts are supported only in Operate App for below list of commands

```
KeyName from
JSON
```
```
Configured
Value in JSON
```
```
Command will
work if pressed
```
```
Operator Landing
Page Command
```
```
Collect TP Page
Command
```
```
Action invoked
```
```
refresh a Alt + a Refresh Refresh will reload the content
```
```
details t Alt + t Details ------ Open the Operation details panel
```
```
navigateToCollectThro
ughput
```
```
r Alt + r Collect Throuput ------ Navigate from Operator Landing Page to
Collect TP Screen
```
```
navigateToForceCollec
tThroughput
```
```
k Alt + k Force Collect Throughput ------- Navigate from Operator Landing Page to
Force Collect TP Screen
```
```
collectThroughputProc
ess
```
```
b Alt + b ---------- Process Will collect the throughput for operation
```
```
materialCall m Alt + m Material Call Material Call View or create Material Request
```
```
containerCall c Alt + c WIP Call WIP Call View or create Container Request
```
## How User can configure the keyboard shortcuts

```
User can configure his own defined shortcuts using the "modMOM-keyboardshortcut.json". But considering the Precautionary Steps
User has to navigate to the file path ../assets/config from the provided artifact for Operate App and edit the JSON file "modMOM-keyboardshortcut.
json"
Refer above table for configuartion description and place correct Configured value against desired key.
Re-zip again and deploy
```
Example : If User wants to configure keyboard shortcut for operation details

```
We are providing User the keyboard shortcut as Alt + t for details command in Operator Landing page.
But User, now wants it to be as Alt + i Follow below flow-diagram
```

Precautionary Steps

There are already few existing keys shortcut available either at browser/ hardware(Special Keyboards) level.
For eg: F5(Page Refresh), Ctrl+C(Copy) and many others.
Make sure we are not using/overlapping the existing key combination while defining your own shortcut keys.

Its always best to use atleast two keys in combination for giving any shortcut.


# DevOps


# Kafka SASL-SCRAM configuration

Invoke the following command lines:

```
./bin/kafka-configs.sh --zookeeper localhost:2181 --alter --add-config 'SCRAM-SHA-256=[iterations=8192,
password=alice-secret],SCRAM-SHA-512=[password=alice-secret]' --entity-type users --entity-name alice
```
```
./bin/kafka-configs.sh --zookeeper localhost:2181 --alter --add-config 'SCRAM-SHA-256=[password=admin-secret],
SCRAM-SHA-512=[password=admin-secret]' --entity-type users --entity-name admin
```
In /config create the following file "kafka_server_jaas.conf":

```
KafkaServer {
org.apache.kafka.common.security.scram.ScramLoginModule required
username="admin"
password="admin-secret";
};
```
modify the file "server.properties" as following:

```
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements. See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License. You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
```
```
# see kafka.server.KafkaConfig for additional details and defaults
```
```
############################# Server Basics #############################
```
```
# The id of the broker. This must be set to a unique integer for each broker.
broker.id=0
```
```
############################# Socket Server Settings #############################
```
```
# The address the socket server listens on. It will get the value returned from
# java.net.InetAddress.getCanonicalHostName() if not configured.
# FORMAT:
# listeners = listener_name://host_name:port
# EXAMPLE:
# listeners = PLAINTEXT://your.host.name:9092
#listeners=PLAINTEXT://:9092
```
```
# Hostname and port the broker will advertise to producers and consumers. If not set,
# it uses the value for "listeners" if configured. Otherwise, it will use the value
# returned from java.net.InetAddress.getCanonicalHostName().
#advertised.listeners=PLAINTEXT://172.23.193.58:9092
```
```
# Maps listener names to security protocols, the default is for them to be the same. See the config
documentation for more details
#listener.security.protocol.map=PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:S
ASL_SSL
```
```
# The number of threads that the server uses for receiving requests from the network and sending res
ponses to the network
num.network.threads=3
```
```
# The number of threads that the server uses for processing requests, which may include disk I/O
num.io.threads=8
```

# The send buffer (SO_SNDBUF) used by the socket server
socket.send.buffer.bytes=102400

# The receive buffer (SO_RCVBUF) used by the socket server
socket.receive.buffer.bytes=102400

# The maximum size of a request that the socket server will accept (protection against OOM)
socket.request.max.bytes=104857600

############################# Log Basics #############################

# A comma separated list of directories under which to store log files
log.dirs=/var/kafka-logs

# The default number of log partitions per topic. More partitions allow greater
# parallelism for consumption, but this will also result in more files across
# the brokers.
num.partitions=1

# The number of threads per data directory to be used for log recovery at startup and flushing at sh
utdown.
# This value is recommended to be increased for installations with data dirs located in RAID array.
num.recovery.threads.per.data.dir=1

############################# Internal Topic Settings #############################
# The replication factor for the group metadata internal topics "__consumer_offsets" and "__transact
ion_state"
# For anything other than development testing, a value greater than 1 is recommended for to ensure a
vailability such as 3.
offsets.topic.replication.factor=1
transaction.state.log.replication.factor=1
transaction.state.log.min.isr=1

############################# Log Flush Policy #############################

# Messages are immediately written to the filesystem but by default we only fsync() to sync
# the OS cache lazily. The following configurations control the flush of data to disk.
# There are a few important trade-offs here:
# 1. Durability: Unflushed data may be lost if you are not using replication.
# 2. Latency: Very large flush intervals may lead to latency spikes when the flush does occur as
there will be a lot of data to flush.
# 3. Throughput: The flush is generally the most expensive operation, and a small flush interval
may lead to excessive seeks.
# The settings below allow one to configure the flush policy to flush data after a period of time or
# every N messages (or both). This can be done globally and overridden on a per-topic basis.

# The number of messages to accept before forcing a flush of data to disk
#log.flush.interval.messages=10000

# The maximum amount of time a message can sit in a log before we force a flush
#log.flush.interval.ms=1000

############################# Log Retention Policy #############################

# The following configurations control the disposal of log segments. The policy can
# be set to delete segments after a period of time, or after a given size has accumulated.
# A segment will be deleted whenever *either* of these criteria are met. Deletion always happens
# from the end of the log.

# The minimum age of a log file to be eligible for deletion due to age
log.retention.hours=2

# A size-based retention policy for logs. Segments are pruned from the log unless the remaining
# segments drop below log.retention.bytes. Functions independently of log.retention.hours.
#log.retention.bytes=1073741824

# The maximum size of a log segment file. When this size is reached a new log segment will be create
d.
log.segment.bytes=1073741824

# The interval at which log segments are checked to see if they can be deleted according
# to the retention policies
log.retention.check.interval.ms=30000

############################# Zookeeper #############################

# Zookeeper connection string (see zookeeper docs for details).
# This is a comma separated host:port pairs, each corresponding to a zk
# server. e.g. "127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002".


modify the file named zookeper.properties as following:

```
# You can also append an optional chroot string to the urls to specify the
# root directory for all kafka znodes.
#zookeeper.connect=172.23.193.58:2181,172.23.193.163:2181
```
```
zookeeper.connect=172.23.193.58:2181
```
```
# Timeout in ms for connecting to zookeeper
zookeeper.connection.timeout.ms=6000
```
```
############################# Group Coordinator Settings #############################
```
```
# The following configuration specifies the time, in milliseconds, that the GroupCoordinator will de
lay the initial consumer rebalance.
# The rebalance will be further delayed by the value of group.initial.rebalance.delay.ms as new memb
ers join the group, up to a maximum of max.poll.interval.ms.
# The default value for this is 3 seconds.
# We override this to 0 here as it makes for a better out-of-the-box experience for development and
testing.
# However, in production environments the default value of 3 seconds is more suitable as this will h
elp to avoid unnecessary, and potentially expensive, rebalances during application startup.
#group.initial.rebalance.delay.ms=0
```
```
# List of enabled mechanisms, can be more than one
sasl.enabled.mechanisms=SCRAM-SHA-256
```
```
# Specify one of of the SASL mechanisms
sasl.mechanism.inter.broker.protocol=SCRAM-SHA-256
```
```
# Configure SASL_SSL if SSL encryption is enabled, otherwise configure SASL_PLAINTEXT
security.inter.broker.protocol=SASL_PLAINTEXT
```
```
# With SSL encryption
listeners=SASL_PLAINTEXT://:9093
advertised.listeners=SASL_PLAINTEXT://172.23.193.58:9093
```
```
#listener.name.sasl_ssl.scram-sha-256.sasl.jaas.config=org.apache.kafka.common.security.scram.ScramL
oginModule required username="admin" password="admin-secret";
```
```
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements. See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License. You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# the directory where the snapshot is stored.
dataDir=/var/zookeeper
# the port at which the clients will connect
clientPort=2181
```
```
tickTime=2000
initLimit=5
#syncLimit=2
#server.1=172.23.193.59:2888:3888
#server.2=172.23.193.198:2888:3888
```
```
# disable the per-ip limit on the number of connections since this is a non-production config
maxClientCnxns=0
```

Before running Kafka, perform the following command:

```
export KAFKA_OPTS=-Djava.security.auth.login.config=/root/kafka/kafka_2.12-2.3.1/config/kafka_server_jaas.conf
```

# Opcenter X with MIO Integration

By properly configuring your scenario, you will be able to exchange messages with the external system through the interaction of the and SAP XML and ER
P App (in case of ProductionOrder only) with the Manufacturing Interoperability product (MIO), which in turn exchanges messages with the external
system, in order to:

```
download Production Orders XML from the external system(in current case SAP) and manage them in the MES system;
download a request for moving an Production Order;
upload to external systems, and receive from external systems, the details of a single Production Order along with the main properties of its
Production schedule;
download a Production Order Result generated by the external system and manage them in the MES system.
```
## Prerequisites

```
Opcenter Execution Foundation 3.0 (hereinafter referred to as Opcenter EX FN) is installed and configured.
Opcenter Execution Process 3.0 is installed and configured.
Manufacturing Interoperability is installed and configured.
If you want to enable the possibility to send data to external systems through Manufacturing Interoperability (for example Opcenter Execution
Foundation allows to send Production Performance data to ERP system), the MIO Client Gateway service must be installed on all hosts.
The user performing the download of messages from external systems should belong to a custom dedicated Role. The commands ImportProduct
ionOrder provided by ERP App, must be associated to the Role. For more information on the Role definition, see chapter How to Configure User
Rights of the Opcenter Execution FoundationDevelopment and Configuration Guide.
In case of Opcenter EX FN distributed scenarios, the MOM Connection must be performed through the MS Application Request Routing (ARR),
or in general Network Load Balancer, address in order to route the calls to an available Service Layer. For more information, see chapter Distribut
ed Scenario of the Opcenter Execution Foundation Installation Guide to lean how to configure distributed scenarios.
```
## Available operations

This chapter, dedicated to the integration of Opcenter Execution Foundation with MIO, will help you to discover:

```
How to Configure MIO for Exchanging Messages with Opcenter Execution Foundation Microservice.
How to Write Messages for Opcenter Execution Foundation
How to Extend XML Files
```

### 1.

### 2.

### 3.

### 4.

### 5.

### 6.

### 7.

### 8.

# How to Configure MIO for Communicating with Serial

# Scales

In order to communicate with a serial scale, Opcenter Execution Foundation requires to make some specific configurations on Opcenter Execution
Foundation side.

The configuration steps provided in this chapter represent one of the possibilities provided by Opcenter Execution Foundation to let different systems
communicate: nothing prevents you from using other Adapters as foreseen by MIO.

Follow these configuration steps:

## Prerequisites

Before you follow the following workflow, it is necessary to configure the MOM Connection.

This is described in a previous chapter of this manual. Please perform the procedure from Configuring the MOM Connection.

## Target user

System Integrator

## Workflow

```
Define the Outbound Message types
Define the Inbound Message types
Define the Channel Sources
Define a Message Channel
Define the Client Gateway
Define the Serial Port Adapters
Configure the MOM Server
Configure the Client machine
```

### 1.

### 2.

### 3.

### 4.

### 5.

### 6.

# Defining a Message Channel For Serial Scales

The next step in the configuration of MIO for communicating with a serial Scale is defining a Message Channel.

You must use the Message Channel configuration object in MIO to a configure message channel for the scale interface.

## Target user

System Integrator

## Procedure

```
Set the attributes of the Message Chanel so that the Base Configuration contains the following information:as
```
```
Attribute Value
```
```
Name POMessageChannel
```
```
Description Pass SAP XML file to ERP microservice after converting through MIO Configuration
```
```
Is Active true
```
```
Map Message true
Add the following Valid Message Types:
```
```
Valid Message Types
```
```
productionschedule
```
```
productionscheduleresponse
```
```
as
Add the following Message Types Functions,
```
```
Attribute Value
```
```
Msg Type Extraction Function IsProductionrequest
```
```
For Above 'MessageType' add below other functions,
```
```
Name Function Type Arguments
```
```
IsProductionrequest conditional fn::checkproductonRequest;productionschedule;unknown
```
```
checkproductonRequest conditional /ProductionSchedule
In the Inbound Filters tab, no need to add any filters.
In the Outbound Filters tab, no need to add any filters.
Set the System Dispatch Rules such as:
```
```
Attribute Value
```
```
Expired Message Rule Expired
```
```
Invalid Message Rule Invalid
```
```
Response ResponseTimeout
```
## Next step

Define the Client Gateway for Serial Scales


### 1.

### 2.

### 3.

# Defining the Channel Sources For Serial Scales

The next step in the configuration of MIO for communicating with a serial Scale is defining the Channel Sources.

## Target user

System Integrator

## Procedure

```
Create a Channel Source instance for every computer running a Channel Adapter host using the Channel Source configuration object in MIO.
```
```
Configure this Channel Source, so that you have setup the following attributes in the Basic Configuration tab:
```
```
Attribute Value
```
```
Server Name IP address or name of the computer.
```
```
Example: 127.0.0.1(localhost)
```
```
Outbound Address https://+:8096/ChannelSource/
```
```
Name miochannelsource
```
```
Description MIO OOB Channel. Requires Router address if not localhost. Version=3.1.127.49.
```
```
Control Address https://+:8096/ChannelSource/
```
```
Admin Address net.tcp://localhost:8086/CIOChannelAdapterHost/Admin
```
```
Is Active true
In the Additional Configuration tab, set the following attributes:
```
```
Attribute Value
```
```
Name miochannelsource
```
```
Inbound Buffer Max Threads^10
```
```
Inbound Retry Interval (milliseconds) 500
```
```
Recent Message Expiration 120
```
```
Backup Server Name
```
```
Backup Poll Interval (seconds) 0
```
```
Admin Binding tcplargemessage
```
```
Outbound Binding basiclargemessage
Add the MIO server name in the Router Host Names.Example: WIN2016MIODEV
```
## Next step

Define a Message Channel For Serial Scales


### 1.

### 2.

### 3.

### 4.

### 5.

# Defining the Client Gateway for Serial Scales

The next step in the configuration of MIO for communicating with a serial Scale is configuring a Client Gateway instance for the MOM Server.

## Target user

System Integrator

## Procedure

```
Create and configure a Client Gateway, so that you have the following setup in the Base Configuration tab:
```
```
Attribute Value
```
```
Name The name of the client gateway personalized for the MOM Server machine
```
```
Example : MIOClient
```
```
Description The description of the client gateway
```
```
Control Address http://+:13024/ClientGateway/
```
```
Control Authentication Type None
```
```
Authentication Expiration (minutes) 20
```
```
Authenticate Plug In Name No value
```
```
Authenticate Plug In Configuration No value
```
```
From MIO Address http://+:13024/ClientGateway/FromRouter
```
```
Message Channel The Message Channel previously defined.
```
```
Example : POMessageChannel
```
```
Channel Adapter Override pofileadapter
```
```
MOM Connection Name The previously defined MOM connection related to the MOM Server.
```
```
Example: ERPConnection
```
```
Message Name Format Test{messagetype}_{messagename}_{messageid}
```
```
Client Gateway Host Name No value
```
```
Is Active True
In Inbound Filters and Outbound Filters no filters are added.
Set the Default values for all fields of the Buffer Settings.
As Host Names, define the MIO Server running the Broker and Router services. Example: ERPConnection
In the Additional Configuration tab, set the following properties:
```
```
Attribute Value
```
```
From MIO Binding Config basiclargemessage
```
```
Throttle Send to MIO 2
```
```
Throttle Add To Buffer 2
```
```
Send To MIO Retry Interval 2500
```
```
Host Exhaustion Interval^10
```
```
Recent Message Expiration 60
```
```
Convert To Json Plugin XmlToJson
```
```
Convert From Json Plugin JsonToXml
```
## Next step


Define the Serial Port Adapters for Serial Scales


### 1.

### 2.

### 3.

### 4.

# Defining the Inbound Message types for Serial Scales

The next step in the configuration of MIO for communicating with a serial Scale is defining the InBound Message Types.

There are several Message Type to create:

```
productionschedule
```
## Target user

System Integrator

## Creating productionschedule Message Type

```
Add a new message type with the following attributes:
```
```
Attribute Value
```
```
Name productionschedule
```
```
Description MIO connection to get production order from SAP and pass it to ERP microservice.
```
```
Dispatch Rule fifowithpredecessors
```
```
Priority 1
```
```
You should have this information the Base Configuration tab.
For Direct Dispatch and Content Converters use default values.
Click on the Message Attributes tab and set the following properties:
```
```
Name Function Function Args Do Not Promote Include In Event
```
```
restcommand equals /odata/ProcessB2MMLProductionSchedule false false
```
```
httpverb equals POST false false
```
```
replacementstring equals « false false
Click on the Additional Configuration tab and set the following properties:
```
```
Attribute Value
```
```
Message Map Message map for messages of this type that can be applied in the message channel or by the workflow.
```
```
Example: SamplewithGK
```
```
Message Map
Revision
```
```
Revision of the selected message map.
```
```
Example: 002
```
```
Override MOM
Connection Name
```
```
Name of override MOM Connection object.
```
```
Example: ERPConnection
```
```
Priority^1
```
```
Workflow Workflow to where messages of this type are dispatched.
```
```
Example: sit_ua_rest_call
```
```
Cache Expiration
(minutes)
```
```
Time from when a cacheable message is dispatched to when it is removed from the cache.
```
```
Example: 120
```
```
Time To Live (minutes) Number of minutes after a message of this type arrives at the message broker before it is expired and can be
evicted if it has not been dispatched.
```
```
Example: 120
```
```
Max Retry Count Number of times the application attempts a message dispatch before it is considered expired and dispatched to the
expired workflow.
```
```
It is written "No value" when the value for an attribute is empty.
```

### 4.

```
Example: 5
```
```
Min Retry Interval
(milliseconds)
```
### 500

```
Min Queue Time
(milliseconds)
```
```
Time between when a message of this type arrives at the message broker and when it is eligible to be dispatched.
```
```
Example: 0
```
```
Response To Type Response to a request. This field identifies the message type of the related request.
```
```
Example: productionscheduleresponse
```
```
Response Timeout
(seconds)
```
### 120

```
Event Only Check box indicating the message is not dispatched to a workflow (Inbound) or sent to an adapter (Outbound).
```
```
Example: false
```
```
Is Request Check box indicating this message type is a request message.
```
```
Example: false
```
```
Outbound On Create Check box indicating the message type is an outbound message and should be dispatched to a channel source
versus a workflow.
```
```
Example: false
```
```
Requires Cache Check box indicating that, after dispatching, messages of this type must be maintained in the message broker’s
cache until the cache expiration period has passed.
```
```
Example: false
```
Next Step

Define the Channel Sources For Serial Scales


### 1.

### 2.

### 3.

# Defining the Outbound Message Types for Serial Scales

The configuration of MIO for communicating with a serial Scale begins by define the Outbound Message Types.

There are several Message Type to create:

```
productionscheduleresponse
```
## Target user

System Integrator

## Creating productionscheduleresponse Message Type

```
Add a new message type with the following attributes:
```
```
Attribute Value
```
```
Name productionscheduleresponse
```
```
Description MIO connection to get production order from SAP and pass it to ERP microservice.
```
```
Dispatch Rule fifowithpredecessors
```
```
Priority 1
```
```
You should have this information the Base Configuration tab.
For Direct Dispatch, Content Converters and Message Attributes use default values.
Click on the Additional Configuration tab and set the following properties:
```
```
Attribute Value
```
```
Message Map Message map for messages of this type that can be applied in the message channel or by the workflow.
```
```
Example: SamplewithGK
```
```
Message Map
Revision
```
```
Revision of the selected message map.
```
```
Example: 002
```
```
Override MOM
Connection Name
```
```
Name of override MOM Connection object.
```
```
Example: ERPConnection
```
```
Priority
```
```
Workflow Workflow to where messages of this type are dispatched.
```
```
Example: sit_ua_rest_call
```
```
Cache Expiration
(minutes)
```
```
Time from when a cacheable message is dispatched to when it is removed from the cache.
```
```
Example: 10
```
```
Time To Live (minutes) Number of minutes after a message of this type arrives at the message broker before it is expired and can be
evicted if it has not been dispatched.
```
```
Example: 120
```
```
Max Retry Count Number of times the application attempts a message dispatch before it is considered expired and dispatched to the
expired workflow.
```
```
Example: 20
```
```
Min Retry Interval
(milliseconds)
```
### 1000

```
Min Queue Time
(milliseconds)
```
```
Time between when a message of this type arrives at the message broker and when it is eligible to be dispatched.
```
```
Example: 0
```
```
It is written "No value" when the value for an attribute is empty.
```

### 3.

```
Response To Type Response to a request. This field identifies the message type of the related request.
```
```
Response Timeout
(seconds)
```
### 120

```
Event Only Check box indicating the message is not dispatched to a workflow (Inbound) or sent to an adapter (Outbound).
```
```
Example: false
```
```
Is Request Check box indicating this message type is a request message.
```
```
Example: false
```
```
Outbound On Create Check box indicating the message type is an outbound message and should be dispatched to a channel source
versus a workflow.
```
```
Example: true
```
```
Requires Cache Check box indicating that, after dispatching, messages of this type must be maintained in the message broker’s
cache until the cache expiration period has passed.
```
```
Example: false
```
Next Step

Define the Inbound Message types for Serial Scales


### 1.

### 2.

### 3.

### 4.

# Defining the Serial Port Adapters for Serial Scales

The next step in the configuration of MIO for communicating with a serial Scale is defining the File Adapters.

In the MIO Adapter Configuration page, on the File Tab, create one File Adapters for each physical scales in your plant.
Apply the following procedure for each File Adapter.

## Target user

System Integrator

## Procedure

```
In the Opcenter Connect MOM Adapter Configuration page, on the File tab, create one File adapter.
In the Basic Configuration tab, set the following attributes as such:
```
```
Attribute Value
```
```
Name The name of the serial adapter personalized for the physical scale (Warning: the name must be in lower case).
```
```
Example: {pofileadapter}
```
```
Description The description of the adapter.
```
```
Channel Source The Channel Source created for the machine on which the scale is connected.
```
```
Example: {miochannelsource}
```
```
Message Channel The Message Channel previously configured for the scale interface.
```
```
Example: {POMessageChannel}
```
```
Content Format Xml
```
```
Inbound Name Format Format of the message trace for Inbound.
```
```
Example: {messagename}
```
```
Outbound Name Format Format of the message trace for Outbound.
```
```
Example: {messagename}_Response
```
```
Is Active true
Leave Default values for the Inbound Filters, Outbound Filters and Buffer Settings.
Set the File Adapter Configuration with the following attributes:
```
```
Attribute Value
```
```
Name The COM port where the scale is connected.
```
```
Example: pofileadapter
```
```
Inbound Drive To Map No value
```
```
Inbound URI The message input source physical path for File Adapter configuration.
```
```
C:\POMessages\inbound
```
```
Outbound Drive To Map No value
```
```
Outbound URI The message outpur/response physical path for File Adapter configuration.
```
```
C:\POMessages\outbound
```
```
Max Read Retries The number of retries to read actual message from Inbound URI.
```
### 1.

### 2.

### 3.

```
Adding a new serial Port Adapter on an already configured environment requires to:
```
```
Perform an iisreset on the MIO server.
Restart the MIO client Gateway service on the MOM server.
Restart the MIO channel host adapter on the client machine.
```

### 4.

### 5.

```
Example: 10
```
```
Retry Delay (milliseconds) Time delay of retries to read actual message from Inbound URI.
```
```
Example: 100
```
```
Delete Interval (seconds) Time delay to delete actual message from Inbound folder after message read.
```
```
Example: 15
```
```
Inbound File Name Filter *.*
```
```
Is XML Format of the message trace for Output.
```
```
Example: true
In the Additional Configuration tab, add below values.
```
```
Attribute Value
```
```
Name pofileadapter
```
```
Inbound Name Format {messagename}
```
```
Outbound Name Format {messagename}_Response
```
```
Wake Up Interval (seconds) 1
```
```
Shutdown Max Time (seconds) 15
```
```
Outbound Retry Interval (seconds) 15000
```
```
Outbound Max Retries 5
```
```
InProcess Message Expiry (minutes) 120
```
```
Unique Cache Expiry (minutes) 0
```
```
Outbound Fail Plugin LogAndDiscard
```
```
Outbound Fail Configuration null
```
```
MOM Connection Name ERPConnection
```
```
Domain Name No value
```
```
User Name CamstarAdmin
```
```
Password ***********
```
```
Do Not Send false
```
```
Do Not Send Outbound false
```
```
Verify Unique false
```
Next step

Configure MOM Server for Serial Scales


### 1.

### 2.

# How to Configure MIO for Exchanging Messages with

# Opcenter X

In order to allow message exchange between MIO and Opcenter X Process, it is required to make some specific configurations on MIO side. The
configuration steps provided in this chapter represent one of the possibilities provided by MIO to let different systems communicate. More specifically, this
chapter focuses on the File Adapter adoption, in order to trigger a business logic provided by Opcenter X Process. Nothing prevents you from using other
Adapters as foreseen by MIO.

Follow these configuration steps:

## Workflow

```
Configuring the MOM Connection
Depending on the required use cases, you can configure the sending or reception of messages:
```
```
configure the download of data from external systems;
configure the upload of data to external systems.
```

### 1.

### 2.

### 3.

### 4.

### 5.

# Configuring the MOM Connection

A MOM Connection allows you to establish the communication, which will then be realized through messages exchange, between Opcenter X and the
Manufacturing Interoperability Web Application.

This is the starting point for the configuration of the use cases designed for the Production Order downloads.

## Procedure

```
Access the Opcenter Connect MOM login page.
Log in to the Opcenter Connect MOM Web Application.
Click the Navigate MIO icon in the Opcenter Connect MOM Configuration tile.
Click MOM Connections.
Click to create a new MOM Connection with the following properties and click Add Object.
```
```
Property Value
```
```
User Name The name of a Opcenter X user.
```
```
Description Connect to ERP Microservice from MIO Machine, configuration to pass data through REST call.
```
```
Password The user password
```
```
Name The name identifying the new MOM connection: e.g. ERPConnection
```
```
IsActive true
```
```
Authentication
Extension Data
```
```
scope=global
```
```
Authentication Address The URL of the Microservice is not required for local MIO machine. Therefore for current Use case it's blank.
```
```
Autentication Type None
```
```
Address The address of the machine where Opcenter X is installed. The name must start with the http/https prefix, for
example http://localhost:7000/
```
## Next step

```
Once you have configured the MOM Connection you can configure the download or upload of messages.
```

### 1.

### 2.

### 3.

### 4.

### 5.

### 6.

### 7.

# How to Download a Message

Opcenter X can facilitate communications between different systems when implemented with the Manufacturing Interoperability product (MIO).

This chapter presents how to download messages from external systems(SAP), through MIO.

More specifically, this chapter focuses on the File Adapter adoption, in order to trigger a business logic provided by Opcenter X.
Nothing prevents you from using other adapters, as foreseen by MIO.

The described procedures are tailored for each type of download messages provided by the system. You will find specific details (when necessary)
depending on which type you mean to implement.

## Workflow

Follow these configuration steps to implement the use case of interest:

```
Import the Workflow
Import the Message Map
Define the Message Types
Define the Message Channel
Define the File Adapter
Reset the IIS and restart the Channel Adapter Host Service, to activate the Use Case configuration
Verify that the configuration works properly
```

### 1.

### 2.

### 3.

### 4.

### 5.

# Checking the Download

To verify if the use case was correctly executed, perform the following steps:

## Procedure

```
Copy the XML file, for the use case of interest, in the folder created for the inbound messages, for example C:\POMessages\inbound.
Verify the content of the response message, created in the folder dedicated to outbound messages, for example C:\POMessages\outbound
folder. Be sure that no errors have occurred.
In Opcenter execution foundation, depending on the use case you have executed check if the respective microservice for fakeDiscovery, Plan
and ERP are running in local environment.
Put respective message in inbound folder to get response in Outbound folder.
To see MIO log messages during the execution of the operation, open Applications and Services Logs -> MIO in Microsoft Event Viewer.
```

### 1.

### 2.

### 3.

### 4.

### 5.

### 6.

### 7.

# Defining the File Adapter

In order to configure Opcenter connect MOM to perform a message download, a File Adapter must be created on Opcenter connect MOM Web
Application.

## Prerequisite

Before you start working, it is recommended to create two dedicated folders where the messages to and from external systems will be stored, e.g.:

```
Inbound Messages: C:\POMessages\inbound
Outbound Messages: C:\POMessages\outbound
```
The Message Channel has been defined.

## Procedure

```
Log in to the Opcenter connect MOM Web Application.
Click in the Opcenter Connect MOM Adapter Configuration tile.
Click the File tab.
Click the Add Object button, to create a new File Adapter with the following properties:
```
```
Attribute Value
```
```
Name pofileadapter(lower case is mandatory)
```
```
Description File Adapter for Production Order XML file from SAP
```
```
Channel Source miochannelsource (already present in the dropdown)
```
```
Message Channel POMessageChannel(already present in the dropdown)
```
```
Content Format Xml
```
```
Inbound Name Format {messagename}
```
```
Outbound NameFormat {messagename}_Response
```
```
IsActive true
Select the pofileadapter File Adapter, and click.
Select File Adapter Configuration and configure the following parameters:
```
```
Parameter Value
```
```
Name pofileadapter
```
```
Inbound Drive To Map No value
```
```
Inbound URI The path to the folder where the input XML file will be placed, for example: C:\POMessages\inbound
```
```
Outbound Drive To Map No value
```
```
Outbound URI The path to the folder where the output XML file will be placed, for example: C:\POMessages\outbound
```
```
Is XML true
```
```
Note: the value of other attributes can be modified according to the project needs. Consider that the procedure as been tested with the default
attribute values.
Select Additional Configuration and configure the following parameter:
```
```
Parameter Value
```
```
Mom Connection Name pofileadapter(already present in the dropdown)
```
```
The File Adapter you are going to configure can be used for all the use cases that are described in this document: so, if you have already
configured a File Adapter, you can skip this configuration step but you must reset the IIS and restart the Channel Adapter Host Service.
```
```
For Sample Upload use case: If you create the dedicated folders as suggested above,
then for Sample Result Download use case: you must create the dedicated folders at a different location, e.g.: c:\EXPRMessages\SampleResult
Download\inbound
```

### 7.

### 8.

### 9.

```
Inbound Name Format {messagename}
```
```
Outbound Name Format {messagename}_Response
```
```
Wake Up Interval (seconds) 1
```
```
Shutdown Max Time (seconds) 15
```
```
Outbound Retry Interval (seconds) 15000
```
```
Outbound Max Retries 5
```
```
InProcess Message Expiry (minutes)^120
```
```
Unique Cache Expiry (minutes)^0
```
```
Outbound Fail Plugin LogAndDiscard
```
```
Outbound Fail Configuration null
```
```
MOM Connection Name ERPConnection
```
```
Domain Name No Value
```
```
User Name CamstarAdmin
```
```
Password ***********
```
```
Do Not Send false
```
```
Do Not Send Outbound false
```
```
Verify Unique false
```
```
Note: the value of other parameters can be modified according to the project needs. Consider that the procedure as been tested with the default
attribute values.
Click Save.
Reset the IIS and restart the Channel Adapter Host Service.
```
Next step

After you have reset the IIS and restarted the Channel Adapter Host Service, to activate the Use Case configuration, you can check if the Download was
successfully executed.


### 1.

### 2.

### 3.

### 4.

### 5.

### 6.

### 7.

### 8.

# Defining the Message Channel

In order to configure Opcenter Connect MOM to perform a message download, a Message Channel must be created on Opcenter Connect MOM Web
Application.

## Prerequisites

The Message Types have been defined.

## Procedure

```
Log in to the Opcenter Connect MOM Web Application..
Click in the Opcenter Connect MOM Configuration tile.
Click the Message Channels tab and click the Add Object icon, to create a new Message Channel with the following properties:
```
```
Parameter Value
```
```
Name POMessageChannel
```
```
Description Pass SAP XML file to ERP microservice after converting through MIO Configuration.
```
```
Is Active true
```
```
Map Message true
Select the PIMessageChannel.
Select the Valid Message Types and click , to add the following data:
```
```
Valid Message Type
```
```
productionschedule
```
```
productionscheduleresponse
Select Message Type Functions, click to edit the Msg Type Extraction Function, type IsProductionrequest and click.
In Message Type Functions area, in Other Functions section, click and insert the following function, which is used to extract the Material
Definition type from the XML file:
```
### NAME FUNCTION TYPE ARGUMENTS

```
IsProductionrequest conditional fn::checkproductonRequest;productionschedule;unknown
```
```
checkproductonRequest checkxpath /ProductionSchedule
Click Add Item.
```
## Next step

You must now define the File Adapter.

```
The Message Channel is used for all use cases described in this document. If it has been already created, skip steps 3, 6, 7 and 8.
```

### 1.

### 2.

### 3.

### 4.

### 5.

### 6.

### 7.

# Defining the Message Types

In order to configure Opcenter Connect MOM to perform a download, the following Message types must be created on Opcenter Connect MOM Web
Application.

## Prerequisite

The Message Map has been imported.

## Configuring the Message Type

```
Log in to the Opcenter Connect MOM Web Application.
Click the Opcenter Connect MOM Configuration tile.
Click the Message Types tab.
Click the Add Object button, to create a new Message Type.
Fill the Name field with one the following Name depending on the required use case:
```
```
Use Case Name Description Priority
```
```
Material Download MaterialDefinition Defines the message type
for the Material Download
```
```
1
```
```
Material Lot MaterialLot Defines the message type
for the Material Lot Download
```
```
1
```
```
Production Order ProductionRequest Defines the message type
for the Production Order Download
```
```
1
```
```
Move Request Move Defines the message type
for the Material Move Request Download
```
```
1
```
```
Work Order
or Work Order Operation
```
```
WorkOrder Defines the message type
for the Work Order or Work Order Operation Download
```
```
1
```
```
Sample Result Download SampleResultRequ
est
```
```
Defines the message type for the Sample Result Download 1
```
```
Production Order for
Opcenter X
```
```
productionscheduleOpcenter Connect MOM connection to get production order from SAP and pass it to ERP
microservice.
```
```
1
```
```
Click the Add Object button at the bottom of the panel, select the Message Type you have just configured and click.
Select Additional Configuration Settings and configure the following parameters:
```
```
Attribute Value
```
```
Message Map SamplewithGK
```
```
Message Map Revision 002
```
```
Override MOM Connection Name ERPConnection
```
```
Workflow sit_ua_rest_call
```
```
Cache Expiration (minutes) 120
```
```
Time To Live (minutes) 120
```
```
Max Retry Count 5
```
```
Min Retry Interval (milliseconds) 500
```
```
Min Queue Time (milliseconds) 0
```
```
Response To Type productionscheduleresponse
```
```
Response Timeout (seconds) 120
```
```
Repeat this procedure for each Message Type that you want to configure.
```
```
Mind keeping the respective description to the name you choose. The proposed priority value can be modified to fit your needs. Please
note highlighted use case is for Opcenter X.
```

### 7.

### 8.

### 9.

### 1.

### 2.

### 3.

### 4.

### 5.

### 6.

```
Event Only false
```
```
Is Request false
```
```
Outbound On Create false
```
```
Requires Cache false
```
```
Note: the value of other attributes can be modified according to the project needs. Consider that the procedure as been tested with the default
attribute values.
Click Save.
Select Message Attributes, click to add the following properties and click Save :
```
```
Name Use Case Function Function Args Do Not Promote Include In Event
```
```
restcommand Material Download equals /odata/ProcessB2MMLProductionSchedule false false
```
```
httpverb All equals POST false false
```
```
replacementstring All equals « false false
```
```
Note: the values in Do Not promote and Include In Event columns can be modified according to the project needs. Consider that the procedure
has been tested with the false values.
```
Configuring the Response Message type

```
Log in to the Opcenter Connect MOM Web Application.
Click the Opcenter Connect MOM Configuration tile.
Click Message Types tab.
Click the Add Object button, to create a new Message Type with the following attributes:
```
```
Download Use Case Name Description Priority
```
```
Material MaterialDefinitionresponse Response for the Material download 1
```
```
Material Lot MaterialLotresponse Response for Material Lot download 1
```
```
Production Order ProductionRequestresponse Response for the Production Order download 1
```
```
Material Move Request Moveresponse Response for the Material Move Request download 1
```
```
Work Order or
```
```
Work Order Operation
```
```
WorkOrderresponse Response for Work Order or Work Order Configuration download 1
```
```
Sample Result SampleResultRequestresponse Response for Sample Result download 1
```
```
Production Order for Opcenter X productionscheduleresponse Response for the Production Order download for Opcenter X 1
Click the Add Object button at the bottom of the panel, select the Message Type you have just configured and click.
Select Additional Configuration Settings and configure the following parameters:
```
```
Note: the value of other attributes can be modified according to the project needs. Consider that the procedure has been tested with the default
attribute values.
```
```
Attribute Value
```
```
Message Map SamplewithGK
```
```
Message Map Revision 002
```
```
Override MOM Connection Name
```
```
Workflow
```
```
Cache Expiration (minutes) 10
```
```
Time To Live (minutes) 120
```
```
Max Retry Count 20
```
```
Be consistent: create a Response Message Type for each Message Type that you have added.
Select the appropriate Response Message Type according to each added Message Type.
```

### 6.

### 7.

```
Min Retry Interval (milliseconds) 1000
```
```
Min Queue Time (milliseconds) 0
```
```
Response To Type
```
```
Response Timeout (seconds) 120
```
```
Event Only false
```
```
Is Request false
```
```
Outbound On Create true
```
```
Requires Cache false
Click Save.
```
Next step

You must now define the Message Channel.


### 1.

### 2.

### 3.

### 4.

### 5.

### 6.

### 7.

# Importing the Message Map

In order to configure MIO to perform a Download, the Message Map file, specific for each provided use case, must be imported into MIO Map Designer.

## Prerequisites

The Workflow has been imported.

## Procedure

```
Open the Opcenter Connect MOM Map Designer Application
In the Opcenter CN MOM Server Settings tab, configure the credentials to connect to the application server. A user with administrative rights
must be entered.
Click Save Settings.
On the menu, click Import Map.
Depending on the use case you are considering import the appropriate message map file contained in %SITUnifiedVAppsData%
\UAPI\MIOIntegration\Mappings folder.
```
```
If you are using MIO 3.0 you must import the message map files from %SITUnifiedVAppsData%\UAPI\MIOIntegration\Mappings\3.0 folder.
```
```
Use Case File Name
```
```
Material Download MaterialMapping_Rev_N.miom
```
```
Material Lot Download LotMapping_Rev_B.miom
```
```
Production Order Download OrderMapping_Rev_P.miom
```
```
Material Move Request Download MoveMapping_Rev_B.miom
```
```
Work Order Download
Work Order Operation Download
```
```
WorkOrderMapping_Rev_C.miom
```
```
Sample Result Download SampleResult_Rev_B.miom
```
```
This message map file is specific to Opcenter Laboratory only.
Click Save Map.
Code Map for ProductionOrder is as below.
```
```
SamplewithGK.miom
```
```
{"Name":"SamplewithGK","Revision":"002","IsRevisionOfRecord":true,"Description":"","type":"
MessageMapConfiguration","ContentFormat":"Xml","JsonToXmlPlugIn":"JsonToXml","JsonToXmlInitString":"","
XmlToJsonPlugIn":"XmlToJson","XmltoJsonInitString":"","TargetTemplate":"
UEQ5NGJXd2dkbVZ5YzJsdmJqMGlNUzR3SWlCbGJtTnZaR2x1WnowaVZWUkdMVGdpSUQ4K0RRbzhZMjl0YldGdVpENE5DanhDTWsxTlRFUmhkR0Ur
RFFvOEwwSXlUVTFNUkdGMFlUNE5Dand2WTI5dGJXRnVaRDQ9","MapSource":{"Contents":"
PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0idXRmLTgiPz4NCjxQcm9kdWN0aW9uU2NoZWR1bGUgeG1sbnM6Ym1sPSJodHRwOi8vd3d3Lm1l
c2Eub3JnL3htbC9CMk1NTC1WMDYwMCIgeG1sbnM6RXh0ZW5kZWQ9Imh0dHA6Ly93d3cubWVzYS5vcmcveG1sL0IyTU1MLVYwNjAwLUFsbEV4dGVu
c2lvbnMiID4NCjxJRD4xMjM0NXM2Nzg8L0lEPg0KPFN0YXJ0VGltZT4yMDE0LTAxLTAxVDIzOjI4OjU2Ljc4Mlo8L1N0YXJ0VGltZT4NCjxEZXNj
cmlwdGlvbj5Qcm9kdWN0aW9uU2NoZWR1bGUgRGVzY3JpcHRpb248L0Rlc2NyaXB0aW9uPg0KPFByb2R1Y3Rpb25SZXF1ZXN0Pg0KCTxEZXNjcmlw
dGlvbj5Qcm9kdWN0aW9uUmVxdWVzdCBEZXNjcmlwdGlvbjwvRGVzY3JpcHRpb24+DQoJPFN0YXJ0VGltZT4yMDE0LTAxLTAxVDIzOjI4OjU2Ljc4
Mlo8L1N0YXJ0VGltZT4NCgk8RW5kVGltZT4yMDE0LTAxLTAxVDIzOjI4OjU2Ljc4Mlo8L0VuZFRpbWU+DQoJPFByaW9yaXR5PiJIaWdoIjwvUHJp
b3JpdHk+DQoJPEV4dGVuZGVkOk5hbWU+Q2FydDwvRXh0ZW5kZWQ6TmFtZT4NCgk8UmVxdWVzdFN0YXRlPmluLXByb2Nlc3M8L1JlcXVlc3RTdGF0
ZT4gICAgICAgDQoJPEV4dGVuZGVkOlF1YW50aXR5Pg0KCQk8RXh0ZW5kZWQ6UXVhbnRpdHlTdHJpbmc+MjAwPC9FeHRlbmRlZDpRdWFudGl0eVN0
cmluZz4NCgkJPEV4dGVuZGVkOlVuaXRPZk1lYXN1cmU+dW5pdHM8L0V4dGVuZGVkOlVuaXRPZk1lYXN1cmU+DQoJPC9FeHRlbmRlZDpRdWFudGl0
eT4NCgk8RXh0ZW5kZWQ6UXVhbnRpdHkyPg0KCQk8RXh0ZW5kZWQ6UXVhbnRpdHkyU3RyaW5nPjUwMDwvRXh0ZW5kZWQ6UXVhbnRpdHkyU3RyaW5n
Pg0KCQk8RXh0ZW5kZWQ6VW5pdE9mTWVhc3VyZTI+VW5pdE9mTWVhc3VyZTI8L0V4dGVuZGVkOlVuaXRPZk1lYXN1cmUyPg0KCTwvRXh0ZW5kZWQ6
UXVhbnRpdHkyPg0KPC9Qcm9kdWN0aW9uUmVxdWVzdD4NCjwvUHJvZHVjdGlvblNjaGVkdWxlPg==","FileName":null},"MapFunctions":
{"ListItem":[{"Name":"getcontent","Description":"","FunctionType":"nodeset","Arguments":".","
LastResolvedValue":"<ProductionSchedule xmlns:Extended=\"http://www.mesa.org/xml/B2MML-V0600-AllExtensions\"
xmlns:bml=\"http://www.mesa.org/xml/B2MML-V0600\">\n <ID>12345s678</ID>\n <StartTime>2014-01-01T23:28:56.782Z<
/StartTime>\n <Description>ProductionSchedule Description</Description>\n <ProductionRequest>\n
<Description>ProductionRequest Description</Description>\n <StartTime>2014-01-01T23:28:56.782Z<
/StartTime>\n <EndTime>2014-01-01T23:28:56.782Z</EndTime>\n <Priority>\"High\"</Priority>\n <Extended:
Name>Cart</Extended:Name>\n <RequestState>in-process</RequestState>\n <Extended:Quantity>\n
<Extended:QuantityString>200</Extended:QuantityString>\n <Extended:UnitOfMeasure>units</Extended:
UnitOfMeasure>\n </Extended:Quantity>\n <Extended:Quantity2>\n <Extended:Quantity2String>500<
/Extended:Quantity2String>\n <Extended:UnitOfMeasure2>UnitOfMeasure2</Extended:UnitOfMeasure2>\n <
/Extended:Quantity2>\n </ProductionRequest>\n</ProductionSchedule>"},{"Name":"finalData","Description":"","
```

```
FunctionType":"concat","Arguments":"«;fn::getcontent;«","LastResolvedValue":"«<ProductionSchedule xmlns:
Extended=\"http://www.mesa.org/xml/B2MML-V0600-AllExtensions\" xmlns:bml=\"http://www.mesa.org/xml/B2MML-V0600\"
>\n <ID>12345s678</ID>\n <StartTime>2014-01-01T23:28:56.782Z</StartTime>\n <Description>ProductionSchedule
Description</Description>\n <ProductionRequest>\n <Description>ProductionRequest Description<
/Description>\n <StartTime>2014-01-01T23:28:56.782Z</StartTime>\n <EndTime>2014-01-01T23:28:56.782Z<
/EndTime>\n <Priority>\"High\"</Priority>\n <Extended:Name>Cart</Extended:Name>\n <RequestState>in-
process</RequestState>\n <Extended:Quantity>\n <Extended:QuantityString>200</Extended:
QuantityString>\n <Extended:UnitOfMeasure>units</Extended:UnitOfMeasure>\n </Extended:Quantity>\n
<Extended:Quantity2>\n <Extended:Quantity2String>500</Extended:Quantity2String>\n <Extended:
UnitOfMeasure2>UnitOfMeasure2</Extended:UnitOfMeasure2>\n </Extended:Quantity2>\n </ProductionRequest>\n<
/ProductionSchedule>«"}]},"MapDetails":{"ListItem":[{"Name":"entrirexml","Description":"Map entire content of
file to command B2MML tag","Sequence":10,"Conditional":"","TargetNamespace":"","TargetValue":"fn::finalData","
TargetExpression":"/command/B2MMLData","TargetFieldType":"Value","TargetFieldName":"","
SourceParentExpression":"","SourceExpression":"","UseCdata":false,"IsDisabled":false}]},"IncludedMaps":
{"ListItem":[]}}
```
Next step

You must now define the Message Types.


### 1.

### 2.

### 3.

### 4.

### 5.

# Importing the Workflow

In order to configure MIO to perform a Download, the sit_ua_rest_call.miow Workflow must be imported into MIO Workflow Designer and saved on the
system.

## Prerequisite

A MOM connection has been configured.

## Procedure

```
Open the Opcenter Connect MOM Configuration Application
In the Settings tab, configure the credentials to connect to the application server. A user with administrative rights must be entered. Click Save.
On the menu, point to Import File.
Import the %SITUnifiedVAppsData%\UAPI\MIOIntegration\Workflows\sit_ua_rest_call.miow file. If file is not available in given location add it
from manual location/physical path.
Click Save Workflow.
```
## Next step

You must now import the Message Map.

```
The sit_ua_rest_call.miow Workflow is common to all use cases: so, if you have already configured a Workflow you can skip this configuration
step.
```
```
If you have already imported a workflow file with a previous version of MIO, in MIO Workflow Designer, click the Open Workflow button to
open the sit_ua_rest_call.miow file. Save it and close the MIO Workflow Designer.
```

### 1.

### 2.

### 3.

### 4.

### 5.

### 6.

### 7.

# How to Upload a Message

Opcenter X can facilitate communications between different systems when implemented with the Manufacturing Interoperability (MIO) product.

This chapter presents how to upload messages to external systems, through MIO.

More specifically, this chapter focuses on the File Adapter adoption, in order to trigger a business logic provided by Opcenter X.
Nothing prevents you from using other adapters, as foreseen by MIO.

The described procedures are tailored for each type of upload messages provided by the system. You will find specific details (when necessary)
depending on which type you mean to implement.

The following types are provided by the system:

```
Upload Message Type Name Description
```
```
Work Order Upload SendWorkOrder Defines the message type for a Work Order and its operations
```
```
or
```
```
Defines a Work Order Operation with the main properties of its respective Work Order
```
```
Sample Upload SendSample Defines the message type for the Sample
```
```
Production Performance Upload ProductionPerformance Defines the message type for the Production Performance
```
## Workflow

```
(Only for Sample Upload Message) Importing the Message Map
Define the Message Types
Define the Message Channel
Define the File Adapter
Define the Client Gateway
Reset the IIS and restart the Channel Adapter Host Service, to activate the Use Case configuration
Verify that the configuration works properly
```
Please note: Currently How to upload of workflow is not clear, therefore not uploading any documentation for above workflow uploading methodology.


# How to Extend XML Files

It is possible to extend both the inbound and outbound messages.

## Extending inbound messages

You can enrich the XML files, containing the message you want to download from the external system, with custom information.

Once downloaded, Opcenter EX PR converts the information provided by the standard message in Opcenter EX PR entities and you can execute a post-
action in order to retrieve and use the custom information you added.

The custom information are stored in a dedicated field, named Bag, for current Production Order case we are not using Bag.

## Extending outbound messages

The custom properties you can add to the B2MMLData to prepare an upload message to the external system, are defined by the system by using the
following tags in the XML file:

```
<ProductionSchedule>
```
The system can identify the right tag according to the entity the custom properties belong to.

B2MMLData is a set of information that can be sent to an external system, for tracing and recording particular actions (for example, status change)
performed on one or more artifacts (for example, a movement).

These actions are identified by a transaction code that can be chosen among those provided by the system or custom-defined by the user.


# Messages for Manufacturing Processes

Opcenter Execution Foundation is compliant with B2MML-V0600-Extensions.xsd file which is an extension schema, of MESA standards. The file has
been enriched in order to cover the Opcenter Execution Foundation use cases, as described in this document.

An example of the file structure is provided for each of the following use cases:

```
Production Order Download Message
```
Furthermore, you can customize the provided XML files as described in How to extend XML Files section.


# Production Order Download Message

This use case can be applied each time it is necessary to automatically download an Order from an external system and make it available in Opcenter
Execution Foundation environment.

In details, it is possible to:

```
Create Production Orders.
Update Production Orders, if already present in the MES system.
Ignore unnecessary content of the message received by the external system.
```
The following description explains the file structure which is to be used to download an Order. You need to configure Manufacturing Interoperability (MIO).

## The file structure

The file contains:

```
the XML declaration consisting of the encoding declaration:
<?xml version="1.0" encoding="UTF-8"?>,
the B2MML namespaces to be used respecting the following format:
<ProductionSchedule xmlns:bml="http://www.mesa.org/xml/B2MML-V0600" xmlns:Extended="http://www.mesa.org/xml/B2MML-V0600-
AllExtensions" >,
the Production Schedule definition.
```
## The Production Schedule definition

Here is an example of XML document to be used to define an Production Order:

```
Production Order Download Message
```
```
<?xml version="1.0" encoding="utf-8"?>
<ProductionSchedule xmlns:bml="http://www.mesa.org/xml/B2MML-V0600" xmlns:Extended="http://www.mesa.org/xml
/B2MML-V0600-AllExtensions" >
<ID>12345s6789</ID>
<StartTime>2014-01-01T23:28:56.782Z</StartTime>
<Description>ProductionSchedule Description</Description>
<ProductionRequest>
<Description>ProductionRequest Description</Description>
<StartTime>2014-01-01T23:28:56.782Z</StartTime>
<EndTime>2014-01-01T23:28:56.782Z</EndTime>
<Priority>In-Progress</Priority>
<Extended:Name>Cart</Extended:Name>
<RequestState>in-process</RequestState>
<Extended:Quantity>
<Extended:QuantityString>200</Extended:QuantityString>
<Extended:UnitOfMeasure>units</Extended:UnitOfMeasure>
</Extended:Quantity>
<Extended:Quantity2>
<Extended:Quantity2String>500</Extended:Quantity2String>
<Extended:UnitOfMeasure2>UnitOfMeasure2</Extended:UnitOfMeasure2>
</Extended:Quantity2>
</ProductionRequest>
</ProductionSchedule>
```
The <ProductionSchedule> tag specifies that the document scope is to define an Order, whose properties are specified by the following tags:

```
Tag Description Occurrence
```
```
<ID> Defines the identifier of the Production Schedule. Mandatory
```
```
<StartTime> Defines the Start Time of the Production Schedule. Optional
```
```
<Description> Defines the description of the Production Schedule. Optional
```
```
<ProductionRequest> Defines the data of the Production Order. Only one Production Request can be defined. Optional
```
The following table lists all the tags that define the properties of a Production Request:


```
Tag Description Occurrence
```
<Description>

<StartTime>

<EndTime>

<Priority>

<Extended:Name>

<RequestState>

<Extended:QuantityString>

<Extended:UnitOfMeasure>

<Extended:Quantity2String>

<Extended:UnitOfMeasure2>


# Traveller's Guide


# Paperwork before and after a business trip (for Siemens PL

# employees)

# Description Supporting tools and details

1. Figure out what you will need for you travel. Usually including flights, hotel, rented car, taxis, meals, etc. Try to make a rough estimation of the costs. E.
g. you can make start a plan in TravelNet, without confirming it. Just to see availability and prices.
https://travelnet.siemens.com/
2. Fill a form to ask for authorization and submit it. You must use EZ-X service. Here you will find the space to insert your expenses estimation. Be careful
that the UI presents you dates formatted as MM/DD/YYYY...
https://ez-x.gss.siemens.com/ezx/welcome.do
3. Create a BTAT document for the trip. This is a screening for possible issues of many kinds (safety, taxes, etc.). Keep the resulting document to be
attached to the expense report.
https://businesstravel.siemens.com/
4. (wait for approval... or just pretend)
5. Actually perform reservations for flight, hotel, car. Back to TravelNet, this time for real.
https://travelnet.siemens.com/
6. Sooner or later you will receive the invoice from BCD Travel. Keep it for the expense report.
7. Remember your flight check-in
8. During the trip, collect the receipts for all the expenses.
9. When done, create an expense report. Go to EZ-X and insert all the expenses. If possible refer to the corresponding authorization and upload the
corresponding BTAT document.
https://ez-x.gss.siemens.com/ezx/welcome.do
10. Create an expense report also on AntexWeb registering here just the "bonus" (indennità) that you deserve for the travel.
https://www.antexweb.net


# What's the name?

Modular MOM is the name currently used for both the project and the product.

Former Names:

```
Cloud MES: was the initial name used by Product Management
RevMOM: was the name of the R&D Project
Opcenter X: has been a name used during a short term. This name was designating the product. This name has been reveal out of our
organization interfering with the business currently done with the other Opcenter products. For this reason, it is not used anymore.
```

# NFR+Assurance_+Chaos+Engineering

## NFR Assurance: Chaos Engineering

## Chaos Engineering Frameworks

```
Name Link Short Description Lizenz Active /
Last
Commit
```
```
Purpo
se
```
```
Target ran
do
m
/
tar
get
ed
Powerful
Seal
```
```
{+}https://github
.com/bloomberg
/powerfulseal+
```
```
PowerfulSeal injects failure into your Kubernetes clusters, so that you can detect problems as early as possible. It
allows for writing scenarios describing complete chaos experiments.
```
```
Apache
2.0 License
```
```
2021-10-
22
```
```
Chaos
Engin
eering
```
```
Pod
Network Delay
Network Failure
VMs
Hosts
```
```
tar
get
ed
```
```
Kuberne
tes Pod
Chaos
Monkey
```
```
{+}https://github
.com/jnewland
/kubernetes-
pod-chaos-
monkey+
```
```
This repository contains a Dockerfile and associated Kubernetes configuration for a Deployment that will randomly
delete pods in a given namespace. This is implemented in Bash mostly.
```
```
MIT License 2019-11-
24
```
```
Chaos
Engin
eering
```
```
Pod ran
dom
```
```
kube-
monkey
```
```
{+}https://github
.com/asobti
/kube-monkey+
```
```
An implementation of Netflix's Chaos Monkey for Kubernetes clusters. It randomly deletes Kubernetes (k8s) pods in
the cluster encouraging and validating the development of failure-resilient services.
```
```
Apache
2.0 License
```
```
2022-03-
03
```
```
Chaos
Engin
eering
```
```
Pod ran
dom
```
```
chaosku
be
```
```
{+}https://github
.com/linki
/chaoskube+
```
```
chaoskube periodically kills random pods in your Kubernetes cluster. MIT License 2022-01-
05
```
```
Chaos
Engin
eering
```
```
Pod ran
dom
```
```
Gremlin {+}https://www.
gremlininc.com
/+
```
```
Chaos-as-a-Service - Gremlin is a platform that offers everything you need to do Chaos Engineering. Supports all
cloud infrastructure providers, Kubernetes, Docker and host-level chaos engineering. Offers an API and control plane.
```
```
External
Service
```
- Chaos
    Engin
    eering

```
CPU, Memory, IO, Disk,
Shutdown, Time Travel,
Process Killer, Blackhole,
Latency, Packet Loss,
DNS
```
```
tar
get
ed
```
```
Pumba {+}https://github
.com/gaia-adm
/pumba+
```
```
Pumba is a chaos testing command line tool for Docker containers. Pumba disturbs your containers by crashing
containerized application, emulating network failures and stress-testing container resources (cpu, memory, fs, io, and
others).
```
```
Apache
2.0 License
```
```
2021-12-
04
```
```
Chaos
Engin
eering
```
```
Container
Network
Pods
Namespaces
```
```
tar
get
ed
```
```
Chaos
Toolkit
```
```
{+}https://github
.com
/chaostoolkit
/chaostoolkit+
{+}https://chaost
oolkit.org/+
{+}https://github
.com
/chaostoolkit
/chaostoolkit-
kubernetes+
```
```
A chaos engineering toolkit to help you build confidence in your software system. Has Extension for Kubernetes, Istio,
Grafana, Prometheus
```
```
Apache
2.0 License 20
22
-0
3-
08
(to
ol
kit
)
20
22
-0
4-
7
(K
ub
er
ne
te
s
Ex
te
ns
io
n)
```
```
Chaos
Engin
eering
```
```
Only through Extensions
Pod
Network
```
```
tar
get
ed
```
```
k8aos
to old
```
```
{+}https://github
.com
/AlexsJones
/k8aos+
```
```
No Futher Investigation (see License and Commit Date) No License
added to Repo
```
```
2017-06-
12
```
```
Chaos
Engin
eering
```
```
Chaos
Monkey
discontin
ued
```
```
{+}https://fabric
8.io/guide
/chaosMonkey.
html+
```
```
No Futher Investigation (see License), as fabric8 is a whole development plattform, but has dicontinued. External
Service, only
with fabric8
```
```
discontin
ued
```
```
Chaos
Engin
eering
```
```
Pod ran
dom
```
```
KubeInv
aders
```
```
{+}https://github
.com/lucky-
sideburn
/KubeInvaders+
```
```
Gamified chaos engineering tool for Kubernetes. It is like Space Invaders but the aliens are pods or worker nodes. Apache
2.0 License
```
```
2021-11-
03
```
```
Chaos
Engin
eering
```
```
Pod
Worker Nodes
```
```
ran
do
m
(as
Ga
me)
wiremock{+}http://wiremo
ck.org/+
```
```
API mocking (Service Virtualization) which enables modeling real world faults and delays Apache
2.0 License
```
```
2022-04-
09
```
```
Chaos
Engin
eering
/
Mocki
ng
```
```
Network Delay tar
get
ed
```
```
mocklab
external
service
```
```
{+}http://get.
mocklab.io/+
```
```
API mocking (Service Virtualization) as a service which enables modeling real world faults and delays. Apache
2.0 License
(but is
wiremock),
```
```
Chaos
Engin
eering
/
```

```
external
service
otherwise
```
```
Mocki
ng
```
Pod-
Reaper

```
{+}https://github
.com/target
/pod-reaper+
```
```
A rules based pod killing container. Pod-Reaper was designed to kill pods that meet specific conditions that can be
used for Chaos testing in Kubernetes.
```
```
MIT License 2022-01-
19
```
```
Chaos
Engin
eering
```
```
Pod tar
get
ed
```
muxy {+}https://github
.com/mefellows
/muxy/+

```
A chaos testing tool for simulating a real-world distributed system failures. Muxy is a proxy that mucks with your
system and application context, operating at Layers 4, 5 and 7, allowing you to simulate common failure scenarios
from the perspective of an application under test; such as an API or a web application.
```
```
MIT License 2019-05-
11
```
```
Chaos
Engin
eering
```
```
Network tar
get
ed
```
Toxiproxy{+}https://github
.com/Shopify
/toxiproxy+

```
A TCP proxy from Shopify to simulate network and system conditions for chaos and resiliency testing. Toxiproxy is a
framework for simulating network conditions. It's made specifically to work in testing, CI and development
environments, supporting deterministic tampering with connections, but with support for randomized chaos and
customization.
```
```
MIT License 2022-04-
11
```
```
Chaos
Engin
eering
```
```
Network tar
get
ed
```
Blockade{+}https://github
.com/worstcase
/blockade+

```
Docker-based utility for testing network failures and partitions in distributed applications. Blockade uses Docker contai
ners to run application processes and manages the network from the host system to create various failure scenarios.
```
```
Apache
2.0 License
```
```
2021-03-
21
```
```
Chaos
Engin
eering
```
```
Network
```
Chaos-
Lambda
only
EC2

```
{+}https://github
.com/bbc
/chaos-lambda+
```
```
Randomly terminate ASG instances during business hours.
EC2 instances are volatile and can be recycled at any time without warning. Amazon recommends running them
under Auto Scaling Groups to ensure overall service availability, but it's easy to forget that instances can suddenly fail
until it happens in the early hours of the morning when everyone is on holiday.
Chaos Lambda increases the rate at which these failures occur during business hours, helping teams to build
services that handle them gracefully.
```
```
Apache
2.0 License
```
```
2021-01-
21
```
```
Chaos
Engin
eering
```
```
EC2 Instances ran
dom
```
Namazu
(formerly
named
Earthqu
ake)
to old

```
{+}https://github
.com/osrg
/namazu+
```
```
Programmable fuzzy scheduler for testing distributed systems. Namazu permutes Java function calls, Ethernet
packets, Filesystem events, and injected faults in various orders so as to find implementation-level bugs of the
distributed system. Namazu can also control non-determinism of the thread interleaving (by calling sched_setattr(2) wit
h randomized parameters). So Namazu can be also used for testing standalone multi-threaded software.
```
```
Apache
2.0 License
```
```
2017-08-
24
```
```
Chaos
Engin
eering
```
```
Docker ran
dom
```
Infection
Monkey

```
{+}https://github
.com/guardicore
/monkey+
```
```
The Infection Monkey is an open source security tool for testing a data center's resiliency to perimeter breaches and
internal server infection. The Monkey uses various methods to self propagate across a data center and reports
success to a centralized Monkey Island server.
```
```
GPL-3.0
License
```
```
2022-04-
07
```
```
Securi
ty /
Chaos
Engin
eering
```
```
Network ran
dom
```
OpenEB
S e2e-
tests

```
{+}https://github
.com/openebs
/e2e-tests+
```
```
Seems to be an addition to litmus Apache
2.0 License
```
```
2021-08-
31
```
```
Chaos
Engin
eering
```
Litmus {+}https://github
.com
/litmuschaos
/litmus+
Scenarios at: {+}
https://hub.
litmuschaos.io/+

```
LitmusChaos is an open source Chaos Engineering platform that enables teams to identify weaknesses & potential
outages in infrastructures by inducing chaos tests in a controlled way. Developers & SREs can practice Chaos
Engineering with Litmus as it is easy to use, based on modern chaos engineering principles & community
collaborated. It is 100% open source & a CNCF project.
```
```
Apache
2.0 License,
dependencies
listed in Fossa
```
```
2022-04-
13
```
```
Chaos
Engin
eering
```
```
Pod
Container
Node
Network
```
```
tar
get
ed
```
Chaos
Mesh

```
{+}https://github
.com/chaos-
mesh/chaos-
mesh+
```
```
Chaos Mesh is a Cloud Native Computing Foundation (CNCF) hosted project. It is a cloud-native Chaos Engineering
platform that orchestrates chaos on Kubernetes environments.
```
```
Apache
2.0 License
```
```
2022-04-
13
```
```
Chaos
Engin
eering
```
```
Pod
Container
Network
```
```
tar
get
ed
```

